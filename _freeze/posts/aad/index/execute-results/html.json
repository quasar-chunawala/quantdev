{
  "hash": "9ca1446ec13ec97eb20605eb4ac307da",
  "result": {
    "markdown": "---\ntitle: AAD(Adjoint Algorithmic Differentiation)\nauthor: Quasar\ndate: '2025-10-23'\ncategories:\n  - xVA\ntags:\n  - aad\nimage: image.jpg\ntoc: true\ntoc-depth: 3\n---\n\n# Introduction\n\nI was skimming through [Luca Capriotti](http://luca-capriotti.net/) and Mike Giles' paper [15 Years of Adjoint Algorithmic Differentiation in Finance](https://people.maths.ox.ac.uk/~gilesm/files/AAD_Review.pdf) and wanted to write a toy implementation in Julia. Algorithmic differentiation (AD) is a set of techniques to accurately and efficiently compute derivatives of a function  in the form of a computer program. Many of my toy examples are borrowed from the excellent [Algorithmic Differentiation in Finance Explained](https://www.amazon.co.uk/Algorithmic-Differentiation-Explained-Financial-Engineering/dp/3319539787/ref=sr_1_1?crid=3ICBI9FQDER6B&dib=eyJ2IjoiMSJ9.OBdGni8_VOEiK_wWQ1g2PA.ChQjNVwVvAHPnbWhiXJDTSw2G9dyuIX9-hsH6BP_ZT4&dib_tag=se&keywords=Algorithmic+Differentiation+in+Finance+Explained&qid=1761218601&sprefix=algorithmic+differentiation+in+finance+explained%2Caps%2C91&sr=8-1), by Marc Henrard.\n\n## Setup\n\nAs a quick example, suppose we have the scalar-valued function $f:\\mathbb{R}^{p_a} \\to \\mathbb{R}$:\n\n$$\ny = \\cos(x_1 + e^{x_2})(\\sin x_3 + \\cos x_4) + x_2^{3/2} + x_4\n$$\n\nThe function inputs are a vector `a[1...p_a]` of dimension $p_a$. All intermediate values in the program will be denoted by `b`s. The output of the function is denoted by the variable `z` of dimension $1$. So, the algorithm starts with the inputs `a`, goes to `z` through a lot of `b`s. The new variables are denoted by `b[j]` with `j` starting with `1` and going upto $p_b$. There are $p_b$ intermediate variables $b$ in the program.\n\n## Forward-mode AD\n\nStandard algorithmic differentiation also called forward algorithmic differentiation or tangent algorithmic differentiation. Our goal is to compute $\\partial z/\\partial a_i$. We achieve this by computing for each $j$($p_a + 1 \\leq j \\leq p_b$) the value:\n\n$$\n\\dot{b}[j,i] = \\frac{\\partial b[j]}{\\partial a[i]}\n$$\n\nWe first initialize the variables $b[j]$ from $1 \\leq j \\leq p_a$ with the inputs values $a[j]$. Note that the derivative is denoted by dot on the variable and $\\dot{b}[j,i]$ is the derivative of $b[j]$ with respect to some other variable $a[i]$. For $j=1:p_a$, then, the derivative of $b[j]$ with respect to $a[i]$ is simply $1$, if $j=i$ and $0$ if $j \\neq i$. This is the starting point of a recursive algorithm. The starting part is the identity matrix : \n\n$$\n\\dot{b}[j,i] = \\delta_{i,j}\n$$\n\nwhere $\\delta_{i,j}$ represents Kronecker's delta.\n\nThe successive derivatives $\\dot{b}[j,i]$ are given by the chain rule:\n\n$$\n\\begin{align*}\n\\dot{b}[j,i] = \\frac{\\partial b[j]}{\\partial a[i]} &= \\sum_{k=p_a + 1}^{k=j-1}\\frac{\\partial b[j]}{\\partial b[k]} \\cdot \\frac{\\partial b[k]}{\\partial a[i]}\\\\\n&= \\sum_{k=p_a + 1}^{j-1} \\frac{\\partial}{\\partial b[k]} (b[j]) \\cdot \\dot{b}[k,i]\\\\\n&= \\sum_{k=p_a + 1}^{j-1} \\frac{\\partial}{\\partial b[k]} g_j(b[p_a + 1 : j - 1]) \\cdot \\dot{b}[k,i]\\\\\n\\end{align*}\n$$\n\nThe numbers $\\dot{b}[p_b,i]$ are equal to the derivatives of $z=b[p_b]$ with respect to $a_i$, $1 \\leq i \\leq p_a$. This concludes the algorithm for the computation of $\\partial z/\\partial a_i$.\n\nThe requirements for such an implementation is that all the intermediary functions $g_j$ have a derivative version. The algorithmic differentiation approach is a bottom-up approach : it can be implemented for an algorithm only if all the components below it, all the components entering into the composition have already been implemented. \n\n![Forward AD](forward_ad.jpg){#fig:forward_ad scale=70%}\n\n## Naive implementation of forward-AD\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nfunction f(a::Array{Real})\n    b = zeros(4)\n    b[1] = a[1] + exp(a[2])\n    b[2] = sin(a[3]) + cos(a[4])\n    b[3] = a[2] ^ (3/2) + a[4]\n    b[4] = cos(b[1]) * b[2] + b[3]\n    b[4]\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nf (generic function with 1 method)\n```\n:::\n:::\n\n\nWe create an AD version of the starter function as follows:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nfunction f_AD(x::Array{Real})\n    b = zeros(4)\n    b[1] = a[1] + exp(a[2])\n    b[2] = sin(a[3]) + cos(a[4])\n    b[3] = a[2] ^ (3/2) + a[4]\n    b[4] = cos(b[1]) * b[2] + b[3]\n    \n    # Forward sweep - derivatives\n    n = length(x)\n\n    b1dot = zeros(n)\n    b1dot[1] = 1\n    b1dot[2] = exp(b[2])\n\n    b2dot = zeros(n)\n    b2dot[3] = cos(b[3])\n    b2dot[4] = -sin(b[4])\n\n    b3dot = zeros(n)\n    b3dot[2] = (3/2)*(b[2] ^ (1/2))\n    b3dot[4] = 1\n\n    b4dot = b[2] * -sin(b[1]) * b1dot + cos(b[1]) * b2dot + b3dot\n    (b[4],b4dot)\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nf_AD (generic function with 1 method)\n```\n:::\n:::\n\n\nNote that, the output of the original function `f` is the function value - a `double`, whilst the output `f_ad` is a 2-tuple : the function value and the value of the jacobian(gradient). \n\n## Adjoint Algorithmic Differentiation\n\nOur goal is to compute $\\frac{\\partial z}{\\partial a_i}$. We achieve this by computing for each intermediate variable $j$ ($p_a + 1 \\leq j \\leq p_b$) the value:\n\n$$\n\\overline{b}[j] = \\frac{\\partial z}{\\partial b[j]}\n$$\n\nNote that, $\\overline{b}[j]$ is the derivative of the output with respect to `b[j]`. It is important to switch the perception between the forward mode and the reverse mode. What is fixed in the reverse approach is the output, we always compute the derivative of the same variable, the output.\n\nThe starting point of the algorithm is easy. For $j = p_b$, the derivative of $z$ with respect to $b[j]$ is simply the derivative of $z$ with respect to itself, which is $1$. This is the starting point of a recursive algorithm. \n\nFrom there, we read the code in reverse order and just apply the chain rule. Each intermediary variable $b[j]$ is used only in the lines of code that follow in the computation. The derivative $\\overline{b}[j]$ is given by:\n\n$$\n\\overline{b}[j] = \\frac{\\partial z}{\\partial b_j} = \\sum_{k=j+1}^{p_b} \\frac{\\partial z}{\\partial b_k} \\cdot \\frac{\\partial b_k}{\\partial b_j} = \\sum_{k=j+1}^{p_b} \\overline{b}[k] \\cdot \\frac{\\partial g_k}{\\partial b_j}\n$$\n\nI think it's easy to visualize this in a computational graph:\n\n![Reverse AD](reverse_ad.jpg){#fig:reverse_ad scale=70%}\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nfunction f_AAD(a::Array{Real})\n    b = zeros(4)\n    b[1] = a[1] + exp(a[2])\n    b[2] = sin(a[3]) + cos(a[4])\n    b[3] = a[2] ^ (3/2) + a[4]\n    b[4] = cos(b[1]) * b[2] + b[3]\n\n    # Backward sweep - derivatives\n    n = length(a)\n    abar = zeros(n)\n    b4bar = 1.0\n    b3bar = 1.0 * b4bar\n    b2bar = cos(b[1]) * b4bar\n    b1bar = b[2] * (-sin(b[1])) * b4bar\n\n    abar[4] = -sin(a[4]) * b2bar + 1.0 * b3bar\n    abar[3] = cos(a[3]) * b2bar\n    abar[2] = exp(a[2]) * b1bar + (1.5) * (a[2] ^ (0.5)) * b3bar\n    abar[1] = 1.0 * b1bar\n\n    (b[4], abar)\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nf_AAD (generic function with 1 method)\n```\n:::\n:::\n\n\n# Algorithmic Differentiation Tools\n\nGiven any computer function `f(x)`, we can build code implementing the tangent or adjoint mode for the calculation of its derivatives. This involves representing the function as a computational graph, calculating the derivatives on each of the edges, and computing the tangents or adjoints in the appropriate direction. This process is mechanical in nature and can be easily automated. \n\nMost AD tools fall into 2 categories : *source code transformation* and *operator overloading*. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "3d475c7287d59124ca069e53a2750d8c",
  "result": {
    "markdown": "---\ntitle: Python memory management\nauthor: Quasar\ndate: '2025-11-27'\ncategories:\n  - Python\nimage: python.jpg\ntoc: true\ntoc-depth: 3\n---\n\n# Introduction \n\nCPython is one of the many implementations of the Python runtime written in human-readable Python and C code. There are other implementations such as PyPy, Jython. In this blog post, I summarize how object memory is alllocated and freed and how CPython manages memory leaks.\n\nPython is a dynamically-typed language. The size of variables can't be calculated at compile-time. Most of Python's core types are dynamically sized. The `list` type can be of any size, a `dict` can have any number of keys, and even `int` is dynamic. The user never has to specify the size of these types. Names in Python can be reused for values of different types.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\na_value = 1\na_value = \"Now, I'm a string\"\na_value = [\"Now\", \"I\", \"am\", \"a\", \"list\"]\n```\n:::\n\n\nTo overcome these constraints, CPython relies heavily on dynamic memory allocation, but adds safety rails to automate the freeing of memory using the garbage collection and reference counting algorithms.\n\nInstead of the Python developer having to allocate memory, Python object memory is allocated automatically by a single, unified API. This design requires that the entire CPython standard library and core modules(written in C) use this API.\n\n# Allocation Domains\n\n- The raw domain is used for allocation from the system heap and large, or non-object related memory.\n\n- The object domain is used for the allocation of all Python object related memory.\n\n- The PyMem domain is the same as `PYMEM_DOMAIN_OBJ`. this exists for legacy purposes.\n\nEach domain implements the same interface of frunctions:\n\n- `_Alloc(size_t size)` allocates memory of `size` bytes and returns a pointer.\n\n- `_Calloc(size_t nelem, size_t el_size)` allocates `nelem` elements each of size `el_size` and returns a pointer.\n\n- `_Realloc(void* ptr, size_t new_size)` reallocates memory of size `new_size`.\n\n- `_Free(void* ptr)` frees memory at `ptr` back to the heap.\n\nThe `PyMemAllocatorDomain` enumeration represents the three domains in CPython as `PYMEM_DOMAIN_RAW`, `PYMEM_DOMAIN_OBJ` and `PYMEM_DOMAIN_MEM`.\n\n\n# CPython interning\n\nThe general rule is that objects are allocated on assignment. Variables just point to objects. There is an exception to this general rule. This is python runtime implementation specific. Often commonly used objects are preallocated and are shared instead of costly new allocations. This is mainly done for performance optimization. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nx = 255\ny = 255\nprint(x is y, x == y)\nx = 1024\ny = 1024\nprint(x is y, x == y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue True\nFalse True\n```\n:::\n:::\n\n\n`int`s in the range $[-5,257)$, empty tuples and all empty or single length strings are preallocated. \n\n## `string` interning explained\n\nString interning is a method of storing only one copy of each distinct string value, which must be immutable. In Python3, we have a function `sys.intern()` and if we use this function, we can enter a string in the table of interned strings. We get a reference to the interned string. \n\nSo, we can gain a little extra performance on dictionary lookup (key comparisons after hashing can be done by a pointer compare instead of a string compare). \n\nNames used in programs are automatically interned. Dictionaries used to hold module, class or instance attributes have interned keys. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sys import intern\na, b = \"strin\", \"string\"\nprint(f\"{a + 'g' is b}\")    # Returns false\nintern(a + 'g') is intern(b) # returns True\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFalse\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nTrue\n```\n:::\n:::\n\n\n# Mutable containers memory allocationn strategy\n\nThere are different mutable containers in Python - `list`s, `set`s and `dict`s. Behind the scenes, there is a strategy for allocating these containers. \n\nA good strategy will plan for growth and shrinkage. It will slightly overallocate memory needed by the container, to leave room for growth. Each time we append to a `list`, we won't have to reallocate the memory. We also have to remember that sometimes we have to shrink the memory for a mutable container. This can help reduce the number of expensive function calls for instance to `realloc` or `memcpy`.\n\n# List allocation strategy\n\nLists are stored as fixed length array of pointers. So, we just point to objects. By design, we overallocate for list growth by append.\n\n- The capacity growth pattern is roughly $4,8,16,25,35,46,\\ldots$\n- \n\nIf we put something at the end of the list, these operations are cheap. But, if we put something in the middle or the beginning, we'll have to copy/shift the elements in memory to perform this operation. \n\nOn my 64-bit machine, with Python 3.13.7, the list allocation size is:\n\n- 64 bits : `56 + 8 * length`\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport sys\nl = []\nfor i in range(17):\n    l.append(i+1)\n    print(f\"List len = {len(l)}, size = {sys.getsizeof(l)} bytes\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList len = 1, size = 88 bytes\nList len = 2, size = 88 bytes\nList len = 3, size = 88 bytes\nList len = 4, size = 88 bytes\nList len = 5, size = 120 bytes\nList len = 6, size = 120 bytes\nList len = 7, size = 120 bytes\nList len = 8, size = 120 bytes\nList len = 9, size = 184 bytes\nList len = 10, size = 184 bytes\nList len = 11, size = 184 bytes\nList len = 12, size = 184 bytes\nList len = 13, size = 184 bytes\nList len = 14, size = 184 bytes\nList len = 15, size = 184 bytes\nList len = 16, size = 184 bytes\nList len = 17, size = 248 bytes\n```\n:::\n:::\n\n\nThere is a fixed overhead of $56$ bytes (the object header, type pointer, reference count ) etc.\n\nFor each size tier, the calculation is approximately:\n\n- 88 bytes = 56 + 32(4 slots)\n- 120 bytes = 56 + 64(8 slots)\n- 184 bytes = 56 + 128(16 slots)\n- 248 bytes =56 + 192(24 slots)\n\nWe shrink when the list size goes below $1/2$ of the allocated storage. This is why appending to Python lists is $O(1)$ amortized time - most appends just fill unused capacity, and only occasional appends trigger a reallocation. \n\n# Overallocation of dictionaries and sets\n\nThese are represented as fixed-length hash tables. The overallocation \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
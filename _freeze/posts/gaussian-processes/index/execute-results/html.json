{
  "hash": "961c61791de606a5e0e97c6a8b8a80f7",
  "result": {
    "markdown": "---\ntitle: \"Gaussian Processes\"\nauthor: \"Quasar\"\ndate: \"2025-10-19\"\ncategories: [Stochastic Calculus]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n# Gaussian Processes.\n\n## Random Vectors.\n\nConsider a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$. We can define several random variables on $\\Omega$. A $n$-tuple of random variables on this space is called a random vector. For example, if $X_{1},X_{2},\\ldots,X_{n}$ are random variables on $(\\Omega,\\mathcal{F},\\mathbb{P})$, then the $n$-tuple $(X_{1},X_{2},\\ldots,X_{n})$ is a random vector on $(\\Omega,\\mathcal{F},\\mathbb{P})$. The vector is said to be $n$-dimensional because it contains $n$-variables. We will sometimes denote a random vector by $X$.\n\nA good point of view is to think of a random vector $X=(X_{1},\\ldots,X_{n})$ as a random variable (point) in $\\mathbf{R}^{n}$. In other words, for an outcome $\\omega\\in\\Omega$, $X(\\omega)$ is a point sampled in $\\mathbf{R}^{n}$, where $X_{j}(\\omega)$ represents the $j$-th coordinate of the point. The distribution of $X$, denoted $\\mu_{X}$ is the probability on $\\mathbf{R}^{n}$defined by the events related to the values of $X$:\n\n$$\\mathbb{P}\\{X\\in A\\}=\\mu_{X}(A)\\quad\\text{for a subset }A\\text{ in }\\mathbf{R}^{n}$$\n\nIn other words, $\\mathbb{P}(X\\in A)=\\mu_{X}(A)$ is the probability that the random point $X$ falls in $A$. The distribution of the vector $X$ is also called the joint distribution of $(X_{1},\\ldots,X_{n})$.\n\n::: defn\nThe **joint distribution function** of $\\mathbf{X}=(X,Y)$ is the function $F:\\mathbf{R}^{2}\\to[0,1]$ given by:\n\n$$F_{\\mathbf{X}}(x,y)=\\mathbb{P}(X\\leq x,Y\\leq y)$$\n:::\n\n::: defn\nThe joint **PDF** $f_{\\mathbf{X}}(x_{1},\\ldots,x_{n})$ of a random vector $\\mathbf{X}$ is a function $f_{\\mathbf{X}}:\\mathbf{R}^{n}\\to\\mathbf{R}$ such that the probability that $X$ falls in a subset $A$ of $\\mathbf{R}^{n}$ and is expressed as the multiple integral of $f(x_{1},x_{2,}\\ldots,x_{n})$ over $A$: :::\n\n$$\\mathbb{P}(X\\in A)=\\int_{A}f(x_{1},x_{2},\\ldots,x_{n})dx_{1}dx_{2}\\ldots dx_{n}$$\n\nNote that: we must have that the integral of $f$ over the whole of $\\mathbf{R}^{n}$ is $1$.\n\nIf $F$ is differentiable at the point $(x,y)$, then we usually specify:\n\n$$f(x,y)=\\frac{\\partial^{2}}{\\partial x\\partial y}F(x,y)$$\n\n\n::: thm\nLet $(X,Y)$ be the random variables with joint density function $f_{X,Y}(x,y)$. The marginal density function $f_{X}(x)$ and $f_{Y}(y)$ of the random variables $X$ and $Y$ respectively is given by:\n\n$$\\begin{aligned}\nf_{X}(x) & =\\int_{-\\infty}^{+\\infty}f_{(X,Y)}(x,y)dy\\\\ f_{Y}(y) & =\\int_{-\\infty}^{+\\infty}f_{(X,Y)}(x,y)dx\\end{aligned}$$ :::\n\n::: proof\n*Proof.* We have:\n\n$$\\begin{aligned}\nF_{X}(x) & =P(X\\leq x)\\\\ & =\\int_{-\\infty}^{x}\\int_{y=-\\infty}^{y=+\\infty}f(x,y)dydx\\end{aligned}$$\n\nDifferentiating both sides with respect to $x$,\n\n$$\\begin{aligned}\nf_{X}(x) & =\\int_{y=-\\infty}^{y=+\\infty}f(x,y)dydx\\end{aligned}$$ ◻ :::\n\n::: defn\nFor continuous random variables $X$ and $Y$ with the joint density function $f_{(X,Y)}$, the conditional density of $Y$ given $X=x$ is:\n\n$$\\begin{aligned}\nf_{Y|X}(y|x) & =\\frac{f_{(X,Y)}(x,y)}{f_{X}(x)}\\end{aligned}$$\n\nfor all $x$ with $f_{X}(x)>0$. This is considered as a function of $y$ for a fixed $x$. As a convention, in order to make $f_{Y|X}(y|x)$ well-defined for all real $x$, let $f_{Y|X}(y|x)=0$ for all $x$ with $f_{X}(x)=0$. :::\n\nWe are essentially slicing the the joint density function of $f_{(X,Y)}(x,y)$ by a thin plane $X=x$. How can we speak of conditioning on $X=x$ for $X$ being a continuous random variable, considering that this event has probability zero. Rigorously speaking, we are actually conditioning on the event that $X$ falls within a small interval containing $x$, say $X\\in(x-\\epsilon,x+\\epsilon)$ and then taking the limit as $\\epsilon$ approaches zero from the right.\n\nWe can recover the joint PDF $f_{(X,Y)}$ if we have the conditional PDF $f_{Y|X}$ and the corresponding marginal $f_{X}$:\n\n$$\\begin{aligned}\nf_{(X,Y)}(x,y) & =f_{Y|X}(y|x)\\cdot f_{X}(x)\\end{aligned}$$\n\n::: thm\n(Bayes rule and LOTP) []{#Bayes-rule-and-LOTP label=\"Bayes-rule-and-LOTP\"} Let $(X,Y)$ be continuous random variables. We have the following continuous form of the Bayes rule:\n\n$$f_{Y|X}(y|x)=\\frac{f_{X|Y}(x|y)\\cdot f_{Y}(y)}{f_{X}(x)}$$\n\nAnd we have the following continuous form of the law of total probability:\n\n$$\\begin{aligned}\nf_{X}(x) & =\\int_{y=-\\infty}^{y=+\\infty}f_{X|Y}(x|y)\\cdot f_{Y}(y)dy\\end{aligned}$$ :::\n\n::: proof\n*Proof.* By the definition of conditional PDFs, we have:\n\n$$\\begin{aligned}\nf_{X|Y}(x|y)\\cdot f_{Y}(y) & =f_{(X,Y)}(x,y)=f_{Y|X}(y|x)\\cdot f_{X}(x)\\end{aligned}$$\n\nDividing throughout by $f_{X}(x)$, we have:\n\n$$\\begin{aligned}\nf_{Y|X}(x) & =\\frac{f_{X|Y}(x|y)\\cdot f_{Y}(y)}{f_{X}(x)}=\\frac{f_{(X,Y)}(x,y)}{f_{X}(x)}\\end{aligned}$$ ◻ :::\n\n::: {#exm-sampling-uniformly}\n(Sampling uniformly in the unit disc). Consider the random vector $\\mathbf{X}=(X,Y)$ corresponding to a random point chosen uniformly in the unit disc $\\{(x,y):x^{2}+y^{2}\\leq1\\}$. $\\mathbf{X}$ is said to have uniform on the unit circle distribution. In this case the PDF is $0$ outside the disc and $\\frac{1}{\\pi}$ inside the disc:\n\n$$\\begin{aligned}\nf(x,y) & =\\frac{1}{\\pi}\\quad\\text{ if }x^{2}+y^{2}\\leq1\\end{aligned}$$\n\nThe random point $(X,Y)$ has $x$-coordinate $X$ and $Y$ coordinate $Y$. Each of these are random variables and their PDFs and CDFs can be computed. This is a valid PDF, because:\n\n$$\\begin{aligned}\n\\int\\int_{D}f(x,y)dydx & =\\int_{-1}^{1}\\int_{-\\sqrt{1-x^{2}}}^{\\sqrt{1-x^{2}}}\\frac{1}{\\pi}dydx\\\\ & =\\frac{1}{\\pi}\\int_{-1}^{1}\\left[y\\right]_{-\\sqrt{1-x^{2}}}^{+\\sqrt{1-x^{2}}}dx\\\\ & =\\frac{2}{\\pi}\\int_{-1}^{1}\\sqrt{1-x^{2}}dx\\end{aligned}$$\n\nSubstituting $x=\\sin\\theta$, we have: $dx=\\cos\\theta d\\theta$ and $\\sqrt{1-x^{2}}=\\cos\\theta$. The limits of integration are $\\theta=-\\pi/2$ to $\\theta=\\pi/2$. Thus,\n\n$$\\begin{aligned}\n\\int\\int_{D}f(x,y)dydx & =\\frac{2}{\\pi}\\int_{-\\pi/2}^{\\pi/2}\\cos^{2}\\theta d\\theta\\\\ & =\\frac{1}{\\pi}\\int_{-\\pi/2}^{\\pi/2}(1+\\cos2\\theta)d\\theta\\\\ & =\\frac{1}{\\pi}\\left[\\theta+\\frac{1}{2}\\sin2\\theta\\right]_{-\\pi/2}^{\\pi/2}\\\\ & =\\frac{1}{\\pi}\\cdot\\pi\\\\ & =1\\end{aligned}$$\n\nThe CDF of $X$ is given by:\n\n$$\\begin{aligned}\nF_{X}(a) & =\\int_{-1}^{a}\\int_{-\\sqrt{1-x^{2}}}^{\\sqrt{1-x^{2}}}\\frac{1}{\\pi}dydx\\\\ & =\\frac{2}{\\pi}\\int_{-1}^{a}\\sqrt{1-x^{2}}dx\\end{aligned}$$\n\nI leave it in this integral form. The PDF of $X$ is obtained by differentiating the CDF, so it is:\n\n$$f_{X}(x)=\\frac{2}{\\pi}\\sqrt{1-x^{2}}\\label{eq:marginal-pdf-of-X}$$\n:::\n\nLet's quickly plot the density of $X$ over the domain of the definition $-1\\leq x\\leq1$.\n\n::: center\nFigure. The PDF of the random variable $X$. :::\n\nNot suprisingly the distribution of the $x$-coordinate is no longer uniform!\n\nIf $(X_{1},X_{2},\\ldots,X_{n})$ is a random vector, the distribution of a single coordinate, say $X_{1}$ is called the *marginal distribution*. In the example [\\[Uniform-on-the-unit-circle-distribution\\]](#Uniform-on-the-unit-circle-distribution){reference-type=\"ref\" reference=\"Uniform-on-the-unit-circle-distribution\"}, the marginal distribution of $X$ is determined by the PDF [\\[eq:marginal-pdf-of-X\\]](#eq:marginal-pdf-of-X){reference-type=\"ref\" reference=\"eq:marginal-pdf-of-X\"}.\n\nRandom variables $X_{1},X_{2},\\ldots,X_{n}$ defined on the same probability space are said to be independent if for any intervals $A_{1},A_{2},\\ldots,A_{n}$ in $\\mathbf{R}$, the probability factors:\n\n$$\\mathbb{P}(X_{1}\\in A_{1},X_{2}\\in A_{2},\\ldots,X_{n}\\in A_{n})=\\mathbb{P}(X_{1}\\in A_{1})\\times\\mathbb{P}(X_{2}\\in A_{2})\\times\\ldots\\times\\mathbb{P}(X_{n}\\in A_{n})$$\nWe say that the random variables are independent and identically distributed (IID) if they are independent and their marginal distributions are the same.\n\nWhen the random vector $(X_{1},X_{2},\\ldots,X_{n})$ has a joint PDF $f(x_{1},x_{2},\\ldots,x_{n})$, the independence of random variables is equivalent to saying that the joint PDF is given by the product of the marginal PDFs:\n\n$$f(x_{1},x_{2},\\ldots,x_{n})=f_{1}(x_{1})\\times f_{2}(x_{2})\\times\\ldots\\times f_{n}(x_{n})$$\n\n## Basic Probabilistic Inequalities.\n\nInequalities are extremely useful tools in the theoretical development of probability theory.\n\n### Jensen's inequality.\n\n::: thm\nIf $g$ is a convex function, and $a>0$, $b>0$, with $p\\in[0,1]$, it follows that:\n\n$$g(pa+(1-p)b)\\leq pg(a)+(1-p)g(b)$$\n:::\n\n::: proof\n*Proof.* This directly follows from the definition of convex functions. ◻ :::\n\n### Jensen's inequality for Random variables.\n\n::: thm\nIf $g$ is a convex function, then it follows that:\n\n$$\\mathbb{E}(g(X))\\geq g(\\mathbb{E}X)$$\n:::\n\n::: proof\n*Proof.* Another way to express the idea, that a function is convex is to observe that the tangent line at an arbitrary point $(t,g(t))$ always lies below the curve. Let $y=a+bx$ be the tangent to $g$ at the point $t$. Then, it follows that:\n\n$$\\begin{aligned}\na+bt & =g(t)\\\\ a+bx & \\leq g(x)\\end{aligned}$$\n\nfor all $x$.\n\nThus, it follows that, for any point $t$, there exists $b$ such that:\n\n$$\\begin{aligned}\ng(x)-g(t) & \\geq b(x-t)\\end{aligned}$$\n\nfor all $x$. Set $t=\\mathbb{E}X$ and $x=X$. Then,\n\n$$\\begin{aligned}\ng(X)-g(\\mathbb{E}X) & \\geq b(X-\\mathbb{E}X)\\end{aligned}$$\n\nTaking expectations on both sides and simplifying:\n\n$$\\begin{aligned}\n\\mathbb{E}\\left(g(X)\\right)-g(\\mathbb{E}X) & \\geq b(\\mathbb{E}X-\\mathbb{E}X)=0\\\\ \\mathbb{E}g(X) & \\geq g(\\mathbb{E}X)\\end{aligned}$$ ◻ :::\n\n### Young's Inequality.\n\n::: thm\nIf $a\\geq0$ and **$b\\geq0$** are non-negative real numbers and if $p>1$ and $q>1$ are real numbers such that $\\frac{1}{p}+\\frac{1}{q}=1$, then:\n\n$$ab\\leq\\frac{a^{p}}{p}+\\frac{b^{q}}{q}$$\n:::\n\n::: proof\n*Proof.* Consider $g(x)=\\log x$. Being a concave function, Jensen's inequality can be reversed. We have:\n\n$$\\begin{aligned}\ng\\left(\\frac{1}{p}a^{p}+\\frac{1}{q}b^{q}\\right) & \\geq\\frac{1}{p}g(a^{p})+\\frac{1}{q}g(b^{q})\\\\ \\log\\left(\\frac{1}{p}a^{p}+\\frac{1}{q}b^{q}\\right) & \\geq\\frac{1}{p}\\log(a^{p})+\\frac{1}{q}\\log(b^{q})\\\\ \\log\\left(\\frac{1}{p}a^{p}+\\frac{1}{q}b^{q}\\right) & \\geq\\frac{1}{p}\\cdot p\\log(a)+\\frac{1}{q}\\cdot q\\log(b)\\\\ \\log\\left(\\frac{1}{p}a^{p}+\\frac{1}{q}b^{q}\\right) & \\geq\\log ab\\end{aligned}$$\n\nBy the Monotonicity of the $\\log x$ function, it follows that :\n\n$$\\begin{aligned}\nab & \\leq\\frac{a^{p}}{p}+\\frac{b^{q}}{q}\\end{aligned}$$ ◻ :::\n\n### Chebyshev's inequality.\n\nOne of the simplest and very useful probabilistic inequalities is a tail bound by expectation: the so called Chebyshev's inequality.\n\n::: thm\n(Chebyshev's inequality) If $X$ is a non-negative random variable, then for every $t\\geq0$:\n\n$$\\mathbb{P}(X\\geq t)\\leq\\frac{1}{t}\\mathbb{E}X$$\n:::\n\n::: proof\n*Proof.* We have:\n\n$$\\begin{aligned}\nt\\cdot\\mathbf{1}_{\\{X\\geq t\\}} & \\leq X\\cdot\\mathbf{1}_{\\{X\\geq t\\}}\\end{aligned}$$\n\nBy the monotonicity of expectations, we have:\n\n$$\\begin{aligned}\n\\mathbb{E}\\mathbf{1}_{\\{X\\geq t\\}} & \\leq\\frac{1}{t}\\mathbb{E}X\\\\ \\implies\\mathbb{P}\\{X\\geq t\\} & \\leq\\frac{1}{t}\\mathbb{E}X\\end{aligned}$$\n\nThis closes the proof. $\\blacksquare$ :::\n\nThere are several variants, easily deduced from Chebyshev's inequality using monotonicity of several functions. For a non-negative random variable $X$ and $t>0$, using the power function $x^{p}$, $p>0$, we get:\n\n$$\\mathbb{P}(X\\geq t)=\\mathbb{P}(X^{p}\\geq t^{p})\\leq\\frac{1}{t^{p}}\\mathbb{E}X^{p}$$\n\nFor a real valued random variable $X$, every $t\\in\\mathbf{R}$, using the square function $x^{2}$ and variance, we have:\n\n$$\\mathbb{P}(|X-\\mathbb{E}X|\\geq t)\\leq\\frac{1}{t^{2}}\\mathbb{E}|X-\\mathbb{E}X|^{2}=\\frac{1}{t^{2}}Var(X)$$\n\nFor a real-valued random variable $X$, every $t\\in\\mathbf{R}$ and $\\lambda>0$, using the exponential function $e^{\\lambda x}$(which is monotonic), we have:\n\n$$\\mathbb{P}(X\\geq t)=\\mathbb{P}(\\lambda X\\geq\\lambda t)=\\mathbb{P}(e^{\\lambda X}\\geq e^{\\lambda t})\\leq\\frac{1}{e^{\\lambda t}}\\mathbb{E}e^{\\lambda X}$$\n\nOur next inequality, the so-called Holder's inequality is a very effective inequality to factor out the expectation of a product.\n\n### Holder's inequality.\n\n::: thm\nLet $p,q\\geq1$ be such that $\\frac{1}{p}+\\frac{1}{q}=1$, For random variables $X$ and $Y$, we have:\n\n$$\\begin{aligned}\n\\mathbb{E}|XY| & \\leq\\left(\\mathbb{E}|X^{p}|\\right)^{1/p}\\left(\\mathbb{E}|Y^{q}|\\right)^{1/q}\\end{aligned}$$ :::\n\n::: proof\n*Proof.* From the Young's inequality, for any $a,b\\in\\mathbf{R}$, $p,q\\geq1$, we have:\n\n$$\\begin{aligned}\nab & \\leq\\frac{a^{p}}{p}+\\frac{b^{q}}{q}\\end{aligned}$$\n\nSetting $a=\\frac{|X|}{\\left(\\mathbb{E}|X^{p}|\\right)^{1/p}}$ and $b=\\frac{|Y|}{\\left(\\mathbb{E}|Y^{q}|\\right)^{1/q}}$, we get:\n\n$$\\begin{aligned}\n\\frac{|XY|}{\\left(\\mathbb{E}|X^{p}|\\right)^{1/p}\\left(\\mathbb{E}|Y^{q}|\\right)^{1/q}} & \\leq\\frac{1}{p}\\cdot\\frac{|X|^{p}}{\\mathbb{E}|X^{p}|}+\\frac{1}{q}\\cdot\\frac{|Y|^{q}}{\\mathbb{E}|Y^{q}|}\\end{aligned}$$\n\nTaking expectations on both sides, and using the monotonicity of expectation property, we get:\n\n$$\\begin{aligned}\n\\frac{\\mathbb{E}|XY|}{\\left(\\mathbb{E}|X^{p}|\\right)^{1/p}\\left(\\mathbb{E}|Y^{q}|\\right)^{1/q}} & \\leq\\frac{1}{p}\\cdot\\frac{\\mathbb{E}|X|^{p}}{\\mathbb{E}|X^{p}|}+\\frac{1}{q}\\cdot\\frac{\\mathbb{E}|Y|^{q}}{\\mathbb{E}|Y^{q}|}=\\frac{1}{p}+\\frac{1}{q}=1\\end{aligned}$$\n\nConsequently,\n\n$$\\begin{aligned}\n\\mathbb{E}|XY| & \\leq\\left(\\mathbb{E}|X^{p}|\\right)^{1/p}\\left(\\mathbb{E}|Y^{q}|\\right)^{1/q}\\end{aligned}$$\n\nLet $p=2$ and $q=2$. Then, we get the Cauchy-Schwarz inequality:\n\n$$\\begin{aligned}\n\\mathbb{E}|XY| & \\leq\\left[\\mathbb{E}(X^{2})\\right]^{1/2}\\left[\\mathbb{E}(Y^{2})\\right]^{1/2}\\end{aligned}$$\n\nIn some ways, the $p$-th moment of a random variable can be thought of as it's length or $p$-norm.\n\nDefine:\n\n$$\\left\\Vert X\\right\\Vert _{p}=\\left(\\mathbb{E}|X|^{p}\\right)^{1/p}$$ ◻\n:::\n\n### Minkowski's Inequality.\n\n::: thm\nFor random variables $X$ and $Y$, and for all $p\\geq1$ we have:\n\n$$\\left\\Vert X+Y\\right\\Vert _{p}\\leq\\left\\Vert X\\right\\Vert _{p}+\\left\\Vert Y\\right\\Vert _{p}$$\n:::\n\n::: proof\n*Proof.* The basic idea of the proof is to use Holder's inequality. Let $\\frac{1}{q}=1-\\frac{1}{p}$ or in other words, $q=\\frac{p}{p-1}$. We have:\n\n$$\\begin{aligned}\n\\mathbb{E}|X||X+Y|^{p-1} & \\leq\\left(\\mathbb{E}|X|^{p}\\right)^{1/p}\\left(\\mathbb{E}|X+Y|^{(p-1)q}\\right)^{1/q} & (a)\\\\ \\mathbb{E}|Y||X+Y|^{p-1} & \\leq\\left(\\mathbb{E}|Y|^{p}\\right)^{1/p}\\left(\\mathbb{E}|X+Y|^{(p-1)q}\\right)^{1/q} & (b)\\end{aligned}$$\n\nAdding the above two equations, we get:\n\n$$\\begin{aligned}\n\\mathbb{E}(|X+Y||X+Y|^{p-1})\\leq\\mathbb{E}(|X|+|Y|)(|X+Y|^{p-1}) & \\leq\\left\\{ \\left(\\mathbb{E}|X|^{p}\\right)^{1/p}+\\left(\\mathbb{E}|Y|^{p}\\right)^{1/p}\\right\\} \\left(\\mathbb{E}|X+Y|^{(p-1)q}\\right)^{1/q}\\\\ \\mathbb{E}|X+Y|^{p} & \\leq\\left\\{ \\left\\Vert X\\right\\Vert _{p}+\\left\\Vert Y\\right\\Vert _{p}\\right\\} \\left(\\mathbb{E}|X+Y|^{p}\\right)^{1/q}\\\\ \\left(\\mathbb{E}|X+Y|^{p}\\right)^{1/p} & \\leq\\left\\Vert X\\right\\Vert _{p}+\\left\\Vert Y\\right\\Vert _{p}\\\\ \\left\\Vert X+Y\\right\\Vert _{p} & \\leq\\left\\Vert X\\right\\Vert _{p}+\\left\\Vert Y\\right\\Vert _{p}\\end{aligned}$$ ◻ :::\n\n## A quick refresher of linear algebra.\n\nMany of the concepts in this chapter have very elegant interpretations, if we think of real-valued random variables on a probability space as vectors in a vector space. In particular, variance is related to the concept of norm and distance, while covariance is related to inner-products. These concepts can help unify some of the ideas in this chapter from a geometric point of view. Of course, real-valued random variables are simply measurable, real-valued functions on the abstract space $\\Omega.$\n\n::: defn\n(Vector Space).\n\nBy a vector space, we mean a non-empty set $V$ with two operations: :::\n\n-   Vector addition: $+:(\\mathbf{x},\\mathbf{y})\\to\\mathbf{x}+\\mathbf{y}$\n\n-   Scalar multiplication: $\\cdot:(\\alpha,\\mathbf{x})\\to\\alpha\\mathbf{x}$\n\nsuch that the following conditions are satisfied:\n\n(A1) Commutativity. $\\mathbf{x}+\\mathbf{y}=\\mathbf{y}+\\mathbf{x}$ for all $\\mathbf{x},\\mathbf{y}\\in V$\n\n(A2) Associativity: $(\\mathbf{x}+\\mathbf{y})+\\mathbf{z}=\\mathbf{x}+(\\mathbf{y}+\\mathbf{z})$ for all $\\mathbf{x},\\mathbf{y},\\mathbf{z}\\in V$\n\n(A3) Zero Element: There exists a zero element, denoted $\\mathbf{0}$ in $V$, for all $\\mathbf{x}\\in V$, such that $\\mathbf{x}+\\mathbf{0}=\\mathbf{x}$.\n\n(A4) Additive Inverse: For all $\\mathbf{x}\\in V$, there exists an additive inverse(negative element) denoted $-\\mathbf{x}$ in $V$, such that $\\mathbf{x}+(-\\mathbf{x})=\\mathbf{0}$.\n\n(M1) Scalar multiplication by identity element in $F$: For all $\\mathbf{x}\\in V$, $1\\cdot\\mathbf{x}=\\mathbf{x}$, where $1$ denotes the multiplicative identity in $F$.\n\n(M2) Scalar multiplication and field multiplication mix well: For all $\\alpha,\\beta\\in F$ and $\\mathbf{v}\\in V$, $(\\alpha\\beta)\\mathbf{v}=\\alpha(\\beta\\mathbf{v})$.\n\n(D1) Distribution of scalar multiplication over vector addition: For all $\\alpha\\in F$, and $\\mathbf{u},\\mathbf{v}\\in V$, $\\alpha(\\mathbf{u}+\\mathbf{v})=\\alpha\\mathbf{u}+\\alpha\\mathbf{v}$.\n\n(D2) Distribution of field addition over scalar multiplication: For all $\\alpha,\\beta\\in F$, and $\\mathbf{v}\\in V$, $(\\alpha+\\beta)\\mathbf{v}=\\alpha\\mathbf{v}+\\beta\\mathbf{v}$.\n\nAs usual, our starting point is a random experiment modeled by a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, so that $\\Omega$ is the set of outcomes, $\\mathscr{\\mathcal{F}}$ is the $\\sigma$-algebra of events and $\\mathbb{P}$ is the probability measure on the measurable space $(\\Omega,\\mathcal{F})$. Our basic vector space $V$ consists of all real-valued random variables defined on $(\\Omega,\\mathcal{F},\\mathbb{P})$. We define vector addition and scalar multiplication in the usual way point-wise.\n\n-   Vector addition: $(X+Y)(\\omega)=X(\\omega)+Y(\\omega)$.\n\n-   Scalar multiplication: $(\\alpha X)(\\omega)=\\alpha X(\\omega)$\n\nClearly, any function $g$ of a random variable $X(\\omega)$ is also a random variable on the same probability space and any linear combination of random variables on $(\\Omega,\\mathcal{F},\\mathbb{P})$ also define a new random variable on the same probability space. Thus, $V$ is closed under vector addition and scalar-multiplication. Since vector-addition and scalar multiplication is defined point-wise, it is easy to see that - all the axioms of a vector space (A1)-(A4), (M1-M2), (D1), (D2) are satisfied. The constantly zero random variable $0(\\omega)=0$ and the indicator random variable $I_{\\Omega}(\\omega)$ can be thought of as the zero and identity vectors in this vector space.\n\n### Inner Products.\n\nIn Euclidean geometry, the angle between two vectors is specified by their dot product, which is itself formalized by the abstract concept of inner products.\n\n::: defn\n(Inner Product). An inner product on the real vector space $V$ is a pairing that takes two vectors $\\mathbf{v},\\mathbf{w}\\in V$ and produces a real number $\\left\\langle \\mathbf{v},\\mathbf{w}\\right\\rangle \\in\\mathbf{R}$. The inner product is required to satisfy the following three axioms for all $\\mathbf{u},\\mathbf{v},\\mathbf{w}\\in V$ and scalars $c,d\\in\\mathbf{R}$.\n\n\\(i\\) Bilinearity:\n$$\\left\\langle c\\mathbf{u}+d\\mathbf{v},\\mathbf{w}\\right\\rangle =c\\left\\langle \\mathbf{u},\\mathbf{w}\\right\\rangle +d\\left\\langle \\mathbf{v},\\mathbf{w}\\right\\rangle$$\n\n$$\\left\\langle \\mathbf{u},c\\mathbf{v}+d\\mathbf{w}\\right\\rangle =c\\left\\langle \\mathbf{u},\\mathbf{v}\\right\\rangle +d\\left\\langle \\mathbf{u},\\mathbf{w}\\right\\rangle$$\n\n\\(ii\\) Symmetry:\n\n$$\\left\\langle \\mathbf{v},\\mathbf{w}\\right\\rangle =\\left\\langle \\mathbf{w},\\mathbf{v}\\right\\rangle$$\n\n\\(iii\\) Positive Definiteness:\n\n$$\\left\\langle \\mathbf{v},\\mathbf{v}\\right\\rangle >0\\quad\\text{ whenever }\\mathbf{v\\neq\\mathbf{0}}$$\n\n$$\\left\\langle \\mathbf{v},\\mathbf{v}\\right\\rangle =0\\quad\\text{ whenever }\\mathbf{v=0}$$\n:::\n\n::: defn\n(Norm). A norm on a real vector space $V$ is a function $\\left\\Vert \\cdot\\right\\Vert :V\\to\\mathbf{R}$ satisfying :\n\n\\(i\\) Positive Definiteness.\n\n$$\\left\\Vert \\mathbf{v}\\right\\Vert \\geq0$$\n\nand\n$$\\left\\Vert \\mathbf{v}\\right\\Vert =0\\quad\\text{if and only if }\\mathbf{v}=\\mathbf{0}$$\n\n\\(ii\\) Scalar multiplication.\n\n$$\\left\\Vert \\alpha\\mathbf{v}\\right\\Vert =|\\alpha|\\left\\Vert \\mathbf{v}\\right\\Vert$$\n\n\\(iii\\) Triangle Inequality.\n\n$$\\left\\Vert \\mathbf{x+y}\\right\\Vert \\leq\\left\\Vert \\mathbf{x}\\right\\Vert +\\left\\Vert \\mathbf{y}\\right\\Vert$$\n:::\n\nAs mentioned earlier, we can define the $p$-norm of a random variable as :\n\n$$\\begin{aligned}\n\\left\\Vert X\\right\\Vert _{p} & =\\left(\\mathbb{E}|X|^{p}\\right)^{1/p}\\end{aligned}$$\n\n\\(i\\) Positive semi-definiteness: Since $|X|$ is a non-negative random variable, $|X|^{p}\\geq0$ and the expectation of a non-negative random variable is also non-negative. Hence, $(\\mathbb{E}|X|^{p})^{1/p}\\geq0$. Moreover, $\\left\\Vert X\\right\\Vert _{p}=0$ implies that $\\mathbb{E}|X|^{p}=0$. From property (iv) of expectations, $X=0$.\n\n\\(ii\\) Scalar-multiplication: We have:\n\n$$\\begin{aligned}\n\\left\\Vert cX\\right\\Vert _{p} & =\\left(\\mathbb{E}|cX|^{p}\\right)^{1/p}\\\\ & =\\left(|c|^{p}\\right)^{1/p}\\left(\\mathbb{E}|X|^{p}\\right)^{1/p}\\\\ & =|c|\\cdot\\left\\Vert X\\right\\Vert _{p}\\end{aligned}$$\n\n\\(iii\\) Triangle Inequality. This followed from the Minkowski's inequality.\n\nThe space of all random variables defined on $(\\Omega,\\mathcal{\\mathcal{F}},\\mathbb{P})$ such that $||X||_{p}<\\infty$ is finite is called the $L^{p}$ space.\n\n### Orthogonal Matrices.\n\n::: defn\n(Orthogonal Matrix). Let $A$ be an $n\\times n$ square matrix. We say that the matrix $A$ is orthogonal, if its transpose is equal to its inverse.\n\n$$\\begin{aligned}\nA' & =A^{-1}\\end{aligned}$$ :::\n\nThis may seem like an odd property to study, but the following theorem explains why it is so useful. Essentially, an orthogonal matrix rotates (or reflects) vectors without distorting angles or distances.\n\n::: prop\nFor an $n\\times n$ square matrix $A$, the following are equivalent:\n\n\\(1\\) $A$ is orthogonal. That is, $A'A=I$.\n\n\\(2\\) $A$ preserves norms. That is, for all $\\mathbf{x}$,\n\n$$\\begin{aligned}\n\\left\\Vert A\\mathbf{x}\\right\\Vert  & =\\left\\Vert \\mathbf{x}\\right\\Vert \\end{aligned}$$\n\n\\(3\\) $A$ preserves inner products, that is, for every $\\mathbf{x}$, $\\mathbf{y}$$\\in\\mathbf{R}^{n}$:\n\n$$\\begin{aligned}\n(A\\mathbf{x})\\cdot(A\\mathbf{y}) & =\\mathbf{x}\\cdot\\mathbf{y}\\end{aligned}$$ :::\n\n::: proof\n*Proof.* We have:\n\n$$\\begin{aligned}\n\\left\\Vert A\\mathbf{x}\\right\\Vert ^{2} & =\\left(A\\mathbf{x}\\right)'(A\\mathbf{x})\\\\ & =\\mathbf{x}'(A'A)\\mathbf{x}\\\\ & =\\mathbf{x}'I\\mathbf{x}\\\\ & =\\mathbf{x}'\\mathbf{x}\\\\ & =||\\mathbf{x}||^{2}\\end{aligned}$$\n\nConsequently, $||A\\mathbf{x}||=||\\mathbf{x}||$. The matrix $A$ preserves norms. Thus, (1) implies (2).\n\nMoreover, consider\n\n$$\\begin{aligned}\n||A(\\mathbf{x}+\\mathbf{y})||^{2} & =\\left(A\\mathbf{x}+A\\mathbf{y}\\right)\\cdot\\left(A\\mathbf{x}+A\\mathbf{y}\\right)\\\\ & =(A\\mathbf{x})\\cdot(A\\mathbf{x})+(A\\mathbf{x})\\cdot(A\\mathbf{y})+(A\\mathbf{y})\\cdot(A\\mathbf{x})+(A\\mathbf{y})\\cdot(A\\mathbf{y})\\\\ & =||A\\mathbf{x}||^{2}+2(A\\mathbf{x})\\cdot(A\\mathbf{y})+||A\\mathbf{y}||^{2} & \\{\\mathbf{x}\\cdot\\mathbf{y}=\\mathbf{y}\\cdot\\mathbf{x}\\}\\\\ & =||\\mathbf{x}||^{2}+2(A\\mathbf{x})\\cdot(A\\mathbf{y})+||\\mathbf{y}||^{2} & \\{A\\text{ preserves norms}\\}\\end{aligned}$$\n\nBut, $||A(\\mathbf{x}+\\mathbf{y})||^{2}=||\\mathbf{x}+\\mathbf{y}||^{2}=||\\mathbf{x}||^{2}+2\\mathbf{x}\\cdot\\mathbf{y}+||\\mathbf{y}||^{2}$. Equating the two expressions, we have the desired result. Hence, (2) implies (3).\n\nLastly, if $A$ preserves inner products, we may write:\n\n$$\\begin{aligned}\n\\left\\langle A\\mathbf{x},A\\mathbf{x}\\right\\rangle  & =\\left\\langle \\mathbf{x},\\mathbf{x}\\right\\rangle \\\\ \\left(A\\mathbf{x}\\right)'(A\\mathbf{x}) & =\\mathbf{x}'\\mathbf{x}\\\\ \\mathbf{x}'A'A\\mathbf{x} & =0\\end{aligned}$$\n\nSince $\\mathbf{x}\\neq\\mathbf{0}$, it must be true that $\\mathbf{x}'A'A-\\mathbf{x}'=0$. Again, since $\\mathbf{x}'\\neq\\mathbf{0}$, it follows that $A'A-I=0$. ◻ :::\n\n::: thm\n[]{#linear-independence-of-orthogonal-vectors label=\"linear-independence-of-orthogonal-vectors\"}If $\\mathbf{q}_{1},\\mathbf{q}_{2},\\ldots,\\mathbf{q}_{k}\\in V$ be mutually orthogonal elements, such that $\\mathbf{q}_{i}\\neq\\mathbf{0}$ for all $i$, then $\\mathbf{q}_{1},\\mathbf{q}_{2},\\ldots,\\mathbf{q}_{k}$ are linearly independent. :::\n\n::: proof\n*Proof.* Let\n\n$$\\begin{aligned}\nc_{1}\\mathbf{q}_{1}+c_{2}\\mathbf{q}_{2}+\\ldots+c_{k}\\mathbf{q}_{k} & =\\mathbf{0}\\end{aligned}$$\n\nSince $\\left\\langle \\mathbf{q}_{i},\\mathbf{q}_{i}\\right\\rangle =1$ and $\\left\\langle \\mathbf{q}_{i},\\mathbf{q}_{j}\\right\\rangle =0$ where $i\\neq j$, we can take the inner product of the vector $(c_{1}\\mathbf{q}_{1}+c_{2}\\mathbf{q}_{2}+\\ldots+c_{i}\\mathbf{q}_{i}+\\ldots+c_{k}\\mathbf{q}_{k})$ with $\\mathbf{q}_{i}$ for each $i=1,2,3,\\ldots,k$. It results in $c_{i}||\\mathbf{q}_{i}||^{2}=0$. Since $\\mathbf{q}_{i}\\neq\\mathbf{0}$, $||\\mathbf{q}_{i}||^{2}>0$. So, $c_{i}=0$. We conclude that $c_{1}=c_{2}=\\ldots=c_{k}=0$. Consequently, $\\mathbf{q}_{1},\\mathbf{q}_{2},\\ldots,\\mathbf{q}_{k}$ are linearly independent. ◻ :::\n\n::: thm\nLet $Q=\\left[\\begin{array}{cccc} \\mathbf{q}_{1} & \\mathbf{q}_{2} & \\ldots & \\mathbf{q}_{n}\\end{array}\\right]$ be an $n\\times n$ orthogonal matrix. Then, $\\{\\mathbf{q}_{1},\\ldots,\\mathbf{q}_{n}\\}$ form an orthonormal basis for $\\mathbf{R}^{n}$. :::\n\n::: proof\n*Proof.* We have $Q\\mathbf{e}_{i}=\\mathbf{q}_{i}$. Consequently,\n\n$$\\begin{aligned}\n\\left\\langle \\mathbf{q}_{i},\\mathbf{q}_{i}\\right\\rangle  & =\\mathbf{q}_{i}'\\mathbf{q}_{i}\\\\ & =(Q\\mathbf{e}_{i})'(Q\\mathbf{e}_{i})\\\\ & =\\mathbf{e}_{i}'Q'Q\\mathbf{e}_{i}\\\\ & =\\mathbf{e}_{i}'I\\mathbf{e}_{i}\\\\ & =\\mathbf{e}_{i}'\\mathbf{e}_{i}\\\\ & =1\\end{aligned}$$\n\nAssume that $i\\neq j$. We have:\n\n$$\\begin{aligned}\n\\left\\langle \\mathbf{q}_{i},\\mathbf{q}_{j}\\right\\rangle  & =\\mathbf{q}_{i}'\\mathbf{q}_{j}\\\\ & =\\mathbf{e}_{i}'Q'Q\\mathbf{e}_{j}\\\\ & =\\mathbf{e}_{i}'\\mathbf{e}_{j}\\\\ & =0\\end{aligned}$$\n\nFrom theorem ([\\[linear-independence-of-orthogonal-vectors\\]](#linear-independence-of-orthogonal-vectors){reference-type=\"ref\" reference=\"linear-independence-of-orthogonal-vectors\"}), $\\{\\mathbf{q}_{1},\\ldots,\\mathbf{q}_{n}\\}$ are linearly independent and hence form an orthonormal basis for $\\mathbf{R}^{n}$. ◻ :::\n\n### Quadratic Forms.\n\nAn expression of the form:\n\n$$\\mathbf{x}'A\\mathbf{x}$$\n\nwhere $\\mathbf{x}$ is a $n\\times1$ column vector and $A$ is an $n\\times n$ matrix is called a quadratic form in $\\mathbf{x}$ and\n\n$$\\begin{aligned}\n\\mathbf{x}'A\\mathbf{x} & =\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}x_{i}x_{j}\\end{aligned}$$\n\nIf $A$ and $B$ are $n\\times n$ and $\\mathbf{x},\\mathbf{y}$ are $n$-vectors, then\n\n$$\\begin{aligned}\n\\mathbf{x}'(A+B)\\mathbf{y} & =\\mathbf{x}'A\\mathbf{y}+\\mathbf{x}'B\\mathbf{y}\\end{aligned}$$\n\nThe quadratic form of the matrix $A$ is called positive definite if:\n\n$$\\begin{aligned}\n\\mathbf{x}'A\\mathbf{x} & >0\\quad\\text{whenever }\\mathbf{x}\\neq\\mathbf{0}\\end{aligned}$$\n\nand positive semidefinite if:\n\n$$\\begin{aligned}\n\\mathbf{x}'A\\mathbf{x} & \\geq0\\quad\\text{whenever }\\mathbf{x}\\neq\\mathbf{0}\\end{aligned}$$\n\nLetting $\\mathbf{e}_{i}$ be the unit vector with it's $i$th coordinate vector $1$, we have:\n\n$$\\begin{aligned}\n\\mathbf{e}_{i}'A\\mathbf{e}_{i} & =\\left[a_{i1}a_{i2}\\ldots a_{ii}\\ldots a_{in}\\right]\\left[\\begin{array}{c} 0\\\\ 0\\\\ \\vdots\\\\ 1\\\\ \\vdots\\\\ 0\n\\end{array}\\right]=a_{ii}\\end{aligned}$$\n\n### Eigenthingies and diagonalizability.\n\nLet $V$ and $W$ be finite dimensional vector spaces with $dim(V)=n$ and $dim(W)=m$. A linear transformation $T:V\\to W$, is defined by its action on the basis vectors. Suppose:\n\n$$\\begin{aligned}\nT(\\mathbf{v}_{j}) & =\\sum_{i=1}^{n}a_{ij}\\mathbf{w}_{i}\\end{aligned}$$\n\nfor all $1\\leq i\\leq m$.\n\nThen, the matrix $A=[T]_{\\mathcal{B}_{V}}^{\\mathcal{B}_{W}}$ of the linear transformation is defined as:\n\n$$\\begin{aligned}\nA & =\\left[\\begin{array}{cccc} a_{11} & a_{12} & \\ldots & a_{1n}\\\\ a_{21} & a_{22} & \\ldots & a_{2n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\ldots & a_{mn}\n\\end{array}\\right]\\end{aligned}$$\n\n::: defn\nA linear transformation $T:V\\to V$ is **diagonalizable** if there exists an ordered basis $\\mathcal{B}=\\{\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{n}\\}$ for $V$ so that the matrix for $T$ with respect to $\\mathcal{B}$ is diagonal. This means precisely that, for some scalars $\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{n}$, we have: :::\n\n$$\\begin{aligned}\nT(\\mathbf{v}_{1}) & =\\lambda_{1}\\mathbf{v}_{1}\\\\ T(\\mathbf{v}_{2}) & =\\lambda_{2}\\mathbf{v}_{2}\\\\ \\vdots\\\\ T(\\mathbf{v}_{n}) & =\\lambda_{n}\\mathbf{v}_{n}\\end{aligned}$$\n\nIn other words, if $A=[T]_{\\mathcal{B}}$, then we have:\n\n$$\\begin{aligned}\nA\\mathbf{v}_{i} & =\\lambda_{i}\\mathbf{v}_{i}\\end{aligned}$$\n\nThus, if we let $P$ be the $n\\times n$ matrix whose columns are the vectors $\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots,\\mathbf{v}_{n}$ and $\\Lambda$ be the $n\\times n$ diagonal matrix with diagonal entries $\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{n}$, then we have:\n\n$$\\begin{aligned}\nA\\left[\\begin{array}{cccc} \\mathbf{v}_{1} & \\mathbf{v}_{2} & \\ldots & \\mathbf{v}_{n}\\end{array}\\right] & =\\left[\\begin{array}{cccc} \\mathbf{v}_{1} & \\mathbf{v}_{2} & \\ldots & \\mathbf{v}_{n}\\end{array}\\right]\\left[\\begin{array}{cccc} \\lambda_{1}\\\\ & \\lambda_{2}\\\\ &  & \\ddots\\\\ &  &  & \\lambda_{n}\n\\end{array}\\right]\\\\\nAP & =P\\Lambda\\\\ A & =P\\Lambda P^{-1}\\end{aligned}$$\n\nThere exists a large class of diagonalizable matrices - the symmetric matrices. A square matrix $A$ is symmetric, if $A=A'$.\n\n::: defn\nLet $T:V\\to V$ be a linear transformation. A **non-zero** vector $\\mathbf{v}\\in V$ is called the eigenvector of $T$, if there is a scalar $\\lambda$ so that $T(\\mathbf{v})=\\lambda\\mathbf{v}$. The scalar $\\lambda$ is called the eigenvalue of $T$. :::\n\n::: lem\nLet $A$ be an $n\\times n$ matrix, and let $\\lambda$ be any scalar. Then,\n\n$$\\begin{aligned}\nE(\\lambda) & =\\{\\mathbf{x}\\in\\mathbf{R}^{n}:A\\mathbf{x}=\\lambda\\mathbf{x}\\}=\\ker(A-\\lambda I)\\end{aligned}$$\n\nis a subspace of $\\mathbf{R}^{n}$. Moreover, if $E(\\lambda)\\neq\\{\\mathbf{0}\\}$ if and only if $\\lambda$ is an eigenvalue, in which case we call $E(\\lambda)$ the $\\lambda$-eigenspace of the matrix $A$. :::\n\n::: proof\n*Proof.* We know that, $E(\\lambda)$ is a subset of $\\mathbf{R}^{n}$. Moreover, if $\\mathbf{u},\\mathbf{v}\\in E(\\lambda)$ , then $A(c_{1}\\mathbf{u}+c_{2}\\mathbf{v})=c_{1}A\\mathbf{u}+c_{2}A\\mathbf{v}=\\lambda(c_{1}\\mathbf{u}+c_{2}\\mathbf{v})$. Consequently, $c_{1}\\mathbf{u}+c_{2}\\mathbf{v}\\in E(\\lambda)$. Thus, $E(\\lambda)$ is a subspace of $\\mathbf{R}^{n}$.\n\nMoreover, by definition, $\\lambda$ is an eigenvalue of $A$ precisely when $\\mathbf{x}\\neq\\mathbf{0}$ vector in $E(\\lambda)$. This closes the proof. ◻ :::\n\n::: thm\nLet $A$ be a $n\\times n$ square matrix. If $A$ is a singular matrix, then $\\det A=0$. :::\n\n::: proof\n*Proof.* By definition, a square matrix is said to be non-singular, if it can be reduced to an upper triangular form with all non-zero elements on the diagonal - the pivots, by elementary row operations. A singular matrix is such that it's echelon form has a row of zeroes, and its row vectors are linearly dependent and $\\det A=0$. ◻ :::\n\n::: thm\nLet $A$ be a $n\\times n$ square matrix. Then, $\\lambda$ is an eigenvalue of $A$ if and only if $\\det(A-\\lambda I)=0$. :::\n\n::: proof\n*Proof.* $\\lambda$ is an eigenvalue of $A$, if and only, the homogenous system of linear equations $(A-\\lambda I)\\mathbf{x}=\\mathbf{0}$ has non-trivial solutions. Consequently, the only possibility is that there are one more free variables (more variables than the number of equations). In other words, $(A-\\lambda I)$ must be a singular matrix and $\\det(A-\\lambda I)=0$. ◻ :::\n\n::: example\nLet's find the eigenvalues and eigenvectors of the matrix\n\n$$\\begin{aligned}\nA & =\\left[\\begin{array}{ccc} 1 & 2 & 1\\\\ 0 & 1 & 0\\\\ 1 & 3 & 1\n\\end{array}\\right]\\end{aligned}$$\n\nWe begin by computing\n\n$$\\begin{aligned}\n\\det(A-\\lambda I) & =\\left[\\begin{array}{ccc} 1-\\lambda & 2 & 1\\\\ 0 & 1-\\lambda & 0\\\\ 1 & 2 & 1-\\lambda\n\\end{array}\\right]\\\\\n& =(1-\\lambda)(1-\\lambda)^{2}-(1-\\lambda)\\\\ & =(1-\\lambda)[(1-\\lambda)^{2}-1)]\\\\ & =-\\lambda(1-\\lambda)(2-\\lambda)\\end{aligned}$$\n\nThus, the eigenvalues of $A$ are $\\lambda=0$, $\\lambda=1$ and $\\lambda=2$.\n\nWe find the respective eigenspaces:\n\n1\\) Fix $\\lambda=0$. We see that:\n\n$$\\begin{aligned}\n\\left[\\begin{array}{ccc} 1 & 2 & 1\\\\ 0 & 1 & 0\\\\ 1 & 3 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\nx_{1}\\\\ x_{2}\\\\ x_{3}\n\\end{array}\\right] & =\\left[\\begin{array}{c}\n0\\\\ 0\\\\ 0\n\\end{array}\\right]\\end{aligned}$$\n\nThe augmented matrix $[A|b]$ is :\n\n$$\\left[\\begin{array}{ccccc}\n1 & 2 & 1 & | & 0\\\\ 0 & 1 & 0 & | & 0\\\\ 1 & 3 & 1 & | & 0\n\\end{array}\\right]$$\n\n$R_{3}-R_{1}$, $R_{3}-R_{2}$ and $R_{1}-2R_{2}$ leaves us with:\n\n$$\\left[\\begin{array}{ccccc}\n1 & 0 & 1 & | & 0\\\\ 0 & 1 & 0 & | & 0\\\\ 0 & 0 & 0 & | & 0\n\\end{array}\\right]$$\n\nSo, $x_{1}+x_{3}=0$ and $x_{2}=0$. Here, $x_{3}$ is a free variable. Thus,\n\n$$\\begin{aligned}\nE(0) & =\\{\\alpha(1,0,-1)|\\alpha\\in\\mathbf{R}\\}\\end{aligned}$$\n\n2\\) Fix $\\lambda=1$. We see that:\n\n$$\\begin{aligned}\n\\left[\\begin{array}{ccc} 0 & 2 & 1\\\\ 0 & 0 & 0\\\\ 1 & 3 & 0\n\\end{array}\\right]\\left[\\begin{array}{c}\nx_{1}\\\\ x_{2}\\\\ x_{3}\n\\end{array}\\right] & =\\left[\\begin{array}{c}\n0\\\\ 0\\\\ 0\n\\end{array}\\right]\\end{aligned}$$\n\nThus, $2x_{2}+x_{3}=0$ and $x_{1}+3x_{2}=0$. Here $x_{3}$ is a free variable. Let $x_{3}=-2\\alpha$. Then, $x_{2}=\\alpha$ and $x_{1}=-3\\alpha$. Consequently,\n\n$$\\begin{aligned}\nE(1) & =\\{\\alpha(-3,1,-2)|\\alpha\\in\\mathbf{R}\\}\\end{aligned}$$\n\n3\\) Fix $\\lambda=3$. We see that:\n\n$$\\begin{aligned}\n\\left[\\begin{array}{ccc} -1 & 2 & 1\\\\ 0 & -1 & 0\\\\ 1 & 3 & -1\n\\end{array}\\right]\\left[\\begin{array}{c}\nx_{1}\\\\ x_{2}\\\\ x_{3}\n\\end{array}\\right] & =\\left[\\begin{array}{c}\n0\\\\ 0\\\\ 0\n\\end{array}\\right]\\end{aligned}$$\n\nThe augmented matrix $[A|b]$ is :\n\n$$\\left[\\begin{array}{ccccc}\n-1 & 2 & 1 & | & 0\\\\ 0 & -1 & 0 & | & 0\\\\ 1 & 3 & -1 & | & 0\n\\end{array}\\right]$$\n\n$R_{3}+R_{1}$, $R_{3}+5R_{2}$ followed by $R_{1}+2R_{2}$ gives:\n\n$$\\left[\\begin{array}{ccccc}\n-1 & 0 & 1 & | & 0\\\\ 0 & -1 & 0 & | & 0\\\\ 0 & 0 & 0 & | & 0\n\\end{array}\\right]$$\n\nThus, $x_{2}=0$ and $x_{1}-x_{3}=0$. Here $x_{3}$ is the free variable. Hence,\n\n$$\\begin{aligned}\nE(2) & =\\{\\alpha(1,0,1):\\alpha\\in\\mathbf{R}\\}\\end{aligned}$$\n\nClearly, there exists a basis $\\mathcal{B}=\\{(1,0,-1),(-3,1,-2),(1,0,1)\\}$ with respect to which the matrix of $T$ is diagonal. Hence, $A$ is diagonalizable. :::\n\nJudging from the previous example, it appears that when an $n\\times n$ square matrix has $n$ distinct eigen values, the corresponding eigenvectors form a linearly independent set and will therefore give a *diagonalizing basis.* Let's begin with a slightly stronger statement.\n\n::: thm\nLet $T:V\\to V$ be a linear transformation. Suppose $\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots,\\mathbf{v}_{k}$ are eigenvectors of $T$ corresponding to the distinct eigenvalues $\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{k}$. Then, $\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots,\\mathbf{v}_{k}\\}$ is a linearly independent set of vectors. :::\n\n::: proof\n*Proof.* Let $m$ be the largest number between $1$ and $k$ (inclusive) so that $\\{\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{m}\\}$ is linearly independent. We proceed by contradiction. We want to see $m=k$. Assume that $m<k$. Then, we know that $\\{\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{m}\\}$ is linearly independent and $\\{\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{m},\\mathbf{v}_{m+1}\\}$ is linearly dependent. Thus, $\\mathbf{v}_{m+1}=c_{1}\\mathbf{v}_{1}+c_{2}\\mathbf{v}_{2}+\\ldots+c_{m}\\mathbf{v}_{m}$ such that atleast one of $c_{1},c_{2},\\ldots,c_{m}$ are non-zero. Then, using repeatedly the fact that $T(\\mathbf{v}_{i})=\\lambda_{i}\\mathbf{v}_{i}$:\n\n$$\\begin{aligned}\n\\mathbf{0} & =(T-\\lambda_{m+1}I)\\mathbf{v}_{m+1}=(T-\\lambda_{m+1}I)(c_{1}\\mathbf{v}_{1}+\\ldots+c_{m}\\mathbf{v}_{m})\\\\ & =c_{1}\\left(T\\mathbf{v}_{1}-\\lambda_{m+1}I\\mathbf{v}_{1}\\right)+c_{2}\\left(T\\mathbf{v}_{2}-\\lambda_{m+1}I\\mathbf{v}_{2}\\right)+\\ldots+c_{m}\\left(T\\mathbf{v}_{m}-\\lambda_{m+1}I\\mathbf{v}_{m}\\right)\\\\ & =c_{1}(\\lambda_{1}-\\lambda_{m+1})\\mathbf{v}_{1}+c_{2}(\\lambda_{2}-\\lambda_{m+1})\\mathbf{v}_{2}+\\ldots+c_{m}(\\lambda_{m}-\\lambda_{m+1})\\mathbf{v}_{m}\\end{aligned}$$\n\nSince $\\lambda_{i}\\neq\\lambda_{m+1}$ for $i=1,2,3,\\ldots,m$ and since $\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots\\mathbf{v}_{m}\\}$ is linearly independent, the only other possibility is $c_{1}=c_{2}=\\ldots=c_{m}=0$. But, this contradicts the fact that $\\mathbf{v}_{m+1}$ is an eigenvector since $\\mathbf{v}_{m+1}\\neq\\mathbf{0}$. Thus, it cannot happen that $m<k$. Consequently, $m=k$. ◻ :::\n\nWhat is underlying this formal argument is the observation that: if $\\mathbf{v}\\in E(\\lambda)\\cap E(\\mu)$, then $T\\mathbf{v}=\\lambda\\mathbf{v}$ and $T\\mathbf{\\mathbf{v}=\\mu\\mathbf{v}}$. Hence, if $\\lambda\\neq\\mu$, then $\\mathbf{v}=\\mathbf{0}$. That is, if $\\lambda\\neq\\mu$, we have $E(\\lambda)\\cap E(\\mu)=\\{\\mathbf{0}\\}$.\n\n::: cor\nSuppose $V$ is an $n$-dimensional vector space and $T:V\\to V$ has $n$ distinct eigenvalues. Then $T$ is diagonalizable. :::\n\n::: proof\n*Proof.* The set of $n$ corresponding eigenvectors must be linearly independent and hence form a basis for $V$. The matrix of $T$ with respect to the eigenbasis is always diagonal. ◻ :::\n\nThe converse of this statement is not true. There are many diagonalizable matrices with repeated eigen-values.\n\n::: defn\nLet $\\lambda$ be an eigenvalue of a linear transformation. The algebraic multiplicity of $\\lambda$ is its multiplicity as a root of the characteristic polynomial $p(t)$ that is, the highest power of $t-\\lambda$ dividing $p(t)$. The geometric multiplicity of $\\lambda$ is the dimension of the eigenspace $E(\\lambda)$. :::\n\n::: prop\nLet $\\lambda$ be an eigenvalue of algebraic multiplicity $m$ and geometric multiplicity $d$. Then, the geometric multiplicity is always bounded by the algebraic multiplicity, and $1\\leq d\\leq m$. :::\n\n::: proof\n*Proof.* Suppose $\\lambda$ is the eigenvalue of the linear transformation $T$. Then, $d=\\dim E(\\lambda)\\geq1$ by definition. Now, choose a basis $\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots,\\mathbf{v}_{d}\\}$ for $E(\\lambda)$ and extend it to a basis $\\mathcal{B}=\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots,\\mathbf{v}_{n}\\}$ for $V$. Then, the matrix of $T$ with respect to $\\mathcal{B}$ is of the form\n\n$$\\begin{aligned}\nA & =\\left[\\begin{array}{cc} \\lambda I_{d} & B\\\\ 0_{(n-d)\\times d} & C\n\\end{array}\\right]\\end{aligned}$$\n\nThe characteristic polynomial $p(t)$ of the matrix $A$ is given by:\n\n$$\\begin{aligned}\np(t) & =\\det(A-tI)\\\\ & =\\det((\\lambda-t)I_{d})\\cdot\\det(C-tI)\\\\ & =(\\lambda-t)^{d}\\cdot\\det(C-tI)\\end{aligned}$$\n\nSince the characteristic polynomial does not depend on the choice of basis, the algebraic multiplicity of $\\lambda$ is atleast $d$. ◻ :::\n\n::: lem\n(Lagrange Multipliers) Suppose $f,g:\\mathbf{R}^{n}\\to\\mathbf{R}$ are scalar-valued $C^{1}$ functions - that is partial derivatives $\\partial_{x_{i}}$ in all variables are continuous. Let $S=\\{\\mathbf{x}\\in\\mathbf{R}^{n}|g(\\mathbf{x})=c\\}$ denote the level set of $g$ at height $c$. Then if $f|_{S}$ (the restriction of $f$ to $S$) has an extremum point $\\mathbf{x}_{0}$ in $S$ such that $\\nabla g(\\mathbf{x}_{0})\\neq\\mathbf{0}$, there exists a scalar $\\lambda$ such that\n\n$$\\nabla f(\\mathbf{x}_{0})=\\lambda\\nabla g(\\mathbf{x}_{0})$$\n:::\n\n::: proof\n*Proof.* Let's visualize the situation for the case $n=3$, where the constraint equation $g(x,y,z)=c$ defines a surface $S$ in $\\mathbf{R}^{3}$.\n\nThus, suppose that $\\mathbf{x}_{0}$ is an extremum of $f$ restricted to $S$. We consider a further restriction of $f$ - to a curve lying in $S$ and passing through $\\mathbf{x}_{0}$. Let $\\mathbf{x}(t)=(x(t),y(t),z(t))$ be the parametric equation of one such arbitrary path $\\mathbf{x}:I\\subseteq\\mathbf{R}\\to\\mathbf{R}^{3}$ lying in $S$ with $\\mathbf{x}(t_{0})=\\mathbf{x}_{0}$ for some $t_{0}\\in I$. Then, the restriction of $f$ to $\\mathbf{x}$ can be written as a function of a single variable $t$. That is:\n\n$$\\begin{aligned}\nF(t) & :=f(\\mathbf{\\mathbf{x}}(t))\\end{aligned}$$\n\nBecause $\\mathbf{x}_{0}$ is an extremum of $f$ on the whole of $S$, it is also an extremum on the path $\\mathbf{x}$. Since $F$ is a differentiable function of $t$, by the interior-extremum theorem, it follows that $F'(t_{0})=0$. The chain rule implies that:\n\n$$\\begin{aligned}\nF'(t) & =\\nabla f(\\mathbf{x})\\cdot\\mathbf{x}'(t)\\end{aligned}$$\n\nEvaluating at $t=t_{0}$, we have:\n\n$$\\begin{aligned}\nF'(t_{0})=0 & =\\nabla f(\\mathbf{x}(t_{0}))\\cdot\\mathbf{x}'(t_{0})\\end{aligned}$$\n\nThus, $\\nabla f(\\mathbf{x}(t_{0}))$ is perpendicular to any curve in $S$ passing through $\\mathbf{x}_{0}$; that is $\\nabla f(\\mathbf{x}_{0})$ is normal to $S$ at $\\mathbf{x}_{0}$. We've already seen previously that the gradient vector $\\nabla g(\\mathbf{x}_{0})$ is also normal to $S$ at $\\mathbf{x}_{0}$. Since the normal direction to the level $S$ is uniquely determined, we must conclude that $\\nabla f(\\mathbf{x}_{0})$ and $\\nabla g(\\mathbf{x}_{0})$ are parallel vectors. Therefore, there exists a scalar $\\lambda$ such that:\n\n$$\\begin{aligned}\n\\nabla f(\\mathbf{x}_{0}) & =\\lambda\\nabla g(\\mathbf{x}_{0})\\end{aligned}$$ ◻ :::\n\n### The Gram-Schmidt Process.\n\nThe advantage of using an orthonormal basis is, that the coordinates of any vector are explicitly given as inner products. Let $\\{\\mathbf{u}_{1},\\mathbf{u}_{2},\\ldots,\\mathbf{u}_{n}\\}$ be an orthonormal basis of $\\mathbf{R}^{n}$. And let $\\mathbf{v}=c_{1}\\mathbf{u}_{1}+\\ldots+c_{n}\\mathbf{u}_{n}$ be an arbitrary vector. Then we have:\n\n$$\\begin{aligned}\nc_{i} & =\\mathbf{v}\\cdot\\mathbf{u}_{i}\\end{aligned}$$\n\nMoreover, the magnitude (norm) of the vector is given by the Pythagorean formula:\n\n$$\\begin{aligned}\n\\left\\Vert \\mathbf{v}\\right\\Vert _{2}^{2} & =\\left\\langle \\mathbf{v},\\mathbf{v}\\right\\rangle \\\\ & =c_{1}^{2}+c_{2}^{2}+\\ldots+c_{n}^{2}\\end{aligned}$$\n\nOnce we are convinced of the utility of orthogonal and orthonormal bases, a natural question arises: how can we construct them? A practical algorithm was discovered Pierre-Simon Laplace in the eighteenth century. Today, the algorithm is known as the *Gram-Schmidt process,* after its rediscovery by Gram and twentieth century mathematician Schmidt.\n\nLet $W$ be a finite dimensional vector space, such that $\\dim W=n$. We assume that, we already know some basis $\\{\\mathbf{w}_{1},\\ldots,\\mathbf{w}_{n}\\}$ of $W$, where $n=\\dim W$. Our goal is to use this information to construct an orthogonal basis $\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots,\\mathbf{v}_{n}$.\n\nWe will construct the orthogonal basis one-by-one. Since initially, we are not worrying about normality, there are no conditions on the first orthogonal basis element $\\mathbf{v}_{1}$, so there is no harm in choosing :\n\n$$\\begin{aligned}\n\\mathbf{v}_{1} & =\\mathbf{w}_{1}\\end{aligned}$$\n\nNote that, $\\mathbf{v}_{1}\\neq\\mathbf{0}$, since $\\mathbf{w}_{1}$ appears in the original basis. Starting with $\\mathbf{w}_{2}$, the second basis vector $\\mathbf{v}_{2}$ must be orthogonal to the first: $\\left\\langle \\mathbf{v}_{2},\\mathbf{v}_{1}\\right\\rangle =0$.\n\n::: center\nFigure. Resolving the vector $\\mathbf{w}_2$ into two components (1) along $\\mathbf{u}_1$ and (2) perpendicular to $\\mathbf{u}_1$. :::\n\nLet us try to arrange this, by subtracting a suitable multiple of $\\mathbf{v}_{1}$, and set:\n\n$$\\begin{aligned}\n\\mathbf{v}_{2} & =\\mathbf{w}_{2}-c\\mathbf{v}_{1}\\end{aligned}$$\n\nThe orthogonality condition\n\n$$\\begin{aligned}\n0 & =\\left\\langle \\mathbf{v}_{2},\\mathbf{v}_{1}\\right\\rangle \\\\ & =(\\mathbf{w}_{2}-c\\mathbf{v}_{1})\\cdot\\mathbf{v}_{1}\\\\ & =\\mathbf{w}_{2}\\cdot\\mathbf{v}_{1}-c\\left\\Vert \\mathbf{v}_{1}\\right\\Vert ^{2}\\\\ c & =\\frac{\\mathbf{w}_{2}\\cdot\\mathbf{v}_{1}}{\\left\\Vert \\mathbf{v}_{1}\\right\\Vert ^{2}}\\end{aligned}$$\n\nand therefore\n\n$$\\begin{aligned}\n\\mathbf{v}_{2} & =\\mathbf{w}_{2}-\\left(\\frac{\\mathbf{w}_{2}\\cdot\\mathbf{v}_{1}}{\\left\\Vert \\mathbf{v}_{1}\\right\\Vert ^{2}}\\right)\\mathbf{v}_{1}\\end{aligned}$$\n\nThe linear independence of $\\mathbf{v}_{1}=\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ ensures that $\\mathbf{v}_{2}\\neq\\mathbf{0}$.\n\nNext, we construct:\n\n$$\\begin{aligned}\n\\mathbf{v}_{3} & =\\mathbf{w}_{3}-c_{1}\\mathbf{v}_{1}-c_{2}\\mathbf{v}_{2}\\end{aligned}$$\n\nby subtracting suitable multiples of the first two orthogonal basis elements from $\\mathbf{w}_{3}$. We want $\\mathbf{v}_{3}$ to be orthogonal to both $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$. Since we already arranged that $\\mathbf{v_{1}\\cdot}\\mathbf{v}_{2}=0$, this requires:\n\n$$\\begin{aligned}\n0=\\mathbf{v}_{3}\\cdot\\mathbf{v}_{1} & =(\\mathbf{w}_{3}\\cdot\\mathbf{v}_{1})-c_{1}\\left\\Vert \\mathbf{v}_{1}\\right\\Vert ^{2}\\\\ 0=\\mathbf{v}_{3}\\cdot\\mathbf{v}_{2} & =(\\mathbf{w}_{3}\\cdot\\mathbf{v}_{2})-c_{2}\\left\\Vert \\mathbf{v}_{2}\\right\\Vert ^{2}\\end{aligned}$$\n\nAnd hence:\n\n$$\\begin{aligned}\nc_{1} & =\\frac{\\mathbf{w}_{3}\\cdot\\mathbf{v}_{1}}{\\left\\Vert \\mathbf{v}_{1}\\right\\Vert ^{2}}\\\\ c_{2} & =\\frac{\\mathbf{w}_{3}\\cdot\\mathbf{v}_{2}}{\\left\\Vert \\mathbf{v}_{2}\\right\\Vert ^{2}}\\end{aligned}$$\n\nTherefore the next orthogonal basis vector is given by the formula:\n\n$$\\begin{aligned}\n\\mathbf{v}_{3} & =\\mathbf{w}_{3}-\\frac{\\mathbf{w}_{3}\\cdot\\mathbf{v}_{1}}{\\left\\Vert \\mathbf{v}_{1}\\right\\Vert ^{2}}\\mathbf{v}_{1}-\\frac{\\mathbf{w}_{3}\\cdot\\mathbf{v}_{2}}{\\left\\Vert \\mathbf{v}_{2}\\right\\Vert ^{2}}\\mathbf{v}_{2}\\end{aligned}$$\n\nSince $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$ are linear combinations of $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$, we must have that $\\mathbf{v}_{3}\\neq\\mathbf{0}$, since otherwise this would imply that $\\mathbf{w}_{3}$ can be written as a linear combination of $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ making them linearly dependent.\n\nContinuing in the same manner, suppose we have already constructed the mutually orthogonal vectors $\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{k-1}$ as linear combinations of $\\mathbf{w}_{1},\\ldots,\\mathbf{w}_{k-1}$. The next orthogonal basis element $\\mathbf{v}_{k}$ will be obtained from $\\mathbf{w}_{k}$ by subtracting a suitable linear combination of the previous orthogonal basis elements. In this fashion we establish the general *Gram-Schmidt* formula -\n\n$$\\mathbf{v}_{k}=\\mathbf{w}_{k}-\\sum_{j=1}^{k-1}\\frac{\\mathbf{w}_{k}\\cdot\\mathbf{v}_{j}}{\\left\\Vert \\mathbf{v}_{j}\\right\\Vert ^{2}}\\mathbf{v}_{j}\\label{eq:gram-schmidt-formula}$$\n\nIf we are after an orthonormal basis $\\mathbf{u}_{1},\\ldots,\\mathbf{u}_{n}$ we merely normalize the resulting orthogonal basis vectors, setting $\\mathbf{u}_{k}=\\frac{\\mathbf{v}_{k}}{\\left\\Vert \\mathbf{v}_{k}\\right\\Vert }$.\n\n### Modifications of the Gram-Schmidt process.\n\nWith the basic Gram-Schmidt algorithm now in hand, it is worth looking at a couple of reformulations that have both practical and theoretical advantages. The first can be used to construct orthonormal basis vectors $\\mathbf{u}_{1},\\mathbf{u}_{2},\\ldots,\\mathbf{u}_{n}$ directly from the basis $\\mathbf{w}_{1},\\ldots,\\mathbf{w}_{n}$.\n\nWe begin by replacing each orthogonal basis vector in the basic Gram-Schmidt formula ([\\[eq:gram-schmidt-formula\\]](#eq:gram-schmidt-formula){reference-type=\"ref\" reference=\"eq:gram-schmidt-formula\"}) by its normalized version $\\mathbf{u}_{j}=\\mathbf{v}_{j}/\\left\\Vert \\mathbf{v}_{j}\\right\\Vert$. The original basis vectors can be expressed in terms of the orthonormal basis via a triangular system.\n\n$$\\begin{aligned}\\mathbf{w}_{1} & =r_{11}\\mathbf{u}_{1}\\\\\n\\mathbf{w}_{2} & =r_{12}\\mathbf{u}_{1}+r_{22}\\mathbf{u}_{2}\\\\ \\mathbf{w}_{3} & =r_{13}\\mathbf{u}_{1}+r_{23}\\mathbf{u}_{2}+r_{33}\\mathbf{u}_{3}\\\\ \\vdots\\\\ \\mathbf{w}_{n} & =r_{1n}\\mathbf{u}_{1}+r_{2n}\\mathbf{u}_{2}+r_{3n}\\mathbf{u}_{3}+\\ldots+r_{nn}\\mathbf{u}_{n}\n\\end{aligned}\n\\label{eq:gram-schmidt-equations}$$\n\nThe coefficients $r_{ij}$ can, in fact, be computed directly from these formulas. Indeed taking, the inner product of the equation for $\\mathbf{w}_{j}$ with the orthonormal basis vector $\\mathbf{u}_{i}$ for $i\\leq j$, we obtain in view of the orthonormality constraints:\n\n$$\\begin{aligned}\n\\mathbf{w}_{j}\\cdot\\mathbf{u}_{i} & =r_{1j}\\mathbf{u}_{1}\\cdot\\mathbf{u}_{i}+\\ldots+r_{ij}\\mathbf{u}_{i}\\cdot\\mathbf{u}_{i}+\\ldots+r_{jj}\\mathbf{u}_{j}\\cdot\\mathbf{u}_{i}\\\\ & =r_{ij}\\end{aligned}$$\n\nand hence:\n\n$$r_{ij}=\\left\\langle \\mathbf{w}_{j},\\mathbf{u}_{i}\\right\\rangle \\label{eq:modified-gram-schmidt-eq1}$$\n\nOn the other hand, we have:\n\n$$\\begin{aligned}\\left\\Vert \\mathbf{w}_{j}\\right\\Vert ^{2} & =\\left\\Vert r_{1j}\\mathbf{u}_{1}+r_{2j}\\mathbf{u}_{2}+\\ldots+r_{jj}\\mathbf{u}_{j}\\right\\Vert ^{2}\\\\\n& =r_{1j}^{2}+r_{2j}^{2}+\\ldots+r_{jj}^{2}\n\\end{aligned}\n\\label{eq:modified-gram-schmidt-eq2}$$\n\nThe pair of equations ([\\[eq:modified-gram-schmidt-eq1\\]](#eq:modified-gram-schmidt-eq1){reference-type=\"ref\" reference=\"eq:modified-gram-schmidt-eq1\"}) and ([\\[eq:modified-gram-schmidt-eq2\\]](#eq:modified-gram-schmidt-eq2){reference-type=\"ref\" reference=\"eq:modified-gram-schmidt-eq2\"}) can be rearranged to devise a recursive procedure to compute the orthonormal basis. We begin by setting $r_{11}=\\left\\Vert \\mathbf{w}_{1}\\right\\Vert$ and so $\\mathbf{u}_{1}=\\mathbf{w}_{1}/r_{11}$. At each subsequent stage, $j\\geq2$, we assume that we have already constructed $\\mathbf{u}_{1},\\ldots,\\mathbf{u}_{j-1}$. We then compute\n\n$$r_{ij}=\\left\\langle \\mathbf{w}_{j},\\mathbf{u}_{i}\\right\\rangle \\quad\\text{for each }\\quad i=1,2,\\ldots,j-1$$\n\nWe obtain next the orthonormal basis vector $\\mathbf{u}_{j}$ by computing\n\n$$\\begin{aligned}r_{jj} & =\\sqrt{\\left\\Vert \\mathbf{w}_{j}\\right\\Vert ^{2}-r_{1j}^{2}-r_{2j}^{2}-\\ldots-r_{j-1,j}^{2}}\\\\\n\\mathbf{u}_{j} & =\\frac{\\mathbf{w}_{j}-r_{1j}\\mathbf{u}_{1}-r_{2j}\\mathbf{u}_{2}-\\ldots-r_{j-1,j}\\mathbf{u}_{j-1}}{r_{jj}}\n\\end{aligned}$$\n\n### The QR Factorization.\n\nThe Gram-Schmidt procedure for orthonormalizing bases of $\\mathbf{R}^{n}$ can be reinterpreted as a matrix factorization.\n\nLet $\\mathbf{w}_{1},\\ldots,\\mathbf{w}_{n}$ be a basis of $\\mathbf{R}^{n}$, and let $\\mathbf{u}_{1},\\ldots,\\mathbf{u}_{n}$ be the corresponding orthonormal basis that results from any one of the implementations of the Gram-Schmidt process. We assemble both sets of column vectors to form non-singular $n\\times n$ matrices:\n\n$$A=\\left[\\begin{array}{cccc}\n\\mathbf{w}_{1} & \\mathbf{w}_{2} & \\ldots & \\mathbf{w}_{n}\\end{array}\\right],\\quad Q=\\left[\\begin{array}{cccc} \\mathbf{u}_{1} & \\mathbf{u}_{2} & \\ldots & \\mathbf{u}_{n}\\end{array}\\right]$$\n\nSince the $\\mathbf{u}_{i}$ form an orthonormal basis, $Q$ is an orthogonal matrix. In view of the matrix multiplication formula, the Gram-Schmidt equations ([\\[eq:gram-schmidt-equations\\]](#eq:gram-schmidt-equations){reference-type=\"ref\" reference=\"eq:gram-schmidt-equations\"}) can be recast into an equivalent matrix form:\n\n$$\\begin{aligned}\nA & =\\left[\\begin{array}{cccc} \\mathbf{u}_{1} & \\mathbf{u}_{2} & \\ldots & \\mathbf{u}_{n}\\end{array}\\right]\\left[\\begin{array}{ccccc} r_{11} & r_{12} & r_{13} & \\ldots & r_{1n}\\\\ & r_{22} & r_{23} & \\ldots & r_{2n}\\\\ &  & r_{33} & \\ldots & r_{3n}\\\\ &  &  & \\ddots\\\\ &  &  & \\ldots & r_{nn}\n\\end{array}\\right]\\\\\n& =QR\\end{aligned}$$\n\nSince the Gram-Schmidt algorithm works on any basis, the only requirement on the matrix $A$ is that it's columns are linearly-independent and form a basis of $\\mathbf{R}^{n}$, and hence $A$ can be any non-singular matrix. We have therefore established the celebrated $QR$-factorization of non-singular matrices.\n\n::: thm\nEvery non-singular matrix $A$ can be factored, $A=QR$ into the product of an orthogonal matrix $Q$ and an upper triangular matrix $R$. :::\n\n### Numerically stable implementation of QR-Factorization.\n\nWe take a slightly different approach to generating orthogonal vectors $\\mathbf{u}_{1},\\ldots,\\mathbf{u}_{n}$. Define:\n\n$$\\begin{aligned}\n\\mathbf{w}_{1}^{(1)} & =r_{11}\\mathbf{u}_{1}\\end{aligned}$$\n\nand define the $j$th iterate of the procedure as:\n\n$$\\begin{aligned}\n\\mathbf{w}_{k}^{(j)} & =r_{1k}\\mathbf{u}_{1}+r_{2k}\\mathbf{u}_{2}+\\ldots+r_{jk}\\mathbf{u}_{j},\\quad j\\leq k\\end{aligned}$$\n\nObserve that:\n\n$$\\begin{aligned}\n\\left\\langle \\mathbf{w}_{k}^{(j)},\\mathbf{u}_{j}\\right\\rangle  & =r_{jk}\\end{aligned}$$\n\nWe can treat all vectors simultaneously instead of sequentially and compute in the $j=1$st iteration:\n\n$$\\begin{aligned}\n\\mathbf{u}_{1} & =\\mathbf{w}_{1}/r_{11}\\\\ \\mathbf{w}_{2}^{(2)} & =\\left(\\mathbf{w}_{2}^{(1)}-\\left\\langle \\mathbf{w}_{2}^{(1)},\\mathbf{u}_{1}\\right\\rangle \\mathbf{u}_{1}\\right)\\\\ \\mathbf{w}_{3}^{(2)} & =\\left(\\mathbf{w}_{3}^{(1)}-\\left\\langle \\mathbf{w}_{3}^{(1)},\\mathbf{u}_{1}\\right\\rangle \\mathbf{u}_{1}\\right)\\\\ \\vdots\\\\ \\mathbf{w}_{n}^{(2)} & =\\left(\\mathbf{w}_{n}^{(1)}-\\left\\langle \\mathbf{w}_{n}^{(1)},\\mathbf{u}_{1}\\right\\rangle \\mathbf{u}_{1}\\right)\\end{aligned}$$\n\nNote that, the updated vectors $\\mathbf{w}_{2}^{(2)},\\mathbf{w}_{3}^{(2)},\\ldots,\\mathbf{w}_{n}^{(2)}$ are orthogonal to $\\mathbf{u}_{1}$.\n\nIn the $j=2$nd iteration, we compute:\n\n$$\\begin{aligned}\n\\mathbf{u}_{2} & =\\mathbf{w}_{2}^{(2)}/r_{22}\\\\ \\mathbf{w}_{3}^{(3)} & =\\left(\\mathbf{w}_{3}^{(2)}-\\left\\langle \\mathbf{w}_{3}^{(2)},\\mathbf{u}_{2}\\right\\rangle \\mathbf{u}_{2}\\right)\\\\ \\mathbf{w}_{4}^{(3)} & =\\left(\\mathbf{w}_{4}^{(2)}-\\left\\langle \\mathbf{w}_{4}^{(2)},\\mathbf{u}_{2}\\right\\rangle \\mathbf{u}_{2}\\right)\\\\ \\vdots\\\\ \\mathbf{w}_{n}^{(3)} & =\\left(\\mathbf{w}_{n}^{(2)}-\\left\\langle \\mathbf{w}_{n}^{(2)},\\mathbf{u}_{2}\\right\\rangle \\mathbf{u}_{2}\\right)\\end{aligned}$$\n\nSince $\\mathbf{w}_{2}^{(2)}$ was orthogonal to $\\mathbf{u}_{1}$, $\\mathbf{u}_{2}$ must also be orthogonal to $\\mathbf{u}_{1}$. Further, $\\mathbf{w}_{3}^{(3)},\\ldots,\\mathbf{w}_{3}^{(n)}$ are orthogonal to both $\\mathbf{u}_{1}$, $\\mathbf{u}_{2}$.\n\nIn particular, in the $j$th iteration we compute:\n\n$$\\begin{aligned}\n\\mathbf{u}_{j} & =\\mathbf{w}_{j}^{(j)}/r_{jj}\\\\ \\mathbf{w}_{j+1}^{(j+1)} & =\\left(\\mathbf{w}_{j+1}^{(j)}-\\left\\langle \\mathbf{w}_{j+1}^{(j)},\\mathbf{u}_{j}\\right\\rangle \\mathbf{u}_{j}\\right)\\\\ \\mathbf{w}_{j+2}^{(j+1)} & =\\left(\\mathbf{w}_{j+2}^{(j)}-\\left\\langle \\mathbf{w}_{j+2}^{(j)},\\mathbf{u}_{j}\\right\\rangle \\mathbf{u}_{j}\\right)\\\\ \\vdots\\\\ \\mathbf{w}_{n}^{(j+1)} & =\\left(\\mathbf{w}_{n}^{(j)}-\\left\\langle \\mathbf{w}_{n}^{(j)},\\mathbf{u}_{j}\\right\\rangle \\mathbf{u}_{j}\\right)\\end{aligned}$$\n\nWe can summarize the above steps as follows. We iterate $j=1$ to $n$. For $j=1$, we start with the initial basis $\\mathbf{w}_{k}^{(1)}=\\mathbf{w}_{k}$, and set $\\mathbf{u}_{1}=\\mathbf{w}_{1}^{(1)}/r_{11}$.\n\nIn the $j$th iteration, we set $\\mathbf{u}_{j}=\\mathbf{w}_{j}^{(j)}/r_{jj}$ and for all $k=j+1$ to $n$, we let $\\mathbf{w}_{k}^{(j+1)}=\\mathbf{w}_{k}^{(j)}-\\left\\langle \\mathbf{w}_{k}^{(j)},\\mathbf{u}_{j}\\right\\rangle \\mathbf{u}_{j}$. Also, we set $r_{jk}=\\left\\langle \\mathbf{w}_{k}^{(j)},\\mathbf{u}_{j}\\right\\rangle$.\n\n\n``` {cpp}\n#include <iostream>\n#include <Eigen/Dense>\n#include <tuple>\n\nusing namespace Eigen;\n\nstd::pair<MatrixXd,MatrixXd> qrFactorization(MatrixXd& A) { \n    int n = A.rows();\n\n// A = [w_1,w_2,...,w_n] = [u_1 u_2 ...u_n]R = QR\n\n    MatrixXd Q = MatrixXd::Zero(n,n); MatrixXd R = MatrixXd::Zero(n,n);\n\n//We proceed iteratively and build the column vectors u_0, u_1, ... u_{n-1} \n    for (int j{ 0 }; j < n; ++j) { // The scalar r_jj = ||w_j^(j)|| \n        for (int i{ 0 }; i < n; ++i) \n            R(j, j) += A(i, j) * A(i, j);\n\n        R(j, j) = sqrt(R(j, j));\n\n        // u_j = w_j / r_jj \n        for (int i{ 0 };i < n; ++i) \n        { \n            Q(i, j) = A(i,j) / R(j, j); \n        }\n\n        for (int k{ j + 1 }; k < n; ++k) { // Dot product of <w_k^(j),u_j> \n            for (int i{ 0 }; i < n; ++i) \n            { \n                R(j,k) += A(i, k) * Q(i, j); \n            }\n\n            // w_k^(j+1) = w_k^(j) - <w_k^(j),u_j> u_j \n            for (int i{ 0 }; i < n; ++i) \n            { \n                A(i, k) = A(i, k) - R(j,k) * Q(i, j); \n            } \n        } \n    } \n    \n    return std::make_pair(Q, R); }\n```\n\n\n### Gram Matrices.\n\nSymmetric matrices whose entries are given by the inner products of elements of an inner product space are called *Gram matrices,* after the Danish mathematician *Jorgen Gram*.\n\n::: defn\nLet $V$ be an inner product space, and let $\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{n}\\in V$. The associated *Gram matrix*\n\n$$\\begin{aligned}\nK & =\\left[\\begin{array}{cccc} \\left\\langle \\mathbf{v}_{1},\\mathbf{v}_{1}\\right\\rangle  & \\left\\langle \\mathbf{v}_{1},\\mathbf{v}_{2}\\right\\rangle  & \\ldots & \\left\\langle \\mathbf{v}_{1},\\mathbf{v}_{n}\\right\\rangle \\\\ \\left\\langle \\mathbf{v}_{2},\\mathbf{v}_{1}\\right\\rangle  & \\left\\langle \\mathbf{v}_{2},\\mathbf{v}_{2}\\right\\rangle  & \\ldots & \\left\\langle \\mathbf{v}_{2},\\mathbf{v}_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\left\\langle \\mathbf{v}_{n},\\mathbf{v}_{1}\\right\\rangle  & \\left\\langle \\mathbf{v}_{n},\\mathbf{v}_{2}\\right\\rangle  & \\ldots & \\left\\langle \\mathbf{v}_{n},\\mathbf{v}_{n}\\right\\rangle\n\\end{array}\\right]\\end{aligned}$$\n\nis the $n\\times n$ symmetric matrix whose entries are the inner-products between the selected vector space elements. :::\n\n::: thm\nAll Gram matrices are positive semi-definite. :::\n\n::: proof\n*Proof.* Let $K$ be an arbitrary Gram matrix. To prove the positive semi-definiteness of $K$, we need to examine the associated quadratic form:\n\n$$\\begin{aligned}\nq(\\mathbf{x}) & =\\mathbf{x}'K\\mathbf{x}\\\\ & =\\sum_{i=1}^{n}\\sum_{j=1}^{n}k_{ij}x_{i}x_{j}\\end{aligned}$$\n\nBut, $k_{ij}=\\left\\langle \\mathbf{v}_{i},\\mathbf{v}_{j}\\right\\rangle$. Substituting the values for the matrix entries, we obtain:\n\n$$\\begin{aligned}\nq(\\mathbf{x}) & =\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\left\\langle \\mathbf{v}_{i},\\mathbf{v}_{j}\\right\\rangle x_{i}x_{j}\\end{aligned}$$\n\nFor intuition, let's choose $n=2$. The quadratic form becomes:\n\n$$\\begin{aligned}\nq(\\mathbf{x}) & =\\left\\langle \\mathbf{v}_{1},\\mathbf{v}_{1}\\right\\rangle x_{1}^{2}+\\left\\langle \\mathbf{v}_{1},\\mathbf{v}_{2}\\right\\rangle x_{1}x_{2}+\\left\\langle \\mathbf{v}_{2},\\mathbf{v}_{1}\\right\\rangle x_{2}x_{1}+\\left\\langle \\mathbf{v}_{2},\\mathbf{v}_{1}\\right\\rangle x_{2}^{2}\\\\ & =\\left\\langle x_{1}\\mathbf{v}_{1}+x_{2}\\mathbf{v}_{2},x_{1}\\mathbf{v}_{1}+x_{2}\\mathbf{v}_{1}\\right\\rangle  & \\{\\text{Bi-linearity of inner products}\\}\\\\ & =\\left\\Vert x_{1}\\mathbf{v}_{1}+x_{2}\\mathbf{v}_{2}\\right\\Vert ^{2}\\end{aligned}$$\n\nTherefore, we can write the original quadratic form as a single inner product:\n\n$$\\begin{aligned}\nq(\\mathbf{x}) & =\\left\\langle \\sum_{i=1}^{n}x_{i}\\mathbf{v}_{i},\\sum_{j=1}^{n}x_{j}\\mathbf{v}_{j}\\right\\rangle \\\\ & =\\left\\Vert \\sum_{i=1}^{n}x_{i}\\mathbf{v}_{i}\\right\\Vert ^{2}\\\\ & =\\left\\Vert \\mathbf{v}\\right\\Vert ^{2} & \\{\\text{Norm }\\left\\Vert \\cdot\\right\\Vert \\text{ is positive semi-definite}\\}\\\\ & \\geq0\\end{aligned}$$ ◻ :::\n\n### Positive Definiteness.\n\nGram matrices furnish us with an almost inexhaustible supply of positive semi-definite matrices. However, we still do not know how to test whether a given symmetric matrix is positive definite.\n\nFrom elementary school, we recall the algebraic technique known as *completing the square*, first arising in the derivation of the formula for the solution to the quadratic equation\n\n$$q(x)=ax^{2}+2bx+c=0\\label{eq:quadratic_equation}$$\n\nThe idea is to combine the first two terms in the equation ([\\[eq:quadratic_equation\\]](#eq:quadratic_equation){reference-type=\"ref\" reference=\"eq:quadratic_equation\"}) to form a perfect square and thereby rewrite the quadratic function in the form :\n\n$$\\begin{aligned}\nq(x) & =a\\left[x^{2}+2\\frac{b}{a}x+\\frac{c}{a}\\right]\\\\ & =a\\left[x^{2}+2x\\cdot\\frac{b}{a}+\\left(\\frac{b}{a}\\right)^{2}+\\frac{c}{a}-\\left(\\frac{b}{a}\\right)^{2}\\right]\\\\ & =a\\left[\\left(x+\\frac{b}{a}\\right)^{2}+\\frac{ac-b^{2}}{a^{2}}\\right]\\end{aligned}$$\n\nAs a consequence,\n\n$$\\begin{aligned}\n\\left(x+\\frac{b}{a}\\right)^{2} & =\\frac{b^{2}-ac}{a^{2}}\\end{aligned}$$\n\nThe familiar *quadratic formula*:\n\n$$\\begin{aligned}\nx & =\\frac{-b\\pm\\sqrt{b^{2}-ac}}{a}\\end{aligned}$$\n\nfollows by taking the square root on both sides and then solving for $x$.\n\nWe can perform the same kind of manipulation on a homogenous quadratic form:\n\n$$q(x_{1},x_{2})=ax_{1}^{2}+2bx_{1}x_{2}+cx_{2}^{2}\\label{eq:original_quadratic_form}$$\n\nIn this case, provided $a\\neq0$, completing the square amounts to writing:\n\n$$\\begin{split}q(x_{1},x_{2}) & =ax_{1}^{2}+2bx_{1}x_{2}+cx_{2}^{2}\\\\\n& =a\\left[x_{1}^{2}+2x_{1}\\cdot\\frac{b}{a}x_{2}+\\left(\\frac{b}{a}x_{2}\\right)^{2}+\\frac{c}{a}x_{2}^{2}-\\frac{b^{2}}{a^{2}}x_{2}^{2}\\right]\\\\ & =a\\left[\\left(x_{1}+\\frac{b}{a}x_{2}\\right)^{2}+\\frac{ac-b^{2}}{a^{2}}x_{2}^{2}\\right]\\\\ & =ay_{1}^{2}+\\frac{ac-b^{2}}{a}y_{2}^{2}\n\\end{split}\n\\label{eq:completing_the_square}$$\n\nThe net result is to re-express $q(x_{1},x_{2})$ as a simpler sum of squares of the new variables:\n\n$$y_{1}=x_{1}+\\frac{b}{a}x_{2},\\quad y_{2}=x_{2}\\label{eq:new_variables_quadratic_form}$$\n\nIt is not hard to see that the final expression in ([\\[eq:completing_the_square\\]](#eq:completing_the_square){reference-type=\"ref\" reference=\"eq:completing_the_square\"}) is positive definite, as a function of $y_{1}$ and $y_{2}$ if and only if both coefficients are positive:\n\n$$a>0,\\quad\\frac{ac-b^{2}}{a}>0$$\n\nOur goal is to adapt this simple idea to analyse the positive semi-definiteness of quadratic forms depending on more than two variables. To this end, let us write the quadratic form identity in the matrix form. The original quadratic form in ([\\[eq:original_quadratic_form\\]](#eq:original_quadratic_form){reference-type=\"ref\" reference=\"eq:original_quadratic_form\"}) can be written as:\n\n$$\\begin{aligned}\nq(\\mathbf{x}) & =\\mathbf{x}'K\\mathbf{x}\\\\ & =\\left[\\begin{array}{cc} x_{1} & x_{2}\\end{array}\\right]\\left[\\begin{array}{cc} a & b\\\\ b & c\n\\end{array}\\right]\\left[\\begin{array}{c}\nx_{1}\\\\ x_{2}\n\\end{array}\\right]\\end{aligned}$$\n\nSimilarly, the right hand side of ([\\[eq:completing_the_square\\]](#eq:completing_the_square){reference-type=\"ref\" reference=\"eq:completing_the_square\"}) can be written as:\n\n$$\\hat{q}(\\mathbf{y})=\\mathbf{y}'D\\mathbf{y},\\quad\\text{where}\\quad D=\\left[\\begin{array}{cc}\na & 0\\\\ 0 & \\frac{ac-b^{2}}{a}\n\\end{array}\\right],\\quad\\mathbf{y}=\\left[\\begin{array}{c}\ny_{1}\\\\ y_{2}\n\\end{array}\\right]\\label{eq:quadratic_form_in_y}$$\n\nAnticipating the final result, the equations ([\\[eq:new_variables_quadratic_form\\]](#eq:new_variables_quadratic_form){reference-type=\"ref\" reference=\"eq:new_variables_quadratic_form\"}) connecting $\\mathbf{x}$ and $\\mathbf{y}$ can themselves be written in the matrix form as:\n\n$$\\mathbf{y}=L'\\mathbf{x}\\quad\\text{or}\\quad\\left[\\begin{array}{c}\ny_{1}\\\\ y_{2}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nx_{1}+\\frac{b}{a}x_{2}\\\\ x_{2}\n\\end{array}\\right],\\quad\\text{where}\\quad L'=\\left[\\begin{array}{cc}\n1 & 0\\\\ b/a & 1\n\\end{array}\\right]$$\n\nSubstituting $\\mathbf{y}$into ([\\[eq:quadratic_form_in_y\\]](#eq:quadratic_form_in_y){reference-type=\"ref\" reference=\"eq:quadratic_form_in_y\"}), we obtain:\n\n$$\\mathbf{y}'D\\mathbf{y}=(L'\\mathbf{x})'D(L'\\mathbf{x})=\\mathbf{x}'LDL'\\mathbf{x}=\\mathbf{x}'K\\mathbf{x},\\quad\\text{where }\\quad K=LDL'\\label{eq:ldl-transpose-factorization}$$\n\nWe are thus led to the realization that completing the square is the same as the $LDL'$ factorization of a symmetric matrix $K$.\n\nFrom basic algebra, we know that, if $A$ is a non-singular matrix, with all it's pivot elements $a_{kk}^{(k)}$ non-zero in the Gaussian elimination process, then $A=LDU$ where $L$ and $U$ are lower and upper uni-triangular matrices and $D$ is a diagonal matrix consisting of the pivots of $A$. If the matrix is symmetric, then it admits the unique factorization $LDL'$.\n\nThe identity ([\\[eq:ldl-transpose-factorization\\]](#eq:ldl-transpose-factorization){reference-type=\"ref\" reference=\"eq:ldl-transpose-factorization\"}) is therefore valid for all real symmetric matrices that are non-singular and can be reduced to an upper triangular matrix by performing elementary row operations (without row interchanges). It also shows how to write the associated quadratic form as a sum of squares:\n\n$$q(\\mathbf{x})=\\mathbf{x}'K\\mathbf{x}=\\mathbf{y}'D\\mathbf{y}=d_{1}y_{1}^{2}+d_{2}y_{2}^{2}+\\ldots+d_{n}y_{n}^{2}\\quad\\text{where}\\quad\\mathbf{y}=L'\\mathbf{x}$$\n\nThe coefficients $d_{i}$ are the diagonal entries of $D$, which are the pivots of $K$. The diagonal quadratic form is positive definite, $\\mathbf{y}'D\\mathbf{y}>0$ for all $\\mathbf{y}\\neq\\mathbf{0}$ if and only if, when performing the Gaussian elimination process, all the pivots are positive. We can now add this to our list of standard results.\n\n::: thm\n(Positive Definiteness) Let $K$ be a $n\\times n$ real symmetric positive definite (SPD) matrix. Then the following statements are equivalent.\n\n\\(i\\) $K$ is non-singular and can be reduced to an upper triangular matrix by performing elementary row operations (without row permutations), and it has positive pivot elements when performing Gaussian elimination.\n\n\\(ii\\) $K$ admits a factorization $K=LDL'$, where $D=diag(d_{1},\\ldots,d_{n})$ such that $d_{i}>0$ for all $i=1,2,3,\\ldots,n$. :::\n\n### Cholesky Factorization.\n\nThe identity ([\\[eq:ldl-transpose-factorization\\]](#eq:ldl-transpose-factorization){reference-type=\"ref\" reference=\"eq:ldl-transpose-factorization\"}) shows us how to write an arbitrary regular quadratic form $q(\\mathbf{x})$ as linear combination of squares. We can push this result slightly further in the positive definite case. Since each pivot $d_{i}$ is positive, we can write the quadratic form as a sum of squares:\n\n$$\\begin{aligned}\nd_{1}y_{1}^{2}+d_{2}y_{2}^{2}+\\ldots+d_{n}y_{n}^{2} & =(\\sqrt{d_{1}}y_{1})^{2}+(\\sqrt{d_{2}}y_{2})^{2}+\\ldots+(\\sqrt{d_{n}}y_{n})^{2}\\\\ & =z_{1}^{2}+z_{2}^{2}+\\ldots+z_{n}^{2}\\end{aligned}$$\n\nwhere $z_{i}=\\sqrt{d_{i}}y_{i}$. In the matrix form, we are writing:\n\n$$\\begin{aligned}\n\\hat{q}(\\mathbf{y}) & =\\mathbf{y}'D\\mathbf{y}\\\\ & =\\mathbf{z}'\\mathbf{z}\\\\ & =\\left\\Vert \\mathbf{z}\\right\\Vert ^{2}\\end{aligned}$$\n\nwhere $\\mathbf{z}=S\\mathbf{y}$, with $S=diag(\\sqrt{d_{1}},\\sqrt{d_{2}},\\ldots,\\sqrt{d_{n}})$. Since $D=S^{2}$, the matrix $S$ can be thought of as a square root of the diagonal matrix $D$. Substituting back into the equation $K=LDL'$, we deduce the *Cholesky factorization:*\n\n$$\\begin{aligned}\nK & =LDL'\\\\ & =LSS'L'\\\\ & =LS(LS)'\\\\ & =MM'\\end{aligned}$$\n\nof a positive definite matrix, first proposed by the early twentieth-century French geographer Andrew Louis Cholesky for solving problems in geodetic surveying. Note that, $M$ is a lower triangular matrix with all positive diagonal entries, namely the square roots of the pivots: $m_{ii}=\\sqrt{d_{i}}$.\n\n::: example\nLet the matrix $K=\\left[\\begin{array}{ccc} 1 & 2 & -1\\\\ 2 & 6 & 0\\\\ -1 & 0 & 9\n\\end{array}\\right]$. Let $KX=I$. We consider the augmented matrix\n$\\left[\\begin{array}{ccc} K & | & I\\end{array}\\right]$. Performing Gaussian elimination, we have:\n\n$$\\left[\\begin{array}{ccccccc}\n1 & 2 & -1 & | & 1 & 0 & 0\\\\ 2 & 6 & 0 & | & 0 & 1 & 0\\\\ -1 & 0 & 9 & | & 0 & 0 & 1\n\\end{array}\\right]$$\n\nThe pivot element $a_{11}^{(1)}=1$. Performing $R_{2}=R_{2}-2R_{1}$and $R_{3}=R_{3}+R_{1}$, the above system is row-equivalent to:\n\n$$\\left[\\begin{array}{ccccccc}\n1 & 2 & -1 & | & 1 & 0 & 0\\\\ 0 & 2 & 2 & | & -2 & 1 & 0\\\\ 0 & 2 & 8 & | & 1 & 0 & 1\n\\end{array}\\right]$$\n\nThe pivot element $a_{22}^{(2)}=2$. Performing $R_{3}=R_{3}-R_{2}$, the above system is row-equivalent to:\n\n$$\\left[\\begin{array}{ccccccc}\n1 & 2 & -1 & | & 1 & 0 & 0\\\\ 0 & 2 & 2 & | & -2 & 1 & 0\\\\ 0 & 0 & 6 & | & 3 & -1 & 1\n\\end{array}\\right]$$\n\nThe pivot element $a_{33}^{(3)}=6$. We have now reduced the system to the form $\\left[\\begin{array}{ccc} DU & | & C\\end{array}\\right]$, where $U$ is an upper uni-triangular matrix. Thus, Gaussian Elimination produces the factors:\n\n$$L=\\left[\\begin{array}{ccc}\n1 & 0 & 0\\\\ 2 & 1 & 0\\\\ -1 & 1 & 1\n\\end{array}\\right],\\quad D=\\left[\\begin{array}{ccc}\n1 & 0 & 0\\\\ 0 & 2 & 0\\\\ 0 & 0 & 6\n\\end{array}\\right],\\quad L^{T}=\\left[\\begin{array}{ccc}\n1 & 2 & -1\\\\ 0 & 1 & 1\\\\ 0 & 0 & 1\n\\end{array}\\right]$$\n\nThus,\n\n$$M=LS=\\left[\\begin{array}{ccc}\n1 & 0 & 0\\\\ 2 & 1 & 0\\\\ -1 & 1 & 1\n\\end{array}\\right]\\left[\\begin{array}{ccc}\n1 & 0 & 0\\\\ 0 & \\sqrt{2} & 0\\\\ 0 & 0 & \\sqrt{6}\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 & 0 & 0\\\\ 2 & \\sqrt{2} & 0\\\\ -1 & \\sqrt{2} & \\sqrt{6}\n\\end{array}\\right]$$\n\nand $K=MM'$. :::\n\nWe conclude our discussion by observing the following:\n\n::: lem\nIf a square matrix $K$ is SPD, it admits a Cholesky factorization of the form $K=MM^{T}$. :::\n\n::: example\nProve that, if $K$ is real SPD(symmetric positive definite matrix), then the diagonal elements of $K$ are positive. :::\n\n::: proof\n*Proof.* Since $K$ is real SPD, $K$ admits a factorization $K=LL^{T}$. Since the diagonal element $(j,j)$ is the inner product of the $j$-th row of $L$ and the $j$-th column of $L^{T}$, we have: $$\\begin{aligned} k_{jj} & =\\sum_{m=1}^{n}l_{jm}l'_{mj}\\end{aligned}$$\n\nBut, $l_{jm}=l'_{mj}$, since $L=\\left(L^{T}\\right)^{T}$ . Hence, $k_{jj}$ is a sum of squares. Further, since the diagonal elements of $L$, that is, all elements $l_{jj}$ are strictly positive, the sum $k_{jj}=l_{j1}^{2}+\\ldots+l_{jj}^{2}+\\ldots+l_{jn}^{2}>0$. Consequently, the diagonal elements of $K$ are positive. ◻ :::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
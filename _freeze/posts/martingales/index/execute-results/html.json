{
  "hash": "7079f61141f7ee48048ba3f28d83cde5",
  "result": {
    "markdown": "---\ntitle: Martingales\nauthor: Quasar\ndate: '2024-07-12'\ncategories:\n  - Stochastic Calculus\nimage: image.jpg\ntoc: true\ntoc-depth: 3\n---\n\n# Martingales.\n\n## Elementary conditional expectation.\n\nIn elementary probability, the conditional expectation of a variable $Y$ given another random variable $X$ refers to the expectation of $Y$ given the conditional distribution $f_{Y|X}(y|x)$ of $Y$ given $X$. To illustrate this, let's go through a simple example. Consider $\\mathcal{B}_{1}$, $\\mathcal{B}_{2}$ to be two independent Bernoulli-distributed random variables with $p=1/2$. Then, construct:\n\n$$\\begin{aligned}\nX=\\mathcal{B}_{1}, & \\quad Y=\\mathcal{B}_{1}+\\mathcal{B}_{2}\n\\end{aligned}$$\n\nIt is easy to compute $\\mathbb{E}[Y|X=0]$ and $\\mathbb{E}[Y|X=1]$. By definition, it is given by:\n\n$$\\begin{aligned}\n\\mathbb{E}[Y|X=0] & =\\sum_{j=0}^{2}j\\mathbb{P}(Y=j|X=0)\\\\\n & =\\sum_{j=0}^{2}j\\cdot\\frac{\\mathbb{P}(Y=j,X=0)}{P(X=0)}\\\\\n & =0+1\\cdot\\frac{(1/4)}{(1/2)}+2\\cdot\\frac{0}{(1/2)}\\\\\n & =\\frac{1}{2}\n\\end{aligned}$$\n\nand\n\n$$\\begin{aligned}\n\\mathbb{E}[Y|X=1] & =\\sum_{j=0}^{2}j\\mathbb{P}(Y=j|X=1)\\\\\n & =\\sum_{j=0}^{2}j\\cdot\\frac{\\mathbb{P}(Y=j,X=1)}{P(X=1)}\\\\\n & =0+1\\cdot\\frac{(1/4)}{(1/2)}+2\\cdot\\frac{(1/4)}{(1/2)}\\\\\n & =\\frac{3}{2}\n\\end{aligned}$$\n\nWith this point of view, the conditional expectation is computed given\nthe information that the event $\\{X=0\\}$ occurred or the event $\\{X=1\\}$\noccurred. It is possible to regroup both conditional expectations in a\nsingle object, if we think of the conditional expectation as a random\nvariable and denote it by $\\mathbb{E}[Y|X]$. Namely, we take:\n\n$$\\begin{aligned}\n\\mathbb{E}[Y|X](\\omega) & =\\begin{cases}\n\\frac{1}{2} & \\text{if }X(\\omega)=0\\\\\n\\frac{3}{2} & \\text{if }X(\\omega)=1\n\\end{cases}\\label{eq:elementary-conditional-expectation-example}\n\\end{aligned}$$\n\nThis random variable is called the *conditional expectation* of $Y$\ngiven $X$. We make two important observations:\n\n\\(i\\) If the value of $X$ is known, then the value of $\\mathbb{E}[Y|X]$\nis determined.\n\n\\(ii\\) If we have another random variable $g(X)$ constructed from $X$,\nthen we have:\n\n$$\\begin{aligned}\n\\mathbb{E}[g(X)Y] & =\\mathbb{E}[g(X)\\mathbb{E}[Y|X]]\n\\end{aligned}$$\n\nIn other words, as far as $X$ is concerned, the conditional expectation\n$\\mathbb{E}[Y|X]$ is a proxy for $Y$ in the expectation. We sometimes\nsay that $\\mathbb{E}[Y|X]$ is the best estimate of $Y$ given the\ninformation of $X$.\n\nThe last observation is easy to verify since:\n\n$$\\begin{aligned}\n\\mathbb{E}[g(X)Y] & =\\sum_{i=0}^{1}\\sum_{j=0}^{2}g(i)\\cdot j\\cdot\\mathbb{P}(X=i,Y=j)\\\\\n & =\\sum_{i=0}^{1}\\mathbb{P}(X=i)g(i)\\left\\{ \\sum_{j=0}^{2}j\\cdot\\frac{\\mathbb{P}(X=i,Y=j)}{\\mathbb{P}(X=i)}\\right\\} \\\\\n & =\\mathbb{E}[g(X)\\mathbb{E}[Y|X]]\n\\end{aligned}$$\n\n::: example\n[]{#ex:elementary-definitions-of-conditional-expectation\nlabel=\"ex:elementary-definitions-of-conditional-expectation\"}(Elementary\nDefinitions of Conditional Expectation).\n\n\\(1\\) $(X,Y)$ discrete. The treatment is similar to the above. If a\nrandom variable $X$ takes values $(x_{i},i\\geq1)$ and $Y$ takes values\n$(y_{j},j\\geq1)$, we have by definition that the conditional expectation\nas a random variable is:\n\n$$\\begin{aligned}\n\\mathbb{E}[Y|X](\\omega) & =\\sum_{j\\geq1}y_{j}\\mathbb{P}(Y=y_{j}|X=x_{i})\\quad\\text{for }\\omega\\text{ such that }X(\\omega)=x_{i}\n\\end{aligned}$$ (2) $(X,Y)$ continuous with joint PDF $f_{X,Y}(x,y)$: In\nthis case, the conditional expectation is the random variable given by\n\n$$\\begin{aligned}\n\\mathbb{E}[Y|X] & =h(X)\n\\end{aligned}$$\n\nwhere\n\n$$\\begin{aligned}\nh(x) & =\\int_{\\mathbf{R}}yf_{Y|X}(y|x)dy=\\int_{\\mathbf{R}}y\\frac{f_{X,Y}(x,y)}{f_{X}(x)}dy=\\frac{\\int_{\\mathbf{R}}yf_{X,Y}(x,y)dy}{\\int_{\\mathbf{R}}f_{X,Y}(x,y)dy}\n\\end{aligned}$$\n:::\n\nIn the two examples above, the expectation of the random variable\n$\\mathbb{E}[Y|X]$ is equal to $\\mathbb{E}[Y]$. Indeed in the discrete\ncase, we have:\n\n$$\\begin{aligned}\n\\mathbb{E}[\\mathbb{E}[Y|X]] & =\\sum_{i=0}^{1}P(X=x_{i})\\cdot\\sum_{j=0}^{2}y_{j}\\mathbb{P}(Y=y_{j}|X=x_{i})\\\\\n & =\\sum_{i=0}^{1}\\sum_{j=0}^{2}y_{j}\\mathbb{P}(Y=y_{j},X=x_{i})\\\\\n & =\\sum_{j=0}^{2}y_{j}\\mathbb{P}(Y=y_{j})\\\\\n & =\\mathbb{E}[Y]\n\\end{aligned}$$\n\n::: example\n(Conditional Probability vs Conditional expectation). The conditional\nprobability of the event $A$ given $B$ can be recast in terms of\nconditional expectation using indicator functions. If\n$0<\\mathbb{P}(B)<1$, it is not hard to check that:\n$\\mathbb{P}(A|B)=\\mathbb{E}[\\mathbf{1}_{A}|\\mathbf{1}_{B}=1]$ and\n$\\mathbb{P}(A|B^{C})=\\mathbb{E}[\\mathbf{1}_{A}|1_{B}=0]$. Indeed the\nrandom variables $\\mathbf{1}_{A}$ and $\\mathbf{1}_{B}$ are discrete. If\nwe proceed as in the discrete case above, we have:\n\n$$\\begin{aligned}\n\\mathbb{E}[\\mathbf{1}_{A}|\\mathbf{1}_{B}=1] & =1\\cdot\\mathbb{P}(\\mathbf{1}_{A}=1|\\mathbf{1}_{B}=1)\\\\\n & =\\frac{\\mathbb{P}(\\mathbf{1}_{A}=1,\\mathbf{1}_{B}=1)}{\\mathbb{P}(\\mathbf{1}_{B}=1)}\\\\\n & =\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\\\\n & =\\mathbb{P}(A|B)\n\\end{aligned}$$\n\nA similar calculation gives $\\mathbb{P}(A|B^{C})$. In particular, the\nformula for total probability for $A$ is a rewriting of the expectation\nof the random variable $\\mathbb{E}[\\mathbf{1}_{A}|\\mathbf{1}_{B}]$:\n\n$$\\begin{aligned}\n\\mathbb{E}[\\mathbb{E}[\\mathbf{1}_{A}|\\mathbf{1}_{B}]] & =\\mathbb{E}[\\mathbf{1}_{A}|\\mathbf{1}_{B}=1]\\mathbb{P}(\\mathbf{1}_{B}=1)+\\mathbb{E}[\\mathbf{1}_{A}|\\mathbf{1}_{B}=0]\\mathbb{P}(\\mathbf{1}_{B}=0)\\\\\n & =\\mathbb{P}(A|B)\\cdot\\mathbb{P}(B)+\\mathbb{P}(A|B^{C})\\cdot\\mathbb{P}(B^{C})\\\\\n & =\\mathbb{P}(A)\n\\end{aligned}$$\n:::\n\n## Conditional Expectation as a projection.\n\n#### Conditioning on one variable. \n\nWe start by giving the definition of conditional expectation given a\nsingle variable. This relates to the two observations (A) and (B) made\npreviously. We assume that the random variable is integrable for the\nexpectations to be well-defined.\n\n::: defn\n[]{#def:conditional-expectation label=\"def:conditional-expectation\"}Let\n$X$ and $Y$ be integrable random variables on\n$(\\Omega,\\mathcal{F},\\mathbb{P})$. The conditional expectation of $Y$\ngiven $X$ is the random variable denoted by $\\mathbb{E}[Y|X]$ with the\nfollowing two properties:\n\n\\(A\\) There exists a function $h:\\mathbf{R}\\to\\mathbf{R}$ such that\n$\\mathbb{E}[Y|X]=h(X)$.\n\n\\(B\\) For any bounded random variable of the form $g(X)$ for some\nfunction $g$,\n\n$$\\mathbb{E}[g(X)Y]=\\mathbb{E}[g(X)\\mathbb{E}[Y|X]]\\label{eq:definition-conditional-expectation}$$\n\nWe can intepret the second property as follows. The conditional\nexpectation $\\mathbb{E}[Y|X]$ serves as a proxy for $Y$ as far as $X$ is\nconcerned. Note that in equation\n([\\[eq:definition-conditional-expectation\\]](#eq:definition-conditional-expectation){reference-type=\"ref\"\nreference=\"eq:definition-conditional-expectation\"}), the expectation on\nthe left can be seen as an average over the joint values of $(X,Y)$,\nwhereas the one on the right is an average over the values of $X$ only!\nAnother way to see this property is to write is as:\n\n$$\\mathbb{E}[g(X)(Y-\\mathbb{E}[Y|X])]=0$$\n\nIn other words, the *random variable $Y-\\mathbb{E}[Y|X]$ is orthogonal\nto any random variable constructed from $X$.*\n\nFinally, it is important to notice that if we take $g(X)=1$, then the\nsecond property implies :\n\n$$\\begin{aligned}\n\\mathbb{E}[Y] & =\\mathbb{E}[\\mathbb{E}[Y|X]]\n\\end{aligned}$$\n\nIn other words, the expectation of the conditional expectation of $Y$ is\nsimply the expectation of $Y$.\n\nThe existence of the conditional expectation $\\mathbb{E}[Y|X]$ is not\nobvious. We know, it exists in particular cases given in example\n([\\[ex:elementary-definitions-of-conditional-expectation\\]](#ex:elementary-definitions-of-conditional-expectation){reference-type=\"ref\"\nreference=\"ex:elementary-definitions-of-conditional-expectation\"}). We\nwill show more generally, that it exists, it is unique whenever $Y$ is\nin $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$ (In fact, it can be shown to\nexist whenever $Y$ is integrable). Before doing so, let's warm up by\nlooking at the case of Gaussian vectors.\n:::\n\n::: example\n[]{#ex:conditional-expectation-of-gaussian-vectors\nlabel=\"ex:conditional-expectation-of-gaussian-vectors\"}(Conditional\nexpectation of Gaussian vectors - I). Let $(X,Y)$ be a Gaussian vector\nof mean $0$. Then:\n\n$$\\mathbb{E}[Y|X]=\\frac{\\mathbb{E}[XY]}{\\mathbb{E}[X^{2}]}X\\label{eq:conditional-expectation-of-gaussian-vector}$$\n\nThis candidate satisfies the two defining properties of conditional\nexpectation : (A) It is clearly a function of $X$; in fact it is a\nsimple multiple of $X$. (B) We have that the random variable\n$\\left(Y-\\frac{\\mathbb{E}[XY]}{\\mathbb{E}[X^{2}]}X\\right)$ is orthogonal\nand thus independent to $X$. This is a consequence of the proposition\n([\\[prop:diagonal-cov-matrix-implies-independence-of-gaussians\\]](#prop:diagonal-cov-matrix-implies-independence-of-gaussians){reference-type=\"ref\"\nreference=\"prop:diagonal-cov-matrix-implies-independence-of-gaussians\"}),\nsince:\n\n$$\\begin{aligned}\n\\mathbb{E}\\left[X\\left(Y-\\frac{\\mathbb{E}[XY]}{\\mathbb{E}[X^{2}]}X\\right)\\right] & =\\mathbb{E}XY-\\frac{\\mathbb{E}[XY]}{\\mathbb{E}[X^{2}]}\\mathbb{E}X^{2}\\\\\n & =\\mathbb{E}XY-\\frac{\\mathbb{E}[XY]}{\\cancel{\\mathbb{E}[X^{2}]}}\\cancel{\\mathbb{E}X^{2}}\\\\\n & =0\n\\end{aligned}$$\n\nTherefore, we have for any bounded function $g(X)$ of $X$:\n\n$$\\begin{aligned}\n\\mathbb{E}[g(X)(Y-\\mathbb{E}(Y|X))] & =\\mathbb{E}[g(X)]\\mathbb{E}[Y-\\mathbb{E}[Y|X]]=0\n\\end{aligned}$$\n:::\n\n::: example\n[]{#ex:brownian-conditioning-I\nlabel=\"ex:brownian-conditioning-I\"}(Brownian conditioning-I) Let\n$(B_{t},t\\geq0)$ be a standard Brownian motion. Consider the Gaussian\nvector $(B_{1/2},B_{1})$. Its covariance matrix is:\n\n$$\\begin{aligned}\nC & =\\left[\\begin{array}{cc}\n1/2 & 1/2\\\\\n1/2 & 1\n\\end{array}\\right]\n\\end{aligned}$$\n\nLet's compute $\\mathbb{E}[B_{1}|B_{1/2}]$ and\n$\\mathbb{E}[B_{1/2}|B_{1}]$. This is easy using the equation\n([\\[eq:conditional-expectation-of-gaussian-vector\\]](#eq:conditional-expectation-of-gaussian-vector){reference-type=\"ref\"\nreference=\"eq:conditional-expectation-of-gaussian-vector\"}). We have:\n\n$$\\begin{aligned}\n\\mathbb{E}[B_{1}|B_{1/2}] & =\\frac{\\mathbb{E}[B_{1}B_{1/2}]}{\\mathbb{E}[B_{1/2}^{2}]}B_{1/2}\\\\\n & =\\frac{(1/2)}{(1/2)}B_{1/2}\\\\\n & =B_{1/2}\n\\end{aligned}$$\n\nIn other words, the best approximation of $B_{1}$ given the information\nof $B_{1/2}$ is $B_{1/2}$. There is no problem in computing\n$\\mathbb{E}[B_{1/2}|B_{1}]$, even though we are conditioning on a future\nposition. Indeed the same formula gives\n\n$$\\begin{aligned}\n\\mathbb{E}[B_{1/2}|B_{1}] & =\\frac{\\mathbb{E}[B_{1}B_{1/2}]}{\\mathbb{E}[B_{1}^{2}]}B_{1}=\\frac{1}{2}B_{1}\n\\end{aligned}$$\n\nThis means that the best approximation of $B_{1/2}$ given the position\nat time $1$, is $\\frac{1}{2}B_{1}$ which makes a whole lot of sense!\n:::\n\nIn example\n([\\[eq:conditional-expectation-of-gaussian-vector\\]](#eq:conditional-expectation-of-gaussian-vector){reference-type=\"ref\"\nreference=\"eq:conditional-expectation-of-gaussian-vector\"}) for the\nGaussian vector $(X,Y)$, the conditional expectation was equal to the\n*orthogonal projection* of $Y$ onto $X$ in $L^{2}$. In particular, the\nconditional expectation was a multiple of $X$. Is this always the case?\nUnfortunately, it is not. For example, in the equation\n([\\[eq:elementary-conditional-expectation-example\\]](#eq:elementary-conditional-expectation-example){reference-type=\"ref\"\nreference=\"eq:elementary-conditional-expectation-example\"}), the\nconditional expectation is clearly not a multiple of the random variable\n$X$. However, it is a function of $X$, as is always the case by\ndefinition\n([\\[def:conditional-expectation\\]](#def:conditional-expectation){reference-type=\"ref\"\nreference=\"def:conditional-expectation\"}).\n\nThe idea to construct the conditional expectation $\\mathbb{E}[Y|X]$ in\ngeneral is to *project $Y$ on the space of all random variables that can\nbe constructed from $X$. To make this precise, consider the following\nsubspace of $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$ :*\n\n::: defn\nLet $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and $X$ a\nrandom variable defined on it. The space\n$L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$ is the linear subspace of\n$L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$ consisting of the\nsquare-integrable random variables of the form $g(X)$ for some function\n$g:\\mathbf{R}\\to\\mathbf{R}$.\n:::\n\nThis is a linear subspace of $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$: It\ncontains the random variable $0$, and any linear combination of random\nvariables of this kind is also a function of $X$ and must have a finite\nsecond moment. We note the following:\n\n::: rem*\n$L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$ is a subspace of\n$L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$, very much how a plane or line\n(going through the origin) is a subspace of $\\mathbf{R}^{3}$.\n\nIn particular, as in the case of a line or a plane, we can project an\nelement of $Y$ of $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$ onto\n$L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$. The resulting projection is an\nelement of $L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$, a square-integrable\nrandom-variable that is a function of $X$. For a subspace $\\mathcal{S}$\nof $\\mathbf{R}^{3}$ (e.g. a line or a plane), the projection of the\nvector $\\mathbf{v}\\in\\mathbf{R}^{3}$ onto the subspace $\\mathcal{S}$,\ndenoted $\\text{Proj}_{\\mathcal{S}}(\\mathbf{v})$ is the closest point to\n$\\mathbf{v}$ lying in the subspace $\\mathcal{S}$. Moreover,\n$\\mathbf{v}-\\text{Proj}_{\\mathcal{S}}(\\mathbf{v})$ is orthogonal to the\nsubspace. This picture of orthogonal projection also holds in $L^{2}$.\nLet $Y$ be a random variable in $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$\nand let $L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$ be the subspace of those\nrandom variables that are functions of $X$. We write $Y^{\\star}$ for the\nrandom variable in $L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$ that is\n*closest* to $Y$. In other words, we have (using the definition of the\n$L^{2}$-distance square):\n\n$$\\inf_{Z\\in L^{2}(\\Omega,\\sigma(X),\\mathbb{P})}\\mathbb{E}[(Y-Z)^{2}]=\\mathbb{E}[(Y-Y^{\\star})^{2}]\\label{eq:Y-star-is-the-closest-to-Y-in-L2-sense}$$\n:::\n\nIt turns out that $Y^{\\star}$ is the right candidate for the conditional\nexpectation.\n\n::: center\nFigure. An illustration of the conditional expectation $\\mathbb{E}[Y|X]$\nas an orthogonal projection of $Y$ onto the subspace\n$L^2(\\Omega,\\sigma(X),\\mathbb{P})$.\n:::\n\n::: thm\n[]{#th:existence-and-uniqueness-of-the-conditional-expectation\nlabel=\"th:existence-and-uniqueness-of-the-conditional-expectation\"}(Existence\nand uniqueness of the conditional expectation) Let $X$ be a random\nvariable on $(\\Omega,\\mathcal{F},\\mathbb{P})$. Let $Y$ be a random\nvariable in $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$. Then the conditional\nexpectation $\\mathbb{E}[Y|X]$ is the random variable $Y^{\\star}$ given\nin the equation\n([\\[eq:Y-star-is-the-closest-to-Y-in-L2-sense\\]](#eq:Y-star-is-the-closest-to-Y-in-L2-sense){reference-type=\"ref\"\nreference=\"eq:Y-star-is-the-closest-to-Y-in-L2-sense\"}). Namely, it is\nthe random variable in $L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$ that is\nclosest to $Y$ in the $L^{2}$-distance.\n\nIn particular we have the following:\n\n1\\) It is the orthogonal projection of $Y$ onto\n$L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$, that is $Y-Y^{\\star}$ is\northogonal to any random variables in the subspace\n$L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$.\n\n2\\) It is unique.\n:::\n\n::: rem*\nThis result reinforces the meaning of the conditional expectation\n$\\mathbb{E}[Y|X]$ as the best estimation of $Y$ given the information of\n$X$: it is the closest random variable to $Y$ among all the functions of\n$X$ in the sense of $L^{2}$.\n:::\n\n::: proof\n*Proof.* We write for short $L^{2}(X)$ for the subspace\n$L^{2}(\\Omega,\\sigma(X),\\mathbb{P})$. Let $Y^{\\star}$ be as in equation\n([\\[eq:Y-star-is-the-closest-to-Y-in-L2-sense\\]](#eq:Y-star-is-the-closest-to-Y-in-L2-sense){reference-type=\"ref\"\nreference=\"eq:Y-star-is-the-closest-to-Y-in-L2-sense\"}). We show\nsuccessively that (1) $Y-Y^{\\star}$ is orthogonal to any element of\n$L^{2}(X)$, so it is the orthogonal projection (2) $Y^{\\star}$ has the\nproperties of conditional expectation in definition\n([\\[eq:definition-conditional-expectation\\]](#eq:definition-conditional-expectation){reference-type=\"ref\"\nreference=\"eq:definition-conditional-expectation\"}) (3) $Y^{\\star}$ is\nunique.\n\n\\(1\\) Let $W=g(X)$ be a random variable in $L^{2}(X)$. We show that $W$\nis orthogonal to $Y-Y^{\\star}$; that is $\\mathbb{E}[(Y-Y^{\\star})W]=0$.\nThis should be intuitively clear from figure above. On the one hand, we\nhave by developing the square:\n\n$$\\begin{aligned}\n\\mathbb{E}[(W-(Y-Y^{\\star}))^{2}] & =\\mathbb{E}[W^{2}-2W(Y-Y^{\\star})+(Y-Y^{\\star})^{2}]\\nonumber \\\\\n & =\\mathbb{E}[W^{2}]-2\\mathbb{E}[W(Y-Y^{\\star})]+\\mathbb{E}(Y-Y^{\\star})^{2}]\\label{eq:developing-the-square}\n\\end{aligned}$$\n\nOn the other hand, $Y^{\\star}+W$ is an arbitrary vector in $L^{2}(X)$(it\nis a linear combination of the elements in $L^{2}(X)$), we must have\nfrom equation\n([\\[eq:Y-star-is-the-closest-to-Y-in-L2-sense\\]](#eq:Y-star-is-the-closest-to-Y-in-L2-sense){reference-type=\"ref\"\nreference=\"eq:Y-star-is-the-closest-to-Y-in-L2-sense\"}):\n\n$$\\begin{aligned}\n\\mathbb{E}[(W-(Y-Y^{\\star}))^{2}] & =\\mathbb{E}[(Y-(Y^{\\star}+W))^{2}]\\nonumber \\\\\n & \\geq\\inf_{Z\\in L^{2}(X)}\\mathbb{E}[(Y-Z)^{2}]\\nonumber \\\\\n & =\\mathbb{E}[(Y-Y^{\\star})^{2}]\\label{eq:lower-bound}\n\\end{aligned}$$\n\nPutting the last two equations\n([\\[eq:developing-the-square\\]](#eq:developing-the-square){reference-type=\"ref\"\nreference=\"eq:developing-the-square\"}),\n([\\[eq:lower-bound\\]](#eq:lower-bound){reference-type=\"ref\"\nreference=\"eq:lower-bound\"}) together, we get that for any\n$W\\in L^{2}(X)$:\n\n$$\\begin{aligned}\n\\mathbb{E}[W^{2}]-2\\mathbb{E}[W(Y-Y^{\\star})] & \\geq0\n\\end{aligned}$$\n\nIn particular, this also holds for $aW$, in which case we get:\n\n$$\\begin{aligned}\na^{2}\\mathbb{E}[W^{2}]-2a\\mathbb{E}[W(Y-Y^{\\star})] & \\geq0\\\\\n\\implies a\\left\\{ a\\mathbb{E}[W^{2}]-2\\mathbb{E}[W(Y-Y^{\\star})]\\right\\}  & \\geq0\n\\end{aligned}$$\n\nIf $a>0$, then:\n\n$$a\\mathbb{E}[W^{2}]-2\\mathbb{E}[W(Y-Y^{\\star})]\\geq0\\label{eq:case-when-a-gt-zero}$$\n\nwhereas if $a<0$, then the sign changes upon dividing throughout by $a$,\nand we have:\n\n$$a\\mathbb{E}[W^{2}]-2\\mathbb{E}[W(Y-Y^{\\star})]\\leq0\\label{eq:case-when-a-lt-zero}$$\n\nRearranging\n([\\[eq:case-when-a-gt-zero\\]](#eq:case-when-a-gt-zero){reference-type=\"ref\"\nreference=\"eq:case-when-a-gt-zero\"}) yields:\n\n$$\\mathbb{E}[W(Y-Y^{\\star})]\\leq a\\mathbb{E}[W^{2}]/2\\label{eq:case-when-a-gt-zero-rearranged}$$\n\nRearranging\n([\\[eq:case-when-a-lt-zero\\]](#eq:case-when-a-lt-zero){reference-type=\"ref\"\nreference=\"eq:case-when-a-lt-zero\"}) yields:\n\n$$\\mathbb{E}[W(Y-Y^{\\star})]\\geq a\\mathbb{E}[W^{2}]/2\\label{eq:case-when-a-lt-zero-rearranged}$$\n\nSince\n([\\[eq:case-when-a-gt-zero-rearranged\\]](#eq:case-when-a-gt-zero-rearranged){reference-type=\"ref\"\nreference=\"eq:case-when-a-gt-zero-rearranged\"}) holds for all $a>0$, the\nstronger inequality, $\\mathbb{E}[W(Y-Y^{\\star})]\\leq0$ must hold. Since,\n([\\[eq:case-when-a-lt-zero-rearranged\\]](#eq:case-when-a-lt-zero-rearranged){reference-type=\"ref\"\nreference=\"eq:case-when-a-lt-zero-rearranged\"}) holds for all $a<0$, the\nstronger inequality $\\mathbb{E}[W(Y-Y^{\\star})]\\geq0$ must hold.\nConsequently,\n\n$$\\mathbb{E}[W(Y-Y^{\\star})]=0$$\n\n\\(2\\) It is clear that $Y^{\\star}$ is a function of $X$ by construction,\nsince it is in $L^{2}(X)$. Moreover, for any $W\\in L^{2}(X)$, we have\nfrom (1) that:\n\n$$\\begin{aligned}\n\\mathbb{E}[W(Y-Y^{\\star})] & =0\n\\end{aligned}$$\n\nwhich is the second defining property of conditional expectations.\n\n\\(3\\) Lastly, suppose there is another element $Y'$ that is in\n$L^{2}(X)$ that minimizes the distance to $Y$. Then we would get:\n\n$$\\begin{aligned}\n\\mathbb{E}[(Y-Y')^{2}] & =\\mathbb{E}[(Y-Y^{\\star}+Y^{\\star}-Y')^{2}]\\\\\n & =\\mathbb{E}[(Y-Y^{\\star})^{2}]+2\\mathbb{E}[(Y-Y^{\\star})(Y^{\\star}-Y')]+\\mathbb{E}[(Y^{\\star}-Y')^{2}]\\\\\n & =\\mathbb{E}[(Y-Y^{\\star})^{2}]+0+\\mathbb{E}[(Y^{\\star}-Y')^{2}]\\\\\n & \\quad\\left\\{ (Y^{\\star}-Y')\\in L^{2}(X)\\perp(Y-Y^{\\star})\\right\\} \n\\end{aligned}$$\n\nwhere we used the fact, that $Y^{\\star}-Y'$ is a vector in $L^{2}(X)$\nand the orthogonality of $Y-Y^{\\star}$ with $L^{2}(X)$ as in (1). But,\nthis implies that:\n\n$$\\begin{aligned}\n\\cancel{\\mathbb{E}[(Y-Y')^{2}]} & =\\cancel{\\mathbb{E}[(Y-Y^{\\star})^{2}]}+\\mathbb{E}[(Y^{\\star}-Y')^{2}]\\\\\n\\mathbb{E}[(Y^{\\star}-Y')^{2}] & =0\n\\end{aligned}$$\n\nSo, $Y^{\\star}=Y'$ almost surely. ◻\n:::\n\n::: example\n\n**Conditional Expectation of continuous random variables.** Let $(X,Y)$\nbe two random variables with joint density $f_{X,Y}(x,y)$ on\n$\\mathbf{R}^{2}$. Suppose for simplicity, that\n$\\int_{\\mathbf{R}}f(x,y)dx>0$ for every $y$ belonging to $\\mathbf{R}$.\nShow that the conditional expectation $\\mathbf{E}[Y|X]$ equals $h(X)$\nwhere $h$ is the function:\n\n$$\\begin{aligned}\nh(x) & =\\frac{\\int_{\\mathbf{R}}yf_{X,Y}(x,y)dy}{\\int_{\\mathbf{R}}f_{X,Y}(x,y)dy}\\label{eq:conditional-expectation-of-continuous-random-variables}\n\\end{aligned}$$\n\nIn particular, verify that $\\mathbf{E}[\\mathbf{E}[Y|X]]=\\mathbf{E}[Y]$.\n\n*Hint*: To prove this, verify that the above formula satisfies both the\nproperties of conditional expectations; then invoke uniqueness to finish\nit off.\n:::\n\n::: sol*\n\\(i\\) The density function $f_{X,Y}(x,y)$ is a map\n$f:\\mathbf{R}^{2}\\to\\mathbf{R}$. The integral\n$\\int_{y=-\\infty}^{y=+\\infty}yf_{X,Y}(x_{0},y)dy$ is the area under the\ncurve $yf(x,y)$ at the point $x=x_{0}$. Let's call it $A(x_{0})$. If\ninstead, we have an arbitrary $x$,\n$\\int_{y=-\\infty}^{y=+\\infty}yf_{X,Y}(x,y)dy$ represents the area $A(x)$\nof an arbitrary slice of the surface $yf_{X,Y}$ at the point $x$. Hence,\nit is a function of $x$. The denominator\n$\\int_{\\mathbf{R}}f_{X,Y}(x,y)dy=f_{X}(x)$, the density of $X$, which is\na function of $x$. Hence, the ratio is a function of $x$.\n\n\\(ii\\) Let $g(X)$ is a bounded random variable. We have:\n\n$$\\begin{aligned}\n\\mathbf{E}[g(X)(Y-h(X))] & =\\mathbf{E}[Yg(X)]-\\mathbf{E}[g(X)h(X)]\\\\\n & =\\int\\int_{\\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx-\\int_{\\mathbf{R}}g(x)h(x)f(x)dx\\\\\n & =\\int\\int_{\\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx\\\\\n & -\\int_{\\mathbf{R}}\\begin{array}{c}\ng(x)\\cdot\\frac{\\int_{\\mathbf{R}}yf_{X,Y}(x,y)dy}{\\int_{\\mathbf{R}}f_{X,Y}(x,y)dy}\\end{array}\\cdot\\int_{\\mathbf{R}}f_{X,Y}(x,y)dy\\ dx\\\\\n & =\\int\\int_{\\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx\\\\\n & -\\int_{\\mathbf{R}}\\begin{array}{c}\ng(x)\\cdot\\frac{\\int_{\\mathbf{R}}yf_{X,Y}(x,y)dy}{\\cancel{\\int_{\\mathbf{R}}f_{X,Y}(x,y)dy}}\\end{array}\\cdot\\cancel{\\int_{\\mathbf{R}}f_{X,Y}(x,y)dy}\\ dx\\\\\n & =\\int\\int_{\\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx-\\int_{\\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)\\cdot dx\\cdot dy\\\\\n & =0\n\\end{aligned}$$\n:::\n\nThus, $h(X)$ is a valid candidate for the conditional expectation\n$\\mathbf{E}[Y|X]$. Moreover, by the existence and uniqueness theorem\n([\\[th:existence-and-uniqueness-of-the-conditional-expectation\\]](#th:existence-and-uniqueness-of-the-conditional-expectation){reference-type=\"ref\"\nreference=\"th:existence-and-uniqueness-of-the-conditional-expectation\"}),\n$\\mathbf{E}[Y|X]$ is unique and equals $h(X)$.\n\n#### Conditioning on several random variables. {#conditioning-on-several-random-variables. .unnumbered}\n\nWe would like to generalize the conditional expectation to the case when\nwe condition on the information of more than one random variable. Taking\nthe $L^{2}$ point of view, we should expect that the conditional\nexpectation is the orthogonal projection of the given random variable on\nthe subspace generated by square integrable functions of all the\nvariables on which we condition.\n\nIt is now useful to study sigma-fields, an object that was defined in\nchapter 1.\n\n::: defn\n[]{#def:sigma-field label=\"def:sigma-field\"}(Sigma-Field) A sigma-field\nor sigma-algebra $\\mathcal{F}$ of a sample space $\\Omega$ is a\ncollection of all measurable events with the following properties:\n\n\\(1\\) $\\Omega$ is in $\\mathcal{F}$.\n\n\\(2\\) Closure under complement. If $A\\in\\mathcal{F}$, then\n$A^{C}\\in\\mathcal{F}$.\n\n\\(3\\) Closure under countable unions. If\n$A_{1},A_{2},\\ldots,\\in\\mathcal{F}$, then\n$\\bigcup_{n=1}^{\\infty}A_{n}\\in\\mathcal{F}$.\n:::\n\nSuch objects play a fundamental role in the rigorous study of\nprobability and real analysis in general. We will focus on the intuition\nbehind them. First let's mention some examples of sigma-fields of a\ngiven sample space $\\Omega$ to get acquainted with the concept.\n\n::: example\n(Examples of sigma-fields).\n\n\\(1\\) *The trivial sigma-field*. Note that the collection of events\n$\\{\\emptyset,\\Omega\\}$ is a sigma-field of $\\Omega$. We generally denote\nit by $\\mathcal{F}_{0}$.\n\n\\(2\\) *The $\\sigma$-field generated by an event $A$.* Let $A$ be an\nevent that is not $\\emptyset$ and not the entire $\\Omega$. Then the\nsmallest sigma-field containing $A$ ought to be:\n\n$$\\begin{aligned}\n\\mathcal{F}_{1} & =\\{\\emptyset,A,A^{C},\\Omega\\}\n\\end{aligned}$$\n\nThis sigma-field is denoted by $\\sigma(A)$.\n\n\\(3\\) The *sigma-field generated by a random variable $X$.*\n\nWe now define the $\\mathcal{F}_{X}$ as follows:\n\n$$\\begin{aligned}\n\\mathcal{F}_{X} & =X^{-1}(\\mathcal{B}):=\\{\\omega:X(\\omega)\\in B\\},\\forall B\\in\\mathcal{B}(\\mathbf{R})\n\\end{aligned}$$\n\nwhere $\\mathcal{B}$ is the Borel $\\sigma$-algebra on $\\mathbf{R}$.\n$\\mathcal{F}_{X}$ is sometimes denoted as $\\sigma(X)$.\n$\\mathcal{F}_{X}$is the set of all events pertaining to $X$. It is a\nsigma-algebra because:\n\n\\(i\\) $\\Omega\\in\\sigma(X)$ because\n$\\Omega=\\{\\omega:X(\\omega)\\in\\mathbf{R}\\}$ and\n$\\mathbf{R}\\in\\mathcal{B}(\\mathbf{R})$.\n\n\\(ii\\) Let any event $C\\in\\sigma(X)$. We need to show that\n$\\Omega\\setminus C\\in\\sigma(X)$.\n\nSince $C\\in\\sigma(X)$, there exists $A\\in\\mathcal{B}(\\mathbf{R})$, such\nthat:\n\n$$\\begin{aligned}\nC & =\\{\\omega\\in\\Omega:X(\\omega)\\in A\\}\n\\end{aligned}$$\n\nNow, we calculate:\n\n$$\\begin{aligned}\n\\Omega\\setminus C & =\\{\\omega\\in\\Omega:X(\\omega)\\in\\mathbf{R}\\setminus A\\}\n\\end{aligned}$$\n\nSince $\\mathcal{B}(\\mathbf{R})$ is a sigma-algebra, it is closed under\ncomplementation. Hence, if $A\\in\\mathcal{B}(\\mathbf{R})$, it implies\nthat $\\mathbf{R}\\setminus A\\in\\mathcal{B}(\\mathbf{R})$. So,\n$\\Omega\\setminus C\\in\\sigma(X)$.\n\n\\(iii\\) Consider a sequence of events\n$C_{1},C_{2},\\ldots,C_{n},\\ldots\\in\\sigma(X)$. We need to prove that\n$\\bigcup_{n=1}^{\\infty}C_{n}\\in\\sigma(X)$.\n\nSince $C_{n}\\in\\sigma(X)$, there exists\n$A_{n}\\in\\mathcal{B}(\\mathbf{R})$ such that:\n\n$$\\begin{aligned}\nC_{n} & =\\{\\omega\\in\\Omega:X(\\omega)\\in A_{n}\\}\n\\end{aligned}$$\n\nNow, we calculuate:\n\n$$\\begin{aligned}\n\\bigcup_{n=1}C_{n} & =\\{\\omega\\in\\Omega:X(\\omega)\\in\\bigcup_{n=1}^{\\infty}A_{n}\\}\n\\end{aligned}$$\n\nBut, $\\bigcup_{n=1}^{\\infty}A_{n}\\in\\mathcal{B}(\\mathbf{R})$. So,\n$\\bigcup_{n=1}^{\\infty}C_{n}\\in\\sigma(X)$.\n\nConsequently, $\\sigma(X)$ is indeed a $\\sigma$-algebra.\n\nIntuitively, we think of $\\sigma(X)$ as containing all information about\n$X$.\n\n\\(4\\) *The sigma-field generated by a stochastic process\n$(X_{s},s\\leq t)$.* Let $(X_{s},s\\geq0)$ be a stochastic process.\nConsider the process restricted to $[0,t]$, $(X_{s},s\\leq t)$. We\nconsider the smallest sigma-field containing all events pertaining to\nthe random variables $X_{s},s\\leq t$. We denote it by\n$\\sigma(X_{s},s\\leq t)$ or $\\mathcal{F}_{t}$.\n:::\n\nThe sigma-fields on $\\Omega$ have a natural (partial) ordering: two\nsigma-fields $\\mathcal{G}$ and $\\mathcal{F}$ of $\\Omega$ are such that\n$\\mathcal{G}\\subseteq\\mathcal{F}$ if all the events in $\\mathcal{G}$ are\nin $\\mathcal{F}$. For example, the trivial $\\sigma$-field\n$\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$ is contained in all the\n$\\sigma$-fields of $\\Omega$. Clearly, the $\\sigma$-field\n$\\mathcal{F}_{t}=\\sigma(X_{s},s\\leq t)$ is contained in\n$\\mathcal{F}_{t'}$ if $t\\leq t'$.\n\nIf all the events pertaining to a random variable $X$ are in the\n$\\sigma$-field $\\mathcal{G}$ (and thus we can compute\n$\\mu(X^{-1}((a,b]))$), we will say that $X$ is $\\mathcal{G}$-measurable.\nThis means that all information about $X$ is contained in $\\mathcal{G}$.\n\n::: defn\nLet $X$ be a random variable defined on\n$(\\Omega,\\mathcal{F},\\mathbb{P})$. Consider another\n$\\mathcal{G}\\subseteq\\mathcal{F}$. Then $X$ is said to be\n$\\mathcal{G}$-measurable, if and only if:\n\n$$\\begin{aligned}\n\\{\\omega:X(\\omega)\\in(a,b]\\} & \\in\\mathcal{G}\\text{ for all intervals }(a,b]\\in\\mathbf{R}\n\\end{aligned}$$\n:::\n\n::: example\n($\\mathcal{F}_{0}$-measurable random variables). Consider the trivial\nsigma-field $\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$. A random variable\nthat is $\\mathcal{F}_{0}$-measurable must be a constant. Indeed, we have\nthat for any interval $(a,b]$, $\\{\\omega:X(\\omega)\\in(a,b]\\}=\\emptyset$\nor $\\{\\omega:X(\\omega)\\in(a,b]\\}=\\Omega$. This can only hold if $X$\ntakes a single value.\n:::\n\n::: example\n[]{#ex:sigma(X)-measurable-random-variables-example\nlabel=\"ex:sigma(X)-measurable-random-variables-example\"}($\\sigma(X)$-measurable\nrandom variables). Let $X$ be a given random variable on\n$(\\Omega,\\mathcal{F},\\mathbb{P})$. Roughly speaking, a\n$\\sigma(X)$-measurable random variable is determined by the information\nof $X$ only. Here is the simplest example of a $\\sigma(X)$-measurable\nrandom variable. Take the indicator function $Y=\\mathbf{1}_{\\{X\\in B\\}}$\nfor some event $\\{X\\in B\\}$ pertaining to $X$. Then the pre-images\n$\\{\\omega:Y(\\omega)\\in(a,b]\\}$ are either $\\emptyset$, $\\{X\\in B\\}$,\n$\\{X\\in B^{C}\\}$ or $\\Omega$ depending on whether $0,1$ are in $(a,b]$\nor not. All of these events are in $\\sigma(X)$. More generally, one can\nconstruct a $\\sigma(X)$-measurable random variable by taking linear\ncombinations of indicator functions of events of the form $\\{X\\in B\\}$.\n\nIt turns out that any (Borel measurable) function of $X$ can be\napproximated by taking limits of such simple functions.\n\nConcretely, this translates to the following statement:\n\n$$\\text{If }Y\\text{ is \\ensuremath{\\sigma}(X)-measurable, then Y=g(X) for some function g}$$\n\nIn the same way, if $Z$ is $\\sigma(X,Y)$-measurable, then $Z=h(X,Y)$ for\nsome $h$. These facts can be proved rigorously using measure theory.\n:::\n\nWe are ready to give the general definition of conditional expectation.\n\n::: example\n(Coin-Tossing Space). Suppose a coin is tossed infinitely many times.\nLet $\\Omega$ be the set of all infinite sequences of $H$s and $T$s. A\ngeneric element of $\\Omega$ is denoted by $\\omega_{1}\\omega_{2}\\ldots$,\nwhere $\\omega_{n}$ indicates the result of the $n$th coin toss. $\\Omega$\nis an uncountable sample space. The trivial sigma-field\n$\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$. Assume that we don't know\nanything about the outcome of the experiement. Even without any\ninformation, we know that the true $\\omega$ belongs to $\\Omega$ and does\nnot belong to $\\emptyset$. It is the information learned at time $0$.\n\nNext, assume that we know the outcome of the first coin toss. Define\n$A_{H}=\\{\\omega:\\omega_{1}=H\\}$=set of all sequences beginning with $H$\nand $A_{T}=\\{\\omega:\\omega_{1}=T\\}$=set of all sequences beginning with\n$T$. The four sets resolved by the first coin-toss form the the\n$\\sigma$-field $\\mathcal{F}_{1}=\\{\\emptyset,A_{H},A_{T},\\Omega\\}$. We\nshall think of this $\\sigma$-field as containing the information learned\nby knowing the outcome of the first coin toss. More precisely, if\ninstead of being told about the first coin toss, we are told for each\nset in $\\mathcal{F}_{1}$, whether or not the true $\\omega$ belongs to\nthat set, then we know the outcome of the first coin toss and nothing\nmore.\n\nIf we are told the first two coin tosses, we obtain a finer resolution.\nIn particular, the four sets:\n\n$$\\begin{aligned}\nA_{HH} & =\\{\\omega:\\omega_{1}=H,\\omega_{2}=H\\}\\\\\nA_{HT} & =\\{\\omega:\\omega_{1}=H,\\omega_{2}=T\\}\\\\\nA_{TH} & =\\{\\omega:\\omega_{1}=T,\\omega_{2}=H\\}\\\\\nA_{TT} & =\\{\\omega:\\omega_{1}=T,\\omega_{2}=T\\}\n\\end{aligned}$$\n\nare resolved. Of course, the sets in $\\mathcal{F}_{1}$ are resolved.\nWhenever a set is resolved, so is its complement, which means that\n$A_{HH}^{C}$, $A_{HT}^{C}$, $A_{TH}^{C}$ and $A_{TT}^{C}$ are resolved,\nso is their union which means that $A_{HH}\\cup A_{TH}$,\n$A_{HH}\\cup A_{TT}$, $A_{HT}\\cup A_{TH}$ and $A_{HT}\\cup A_{TT}$ are\nresolved. The other two pair-wise unions $A_{HH}\\cup A_{HT}=A_{H}$ and\n$A_{TH}\\cup A_{TT}=A_{T}$ are already resolved. Finally, the triple\nunions are also resolved, because\n$A_{HH}\\cup A_{HT}\\cup A_{TH}=A_{TT}^{C}$ and so forth. Hence, the\ninformation pertaining to the second coin-toss is contained in:\n\n$$\\begin{aligned}\n\\mathcal{F}_{2} & =\\{\\emptyset,\\Omega,\\\\\n & A_{H},A_{T},\\\\\n & A_{HH},A_{HT},A_{TH},A_{TT},\\\\\n & A_{HH}^{C},A_{HT}^{C},A_{TH}^{C},A_{TT}^{C},\\\\\n & A_{HH}\\cup A_{TH},A_{HH}\\cup A_{TT},A_{HT}\\cup A_{TH},A_{HT}\\cup A_{TT}\\}\n\\end{aligned}$$\n\nHence, if the outcome of the first two coin tosses is known, all of the\nevents in $\\mathcal{F}_{2}$ are resolved - we exactly know, if each\nevent has ocurred or not. $\\mathcal{F}_{2}$ is the information learned\nby observing the first two coin tosses.\n:::\n\n::: xca\n(**Exercises on sigma-fields**).\n\n\\(a\\) Let $A$, $B$ be two proper subsets of $\\Omega$ such that\n$A\\cap B\\neq\\emptyset$ and $A\\cup B\\neq\\Omega$. Write down\n$\\sigma(\\{A,B\\})$, the smallest sigma-field containing $A$ and $B$\nexplicitly. What if $A\\cap B=\\emptyset$?\n\n\\(b\\) The Borel sigma-field is the smallest sigma-field containing\nintervals of the form $(a,b]$ in $\\mathbf{R}$. Show that all singletons\n$\\{b\\}$ are in $\\mathcal{B}(\\mathbf{R})$ by writing $\\{b\\}$ as a\ncountable intersection of intervals $(a,b]$. Conclude that all open\nintervals $(a,b)$ and all closed intervals $[a,b]$ are in\n$\\mathcal{B}(\\mathbf{R})$. Is the subset $\\mathbf{Q}$ of rational\nnumbers a Borel set?\n:::\n\n::: proof\n*Proof.* (a) The sigma-field generated by the two events $A$, $B$ is\ngiven by:\n\n$$\\begin{aligned}\n\\sigma(\\{A,B\\}) & =\\{\\emptyset,\\Omega,\\\\\n & A,B,A^{C},B^{C},\\\\\n & A\\cup B,A\\cap B,\\\\\n & A\\cup B^{C},A^{C}\\cup B,A^{C}\\cup B^{C},\\\\\n & A\\cap B^{C},A^{C}\\cap B,A^{C}\\cap B^{C},\\\\\n & (A\\cup B)\\cap(A\\cap B)^{C},\\\\\n & (A\\cup B)^{C}\\cup(A\\cap B)\\}\n\\end{aligned}$$\n\n\\(b\\) Firstly, recall that:\n\n$$\\begin{aligned}\n\\mathcal{B}(\\mathbf{R}) & =\\bigcap_{\\alpha\\in\\Lambda}\\mathcal{F}_{\\alpha}=\\bigcap\\sigma(\\{I:I\\text{ is an interval }(a,b]\\subseteq\\mathbf{R}\\})\n\\end{aligned}$$\n\nWe can write:\n\n$$\\begin{aligned}\n\\{b\\} & =\\bigcap_{n=1}^{\\infty}\\left(b-\\frac{1}{n},b\\right]\n\\end{aligned}$$\n\nAs $\\mathcal{B}(\\mathbf{R})$ is a sigma-field, it is closed under\ncountable intersections. Hence, the singleton set $\\{b\\}$is a Borel set.\n\nSimilarly, we can write, any open interval as the countable union:\n\n$$\\begin{aligned}\n(a,b) & =\\bigcup_{n=1}^{\\infty}\\left(a,b-\\frac{1}{n}\\right]\n\\end{aligned}$$\n\nWe can convince ourselves, that equality indeed holds. Let $x\\in(a,b)$\nand choose $N$, such that $\\frac{1}{N}<|b-x|$. Then, for all $n\\geq N$,\n$x\\in(a,b-1/n]$. Thus, it belongs to the RHS. In the reverse direction,\nlet $x$ belong to $\\bigcup_{n=1}^{\\infty}\\left(a,b-\\frac{1}{n}\\right]$.\nSo, $x$ belongs to atleast one of these sets. Therefore, $x\\in(a,b)$ is\ntrivially true. So, the two sets are equal.\n\nHence, open intervals are Borel sets.\n\nSimilarly, we may write:\n\n$$\\begin{aligned}\n[a,b] & =\\bigcap_{n=1}^{\\infty}\\left(a-\\frac{1}{n},b+\\frac{1}{n}\\right)\n\\end{aligned}$$\n\nConsequently, closed intervals are Borel sets. Since $\\mathbf{Q}$ is\ncountable, it is a Borel set. Moreover, the empty set $\\emptyset$ and\n$\\mathbf{R}$ are Borel sets. So, $\\mathbf{R}\\backslash\\mathbf{Q}$ is\nalso a Borel set. ◻\n:::\n\n::: xca\nLet $(X,Y)$ be a Gaussian vector with mean $0$ and\ncovariance matrix\n\n$$\\begin{aligned}\nC & =\\left[\\begin{array}{cc}\n1 & \\rho\\\\\n\\rho & 1\n\\end{array}\\right]\n\\end{aligned}$$\n\nfor $\\rho\\in(-1,1)$. We verify that the example\n([\\[ex:conditional-expectation-of-gaussian-vectors\\]](#ex:conditional-expectation-of-gaussian-vectors){reference-type=\"ref\"\nreference=\"ex:conditional-expectation-of-gaussian-vectors\"}) and\nexercise\n([\\[ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables\\]](#ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables){reference-type=\"ref\"\nreference=\"ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables\"})\nyield the same conditional expectation.\n\n\\(a\\) Use equation\n([\\[eq:conditional-expectation-of-gaussian-vector\\]](#eq:conditional-expectation-of-gaussian-vector){reference-type=\"ref\"\nreference=\"eq:conditional-expectation-of-gaussian-vector\"}) to show that\n$\\mathbf{E}[Y|X]=\\rho X$.\n\n\\(b\\) Write down the joint PDF $f(x,y)$ of $(X,Y)$.\n\n\\(c\\) Show that $\\int_{\\mathbf{R}}yf(x,y)dy=\\rho x$ and that\n$\\int_{\\mathbf{R}}f(x,y)dy=1$.\n\n\\(d\\) Deduce that $\\mathbf{E}[Y|X]=\\rho X$ using the equation\n([\\[eq:conditional-expectation-of-continuous-random-variables\\]](#eq:conditional-expectation-of-continuous-random-variables){reference-type=\"ref\"\nreference=\"eq:conditional-expectation-of-continuous-random-variables\"}).\n:::\n\n::: proof\n*Proof.* (a) Since $(X,Y)$ have mean $0$ and variance $1$, it follows\nthat:\n\n$$\\begin{aligned}\n\\mathbf{E}[(X-EX)(Y-EY)] & =\\mathbf{E}(XY)\\\\\n\\sqrt{(\\mathbf{E}[X^{2}]-(\\mathbf{E}X)^{2})}\\cdot\\sqrt{(\\mathbf{E}[Y^{2}]-(\\mathbf{E}Y)^{2})} & =\\sqrt{(1-0)(1-0)}\\\\\n & =1\n\\end{aligned}$$\n\nand therefore,\n\n$$\\begin{aligned}\n\\rho & =\\frac{\\mathbf{E}(XY)}{1}=\\frac{\\mathbf{E}[XY]}{\\mathbf{E}[X^{2}]}\n\\end{aligned}$$\n\nSince $(X,Y)$ is a Gaussian vector, using\n([\\[eq:conditional-expectation-of-gaussian-vector\\]](#eq:conditional-expectation-of-gaussian-vector){reference-type=\"ref\"\nreference=\"eq:conditional-expectation-of-gaussian-vector\"}), we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[Y|X] & =\\frac{\\mathbf{E}[XY]}{\\mathbf{E}[X^{2}]}X=\\rho X\n\\end{aligned}$$\n\n\\(b\\) Consider the augmented matrix $[C|I]$. We have:\n\n$$\\begin{aligned}\n[C|I] & =\\left[\\left.\\begin{array}{cc}\n1 & \\rho\\\\\n\\rho & 1\n\\end{array}\\right|\\begin{array}{cc}\n1 & 0\\\\\n0 & 1\n\\end{array}\\right]\n\\end{aligned}$$\n\nPerforming $R_{2}=R_{2}-\\rho R_{1}$, the above system is row-equivalent\nto:\n\n$$\\left[\\left.\\begin{array}{cc}\n1 & \\rho\\\\\n0 & 1-\\rho^{2}\n\\end{array}\\right|\\begin{array}{cc}\n1 & 0\\\\\n-\\rho & 1\n\\end{array}\\right]$$\n\nPerforming $R_{2}=\\frac{1}{1-\\rho^{2}}R_{2}$, the above system is\nrow-equivalent to:\n\n$$\\left[\\begin{array}{cc}\n1 & \\rho\\\\\n0 & 1\n\\end{array}\\left|\\begin{array}{cc}\n1 & 0\\\\\n\\frac{-\\rho}{1-\\rho^{2}} & \\frac{1}{1-\\rho^{2}}\n\\end{array}\\right.\\right]$$\n\nPerforming $R_{1}=R_{1}-\\rho R_{2}$, we have:\n\n$$\\left[\\begin{array}{cc}\n1 & 0\\\\\n0 & 1\n\\end{array}\\left|\\begin{array}{cc}\n\\frac{1}{1-\\rho^{2}} & -\\frac{\\rho}{1-\\rho^{2}}\\\\\n\\frac{-\\rho}{1-\\rho^{2}} & \\frac{1}{1-\\rho^{2}}\n\\end{array}\\right.\\right]$$\n\nThus, $$\\begin{aligned}\nC^{-1} & =\\frac{1}{1-\\rho^{2}}\\left[\\begin{array}{cc}\n1 & -\\rho\\\\\n-\\rho & 1\n\\end{array}\\right]\n\\end{aligned}$$\n\nMoreover, $\\det C=1-\\rho^{2}.$\n\nTherefore, the joint density of $(X,Y)$ is given by:\n\n$$\\begin{aligned}\nf(x,y) & =\\frac{1}{2\\pi\\sqrt{1-\\rho^{2}}}\\exp\\left[-\\frac{1}{2(1-\\rho^{2})}\\left[\\begin{array}{cc}\nx & y\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & -\\rho\\\\\n-\\rho & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\nx\\\\\ny\n\\end{array}\\right]\\right]\\\\\n & =\\frac{1}{2\\pi\\sqrt{1-\\rho^{2}}}\\exp\\left[-\\frac{1}{2(1-\\rho^{2})}\\left[\\begin{array}{cc}\nx-\\rho y & -\\rho x+y\\end{array}\\right]\\left[\\begin{array}{c}\nx\\\\\ny\n\\end{array}\\right]\\right]\\\\\n & \\frac{1}{2\\pi\\sqrt{1-\\rho^{2}}}\\exp\\left[-\\frac{1}{2(1-\\rho^{2})}(x^{2}-2\\rho xy+y^{2})\\right]\n\\end{aligned}$$\n\n\\(c\\) Claim I. $\\int_{\\mathbf{R}}yf(x,y)dy=\\rho x$.\n\nCompleting the square, we have:\n\n$$\\begin{aligned}\n(x^{2}-2\\rho xy+y^{2}) & =(y-\\rho x)^{2}+x^{2}(1-\\rho^{2})\n\\end{aligned}$$\n\nThus, we can write:\n\n$$\\begin{aligned}\n\\int_{\\mathbf{R}}yf(x,y)dy & =\\frac{1}{2\\pi\\sqrt{1-\\rho^{2}}}e^{-\\frac{1}{2}x^{2}}\\int_{\\mathbf{R}}ye^{-\\frac{1}{2}\\frac{(y-\\rho x)^{2}}{(1-\\rho^{2})}}dy\n\\end{aligned}$$\n\nLet's substitute\n\n$$\\begin{aligned}\nz & =\\frac{(y-\\rho x)}{\\sqrt{1-\\rho^{2}}}\\\\\ndz & =\\frac{dy}{\\sqrt{1-\\rho^{2}}}\n\\end{aligned}$$\n\nTherefore,\n\n$$\\begin{aligned}\n\\int_{\\mathbf{R}}ye^{-\\frac{1}{2}\\frac{(y-\\rho x)^{2}}{(1-\\rho^{2})}}dy & =\\sqrt{1-\\rho^{2}}\\int_{\\mathbf{R}}(\\rho x+\\sqrt{1-\\rho^{2}}z)e^{-\\frac{z^{2}}{2}}dz\\\\\n & =\\rho x\\cdot\\sqrt{1-\\rho^{2}}\\int_{\\mathbf{R}}e^{-\\frac{z^{2}}{2}}dz+(1-\\rho^{2})\\int_{\\mathbf{R}}ze^{-\\frac{z^{2}}{2}}dz\\\\\n & =\\rho x\\cdot\\sqrt{1-\\rho^{2}}\\cdot\\sqrt{2\\pi}+(1-\\rho^{2})\\cdot0\\\\\n & =\\rho x\\cdot\\sqrt{1-\\rho^{2}}\\cdot\\sqrt{2\\pi}\n\\end{aligned}$$\n\nConsequently,\n\n$$\\begin{aligned}\n\\int_{\\mathbf{R}}yf(x,y)dy & =\\frac{1}{2\\pi\\cancel{\\sqrt{1-\\rho^{2}}}}e^{-\\frac{1}{2}x^{2}}\\rho x\\cdot\\cancel{\\sqrt{1-\\rho^{2}}}\\cdot\\sqrt{2\\pi}\\\\\n & =\\rho x\\cdot\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^{2}}\\\\\n & =\\rho x\\cdot f_{X}(x)\\\\\n\\frac{\\int_{\\mathbf{R}}yf(x,y)dy}{f_{X}(x)} & =\\frac{\\int_{\\mathbf{R}}yf(x,y)dy}{\\int_{\\mathbf{R}}f(x,y)}=\\rho x\n\\end{aligned}$$\n\n\\(d\\) For a Gaussian vector $(X,Y),$ the conditional expectation\n$\\mathbf{E}[Y|X]=h(X)$. Hence, $\\mathbf{E}[Y|X]=\\rho X$. ◻\n:::\n\n::: defn\n(Conditional Expectation) Let $Y$ be an integrable random variable on\n$(\\Omega,\\mathcal{F},\\mathbb{P})$ and let\n$\\mathcal{G}\\subseteq\\mathcal{F}$ be a sigma-field of $\\Omega$. The\nconditional expectation of $Y$ given $\\mathcal{G}$ is the random\nvariable denoted by $\\mathbb{E}[Y|\\mathcal{G}]$ such that the following\nhold:\n\n\\(a\\) $\\mathbb{E}[Y|\\mathcal{G}]$ is $\\mathcal{G}$-measurable.\n\nIn other words, all events pertaining to the random variable\n$\\mathbb{E}[Y|\\mathcal{G}]$ are in $\\mathcal{G}$.\n\n\\(b\\) For any (bounded) random variable $W$, that is\n$\\mathcal{G}$-measurable,\n\n$$\\begin{aligned}\n\\mathbb{E}[WY] & =\\mathbb{E}[W\\mathbb{E}[Y|\\mathcal{G}]]\n\\end{aligned}$$\n\nIn other words, $\\mathbf{E}[Y|\\mathcal{G}]$ is a proxy for $Y$ as far as\nthe events in $\\mathcal{G}$ are concerned.\n\nNote that, by taking $W=1$ in the property (B), we recover:\n\n$$\\begin{aligned}\n\\mathbf{E}[\\mathbf{E}[Y|\\mathcal{G}]] & =\\mathbf{E}[Y]\n\\end{aligned}$$\n:::\n\n::: rem*\nBeware of the notation! If $\\mathcal{G}=\\sigma(X)$, then the conditional\nexpectation $\\mathbf{E}[Y|\\sigma(X)]$ is usually denoted by\n$\\mathbf{E}[Y|X]$ for short. However, one should always keep in mind\nthat conditioning on $X$ is in fact projecting on the linear subspace\n*generated by all variables constructed from $X$* and not on the linear\nspace generated by generated by $X$ alone. In the same way, the\nconditional expectation $\\mathbf{E}[Z|\\sigma(X,Y)]$ is often written\n$\\mathbf{E}[Z|X,Y]$ for short.\n\nAs expected, if $Y$ is in $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$, then\n$\\mathbf{E}[Y|\\mathcal{G}]$ is given by the orthogonal projection of $Y$\nonto the subspace $L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$, the subspace\nof square integrable random variables that are $\\mathcal{G}$-measurable.\nWe write $Y^{\\star}$ for the random variable in\n$L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$ that is closest to $Y$ that is:\n\n$$\\begin{aligned}\n\\min_{Z\\in L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})}\\mathbf{E}[(Y-Z)^{2}] & =\\mathbf{E}[(Y-Y^{\\star})^{2}]\\label{eq:conditional-expectation}\n\\end{aligned}$$\n:::\n\n::: thm\n[]{#th:existence-and-uniqueness-of-conditional-expectations-II\nlabel=\"th:existence-and-uniqueness-of-conditional-expectations-II\"}(Existence\nand Uniqueness of Conditional Expectations) Let\n$\\mathcal{G}\\subset\\mathcal{F}$ be a sigma-field of $\\Omega$. Let $Y$ be\na random variable in $L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})$. Then, the\nconditional expectation $\\mathbf{E}[Y|\\mathcal{G}]$ is the random\nvariable $Y^{\\star}$ given in the equation\n([\\[eq:conditional-expectation\\]](#eq:conditional-expectation){reference-type=\"ref\"\nreference=\"eq:conditional-expectation\"}). Namely, it is the random\nvariable in $L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$ that is closest to\n$Y$ in the $L^{2}$-distance. In particular we have the following:\n:::\n\n-   It is the orthogonal projection of $Y$ onto\n    $L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$, that is, $Y-Y^{\\star}$ is\n    orthogonal to the random variables in\n    $L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$.\n\n-   It is unique.\n\nAgain, the result should be interpreted as follows: The conditional\nexpectation $\\mathbf{E}[Y|\\mathcal{G}]$ is the best approximation of $Y$\ngiven the information included in $\\mathcal{G}$.\n\n::: rem*\nThe conditional expectation in fact exists and is unique for any\nintegrable random variable $Y$(i.e.\n$Y\\in L^{1}(\\Omega,\\mathcal{F},\\mathbb{P})$ as the definition suggests.\nHowever, there is no orthogonal projection in $L^{1}$, so the intuitive\ngeometric picture is lost.\n:::\n\n::: center\nFigure. An illustration of the conditional expectation\n$\\mathbb{E}[Y|\\mathcal{G}]$ as an orthogonal projection of $Y$ onto the\nsubspace $L^2(\\Omega,\\mathcal{G},\\mathbb{P})$.\n:::\n\n::: example\n(Conditional Expectation for Gaussian Vectors. II.) Consider the\nGaussian vector $(X_{1},\\ldots,X_{n})$. Without loss of generality,\nsuppose it has mean $0$ and is non-degenerate. What is the best\napproximation of $X_{n}$ given the information $X_{1},\\ldots,X_{n-1}$?\nIn other words, what is:\n\n$$\\mathbf{E}[X_{n}|\\sigma(X_{1},\\ldots,X_{n-1})$$\n\nWith example\n([\\[ex:sigma(X)-measurable-random-variables-example\\]](#ex:sigma(X)-measurable-random-variables-example){reference-type=\"ref\"\nreference=\"ex:sigma(X)-measurable-random-variables-example\"}) in mind,\nlet's write $\\mathbf{E}[X_{n}|X_{1}\\ldots X_{n-1}]$ for short. From\nexample\n([\\[ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables\\]](#ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables){reference-type=\"ref\"\nreference=\"ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables\"}),\nwe know that if $(X,Y)$ is a Gaussian vector with mean $0$, then\n$\\mathbf{E}[Y|X]$ is a multiple of $X$. Thus, we expect, that\n$\\mathbf{E}[X_{n}|X_{1}X_{2}\\ldots X_{n-1}]$ is a linear combination of\n$X_{1},X_{2},\\ldots,X_{n-1}$. That is, there exists\n$a_{1},\\ldots,a_{n-1}$ such that:\n\n$$\\begin{aligned}\n\\mathbf{E}[X_{n}|X_{1}X_{2}\\ldots X_{n-1}] & =a_{1}X_{1}+a_{2}X_{2}+\\ldots+a_{n-1}X_{n-1}\n\\end{aligned}$$ In particular, since the conditional expectation is a\nlinear combination of the $X$'s, it is itself a Gaussian random\nvariable. The best way to find the coefficient $a$'s is to go back to\nIID decomposition of Gaussian vectors.\n\nLet $(Z_{1},Z_{2},\\ldots,Z_{n-1})$ be IID standard Gaussians constructed\nfrom the linear combination of $(X_{1},X_{2},\\ldots,X_{n-1})$. Then, we\nhave:\n\n$$\\begin{aligned}\n\\mathbf{E}[X_{n}|X_{1}X_{2}\\ldots X_{n-1}] & =b_{1}Z_{1}+\\ldots+b_{n-1}Z_{n-1}\n\\end{aligned}$$\n\nNow, recall, that we construct the random variables $Z_{1}$, $Z_{2}$,\n$\\ldots$, $Z_{n}$ using Gram-Schmidt orthogonalization:\n\n$$\\begin{aligned}\n\\tilde{Z_{1}} & =X_{1}, & Z_{1} & =\\frac{\\tilde{Z_{1}}}{\\mathbf{E}(\\tilde{Z}_{1}^{2})}\\\\\n\\tilde{Z_{2}} & =X_{2}-\\mathbf{E}(X_{2}Z_{1})Z_{1} & Z_{2} & =\\frac{\\tilde{Z}_{2}}{\\mathbf{E}(\\tilde{Z}_{2}^{2})}\\\\\n\\tilde{Z_{3}} & =X_{3}-\\sum_{i=1}^{2}\\mathbf{E}(X_{3}Z_{i})Z_{i} & Z_{3} & =\\frac{\\tilde{Z}_{3}}{\\mathbf{E}(\\tilde{Z}_{3}^{2})}\\\\\n & \\vdots\n\\end{aligned}$$\n:::\n\n**The simple case for $n=2$ random variables.**\n\nWe have already seen before:\n\n$$\\begin{aligned}\n\\mathbf{E}[X_{1}(X_{2}-\\mathbf{E}(X_{2}Z_{1})Z_{1})] & =\\mathbf{E}[\\tilde{Z}_{1}(X_{2}-\\mathbf{E}(X_{2}Z_{1})Z_{1})]\\\\\n & =\\frac{\\mathbf{E}[\\tilde{Z}_{1}^{2}]}{\\mathbf{E}[\\tilde{Z}_{1}^{2}]}\\times\\mathbf{E}\\left[\\tilde{Z}_{1}(X_{2}-\\mathbf{E}(X_{2}Z_{1})Z_{1})\\right]\\\\\n & =\\mathbf{E}[\\tilde{Z}_{1}^{2}]\\times\\mathbf{E}\\left[\\frac{\\tilde{Z}_{1}}{\\mathbf{E}[\\tilde{Z}_{1}^{2}]}(X_{2}-\\mathbf{E}(X_{2}Z_{1})Z_{1})\\right]\\\\\n & =\\mathbf{E}[\\tilde{Z}_{1}^{2}]\\times\\mathbf{E}[Z_{1}(X_{2}-\\mathbf{E}(X_{2}Z_{1})Z_{1})]\\\\\n & =\\mathbf{E}[\\tilde{Z}_{1}^{2}]\\times\\left(\\mathbf{E}[Z_{1}X_{2}]-\\mathbf{E}(X_{2}Z_{1})\\mathbf{E}[Z_{1}^{2}]\\right)\\\\\n & =0\n\\end{aligned}$$\n\nSo,$X_{2}-\\mathbf{E}(X_{2}Z_{1})Z_{1}$ is orthogonal to $X_{1}$.\n\nMoreover, $\\mathbf{E}(X_{2}Z_{1})Z_{1}$ is a function of $X_{1}$. Thus,\nboth the properties of conditional expectation are satisfied. Since\nconditional expectations are unique, we must have,\n$\\mathbf{E}[X_{2}|X_{1}]=\\mathbf{E}(X_{2}Z_{1})Z_{1}$.\n\n**The case for $n=3$ random variables.**\n\nWe have seen that:\n\n$$\\begin{aligned}\n\\mathbf{E}[X_{1}(X_{3}-\\mathbf{E}(X_{3}Z_{1})Z_{1}-\\mathbf{E}(X_{3}Z_{2})Z_{2})] & =\\frac{\\mathbf{E}[\\tilde{Z}_{1}^{2}]}{\\mathbf{E}[\\tilde{Z}_{1}^{2}]}\\times\\mathbf{E}[\\tilde{Z}_{1}(X_{3}-\\mathbf{E}(X_{3}Z_{1})Z_{1}-\\mathbf{E}(X_{3}Z_{2})Z_{2})]\\\\\n & =\\mathbf{E}[\\tilde{Z}_{1}^{2}]\\times\\mathbf{E}\\left\\{ \\frac{\\tilde{Z}_{1}}{\\mathbf{E}[\\tilde{Z}_{1}^{2}]}(X_{3}-\\mathbf{E}(X_{3}Z_{1})Z_{1}-\\mathbf{E}(X_{3}Z_{2})Z_{2})\\right\\} \\\\\n & =\\mathbf{E}[\\tilde{Z}_{1}^{2}]\\times\\mathbf{E}\\left\\{ Z_{1}(X_{3}-\\mathbf{E}(X_{3}Z_{1})Z_{1}-\\mathbf{E}(X_{3}Z_{2})Z_{2})\\right\\} \\\\\n & =\\mathbf{E}[\\tilde{Z}_{1}^{2}]\\times\\mathbf{E}[X_{3}Z_{1}]-\\mathbf{E}[X_{3}Z_{1}]\\mathbf{E}[Z_{1}^{2}]-\\mathbf{E}[X_{3}Z_{2}]\\mathbf{E}[Z_{1}Z_{2}]\\\\\n & =0\n\\end{aligned}$$\n\nIt is an easy exercise to show that it is orthogonal to $X_{2}$.\n\nHence, $X_{3}-\\mathbf{E}(X_{3}Z_{1})Z_{1}-\\mathbf{E}(X_{3}Z_{2})Z_{2}$\nis orthogonal to $X_{1}$ and $X_{2}$. Moreover,\n$\\mathbf{E}(X_{3}Z_{1})Z_{1}+\\mathbf{E}(X_{3}Z_{2})Z_{2}$ is a function\nof $X_{1}$, $X_{2}$. Thus, we must have:\n\n$$\\begin{aligned}\n\\mathbf{E}[X_{3}|X_{1}X_{2}] & =\\mathbf{E}(X_{3}Z_{1})Z_{1}+\\mathbf{E}(X_{3}Z_{2})Z_{2}\n\\end{aligned}$$\n\nIn general, $X_{n}-\\sum_{i=1}^{n-1}\\mathbf{E}(X_{n}Z_{i})Z_{i}$ is\northogonal to $X_{1}$, $X_{2}$, $\\ldots$, $X_{n-1}$. Hence,\n\n$$\\begin{aligned}\n\\mathbf{E}[X_{n}|X_{1}X_{2}\\ldots X_{n-1}] & =\\sum_{i=1}^{n-1}\\mathbf{E}(X_{n}Z_{i})Z_{i}\n\\end{aligned}$$\n\n### Properties of Conditional Expectation.\n\nWe now list the properties of conditional expectation that follow from\nthe two defining properties (A), (B) in the definition. They are\nextremely useful, when doing explicit computations on martingales. A\ngood way to remember them is to understand how they relate to the\ninterpretation of conditional expectation as an orthogonal projection\nonto a subspace or, equivalently, as the best approximation of the\nvariable given the information available.\n\n::: prop\n[]{#prop:properties-of-conditional-expectation\nlabel=\"prop:properties-of-conditional-expectation\"}Let $Y$ be an\nintegrable random variable on $(\\Omega,\\mathcal{F},\\mathbb{P})$. Let\n$\\mathcal{G}\\subseteq\\mathcal{F}$ be another sigma-field of $\\Omega$.\nThen, the conditional expectation $\\mathbf{E}[Y|\\mathcal{G}]$ has the\nfollowing properties:\n\n\\(1\\) If $Y$ is $\\mathcal{G}$-measurable, then :\n\n$$\\begin{aligned}\n\\mathbf{E}[Y|\\mathcal{G}] & =Y\n\\end{aligned}$$\n\n\\(2\\) Taking out what is known. More generally, if $Y$ is\n$\\mathcal{G-}$measurable and $X$ is another integrable random variable\n(with $XY$ also integrable), then :\n\n$$\\begin{aligned}\n\\mathbf{E}[XY|\\mathcal{G}] & =Y\\mathbf{E}[X|\\mathcal{G}]\n\\end{aligned}$$\n\nThis makes sense, since $Y$ is determined by $\\mathcal{G}$, so we can\ntake out what is known; it can be treated as a constant for the\nconditional expectation.\n\n\\(3\\) Independence. If $Y$ is independent of $\\mathcal{G}$, that is, for\nany events $\\{Y\\in(a,b]\\}$ and $A\\in\\mathcal{G}$:\n\n$$\\begin{aligned}\n\\mathbb{P}(\\{Y\\in I\\}\\cap A) & =\\mathbb{P}(\\{Y\\in I\\})\\cdot\\mathbb{P}(A)\n\\end{aligned}$$\n\nthen\n\n$$\\begin{aligned}\n\\mathbf{E}[Y|\\mathcal{G}] & =\\mathbf{E}[Y]\n\\end{aligned}$$\n\nIn other words, if you have no information on $Y$, your best guess for\nits value is simply plain expectation.\n\n\\(4\\) Linearity of conditional expectations. Let $X$ be another\nintegrable random variable on $(\\Omega,\\mathcal{F},\\mathbb{P})$. Then,\n\n$$\\begin{aligned}\n\\mathbf{E}[aX+bY|\\mathcal{G}] & =a\\mathbf{E}[X|\\mathcal{G}]+b\\mathbf{E}[Y|\\mathcal{G}],\\quad\\text{for any }a,b\\in\\mathbf{R}\n\\end{aligned}$$\n\nThe linearity justifies the cumbersom choice of notation\n$\\mathbf{E}[Y|\\mathcal{G}]$ for the random variable.\n\n\\(5\\) Tower Property : If $\\mathcal{H}\\subseteq\\mathcal{G}$ is another\nsigma-field of $\\Omega$, then:\n\n$$\\begin{aligned}\n\\mathbf{E}[Y|\\mathcal{H}] & =\\mathbf{E}[\\mathbf{E}[Y|\\mathcal{G}]|\\mathcal{H}]\n\\end{aligned}$$\n\nThink in terms of two successive projections: first on a plane, then on\na line in the plane.\n\n\\(6\\) Pythagoras Theorem. We have:\n\n$$\\begin{aligned}\n\\mathbf{E}[Y^{2}] & =\\mathbf{E}\\left[\\left(\\mathbf{E}[Y|\\mathcal{G}]\\right)^{2}\\right]+\\mathbf{E}\\left[\\left(Y-\\mathbf{E}[Y|\\mathcal{G}]\\right)^{2}\\right]\n\\end{aligned}$$\n\nIn particular:\n\n$$\\begin{aligned}\n\\mathbf{E}\\left[\\left(\\mathbf{E}\\left[Y|\\mathcal{G}\\right]\\right)^{2}\\right] & \\leq\\mathbf{E}[Y^{2}]\n\\end{aligned}$$\n\nIn words, the $L^{2}$ norm of $\\mathbf{E}[X|\\mathcal{G}]$ is smaller\nthan the one of $X$, which is clear if you think in terms of orthogonal\nprojection.\n\n\\(7\\) Expectation of the conditional expectation.\n\n$$\\begin{aligned}\n\\mathbf{E}\\left[\\mathbf{E}[Y|\\mathcal{G}]\\right] & =\\mathbf{E}[Y]\n\\end{aligned}$$\n:::\n\n*Proof.*\n\nThe uniqueness property of conditional expectations in theorem\n([\\[th:existence-and-uniqueness-of-conditional-expectations-II\\]](#th:existence-and-uniqueness-of-conditional-expectations-II){reference-type=\"ref\"\nreference=\"th:existence-and-uniqueness-of-conditional-expectations-II\"})\nmight appear to be an academic curiosity. On the contrary, it is very\npractical, since it ensures, that if we find a candidate for the\nconditional expectation that has the two properties in Definition\n([\\[def:conditional-expectation\\]](#def:conditional-expectation){reference-type=\"ref\"\nreference=\"def:conditional-expectation\"}), then it must be *the*\nconditional expectation. To see this, let's prove property (1).\n\n::: claim\nIf $Y$ is $\\mathcal{G}$-measurable, then $\\mathbf{E}[Y|\\mathcal{G}]=Y$.\n\nIt suffices to show that $Y$ has the two defining properties of\nconditional expectation.\n\n\\(1\\) We are given that, $Y$ is $\\mathcal{G}$-measurable. So, property\n(A) is satisfied.\n\n\\(2\\) For any bounded random variable $W$ that is\n$\\mathcal{G}$-measurable, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[W(Y-Y)] & =\\mathbf{E}[0]=0\n\\end{aligned}$$\n\nSo, property (B) is also a triviality.\n:::\n\n::: claim\n(Taking out what is known.) If $Y$ is $\\mathcal{G}$-measurable and $X$\nis another integrable random variable, then:\n\n$$\\begin{aligned}\n\\mathbf{E}[XY|\\mathcal{G}] & =Y\\mathbf{E}[X|\\mathcal{G}]\n\\end{aligned}$$\n\nIn a similar vein, it suffices to show that,\n$Y\\mathbf{E}[X|\\mathcal{G}]$ has the two defining properties of\nconditional expectation.\n\n\\(1\\) We are given that $Y$ is $\\mathcal{G}$-measurable; from property\n(1), $\\mathbf{E}[X|\\mathcal{G}]$ is $\\mathcal{G}$-measurable. It follows\nthat, $Y\\mathbf{E}[X|\\mathcal{G}]$ is $\\mathcal{G}$-measurable.\n\n\\(2\\) From theorem\n([\\[th:existence-and-uniqueness-of-conditional-expectations-II\\]](#th:existence-and-uniqueness-of-conditional-expectations-II){reference-type=\"ref\"\nreference=\"th:existence-and-uniqueness-of-conditional-expectations-II\"}),\n$X-\\mathbf{E}[X|\\mathcal{G}]$ is orthogonal to the random variables\n$L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$. So, if $W$ is any bounded\n$\\mathcal{G}$-measurable random variable, it follows that:\n\n$$\\begin{aligned}\n\\mathbf{E}[WY(X-\\mathbf{E}[X|\\mathcal{G}])] & =0\\\\\n\\implies\\mathbf{E}[W\\cdot XY] & =\\mathbf{E}[WY\\mathbf{E}[X|\\mathcal{G}]]\n\\end{aligned}$$\n\nThis closes the proof.\n:::\n\n::: claim\n(Independence.) If $Y$ is independent of $\\mathcal{G}$, that is, for all\nevents $\\{Y\\in(a,b]\\}$ and $A\\in\\mathcal{G}$,\n\n$$\\begin{aligned}\n\\mathbf{\\mathbb{P}}\\{Y\\in(a,b]\\cap A\\} & =\\mathbb{P}\\{Y\\in(a,b]\\}\\cdot\\mathbb{P}(A)\n\\end{aligned}$$\n\nthen\n\n$$\\begin{aligned}\n\\mathbf{E}[Y|\\mathcal{G}] & =\\mathbf{E}[Y]\n\\end{aligned}$$\n\nLet us show that $\\mathbf{E}[Y]$ has the two defining properties of\nconditional expectations.\n\n\\(1\\) $\\mathbf{E}[Y]$ is a constant and so it is $\\mathcal{F}_{0}$\nmeasurable. Hence, it is $\\mathcal{G}$ measurable.\n\n\\(2\\) If $W$ is another $\\mathcal{G}$-measurable random variable,\n\n$$\\begin{aligned}\n\\mathbf{E}[WY] & =\\mathbf{E}[W]\\cdot\\mathbf{E}[Y]\n\\end{aligned}$$\n\nsince $Y$ is independent of $\\mathcal{G}$ and therefore it is\nindependent of $Y$. Hence,\n\n$$\\begin{aligned}\n\\mathbf{E}[W(Y-\\mathbf{E}[Y])] & =0\n\\end{aligned}$$\n\nConsequently, $\\mathbf{E}[Y|\\mathcal{G}]=\\mathbf{E}[Y]$.\n:::\n\n::: claim\n(Linearity of conditional expectations) Let $X$ be another integrable\nrandom variable on $(\\Omega,\\mathcal{F},\\mathbb{P})$. Then,\n\n$$\\begin{aligned}\n\\mathbf{E}[aX+bY|\\mathcal{G}] & =a\\mathbf{E}[X|\\mathcal{G}]+b\\mathbf{E}[Y|\\mathcal{G}],\\quad\\text{for any }a,b\\in\\mathbf{R}\n\\end{aligned}$$\n:::\n\nSince $\\mathbf{E}[X|\\mathcal{G}]$ and $\\mathbf{E}[Y|\\mathcal{G}]$ are\n$\\mathcal{G}-$measurable, any linear combination of these two random\nvariables is also $\\mathcal{G}$-measurable.\n\nAlso, if $W$ is any bounded $\\mathcal{G}-$measurable random variable, we\nhave:\n\n$$\\begin{aligned}\n\\mathbf{E}[W(aX+bY-(a\\mathbf{E}[X|\\mathcal{G}]+b\\mathbf{E}[Y|\\mathcal{G}]))] & =a\\mathbf{E}[W(X-\\mathbf{E}[X|\\mathcal{G}])]\\\\\n & +b\\mathbf{E}[W(Y-\\mathbf{E}[Y|\\mathcal{G}])]\n\\end{aligned}$$\n\nBy definition, $X-\\mathbf{E}(X|\\mathcal{G})$ is orthogonal t o the\nsubspace $L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$ and hence to all\n$\\mathcal{G}$-measurable random-variables. Hence, the two expectations\non the right hand side of the above expression are $0$. Since,\nconditional expectations are unique, we have the desired result.\n\n::: claim\nIf $\\mathcal{H}\\subseteq\\mathcal{G}$ is another sigma-field of $\\Omega$,\nthen\n\n$$\\begin{aligned}\n\\mathbf{E}[Y|\\mathcal{H}] & =\\mathbf{E}[\\mathbf{E}[Y|\\mathcal{G}]|\\mathcal{H}]\n\\end{aligned}$$\n\nDefine $U:=\\mathbf{E}[Y|\\mathcal{G}]$. By definition,\n$\\mathbf{E}[U|\\mathcal{H}]$ is $\\mathcal{H}$-measurable.\n\nLet $W$ be any bounded $\\mathcal{H}$-measurable random variable. We\nhave:\n\n$$\\begin{aligned}\n\\mathbf{E}[W\\{\\mathbf{E}(Y|\\mathcal{G})-\\mathbf{E}(\\mathbf{E}(Y|\\mathcal{G})|\\mathcal{H})\\}] & =\\mathbf{E}[W(U-\\mathbf{E}(U|\\mathcal{H})]\n\\end{aligned}$$\n\nBut, by definition $U-\\mathbf{E}(U|\\mathcal{H})$ is always orthogonal to\nthe subspace $L^{2}(\\Omega,\\mathcal{H},\\mathbb{P})$ and hence,\n$\\mathbf{E}[W(U-\\mathbf{\\mathbf{E}}(U|\\mathcal{H})]=0$. Since,\nconditional expectations are unique, we have the desired result.\n:::\n\n::: claim\n**Pythagoras's theorem**. We have:\n\n$$\\begin{aligned}\n\\mathbf{E}[Y^{2}] & =\\mathbf{E}[(\\mathbf{E}[Y|\\mathcal{G}])^{2}]+\\mathbf{E}[(Y-\\mathbf{E}(Y|\\mathcal{G}))^{2}]\n\\end{aligned}$$\n\nIn particular,\n\n$$\\begin{aligned}\n\\mathbf{E}[(\\mathbf{E}[Y|\\mathcal{G}])^{2}] & \\leq\\mathbf{E}[Y^{2}]\n\\end{aligned}$$\n\nConsider the orthogonal decomposition:\n\n$$\\begin{aligned}\nY & =\\mathbf{E}[Y|\\mathcal{G}]+(Y-\\mathbf{E}[Y|\\mathcal{G}])\n\\end{aligned}$$\n\nSquaring on both sides and taking expectations, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[Y^{2}] & =\\mathbf{E}[(\\mathbf{E}(Y|\\mathcal{G}))^{2}]+\\mathbf{E}[(Y-\\mathbf{E}[Y|\\mathcal{G}])^{2}]+2\\mathbf{E}\\left[\\mathbf{E}[Y|\\mathcal{G}](Y-\\mathbf{E}[Y|\\mathcal{G}])\\right]\n\\end{aligned}$$\n\nBy definition of conditional expectation,\n$(Y-\\mathbf{E}[Y|\\mathcal{G}])$ is orthogonal to the subspace\n$L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$. By the properties of conditional\nexpectation, $\\mathbf{E}[Y|\\mathcal{G}]$ is $\\mathcal{G}-$measurable, so\nit belongs to $L^{2}(\\Omega,\\mathcal{G},\\mathbb{P})$. Hence, the\ndot-product on the right-hand side is $0$. Consequently, we have the\ndesired result.\n\nMoreover, since $(Y-\\mathbf{E}[Y|\\mathcal{G}])^{2}$ is a non-negative\nrandom variable, $\\mathbf{E}[(Y-\\mathbf{E}[Y|\\mathcal{G}])^{2}]\\geq0$.\nIt follows that:\n$\\mathbf{E}[Y^{2}]\\geq\\mathbf{E}[(\\mathbf{E}(Y|\\mathcal{G}))^{2}]$.\n:::\n\n::: claim\nOur claim is:\n\n$$\\begin{aligned}\n\\mathbf{E}\\left[\\mathbf{E}[Y|\\mathcal{G}]\\right] & =\\mathbf{E}[Y]\n\\end{aligned}$$\n\nWe know that, if $W$ is any bounded $\\mathcal{G}$-measurable random\nvariable:\n\n$$\\begin{aligned}\n\\mathbf{E}\\left[WY\\right] & =\\mathbf{E}[W\\mathbf{E}[Y|\\mathcal{G}]]\n\\end{aligned}$$\n\nTaking $W=1$, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}\\left[Y\\right] & =\\mathbf{E}[\\mathbf{E}[Y|\\mathcal{G}]]\n\\end{aligned}$$\n:::\n\n::: example\n(Brownian Conditioning II). We continue the example\n([\\[ex:brownian-conditioning-I\\]](#ex:brownian-conditioning-I){reference-type=\"ref\"\nreference=\"ex:brownian-conditioning-I\"}). Let's now compute the\nconditional expectations $\\mathbf{E}[e^{aB_{1}}|B_{1/2}]$ and\n$\\mathbf{E}[e^{aB_{1/2}}|B_{1}]$ for some parameter $a$. We shall need\nthe properties of conditional expectation in proposition\n([\\[prop:properties-of-conditional-expectation\\]](#prop:properties-of-conditional-expectation){reference-type=\"ref\"\nreference=\"prop:properties-of-conditional-expectation\"}). For the first\none we use the fact that $B_{1/2}$ is independent of **$B_{1}-B_{1/2}$**\nto get:\n\n$$\\begin{aligned}\n\\mathbf{E}[e^{aB_{1}}|B_{1/2}] & =\\mathbf{E}[e^{a((B_{1}-B_{1/2})+B_{1/2})}|B_{1/2}]\\\\\n & =\\mathbf{E}[e^{a(B_{1}-B_{2})}\\cdot e^{aB_{1/2}}|B_{1/2}]\\\\\n & \\quad\\left\\{ \\text{Taking out what is known}\\right\\} \\\\\n & =e^{aB_{1/2}}\\mathbf{E}[e^{a(B_{1}-B_{1/2})}|B_{1/2}]\\\\\n & =e^{aB_{1/2}}\\cdot\\mathbf{E}[e^{a(B_{1}-B_{1/2})}]\\\\\n & \\quad\\{\\text{Independence}\\}\n\\end{aligned}$$\n\nWe know that, $a(B_{1}-B_{1/2})$ is a gaussian random variable with mean\n$0$ and variance $a^{2}/2$. We also know that,\n$\\mathbf{E}[e^{tZ}]=e^{t^{2}/2}$. So,\n$\\mathbf{E}[e^{a(B_{1}-B_{1/2})}]=e^{a^{2}/4}$. Consequently,\n$\\mathbf{E}[e^{aB_{1}}|B_{1/2}]=e^{aB_{1/2}+a^{2}/4}$.\n\nThe result itself has the form of the MGF of a Gaussian with mean\n$B_{1/2}$ and variance $1/2$. (The MGF of $X=\\mu+\\sigma Z$, $Z=N(0,1)$\nis $M_{X}(a)=\\exp\\left[\\mu+\\frac{1}{2}\\sigma^{2}a^{2}\\right]$.) In fact,\nthis shows that the conditional distribution of $B_{1}$ given $B_{1/2}$\nis Gaussian of mean $B_{1/2}$ and variance $1/2$.\n\nFor the other expectation, note that $B_{1/2}-\\frac{1}{2}B_{1}$ is\nindependent of $B_{1}$. We have: $$\\begin{aligned}\n\\mathbf{E}\\left[\\left(B_{1/2}-\\frac{1}{2}B_{1}\\right)B_{1}\\right] & =\\mathbf{E}(B_{1/2}B_{1})-\\frac{1}{2}\\mathbf{E}[B_{1}^{2}]\\\\\n & =\\frac{1}{2}-\\frac{1}{2}\\cdot1\\\\\n & =0\n\\end{aligned}$$\n\nTherefore, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[e^{aB_{1/2}}|B_{1}] & =\\mathbf{E}[e^{a(B_{1/2}-\\frac{1}{2}B_{1})+\\frac{a}{2}B_{1}}|B_{1}]\\\\\n & =\\mathbf{E}[e^{a(B_{1/2}-\\frac{1}{2}B_{1})}\\cdot e^{\\frac{a}{2}B_{1}}|B_{1}]\\\\\n & =e^{\\frac{a}{2}B_{1}}\\mathbf{E}[e^{a(B_{1/2}-\\frac{1}{2}B_{1})}|B_{1}]\\\\\n & \\quad\\{\\text{Taking out what is known }\\}\\\\\n & =e^{\\frac{a}{2}B_{1}}\\mathbf{E}[e^{a(B_{1/2}-\\frac{1}{2}B_{1})}]\\\\\n & \\quad\\{\\text{Independence}\\}\n\\end{aligned}$$\n\nNow, $a(B_{1/2}-\\frac{1}{2}B_{1})$ is a random variable with mean $0$\nand variance $a^{2}(\\frac{1}{2}-\\frac{1}{4})=\\frac{a^{2}}{4}$.\nConsequently, $\\mathbf{E}[e^{(a/2)Z}]=e^{\\frac{a^{2}}{8}}$. Thus,\n$\\mathbf{E}[e^{aB_{1/2}}|B_{1}]=e^{\\frac{a}{2}B_{1}+\\frac{a^{2}}{8}}$.\n:::\n\n::: example\n(Brownian bridge is conditioned Brownian motion). We know that the\nBrownian bridge $M_{t}=B_{t}-tB_{1}$, $t\\in[0,1]$ is independent of\n$B_{1}$. We use this to show that the conditional distribution of the\nBrownian motion given the value at the end-point $B_{1}$ is the one of a\nBrownian bridge shifted by the straight line going from $0$ to $B_{1}$.\nTo see this, we compute the conditional MGF of\n$(B_{t_{1}},B_{t_{2}},\\ldots,B_{t_{n}})$ given $B_{1}$ for some\narbitrary choices of $t_{1},t_{2},\\ldots,t_{n}$ in $[0,1]$. We get the\nfollowing by adding and subtracting $t_{j}B_{1}$:\n\n$$\\begin{aligned}\n\\mathbf{E}[e^{a_{1}B_{t_{1}}+\\ldots+a_{n}B_{t_{n}}}|B_{1}] & =\\mathbf{E}[e^{a_{1}(B_{t_{1}}-t_{1}B_{1})+\\ldots+a_{n}(B_{t_{n}}-t_{n}B_{1})}\\cdot e^{(a_{1}t_{1}B_{1}+\\ldots+a_{n}t_{n}B_{1})}|B_{1}]\\\\\n & =e^{(a_{1}t_{1}B_{1}+\\ldots+a_{n}t_{n}B_{1})}\\mathbf{E}[e^{a_{1}M_{t_{1}}+\\ldots+a_{n}M_{t_{n}}}|B_{1}]\\\\\n & \\quad\\{\\text{Taking out what is known}\\}\\\\\n & =e^{(a_{1}t_{1}B_{1}+\\ldots+a_{n}t_{n}B_{1})}\\mathbf{E}[e^{a_{1}M_{t_{1}}+\\ldots+a_{n}M_{t_{n}}}]\\\\\n & \\quad\\{\\text{Independence}\\}\n\\end{aligned}$$\n\nThe right side is exactly the MGF of the process\n$M_{t}+tB_{1},t\\in[0,1]$ (for a fixed value $B_{1})$, where\n$(M_{t},t\\in[0,1])$ is a Brownian bridge. This proves the claim.\n:::\n\n::: lem\n(Conditional Jensen's Inequality) If $c$ is a convex function on\n$\\mathbf{R}$ and $X$ is a random variable on\n$(\\Omega,\\mathcal{F},\\mathbb{P})$, then:\n\n$$\\begin{aligned}\n\\mathbf{E}[c(X)] & \\geq c(\\mathbf{E}[X])\n\\end{aligned}$$\n\nMore generally, if $\\mathcal{G}\\subseteq\\mathcal{F}$ is a sigma-field,\nthen:\n\n$$\\mathbf{E}[c(X)|\\mathcal{G}]\\geq c(\\mathbf{E}[X|\\mathcal{G}])$$\n:::\n\n::: proof\n*Proof.* We know that, if $c(x)$ is a convex function, the tangent to\nthe curve $c$ at any point lies below the curve. The tangent to the cuve\nat this point, is a straight-line of the form:\n\n$$\\begin{aligned}\nc(t)=y & =mt+c\n\\end{aligned}$$\n\nwhere $m(t)=c'(t)$. This holds for all $t\\in\\mathbf{R}$. At an arbitrary\npoint $x$ we have:\n\n$$\\begin{aligned}\nc(x)\\geq & y=mx+c\n\\end{aligned}$$\n\nTherefore, we have:\n\n$$\\begin{aligned}\nc(x)-c(t) & \\geq m(t)(x-t)\n\\end{aligned}$$\n\nfor any $x$ and any point of tangency $t$.\n\n$$\\begin{aligned}\nc(X)-c(Y) & \\geq m(Y)(X-Y)\n\\end{aligned}$$\n\nSubstituting $Y=\\mathbf{E}[X|\\mathcal{G}]$, we get:\n\n$$\\begin{aligned}\nc(X)-c(\\mathbf{E}[X|\\mathcal{G}]) & \\geq m(\\mathbf{E}[X|\\mathcal{G}])(X-\\mathbf{E}[X|\\mathcal{G}])\n\\end{aligned}$$\n\nTaking expectations on both sides, we get:\n\n$$\\begin{aligned}\n\\mathbf{E}[(c(X)-c(\\mathbf{E}[X|\\mathcal{G}]))|\\mathcal{G}] & \\geq\\mathbf{E}[m(\\mathbf{E}[X|\\mathcal{G}])(X-\\mathbf{E}[X|\\mathcal{G}])|\\mathcal{G}]\n\\end{aligned}$$\n\nThe left-hand side simplifies as:\n\n$$\\begin{aligned}\n\\mathbf{E}[(c(X)-c(\\mathbf{E}[X|\\mathcal{G}]))|\\mathcal{G}] & =\\mathbf{E}[c(X)|\\mathcal{G}]-\\mathbf{E}[c(\\mathbf{E}[X|\\mathcal{G}]))|\\mathcal{G}]\\\\\n & \\quad\\{\\text{Linearity}\\}\\\\\n & =\\mathbf{E}[c(X)|\\mathcal{G}]-c(\\mathbf{E}[X|\\mathcal{G}])\\\\\n & \\quad\\{\\text{c(\\ensuremath{\\mathbf{E}}[X|\\ensuremath{\\mathcal{G}}])) is \\ensuremath{\\mathcal{G}}-measurable}\\}\n\\end{aligned}$$\n\nOn the right hand side, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[m(\\mathbf{E}[X|\\mathcal{G}])(X-\\mathbf{E}[X|\\mathcal{G}])|\\mathcal{G}] & =\\mathbf{E}[m(\\mathbf{E}[X|\\mathcal{G}])\\cdot X|\\mathcal{G}]-\\mathbf{E}[m(\\mathbf{E}[X|\\mathcal{G}])\\cdot\\mathbf{E}[X|\\mathcal{G}]|\\mathcal{G}]\\\\\n & =\\mathbf{E}[X|\\mathcal{G}]m(\\mathbf{E}[X|\\mathcal{G}])-m(\\mathbf{E}[X|\\mathcal{G}])\\cdot\\mathbf{E}[X|\\mathcal{G}]\\\\\n & =0\n\\end{aligned}$$\n\nConsequently, it follows that\n$\\mathbf{E}[c(X)|\\mathcal{G}]\\geq c(\\mathbf{E}[X|\\mathcal{G}])$. ◻\n:::\n\n::: example\n(Embeddings of $L^{p}$ spaces) Square-integrable random variables are in\nfact integrable. In other words, there is always the inclusion\n$L^{2}(\\Omega,\\mathcal{F},\\mathbb{P})\\subseteq L^{1}(\\Omega,\\mathcal{F},\\mathbb{P})$.\nIn particular, square integrable random variables always have a\nwell-defined variance. This embedding is a simple consequence of\nJensen's inequality since:\n\n$$\\begin{aligned}\n|\\mathbf{E}[X]|^{2} & \\leq\\mathbf{E}[|X|^{2}]\n\\end{aligned}$$\n\nas $f(x)=|x|^{2}$ is convex. By taking the square root on both sides, we\nget:\n\n$$\\begin{aligned}\n\\left\\Vert X\\right\\Vert _{1} & \\leq\\left\\Vert X\\right\\Vert _{2}\n\\end{aligned}$$\n\nMore generally, for any $1<p<\\infty$, we can define\n$L^{p}(\\Omega,\\mathcal{F},\\mathbb{P})$ to be the linear space of random\nvariables such that $\\mathbf{E}[|X|^{p}]<\\infty$. Then for $p<q$, since\n$x^{q/p}$ is convex, we get by Jensen's inequality :\n\n$$\\begin{aligned}\n\\mathbf{E}[|X|^{q}] & =\\mathbf{E}[(|X|^{p})^{\\frac{q}{p}}]\\geq\\left(\\mathbf{E}[|X|^{p}]\\right)^{\\frac{q}{p}}\n\\end{aligned}$$\n\nTaking the $q$-th root on both sides:\n\n$$\\begin{aligned}\n\\mathbf{E}[|X|^{p}]^{1/p} & \\leq\\mathbf{E}[|X|^{q}]^{1/q}\n\\end{aligned}$$\n\nSo, if $X\\in L^{q}$, then it must also be in $L^{p}$. Concretely, this\nmeans that any random variable with a finite $q$-moment will also have a\nfinite $p$-moment, for $q>p$.\n:::\n\n## Martingales.\n\nWe now have all the tools to define martingales.\n\n::: defn\n[]{#def:Filtration label=\"def:Filtration\"}(Filtration). A *filtration*\n$(\\mathcal{F}_{t}:t\\geq0)$ of $\\Omega$ is an increasing sequence of\n$\\sigma$-fields of $\\Omega$. That is,\n\n$$\\begin{aligned}\n\\mathcal{F}_{s} & \\subseteq\\mathcal{F}_{t},\\quad\\forall s\\leq t\n\\end{aligned}$$\n\nWe will usually take $\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$. The\ncanonical example of a filtration is the natural filtration of a given\nprocess $(M_{s}:s\\geq0)$. This is the filtration given by\n$\\mathcal{F}_{t}=\\sigma(M_{s},s\\leq t)$. The inclusions of the\n$\\sigma$-fields are then clear. For a given Brownian motion\n$(B_{t},t\\geq0)$, the filtration $\\mathcal{F}_{t}=\\sigma(B_{s},s\\leq t)$\nis sometimes called the *Brownian filtration*. We think of the\nfiltration as the *flow of information of the process*.\n:::\n\n::: defn\n[]{#def:Adapted-process label=\"def:Adapted-process\"}A stochastic process\n$(X_{t}:t\\geq0)$ is said to be adapted to $(\\mathcal{F}_{t}:t\\geq0)$, if\nfor each $t$, the random variable $X_{t}$ is\n$\\mathcal{F}_{t}-$measurable.\n:::\n\n::: defn\n(Martingale). A process $(M_{t}:t\\geq0)$ is a martingale for the\nfiltration $(\\mathcal{F}_{t}:t\\geq0)$ if the following hold:\n\n\\(1\\) The process is *adapted*, that is $M_{t}$ is\n$\\mathcal{F}_{t}-$measurable for all $t\\geq0$.\n\n\\(2\\) $\\mathbf{E}[|M_{t}|]<\\infty$ for all $t\\geq0$. (This ensures that\nthe conditional expectation is well defined.)\n\n\\(3\\) *Martingale property:*\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{t}|\\mathcal{F}_{s}] & =M_{s}\\quad\\forall s\\leq t\n\\end{aligned}$$\n\nRoughly, speaking this means that the best approximation of a process at\na future time $t$ is its value at the present.\n:::\n\nIn particular, the martingale property implies that:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{t}|\\mathcal{F}_{0}] & =M_{0}\\nonumber \\\\\n\\mathbf{E}[\\mathbf{E}[M_{t}|\\mathcal{F}_{0}]] & =\\mathbf{E}[M_{0}]\\nonumber \\\\\n\\mathbf{E}[M_{t}] & =\\mathbf{E}[M_{0}]\\label{eq:expected-value-of-martingale-at-any-time-is-constant}\\\\\n & \\quad\\{\\text{Tower Property}\\}\\nonumber \n\\end{aligned}$$\n\nUsually, we take $\\mathcal{F}_{0}$ to be the trivial sigma-field\n$\\{\\emptyset,\\Omega\\}$. A random variable that is\n$\\mathcal{F}_{0}$-measurable must be a constant, so $M_{0}$ is a\nconstant. In this case, $\\mathbf{E}[M_{t}]=M_{0}$ for all $t$. If\nproperties (1) and (2) are satisfied, but the best approximation is\nlarger, $\\mathbf{E}[M_{t}|\\mathcal{F}_{s}]\\geq M_{s}$, the process is\ncalled a *submartingale*. If it is smaller on average,\n$\\mathbf{E}[M_{t}|\\mathcal{F}_{s}]\\leq\\mathbf{E}[M_{s}]$, we say it is a\nsupermartingale.\n\nWe will be mostly interested in martingales that are continuous and\nsquare-integrable. Continuous martingales are martingales whose paths\n$t\\mapsto M_{t}(\\omega)$ are continuous almost surely. Square-integrable\nmartingales are such that $\\mathbf{E}[|M_{t}|^{2}]<\\infty$ for all\n$t$'s. This condition is stronger than $\\mathbf{E}[|M_{t}|]<\\infty$ due\nto Jensen's inequality.\n\n::: rem*\n(Martingales in Discrete-time). Martingales can be defined the same way\nif the index set of the process is discrete. For example, the filtration\n$(\\mathcal{F}_{n}:n\\in\\mathbf{N})$ is a countable set and the martingale\nproperty is then replaced by $\\mathbf{E}[M_{n+1}|\\mathcal{F}_{n}]=M_{n}$\nas expected. The tower-property then yields the martingale property\n$\\mathbf{E}[M_{n+k}|\\mathcal{F}_{n}]=M_{n}$ for $k\\geq1$.\n:::\n\n::: rem*\n(Continuous Filtrations). Filtrations with continuous time can be tricky\nto handle rigorously. For example, one has to make sense of what it\nmeans for $\\mathcal{F}_{s}$ as $s$ approaches $t$ from the left. Is it\nequal to $\\mathcal{F}_{t}$? Or is there actually less information in\n$\\lim_{s\\to t^{-}}\\mathcal{F}_{s}$ than in $\\mathcal{F}_{t}$? This is a\nbit of headache when dealing with processes with jumps, like the Poisson\nprocess. However, if the paths are continuous, the technical problems\nare not as heavy.\n\nLet's look at some of the important examples of martingales constructed\nfrom Brownian Motion.\n:::\n\n::: example\n(Examples of Brownian Martingales)\n\n\\(i\\) *Standard Brownian Motion.* Let $(B_{t}:t\\geq0)$ be a standard\nBrownian motion and let $(\\mathcal{F}_{t}:t\\geq0)$ be a *Brownian\nfiltration*. Then $(B_{t}:t\\geq0)$ is a square integrable martingale for\nthe filtration $(\\mathcal{F}_{t}:t\\geq0)$. Property (1) is obvious,\nbecause all the sets in $\\mathcal{F}_{t}$ are resolved, upon observing\nthe outcome of $B_{t}$. Similarly, $\\mathbf{E}[|B_{t}|]=0$. As for the\nmartingale property, note that, by the properties of conditional\nexpectation in proposition\n([\\[prop:properties-of-conditional-expectation\\]](#prop:properties-of-conditional-expectation){reference-type=\"ref\"\nreference=\"prop:properties-of-conditional-expectation\"}), we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[B_{t}|\\mathcal{F}_{s}] & =\\mathbf{E}[B_{t}|B_{s}]\\\\\n & =\\mathbf{E}[B_{t}-B_{s}+B_{s}|B_{s}]\\\\\n & =\\mathbf{E}[B_{t}-B_{s}|B_{s}]+\\mathbf{E}[B_{s}|B_{s}]\\\\\n & \\quad\\{\\text{Linearity}\\}\\\\\n & =\\mathbf{E}[B_{t}-B_{s}]+B_{s}\\\\\n & \\quad\\{\\text{Independence}\\}\\\\\n & =B_{s}\n\\end{aligned}$$\n\n\\(ii\\) *Geometric Brownian Motion.* Let $(B_{t},t\\ge0)$ be a standard\nbrownian motion, and $\\mathcal{F}_{t}=\\sigma(B_{s},s\\leq t)$. A\n*geometric brownian motion* is a process $(S_{t},t\\geq0)$ defined by:\n\n$$\\begin{aligned}\nS_{t} & =S_{0}\\exp\\left(\\sigma B_{t}+\\mu t\\right)\n\\end{aligned}$$\n\nfor some parameter $\\sigma>0$ and $\\mu\\in\\mathbf{R}$. This is simply the\nexponential of the Brownian motion with drift. This is not a martingale\nfor most choices of $\\mu$! In fact, one must take\n\n$$\\begin{aligned}\n\\mu & =-\\frac{1}{2}\\sigma^{2}\n\\end{aligned}$$ for the process to be a martingale for the Brownian\nfiltration. Let's verify this. Property (1) is obvious since $S_{t}$ is\na function of $B_{t}$ for each $t$. So, it is $\\mathcal{F}_{t}$\nmeasurable. Moreover, property (2) is clear:\n$\\mathbf{E}[\\exp(\\sigma B_{t}+\\mu t)]=\\mathbf{E}[\\exp(\\sigma\\sqrt{t}Z+\\mu t)]=\\exp(\\mu t+\\frac{1}{2}\\sigma^{2}t)$.\nSo, its a finite quantity. As for the martingale property, note that by\nthe properties of conditional expectation, and the MGF of Gaussians, we\nhave for $s\\leq t$:\n\n$$\\begin{aligned}\n\\mathbf{E}[S_{t}|\\mathcal{F}_{s}] & =\\mathbf{E}\\left[S_{0}\\exp\\left(\\sigma B_{t}-\\frac{1}{2}\\sigma^{2}t\\right)|\\mathcal{F}_{s}\\right]\\\\\n & =S_{0}\\exp(-\\frac{1}{2}\\sigma^{2}t)\\mathbf{E}[\\exp(\\sigma(B_{t}-B_{s}+B_{s}))|\\mathcal{F}_{s}]\\\\\n & =S_{0}\\exp(-\\frac{1}{2}\\sigma^{2}t)\\exp(\\sigma B_{s})\\mathbf{E}[\\exp(\\sigma(B_{t}-B_{s}))|\\mathcal{F}_{s}]\\\\\n & \\quad\\{\\text{Taking out what is known}\\}\\\\\n & =S_{0}\\exp\\left(\\sigma B_{s}-\\frac{1}{2}\\sigma^{2}t\\right)\\mathbf{E}\\left[\\exp\\left(\\sigma(B_{t}-B_{s})\\right)\\right]\\\\\n & \\quad\\{\\text{Independence}\\}\\\\\n & =S_{0}\\exp\\left(\\sigma B_{s}-\\frac{1}{2}\\sigma^{2}t+\\frac{1}{2}\\sigma^{2}(t-s)\\right)\\\\\n & =S_{0}\\exp(\\sigma B_{s}-\\frac{1}{2}\\sigma^{2}s)\\\\\n & =S_{s}\n\\end{aligned}$$\n\nWe will sometimes abuse terminology and refer to the martingale case of\ngeometric brownian motion simply as geometric Brownian Motion when the\ncontext is clear.\n\n\\(iii\\) *The square of the Brownian motion, compensated*. It is easy to\ncheck $(B_{t}^{2},t\\geq0)$ is a submartingale by direct computation\nusing increments or by Jensen's inequality:\n$\\mathbf{E}[B_{t}^{2}|\\mathcal{F}_{s}]>(\\mathbf{E}[B_{t}|\\mathcal{F}_{s}])^{2}=B_{s}^{2}$,\n$s<t$. It is nevertheless possible to compensate to get a martingale:\n\n$$\\begin{aligned}\nM_{t} & =B_{t}^{2}-t\n\\end{aligned}$$\n\nIt is an easy exercise to verify that $(M_{t}:t\\geq0)$ is a martingale\nfor the Brownian filtration $(\\mathcal{F}_{t}:t\\geq0)$.\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{t}|\\mathcal{F}_{s}] & =\\mathbf{E}[B_{t}^{2}-t|\\mathcal{F}_{s}]\\\\\n & =\\mathbf{E}[B_{t}^{2}|\\mathcal{F}_{s}]-t\\\\\n & =\\mathbf{E}[(B_{t}-B_{s}+B_{s})^{2}|\\mathcal{F}_{s}]-t\\\\\n & =\\mathbf{E}[(B_{t}-B_{s})^{2}|\\mathcal{F}_{s}]+2\\mathbf{E}[(B_{t}-B_{s})B_{s}|\\mathcal{F}_{s}]+\\mathbf{E}[B_{s}^{2}|\\mathcal{F}_{s}]-t\\\\\n & =\\mathbf{E}[(B_{t}-B_{s})^{2}]+2B_{s}\\mathbf{E}[(B_{t}-B_{s})|\\mathcal{F}_{s}]+B_{s}^{2}-t\\\\\n & =\\mathbf{E}[(B_{t}-B_{s})^{2}]+2B_{s}\\mathbf{E}[(B_{t}-B_{s})]+B_{s}^{2}-t\\\\\n & \\left\\{ \\begin{array}{c}\n\\text{\\ensuremath{(B_{t}-B_{s})} is independent of \\ensuremath{\\mathcal{F}_{s}}}\\\\\n\\text{Also, \\ensuremath{B_{s}} is known at time \\ensuremath{s}}\n\\end{array}\\right\\} \\\\\n & =(t-s)+2B_{s}\\cdot0+B_{s}^{2}-t\\\\\n & =B_{s}^{2}-s\\\\\n & =M_{s}\n\\end{aligned}$$\n:::\n\n::: example\n(Other important martingales).\n\n\\(1\\) *Symmetric random walks.* This is an example of a martingale in\ndiscrete time. Take $(X_{i}:i\\in\\mathbf{N})$ to be IID random variables\nwith $\\mathbf{E}[X_{i}]=0$ and $\\mathbf{E}[|X_{i}|]<\\infty$. Take\n$\\mathcal{F}_{n}=\\sigma(X_{i},i\\leq n)$ and\n\n$$\\begin{aligned}\nS_{n} & =X_{1}+X_{2}+\\ldots+X_{n},\\quad S_{0}=0\n\\end{aligned}$$\n\nFirstly, the information learned by observing the outcomes of\n$X_{1}$,$\\ldots$,$X_{n}$ is enough to completely determine $S_{n}$.\nHence, $S_{n}$ is $\\mathcal{F}_{n}-$measurable.\n\nNext, $$\\begin{aligned}\n|S_{n}| & =\\left|\\sum_{i=1}^{n}X_{i}\\right|\\\\\n & \\leq\\sum_{i=1}^{n}|X_{i}|\n\\end{aligned}$$\n\nConsequently, by the montonocity of expectations, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[|S_{n}|] & \\leq\\sum_{i=1}^{n}\\mathbf{E}[|X_{i}|]<\\infty\n\\end{aligned}$$\n\nThe martingale property is also satisfied. We have:\n\n$$\\begin{aligned}\n\\mathbf{E}[S_{n+1}|\\mathcal{F}_{n}] & =\\mathbf{E}[S_{n}+X_{n+1}|\\mathcal{F}_{n}]\\\\\n & =\\mathbf{E}[S_{n}|\\mathcal{F}_{n}]+\\mathbf{E}[X_{n+1}|\\mathcal{F}_{n}]\\\\\n & =S_{n}+\\mathbf{E}[X_{n+1}]\\\\\n & \\left\\{ \\begin{array}{c}\n\\text{\\ensuremath{S_{n}} is \\ensuremath{\\mathcal{F}_{n}}-measurable}\\\\\n\\text{\\ensuremath{X_{n+1}} is independent of \\ensuremath{\\mathcal{F}_{n}}}\n\\end{array}\\right\\} \\\\\n & =S_{n}+0\\\\\n & =S_{n}\n\\end{aligned}$$\n\n\\(2\\) *Compensated Poisson process*. Let $(N_{t}:t\\geq0)$ be a Poisson\nprocess with rate $\\lambda$ and $\\mathcal{F}_{t}=\\sigma(N_{s},s\\leq t)$.\nThen, $N_{t}$ is a submartingale for its natural filtration. Again,\nproperties (1) and (2) are easily checked. $N_{t}$ is $\\mathcal{F}_{t}$\nmeasurable. Moreover,\n$\\mathbf{E}[|N_{t}|]=\\mathbf{E}[N_{t}]=\\frac{1}{\\lambda t}<\\infty$. The\nsubmartingale property follows by the independence of increments : for\n$s\\leq t$,\n\n$$\\begin{aligned}\n\\mathbf{E}[N_{t}|\\mathcal{F}_{s}] & =\\mathbf{E}[N_{t}-N_{s}+N_{s}|\\mathcal{F}_{s}]\\\\\n & =\\mathbf{E}[N_{t}-N_{s}|\\mathcal{F}_{s}]+\\mathbf{E}[N_{s}|\\mathcal{F}_{s}]\\\\\n & =\\mathbf{E}[N_{t}-N_{s}]+N_{s}\\\\\n & =\\lambda(t-s)+N_{s}\\\\\n & \\left\\{ \\because\\mathbf{E}[N_{t}]=\\lambda t\\right\\} \n\\end{aligned}$$\n\nMore importantly, we get a martingale by slightly modifying the process.\nIndeed, if we subtract $\\lambda t$, we have that the process :\n\n$$\\begin{aligned}\nM_{t} & =N_{t}-\\lambda t\n\\end{aligned}$$\n\nis a martingale. We have:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{t}|\\mathcal{F}_{s}] & =\\mathbf{E}[N_{t}-\\lambda t|\\mathcal{F}_{s}]\\\\\n & =\\lambda t-\\lambda s+N_{s}-\\lambda t\\\\\n & =N_{s}-\\lambda s\\\\\n & =M_{s}\n\\end{aligned}$$\n\nThis is called the *compensated Poisson process*. Let us simulate $10$\npaths of the compensated poisson process on $[0,10]$.\n:::\n\n``` {caption=\"Generating 10 paths of a compensated Poisson process\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generates a sample path of a compensated poisson process \n# with rate : `lambda_` per unit time\n# on the interval [0,T], and subintervals of size `stepSize`.\n\ndef generateCompensatedPoissonPath(lambda_,T,stepSize):\n    N = int(T/stepSize)   \n\n    poissonParam = lambda_ * stepSize   \t \n\n    x = np.random.poisson(lam=poissonParam,size=N)  \n    x = np.concatenate([[0.0], x])\n    N_t = np.cumsum(x)  \n    t = np.linspace(start=0.0,stop=10.0,num=1001)\n\n    M_t = np.subtract(N_t,lambda_ * t)  \n    return M_t\n\n\nt = np.linspace(0,10,1001)\nplt.grid(True)\n\nplt.xlabel(r'Time $t$')\nplt.ylabel(r'Compensated poisson process $M(t)$')\nplt.grid(True)\nplt.title(r'$10$ paths of the compensated Poisson process on $[0,10]$')\n\nfor i in range(10):\n    # Generate a poisson path with rate 1 /sec = 0.01 /millisec\n    n_t = generateCompensatedPoissonPath(lambda_=1.0, T=10, stepSize=0.01)\n    plt.plot(t, n_t)\n\n\nplt.show()\nplt.close()\n```\n\nWe saw in the two examples, that, even though a process is not itself a\nmartingale, we can sometimes *compensate* to obtain a martingale! Ito\nCalculus will greatly extend this perspective. We will have systematic\nrules that show when a function of Brownian motion is a martingale and\nif not, how to modify it to get one.\n\nFor now, we observe that a convex function of a martingale is always a\nsubmartingale by Jensen's inequality.\n\n::: cor\n[]{#corollary:the-convex-function-of-martingale-is-a-submartingale\nlabel=\"corollary:the-convex-function-of-martingale-is-a-submartingale\"}If\n$c$ is a convex function on $\\mathbf{R}$ and $(M_{t}:t\\geq0)$ is a\nmartingale for $(\\mathcal{F}_{t}:t\\geq0)$, then the process\n$(c(M_{t}):t\\geq0)$ is a submartingale for the same filtration, granted\nthat $\\mathbf{E}[|c(M_{t})|]<\\infty$.\n:::\n\n::: proof\n*Proof.* The fact that $c(M_{t})$ is adapted to the filtration is clear\nsince it is an explicit function of $M_{t}$. The integrability is by\nassumption. The submartingale property is checked as follows:\n\n$$\\begin{aligned}\n\\mathbf{E}[c(M_{t})|\\mathcal{F}_{s}] & \\geq c(\\mathbf{E}[M_{t}|\\mathcal{F}_{s}])=c(M_{s})\n\\end{aligned}$$ ◻\n:::\n\n::: rem*\n(The Doob-Meyer Decomposition Theorem). Let $(X_{n}:n\\in\\mathbf{N})$ be\na submartingale with respect to a filtration\n$(\\mathcal{F}_{n}:n\\in\\mathbf{N})$. Define a sequence of random\nvariables $(A_{n}:n\\in\\mathbf{N})$ by $A_{0}=0$ and\n\n$$\\begin{aligned}\nA_{n} & =\\sum_{i=1}^{n}(\\mathbf{E}[X_{i}|\\mathcal{F}_{i-1}]-X_{i-1}),\\quad n\\geq1\n\\end{aligned}$$\n\nNote that $A_{n}$ is $\\mathcal{F}_{n-1}$-measurable. Moreover, since\n$(X_{n}:n\\in\\mathbf{N})$ is a submartingale, we have\n$\\mathbf{E}[X_{i}|\\mathcal{F}_{i-1}]-X_{i-1}\\geq0$ almost surely. Hence,\n$(A_{n}:n\\in\\mathbf{N})$ is an increasing sequence almost surely. Let\n$M_{n}=X_{n}-A_{n}$.\n\nWe have:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{n}|\\mathcal{F}_{n-1}] & =\\mathbf{E}[X_{n}-A_{n}|\\mathcal{F}_{n-1}]\\\\\n & =\\mathbf{E}[X_{n}|\\mathcal{F}_{n-1}]-\\mathbf{E}[A_{n}|\\mathcal{F}_{n-1}]\\\\\n & =\\mathbf{E}[X_{n}|\\mathcal{F}_{n-1}]-\\mathbf{E}\\left[\\left.\\mathbf{E}[X_{n}|\\mathcal{F}_{n-1}]-X_{n-1}+A_{n-1}\\right|\\mathcal{F}_{n-1}\\right]\\\\\n & =\\mathbf{E}[X_{n}|\\mathcal{F}_{n-1}]-\\mathbf{E}[X_{n}|\\mathcal{F}_{n-1}]+\\mathbf{E}[X_{n-1}|\\mathcal{F}_{n-1}]-\\mathbf{E}[A_{n-1}|\\mathcal{F}_{n-1}]\\\\\n & =\\cancel{\\mathbf{E}[X_{n}|\\mathcal{F}_{n-1}]}-\\cancel{\\mathbf{E}[X_{n}|\\mathcal{F}_{n-1}]}+X_{n-1}-A_{n-1}\\\\\n & =M_{n-1}\n\\end{aligned}$$\n\nThus, $(M_{n}:n\\in\\mathbf{N})$ is a martingale. Thus, we have obtained\nthe Doob decomposition:\n\n$$\\begin{aligned}\nX_{n} & =M_{n}+A_{n}\\label{eq:doob-decomposition}\n\\end{aligned}$$\n\nThis decomposition of a submartingale as a sum of a martingale and an\nadapted increasing sequence is unique, if we require that $A_{0}=0$ and\nthat $A_{n}$ is $\\mathcal{F}_{n-1}$-measurable.\n\nFor the continuous-time case, the situation is much more complicated.\nThe analogue of equation\n([\\[eq:doob-decomposition\\]](#eq:doob-decomposition){reference-type=\"ref\"\nreference=\"eq:doob-decomposition\"}) is called the *Doob-Meyer\ndecomposition*. We briefly describe this decomposition and avoid the\ntechnical details. All stochastic processes $X(t)$ are assumed to be\nright-continuous with left-hand limits $X(t-)$.\n\nLet $X(t)$, $a\\leq t\\leq b$ be a submartingale with respect to a\nright-continuous filtration $(\\mathcal{F}_{t}:a\\leq t\\leq b)$. If $X(t)$\nsatisfies certain conditions, then it can be uniquely decomposed as:\n\n$$\\begin{aligned}\nX(t) & =M(t)+C(t),\\quad a\\leq t\\leq b\n\\end{aligned}$$\n\nwhere $M(t)$, $a\\leq t\\leq b$ is a martingale with respect to\n$(\\mathcal{F}_{t};a\\leq t\\leq b)$, $C(t)$ is right-continuous and\nincreasing almost surely with $\\mathbf{E}[C(t)]<\\infty$.\n:::\n\n::: example\n(Square of a Poisson Process). Let $(N_{t}:t\\geq0)$ be a Poisson process\nwith rate $\\lambda$. We consider the compensated process\n$M_{t}=N_{t}-\\lambda t$. By\n([\\[corollary:the-convex-function-of-martingale-is-a-submartingale\\]](#corollary:the-convex-function-of-martingale-is-a-submartingale){reference-type=\"ref\"\nreference=\"corollary:the-convex-function-of-martingale-is-a-submartingale\"}),\nthe process $(M_{t}^{2}:t\\geq0)$ is a submartingale for the filtration\n$(\\mathcal{F}_{t}:t\\geq0)$ of the Poisson process. How should we\ncompensated $M_{t}^{2}$ to get a martingale? A direct computation using\nthe properties of conditional expectation yields:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{t}^{2}|\\mathcal{F}_{s}] & =\\mathbf{E}[(M_{t}-M_{s}+M_{s})^{2}|\\mathcal{F}_{s}]\\\\\n & =\\mathbf{E}[(M_{t}-M_{s})^{2}+2(M_{t}-M_{s})M_{s}+M_{s}^{2}|\\mathcal{F}_{s}]\\\\\n & =\\mathbf{E}[(M_{t}-M_{s})^{2}|\\mathcal{F}_{s}]+2\\mathbf{E}[(M_{t}-M_{s})M_{s}|\\mathcal{F}_{s}]+\\mathbf{E}[M_{s}^{2}|\\mathcal{F}_{s}]\\\\\n & =\\mathbf{E}[(M_{t}-M_{s})^{2}]+2M_{s}\\underbrace{\\mathbf{E}[M_{t}-M_{s}]}_{\\text{equals \\ensuremath{0}}}+M_{s}^{2}\\\\\n & =\\mathbf{E}[(M_{t}-M_{s})^{2}]+M_{s}^{2}\n\\end{aligned}$$\n\nNow, if $X\\sim\\text{Poisson\\ensuremath{(\\lambda t)}}$, then\n$\\mathbf{E}[X]=\\lambda t$ and\n$\\mathbf{E}\\ensuremath{[X^{2}]}=\\lambda t(\\lambda t+1)$.\n\n$$\\begin{aligned}\n\\mathbf{E}[(M_{t}-M_{s})^{2}] & =\\mathbf{E}\\left[\\left\\{ (N_{t}-N_{s})-\\lambda(t-s)\\right\\} ^{2}\\right]\\\\\n & =\\mathbf{E}\\left[(N_{t}-N_{s})^{2}\\right]-2\\lambda(t-s)\\mathbf{E}\\left[(N_{t}-N_{s})\\right]+\\lambda^{2}(t-s)^{2}\\\\\n & =\\lambda^{2}(t-s)^{2}+\\lambda(t-s)-2\\lambda(t-s)\\cdot\\lambda(t-s)+\\lambda^{2}(t-s)^{2}\\\\\n & =\\lambda(t-s)\n\\end{aligned}$$\n\nThus,\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{t}^{2}-\\lambda t|\\mathcal{F}_{s}] & =M_{s}^{2}-\\lambda s\n\\end{aligned}$$\n\nWe conclude that the process $(M_{t}^{2}-\\lambda t:t\\geq0)$ is a\nmartingale. The Doob-Meyer decomposition of the submartingale\n$M_{t}^{2}$ is then:\n\n$$\\begin{aligned}\nM_{t}^{2} & =(M_{t}^{2}-\\lambda t)+\\lambda t\n\\end{aligned}$$\n:::\n\n::: example\nConsider a Brownian motion $B(t)$. The quadratic variation of the\nprocess $(B(t):t\\geq0)$ over the interval $[0,t]$ is given by\n$[B]_{t}=t$. On the other hand, we saw, that the square of Brownian\nmotion compensated, $(B_{t}^{2}-t:t\\geq0)$ is a martingale. Hence, the\nDoob-Meyer decomposition of $B(t)^{2}$ is given by:\n\n$$\\begin{aligned}\nB(t)^{2} & =(B(t)^{2}-t)+t\n\\end{aligned}$$\n:::\n\n## Computations with Martingales.\n\nMartingales are not only conceptually interesting, they are also\nformidable tools to compute probabilities and expectations of processes.\nFor example, in this section, we will solve the *gambler's ruin* problem\nfor Brownian motion. For convenience, we introduce the notion of\n*stopping time* before doing so.\n\n### Stopping times.\n\n::: {#def-stopping-time}\n\n### Stopping Time.\nA random variable $\\tau:\\Omega\\to\\mathbf{N}\\cup\\{+\\infty\\}$ is said to\nbe a *stopping time* for the filtration $(\\mathcal{F}_{t}:t\\geq0)$ if\nand only if:\n\n$$\\begin{aligned}\n\\{\\omega:\\tau(\\omega)\\leq t\\} & \\in\\mathcal{F}_{t},\\quad\\forall t\\geq0\n\\end{aligned}$$ Note that since $\\mathcal{F}_{t}$ is a sigma-field, if\n$\\tau$ is a stopping time, then we must also have that\n$\\{\\omega:\\tau(\\omega)>t\\}\\in\\mathcal{F}_{t}$.\n\nIn other words, $\\tau$ is a stopping time, if we can decide if the\nevents $\\{\\tau\\leq t\\}$ occurred or not based on the information\navailable at time $t$.\n\nThe term *stopping time* comes from gambling: a gambler can decide to\nstop playing at a random time (depending for example on previous gains\nor losses), but when he or she decides to stop, his/her decision is\nbased solely upon the knowledge of what happened before, and does not\ndepend on future outcomes. In other words, the stopping policy/strategy\ncan only depend on past outcomes. Otherwise, it would mean that he/she\nhas a crystall ball.\n:::\n\n::: example\n(Examples of stopping times).\n\n\\(i\\) *First passage time*. This is the first time when a process\nreaches a certain value. To be precise, let $X=(X_{t}:t\\geq0)$ be a\nprocess and $(\\mathcal{F}_{t}:t\\geq0)$ be its natural filtration. For\n$a>0$, we define the first passage time at $a$ to be:\n\n$$\\begin{aligned}\n\\tau(\\omega) & =\\inf\\{s\\geq0:X_{s}(\\omega)\\geq a\\}\n\\end{aligned}$$\n\nIf the path $\\omega$ never reaches $a$, we set $\\tau(\\omega)=\\infty$.\nNow, for $t$ fixed and for a given path $X(\\omega)$, it is possible to\nknow if $\\{\\tau(\\omega)\\leq t\\}$ (the path has reached $a$ before time\n$t$) or $\\{\\tau(\\omega)>t\\}$ (the path has not reached $a$ before time\n$t$) with the information available at time $t$, since we are looking at\nthe first time the process reaches $a$. Hence, we conclude that $\\tau$\nis a stopping time.\n\n\\(ii\\) *Hitting time*. More generally, we can consider the first time\n(if ever) that the path of a process $(X_{t}:t\\geq0)$ enters or hits a\nsubset $B$ of $\\mathbf{R}$:\n\n$$\\begin{aligned}\n\\tau(\\omega) & =\\min\\{s\\geq0:X_{s}(\\omega)\\in B\\}\n\\end{aligned}$$\n\nThe first passage time is the particular case in which $B=[a,\\infty)$.\n\n\\(iii\\) *Minimum of two stopping times.* If $\\tau$ and $\\tau'$ are two\nstopping times for the same filtration $(\\mathcal{F}_{t}:t\\geq0)$, then\nso is the minimum $\\tau\\land\\tau'$ between the two, where\n\n$$\\begin{aligned}\n(\\tau\\land\\tau')(\\omega) & =\\min\\{\\tau(\\omega),\\tau'(\\omega)\\}\n\\end{aligned}$$\n\nThis is because for any $t\\geq0$:\n\n$$\\begin{aligned}\n\\{\\omega: & (\\tau\\land\\tau')(\\omega)\\leq t\\}=\\{\\omega:\\tau(\\omega)\\leq t\\}\\cup\\{\\omega:\\tau'(\\omega)\\leq t\\}\n\\end{aligned}$$\n\nSince the right hand side is the union of two events in\n$\\mathcal{F}_{t}$, it must also be in $\\mathcal{F}_{t}$ by the\nproperties of a sigma-field. We conclude that $\\tau\\land\\tau'$ is a\nstopping time. Is it also the case that the maximum $\\tau\\lor\\tau'$ is a\nstopping time?\n\nFor any fixed $t\\geq0$, we have:\n\n$$\\begin{aligned}\n\\{\\omega:(\\tau\\lor\\tau')(\\omega)\\leq t\\} & =\\{\\omega:\\tau(\\omega)\\leq t\\}\\cap\\{\\omega:\\tau'(\\omega)\\leq t\\}\n\\end{aligned}$$\n\nSince the right hand side is the intersection of two events in\n$\\mathcal{F}_{t}$, it must also be in $\\mathcal{F}_{t}$ by the\nproperties of a sigma-field. We conclude that $\\tau\\lor\\tau'$ is a\nstopping time.\n:::\n\n::: example\n(Last passage time is not a stopping time). What if we look at the last\ntime the process reaches $a$, that is:\n\n$$\\begin{aligned}\n\\rho(\\omega) & =\\sup\\{t\\geq0:X_{t}(\\omega)\\geq a\\}\n\\end{aligned}$$\n\nThis is a well-defined random variable, but it is not a stopping time.\nBased on the information available at time $t$, we are not able to\ndecide whether or not $\\{\\rho(\\omega)\\leq t\\}$ occurred or not, as the\npath can always reach $a$ one more time after $t$.\n:::\n\nIt turns out that a martingale that is stopped when the stopping time is\nattained remains a martingale.\n\n::: {#prp-stopped-martingale}\n### Stopped Martingale. \n\nIf $(M_{t}:t\\geq0)$ is a continuous martingale for the filtration $(\\mathcal{F}_{t}:t\\geq0)$ and $\\tau$ is a stopping time for the same filtration, then the stopped process defined by\n\n$$\\begin{aligned}\nM_{t\\land\\tau} & =\\begin{cases}\nM_{t} & t\\leq\\tau\\\\\nM_{\\tau} & t>\\tau\n\\end{cases}\n\\end{aligned}$$\n\nis also a continuous martingale for the same filtration.\n:::\n\n*Proof*.\n\nLet $(M_n,n=0,1,2,\\ldots)$ be a martingale for the filtration $(\\mathcal{F}_n,n\\geq 0)$. Let $\\tau$ be a stopping time for the same filtration and consider the martingale transform with the process:\n\n$$\nX_n(\\omega) = \\begin{cases}\n1 & \\quad \\text{ if } n < \\tau(\\omega) \\\\\n0   & \\quad \\text{ if } n \\geq \\tau(\\omega)\n\\end{cases}\n$$\n\nThen, the stopped martingale $(M_{t \\land \\tau},t\\geq 0)$ can be written as an a martingale transform of the process $(X_n)_{n=0}^{\\infty}$.\n\n$$\nM = \\sum_{j=0}^{n-1} X_j (M_{j+1} - M_j)\n$$\n\nfor all $n=0,1,2,\\ldots$. Invoking the property martingale transforms are continuous martingales, we have that $M_t = \\int X_t dM_t$ must be a continous martingale. Consequently, the stopped process $(M_{t \\land \\tau}, t\\geq 0)$ must be a martingale. $\\blacksquare$\n\n::: {#thm-Doob-optional-stopping}\n\n### Doob's Optional Stopping Theorem.\n\nIf $(M_{t}:t\\geq0)$ is a continuous martingale for the filtration $(\\mathcal{F}_{t}:t\\geq0)$ and $\\tau$ is a stopping time such that $\\tau<\\infty$ and the stopped process $(M_{t\\land\\tau}:t\\geq0)$ is bounded, then:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{\\tau}] & =M_{0}\n\\end{aligned}$$\n:::\n\n*Proof.* Since $(M_{\\tau\\land t}:t\\geq0)$ is a martingale, we always\nhave:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{\\tau\\land t}] & =M_{0}\n\\end{aligned}$$\n\nNow, since $\\tau(\\omega)<\\infty$, we must\n\nhave that $\\lim_{t\\to\\infty}M_{\\tau\\land t}=M_{\\tau}$ almost surely. In\nparticular, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{\\tau}] & =\\mathbf{E}\\left[\\lim_{t\\to\\infty}M_{\\tau\\land t}\\right]=\\lim_{t\\to\\infty}\\mathbf{E}[M_{\\tau\\land t}]=\\lim_{t\\to\\infty}M_{0}\n\\end{aligned}$$\n\nwhere we passed to the limit, using the dominated convergence theorem. Recall, that the dominated convergence theorem allows us to commute abstract integration $\\int$ and $\\lim_{n \\to \\infty}$. \n\n$\\lim_{n \\to \\infty} \\int f_n = \\lim_{n \\to \\infty} \\int f_n$ as long as the $f_n$'s are dominated. $\\blacksquare$\n\n::: {exm-gamblers-ruin-with-brownian-motion}\n    \n### Gambler's ruin with Brownian motion. \n\nThe *gambler's ruin problem* is known in different forms. Roughly speaking, it refers to the problem of computing the probability of a gambler making a series of bets reaching a certain amount before going broke. In terms of Brownian motion (and stochastic processes in general), it translates to the following questions: Let $(B_{t}:t\\geq0)$ be a standard brownian motion starting at $B_{0}=0$ and $a,b>0$.\n\n\\(1\\) What is the probability that a Brownian path reaches $a$ before\n$-b$?\n\n\\(2\\) What is the expected waiting time for the path to reach $a$ or\n$-b$?\n\nFor the first question, it is a simple computation using stopping time\nand martingale properties. Define the hitting time:\n\n$$\\begin{aligned}\n\\tau(\\omega) & =\\inf\\{t\\geq0:B_{t}(\\omega)\\geq a\\text{ or }B_{t}(\\omega)\\leq-b\\}\n\\end{aligned}$$\n\nNote that $\\tau$ is the minimum between the first passage time at $a$ and the one at $-b$.\n\nWe first show that $\\tau<\\infty$ almost surely. In other words, all Brownian paths reach $a$ or $-b$ eventually. To see this, consider the event $E_{n}$ that the $n$-th increment exceeds $a+b$\n\n$$\\begin{aligned}\nE_{n} & :=\\left\\{ |B_{n}-B_{n-1}|>a+b\\right\\} \n\\end{aligned}$$\n\nNote that, if $E_{n}$ occurs, then we must have that the Brownian motion path exits the interval $[-b,a].$ Moreover, we have $\\mathbb{P}(E_{n})=\\mathbb{P}(E_{1})$ for all $n$. Call this probability $p$.\n\nSince the events $E_{n}$ are independent, we have:\n\n$$\\begin{aligned}\n\\mathbb{P}(E_{1}^{C}\\cap E_{2}^{C}\\cap\\ldots\\cap E_{n}^{C}) & =(1-p)^{n}\n\\end{aligned}$$\n\nAs $n\\to\\infty$ we have:\n\n$$\\begin{aligned}\n\\lim_{n\\to\\infty}\\mathbb{P}(E_{1}^{C}\\cap E_{2}^{C}\\cap\\ldots\\cap E_{n}^{C}) & =0\n\\end{aligned}$$\n\nThe sequence of events $(F_{n})$ where $F_{n}=E_{1}^{C}\\cap E_{2}^{C}\\cap\\ldots\\cap E_{n}^{C}$ is a decreasing sequence of events. By the continuity of probability measure lemma, we conclude that:\n\n$$\\begin{aligned}\n\\lim_{n\\to\\infty}\\mathbb{P}\\left(F_{n}\\right) & =\\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty}F_{n}\\right)=0\n\\end{aligned}$$\n\nTherefore, it must be the case $\\mathbb{P}(\\cup_{n=1}^{\\infty}E_{n})=1$.\nSo, $E_{n}$ must occur for some $n$, so all brownian motion paths reach\n$a$ or $-b$ almost surely.\n\nSince $\\tau<\\infty$ with probability one, the random variable $B_{\\tau}$\nis well-defined : $B_{\\tau}(\\omega)=B_{t}(\\omega)$ if $\\tau(\\omega)=t$.\nIt can only take two values: $a$ or $-b$. Question (1) above translates\ninto computing $\\mathbb{P}(B_{\\tau}=a)$. On one hand, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[B_{\\tau}] & =a\\mathbb{P}(B_{\\tau}=a)+(-b)(1-\\mathbb{P}(B_{\\tau}=a))\n\\end{aligned}$$\n\nOn the other hand, by Doob's optional stopping theorem (@thm-Doob-optional-stopping), we have\n$\\mathbf{E}[B_{\\tau}]=\\mathbf{E}[B_{0}]=0$. (Note that the stopped process $(B_{t\\land\\tau}:t\\geq0)$ is bounded above by $a$ and by $-b$ below). Putting these two observations together, we get:\n\n$$\\begin{aligned}\n\\mathbb{P}(B_{\\tau}=a) & =\\frac{b}{a+b}\n\\end{aligned}$$\n\nA very simple and elegant answer!\n\nWe will revisit this problem again and again. In particular, we will\nanswer the question above for Brownian motion with a drift at length\nfurther ahead.\n:::\n\n::: {#exm-expected-waiting-time}\n\n### Expected Waiting Time. \nLet $\\tau$ be as in the last example. We now answer question (2) of the gambler's ruin\nproblem:\n\n$$\\begin{aligned}\n\\mathbf{E}[\\tau] & =ab\n\\end{aligned}$$\n\nNote that the expected waiting time is consistent with the rough\nheuristic that Brownian motion travels a distance $\\sqrt{t}$ by time $t$. We now use the martingale $M_{t}=B_{t}^{2}-t$. On the one hand, if we apply optional stopping @thm-Doob-optional-stopping, we get:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{\\tau}] & =M_{0}=0\n\\end{aligned}$$\n\nMoreover, we know the distribution of $B_{\\tau}$, thanks to the probability calculated in the last example. We can therefore compute $\\mathbf{E}[M_{\\tau}]$ directly:\n\n$$\\begin{aligned}\n0 & =\\mathbf{E}[M_{\\tau}]\\\\\n & =\\mathbf{E}[B_{\\tau}^{2}-\\tau]\\\\\n & =\\mathbf{E}[B_{\\tau}^{2}]-\\mathbf{E}[\\tau]\\\\\n & =a^{2}\\cdot\\frac{b}{a+b}+b^{2}\\cdot\\frac{a}{a+b}-\\mathbf{E}[\\tau]\\\\\n\\mathbf{E}[\\tau] & =\\frac{a^{2}b+b^{2}a}{a+b}\\\\\n & =\\frac{ab\\cancel{(a+b)}}{\\cancel{(a+b)}}=ab\n\\end{aligned}$$\n\nWhy can we apply optional stopping here? The random variable $\\tau$ is\nfinite with probability $1$ as before. However, the stopped martingale\nis not necessarily bounded as before: $B_{\\tau\\land t}$ is bounded but\n$\\tau$ is not. However, the conclusion of optional stopping still holds.\nIndeed, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{t\\land\\tau}] & =\\mathbf{E}[B_{t\\land\\tau}^{2}]-\\mathbf{E}[t\\land\\tau]\n\\end{aligned}$$\n\nBy the bounded convergence theorem, we get\n$\\lim_{t\\to\\infty}\\mathbf{E}[B_{t\\land\\tau}^{2}]=\\mathbf{E}[\\lim_{t\\to\\infty}B_{t\\land\\tau}^{2}]=\\mathbf{E}[B_{\\tau}^{2}]$.\nSince $\\tau\\land t$ is a non-decreasing sequence and as $t\\to\\infty$,\n$t\\land\\tau\\to\\tau$ almost surely, as $\\tau<\\infty$, by the monotone\nconvergence theorem,\n$\\lim_{t\\to\\infty}\\mathbf{E}[t\\land\\tau]=\\mathbf{E}[\\tau]$.\n:::\n\n::: {#exm-first-passage-time-of-brownian-motion}\n\n### First passage time of Brownian Motion. \n\nWe can use the previous two examples to get some very interesting information on the first passage time:\n\n$$\\begin{aligned}\n\\tau_{a} & =\\inf\\{t\\geq0:B_{t}\\geq a\\}\n\\end{aligned}$$\n\nLet $\\tau=\\tau_{a}\\land\\tau_{-b}$ be as in the previous examples with\n$\\tau_{-b}=\\inf\\{t\\geq0:B_{t}\\leq-b\\}$. Note that\n$(\\tau_{-b},b\\in\\mathbf{R}_{+})$ is a sequence of random variables that\nis increasing in $b$. A brownian motion path must cross through $-1$\nbefore it hits $-2$ for the first time and in general\n$\\tau_{-n}(\\omega)\\leq\\tau_{-(n+1)}(\\omega)$. Moreover, we have\n$\\tau_{-b}\\to\\infty$ almost surely as $b\\to\\infty$. That's because,\n$\\mathbb{P}\\{\\tau<\\infty\\}=1$. Moreover, the event $\\{B_{\\tau}=a\\}$ is\nthe same as $\\{\\tau_{a}<\\tau_{-b}\\}$. Now, the events\n$\\{\\tau_{a}<\\tau_{-b}\\}$ are increasing in $b$, since if a path reaches\n$a$ before $-b$, it will do so as well for a more negative value of\n$-b$. On one hand, this means by the continuity of probability measure\nlemma\n([\\[th:continuity-property-of-lebesgue-measure\\]](#th:continuity-property-of-lebesgue-measure){reference-type=\"ref\"\nreference=\"th:continuity-property-of-lebesgue-measure\"}) that:\n\n$$\\begin{aligned}\n\\lim_{b\\to\\infty}\\mathbb{P}\\left\\{ \\tau_{a}<\\tau_{-b}\\right\\}  & =\\mathbb{P}\\{\\lim_{b\\to\\infty}\\tau_{a}<\\tau_{-b}\\}\\\\\n & =\\mathbb{P}\\{\\tau_{a}<\\infty\\}\n\\end{aligned}$$\n\nOn the other hand, we have by example\n([\\[example:probability-of-hitting-times\\]](#example:probability-of-hitting-times){reference-type=\"ref\"\nreference=\"example:probability-of-hitting-times\"})\n\n$$\\begin{aligned}\n\\lim_{b\\to\\infty}\\mathbb{P}\\left\\{ \\tau_{a}<\\tau_{-b}\\right\\}  & =\\lim_{b\\to\\infty}\\mathbb{P}\\{B_{\\tau}=a\\}\\\\\n & =\\lim_{b\\to\\infty}\\frac{b}{b+a}\\\\\n & =1\n\\end{aligned}$$\n\nWe just showed that:\n\n$$\\begin{aligned}\n\\mathbb{P}\\left\\{ \\tau_{a}<\\infty\\right\\}  & =1\\label{eq:first-passage-time-to-a-is-finite-almost-surely}\n\\end{aligned}$$\n\nIn other words, every Brownian path will reach $a$, no matter how large\n$a$ is!\n\nHow long will it take to reach $a$ on average? Well, we know from\nexample\n([\\[ex:expected-waiting-times\\]](#ex:expected-waiting-times){reference-type=\"ref\"\nreference=\"ex:expected-waiting-times\"}) that\n$\\mathbf{E}[\\tau_{a}\\land\\tau_{-b}]=ab$. On one hand this means,\n\n$$\\begin{aligned}\n\\lim_{b\\to\\infty}\\mathbf{E}[\\tau_{a}\\land\\tau_{-b}] & =\\lim_{b\\to\\infty}ab=\\infty\n\\end{aligned}$$\n\nOn the other hand, since the random variables $\\tau_{-b}$ are\nincreasing,\n\n$$\\begin{aligned}\n\\lim_{b\\to\\infty}\\mathbf{E}[\\tau_{a}\\land\\tau_{-b}] & =\\mathbf{E}\\left[\\lim_{b\\to\\infty}\\tau_{a}\\land\\tau_{-b}\\right]=\\mathbf{E}[\\tau_{a}]\n\\end{aligned}$$\n\nby the monotone convergence theorem\n([\\[th:monotone-convergence-theorem\\]](#th:monotone-convergence-theorem){reference-type=\"ref\"\nreference=\"th:monotone-convergence-theorem\"}). We just proved that:\n\n$$\\begin{aligned}\n\\mathbf{E}[\\tau_{a}] & =\\infty\n\\end{aligned}$$\n\nIn other words, any Brownian motion path will reach $a$, but the\nexpected waiting time for this to occur is infinite, no matter, how\nsmall $a$ is! What is happening here? No matter, how small $a$ is, there\nis always paths that reach very large negative values before hitting\n$a$. These paths might be unlikely. However, the first passage time for\nthese paths is so large that they affect the value of the expectation\nsubstantially. In other words, $\\tau_{a}$ is a *heavy-tailed random\nvariable*. We look at the distribution of $\\tau_{a}$ in more detail in\nthe next section.\n:::\n\n::: example\n(When option stopping fails). Consider $\\tau_{a}$, the first passage\ntime at $a>0$. The random variable $B_{\\tau_{a}}$ is well-defined since\n$\\tau_{a}<\\infty$. In fact, we have $B_{\\tau_{a}}=a$ with probability\none. Therefore, the following must hold:\n\n$$\\begin{aligned}\n\\mathbf{E}[B_{\\tau_{a}}] & =a\\neq B_{0}\n\\end{aligned}$$\n\nOptional stopping theorem corollary\n([\\[th:doob\\'s-optional-sampling-theorem\\]](#th:doob's-optional-sampling-theorem){reference-type=\"ref\"\nreference=\"th:doob's-optional-sampling-theorem\"}) does not apply here,\nsince the stopped process $(B_{t\\land\\tau_{a}}:t\\geq0)$ is not bounded.\n$B_{t\\land\\tau_{a}}$ can become infinitely negative before hitting $a$.\n:::\n\n## Reflection principle for Brownian motion.\n\n::: prop\n[]{#prop:bacheliers-formula label=\"prop:bacheliers-formula\"}(Bachelier's\nformula). Let $(B_{t}:t\\leq T)$ be a standard brownian motion on\n$[0,T].$ Then, the CDF of the random variable\n$\\sup_{0\\leq t\\leq T}B_{t}$ is:\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\sup_{0\\leq t\\leq T}B_{t}\\leq a\\right) & =\\mathbb{P}\\left(|B_{T}|\\leq a\\right)\n\\end{aligned}$$\n\nIn particular, its PDF is:\n\n$$\\begin{aligned}\nf_{\\max}(a) & =\\frac{2}{\\sqrt{2\\pi T}}e^{-\\frac{a^{2}}{2T}}\n\\end{aligned}$$\n:::\n\n::: rem*\nWe can verify these results empirically. Note that the paths of the random variables $\\max_{0\\leq s\\leq t}B_{s}$ and $|B_{t}|$ are very different as $t$ varies for a given $\\omega$. One is increasing and the other is not. The equality holds in distribution for a fixed $t$. As a bonus corollary, we get the distribution of the first passage time at $a$.\n:::\n\n::: cor\nLet $a\\geq0$ and $\\tau_{a}=\\inf\\{t\\geq0:B_{t}\\geq a\\}$. Then:\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\tau_{a}\\leq T\\right) & =\\mathbb{P}\\left(\\max_{0\\leq t\\leq T}B_{t}\\geq a\\right)=\\int_{a}^{\\infty}\\frac{2}{\\sqrt{2\\pi T}}e^{-\\frac{x^{2}}{2T}}dx\n\\end{aligned}$$\n\nIn particular, the random variable $\\tau_{a}$ has the PDF:\n\n$$\\begin{aligned}\nf_{\\tau_{a}}(t) & =\\frac{a}{\\sqrt{2\\pi}}\\frac{e^{-\\frac{a^{2}}{2t}}}{t^{3/2}},\\quad t>0\n\\end{aligned}$$\n\nThis implies that it is heavy-tailed with $\\mathbf{E}[\\tau_{a}]=\\infty$.\n:::\n\n::: proof\n*Proof.* The maximum on $[0,T]$ is larger than or equal to $a$ if and\nonly if $\\tau_{a}\\leq T$. Therefore, the events\n$\\{\\max_{0\\leq t\\leq T}B_{t}\\geq a\\}$ and $\\{\\tau_{a}\\leq T\\}$ are the\nsame. So, the CDF $\\mathbb{P}(\\tau_{a}\\leq t)$ of $\\tau_{a}$, by\nproposition\n([\\[prop:bacheliers-formula\\]](#prop:bacheliers-formula){reference-type=\"ref\"\nreference=\"prop:bacheliers-formula\"})\n$\\int_{a}^{\\infty}f_{\\max}(x)dx=\\int_{a}^{\\infty}\\frac{2}{\\sqrt{2\\pi T}}e^{-\\frac{x^{2}}{2T}}dx$.\n\n$$\\begin{aligned}\nf_{\\tau_{a}}(t) & =-2\\phi(a/\\sqrt{t})\\cdot a\\cdot\\left(-\\frac{1}{2t^{3/2}}\\right)\\\\\n & =\\frac{a}{t^{3/2}}\\phi\\left(\\frac{a}{\\sqrt{t}}\\right)\\\\\n & =\\frac{a}{t^{3/2}}\\cdot\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{a^{2}}{2t}}\n\\end{aligned}$$\n\nTo estimate the expectation, it suffices to realize that for $t\\geq1$,\n$e^{-\\frac{a^{2}}{2t}}$ is larger than $e^{-\\frac{a^{2}}{2}}$.\nTherefore, we have:\n\n$$\\begin{aligned}\n\\mathbf{E}[\\tau_{a}] & =\\int_{0}^{\\infty}t\\frac{a}{\\sqrt{2\\pi}}\\frac{e^{-a^{2}/2t}}{t^{3/2}}dt\\geq\\frac{ae^{-a^{2}/2}}{\\sqrt{2\\pi}}\\int_{1}^{\\infty}t^{-1/2}dt\n\\end{aligned}$$\n\nThis is an improper integral and it diverges like $\\sqrt{t}$ and is\ninfinite as claimed. ◻\n:::\n\nTo prove proposition\n([\\[prop:bacheliers-formula\\]](#prop:bacheliers-formula){reference-type=\"ref\"\nreference=\"prop:bacheliers-formula\"}), we will need an important\nproperty of Brownian motion called the *reflection principle*. To\nmotivate it, recall the reflection symmetry of Brownian motion at time\n$s$ in proposition\n([\\[prop:brownian-motion-symmetry-of-reflection-at-time-s\\]](#prop:brownian-motion-symmetry-of-reflection-at-time-s){reference-type=\"ref\"\nreference=\"prop:brownian-motion-symmetry-of-reflection-at-time-s\"}). It\nturns out that this reflection property also holds if $s$ is replaced by\na stopping time.\n\n::: lem\n[]{#lemma:BM-reflection-principle\nlabel=\"lemma:BM-reflection-principle\"}(Reflection principle). Let\n$(B_{t}:t\\geq0)$ be a standard Brownian motion and let $\\tau$ be a\nstopping time for its filtration. Then, the process\n$(\\tilde{B}_{t}:t\\geq0)$ defined by the reflection at time $\\tau$:\n\n$$\\begin{aligned}\n\\tilde{B}_{t} & =\\begin{cases}\nB_{t} & \\text{if \\ensuremath{t\\leq\\tau}}\\\\\nB_{\\tau}-(B_{t}-B_{\\tau}) & \\text{if \\ensuremath{t>\\tau}}\n\\end{cases}\n\\end{aligned}$$\n\nis also a standard brownian motion.\n:::\n\n::: rem*\nWe defer the proof of the reflection property of Brownian motion to a\nfurther section. It is intuitive and instructive to quickly picture this\nin the discrete-time setting. I adopt the approach as in Shreve-I.\n\nWe repeatedly toss a fair coin ($p$, the probability of $H$ on each\ntoss, and $q=1-p$, the probability of $T$ on each toss, are both equal\nto $\\frac{1}{2}$). We denote the successive outcomes of the tosses by\n$\\omega_{1}\\omega_{2}\\omega_{3}\\ldots$. Let\n\n$$\\begin{aligned}\nX_{j} & =\\begin{cases}\n-1 & \\text{if \\ensuremath{\\omega_{j}=H}}\\\\\n+1 & \\text{if \\ensuremath{\\omega_{j}=T}}\n\\end{cases}\n\\end{aligned}$$\n\nand define $M_{0}=0$, $M_{n}=\\sum_{j=1}^{n}X_{n}$. The process\n$(M_{n}:n\\in\\mathbf{N})$ is a symmetric random walk.\n\nSuppose we toss a coin an odd number $(2j-1)$ of times. Some of the\npaths will reach level $1$ in the first $2j-1$ steps and other will not\nreach. In the case of $3$ tosses, there are $2^{3}=8$ possible paths and\n$5$ of these reach level $1$ at some time $\\tau_{1}\\leq2j-1$. From that\nmoment on, we can create a reflected path, which steps up each time the\noriginal path steps down and steps down each time the original path\nsteps up. If the original path ends above $1$ at the final time $2j-1$,\nthe reflected path ends below $1$ and vice versa. If the original path\nends at $1$, the reflected path does also. In fact, the reflection at\nthe first hitting time has the same distribution as the original random\nwalk.\n\nThe key here is, out of the $5$ paths that reach level $1$ at some time,\nthere are as many reflected paths that exceed $1$ at time $(2j-1)$ as\nthere are original paths that exceed $1$ at time $(2j-1)$. So, to count\nthe total number of paths that reach level $1$ by time $(2j-1)$, we can\ncount the paths that are at $1$ at time $(2j-1)$ and then add on *twice*\nthe number of paths that exceed $1$ at time $(2j-1)$.\n:::\n\nWith this new tool, we can now prove proposition\n([\\[prop:bacheliers-formula\\]](#prop:bacheliers-formula){reference-type=\"ref\"\nreference=\"prop:bacheliers-formula\"}).\n\n::: proof\n*Proof.* Consider $\\mathbb{P}(\\max_{t\\leq T}B_{t}\\geq a)$. By splitting\nthis probability over the event of the endpoint, we have:\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}\\geq a\\right) & =\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}\\geq a,B_{T}>a\\right)+\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}\\geq a,B_{T}\\leq a\\right)\n\\end{aligned}$$\n\nNote also, that $\\mathbb{P}(B_{T}=a)=0$. Hence, the first probability\nequals $\\mathbb{P}(B_{T}\\geq a)$. As for the second, consider the time\n$\\tau_{a}$. On the event considered, we have $\\tau_{a}\\leq T$ and using\nlemma\n([\\[lemma:BM-reflection-principle\\]](#lemma:BM-reflection-principle){reference-type=\"ref\"\nreference=\"lemma:BM-reflection-principle\"}) at that time, we get\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}\\geq a,B_{T}\\leq a\\right) & =\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}\\geq a,\\tilde{B}_{T}\\geq a\\right)\n\\end{aligned}$$\n\nObserve that the event $\\{\\max_{t\\leq T}B_{t}\\geq a\\}$ is the same as\n$\\{\\max_{t\\leq T}\\tilde{B}_{T}\\geq a\\}$. (A rough picture might help\nhere.) Thereforem the above probability is\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}\\geq a,B_{T}\\leq a\\right) & =\\mathbb{P}\\left(\\max_{t\\leq T}\\tilde{B}_{t}\\geq a,\\tilde{B}_{T}\\geq a\\right)=\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}\\geq a,B_{T}\\geq a\\right)\n\\end{aligned}$$\n\nwhere the last equality follows from the reflection principle\n($\\tilde{B}_{t}$ is also a standard brownian motion, and $B_{T}$ and\n$\\tilde{B}_{T}$ have the same distribution.) But, as above, the last\nprobability is equal to $\\mathbb{P}(B_{T}\\geq a)$. We conclude that:\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}\\geq a\\right) & =2\\mathbb{P}(B_{T}\\geq a)=\\frac{2}{\\sqrt{2\\pi T}}\\int_{a}^{\\infty}e^{-\\frac{x^{2}}{2T}}dx=\\mathbb{P}(|B_{T}|\\geq a)\n\\end{aligned}$$\n\nThis implies in particular that\n$\\mathbb{P}\\left(\\max_{t\\leq T}B_{t}=a\\right)=0$. Thus, we also have\n$\\mathbb{P}(\\max_{t\\leq T}B_{t}\\leq a)=\\mathbb{P}(|B_{T}|\\leq a)$ as\nclaimed. ◻\n:::\n\n## Numerical Simulations\n::: example\n(Simulating Martingales) Sample $10$ paths of the following process with\na step-size of $0.01$:\n\n\\(a\\) $B_{t}^{2}-t$, $t\\in[0,1]$\n\n\\(b\\) Geometric Brownian motion : $S_{t}=\\exp(B_{t}-t/2)$, $t\\in[0,1]$.\n\nLet's write a simple $\\texttt{BrownianMotion}$ class, that we shall use\nto generate sample paths.\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport attrs\nfrom attrs import define, field\n\n@define\nclass BrownianMotion:\n    _step_size = field(validator=attrs.validators.and_(attrs.validators.instance_of(float),\n                                                       attrs.validators.ge(0.0)))\n    # Time T\n    _T = field(validator=attrs.validators.and_(attrs.validators.instance_of(float),\n                                               attrs.validators.ge(0.0)))\n    # number of paths\n    _N = field(validator=attrs.validators.and_(attrs.validators.instance_of(int),\n                                               attrs.validators.gt(0)))\n\n    _num_steps = field(init=False)\n\n    def __attrs_post_init__(self):\n        self._num_steps = int(self._T/self._step_size)\n\n    def covariance_matrix(self):\n        C = np.zeros((self._num_steps,self._num_steps))\n\n        for i in range(self._num_steps):\n            for j in range(self._num_steps):\n                s = (i+1) * self._step_size\n                t = (j+1) * self._step_size\n                C[i,j] = min(s,t)\n        return C\n\n    # Each column vector represents a sample path\n    def generate_paths(self):\n        C = self.covariance_matrix()\n        A = np.linalg.cholesky(C)\n        Z = np.random.standard_normal((self._num_steps, self._N))\n        X = np.matmul(A,Z)\n        X = np.concatenate((np.zeros((1,self._N)),X),axis=0)\n        return X.transpose()\n```\n:::\n\n\nNow, the process $B_{t}^{2}-t$ can be sampled as follows:\n\n``` {caption=\"10 paths of $B_t^2 - t$\"}\n\ndef generateSquareOfBMCompensated(numOfPaths,stepSize,T):\n    N = int(T/stepSize)\n\n    X = []\n    brownianMotion = BrownianMotion(stepSize,T)\n    for n in range(numOfPaths):\n\n        B_t = brownianMotion.samplePath()\n\n        B_t_sq = np.square(B_t)\n\n        t = np.linspace(start=0.0,stop=1.0,num=N+1)\n        M_t = np.subtract(B_t_sq,t)\n        X.append(M_t)\n\n    return X\n```\n\nThe gBM process can be sampled similarly, with\n$\\texttt{\\ensuremath{M_{t}} = np.exp(np.subtract(\\ensuremath{B_{t}},t/2))}$.\n\n::: example\n**(Maximum of Brownian Motion.)** Consider the maximum of Brownian\nmotion on $[0,1]$: $\\max_{s\\leq1}B_{s}$.\n\n\\(a\\) Draw the histogram of the random variable\n$\\max_{s\\leq1}B_{s}$using $10,0000$ sampled Brownian paths with a step\nsize of $0.01$.\n\n\\(b\\) Compare this to the PDF of the random variable $|B_{1}|$.\n:::\n\n*Solution.*\n\nI use the $\\texttt{itertools}$ python library to compute the running\nmaximum of a brownian motion path.\n\n``` {caption=\"The process $\\\\sup_{s\\\\leq 1}B_s$\"}\n\nbrownianMotion = BrownianMotion(stepSize=0.01,T=1)\ndata = []\n\nfor i in range(10000):\n    B_t = brownianMotion.samplePath()\n    max_B_t = list(itertools.accumulate(B_t,max))\n    data.append(max_B_t[100])\n```\n\nAnalytically, we know that $B_{1}$ is a gaussian random variable with\nmean $0$ and variance $1$.\n\n$$\\begin{aligned}\n\\mathbb{P}(|B_{1}|\\leq z) & =\\mathbb{P}(|Z|\\leq z)\\\\\n & =\\mathbb{P}(-z\\leq Z\\leq z)\\\\\n & =\\mathbb{P}(Z\\leq z)-\\mathbb{P}(Z\\leq-z)\\\\\n & =\\mathbb{P}(Z\\leq z)-(1-\\mathbb{P}(Z\\leq z))\\\\\nF_{|B_{1}|}(z) & =2\\Phi(z)-1\n\\end{aligned}$$\n\nDifferentiating on both sides, we get:\n\n$$\\begin{aligned}\nf_{|B_{1}|}(z) & =2\\phi(z)=\\frac{2}{\\sqrt{2\\pi}}e^{-\\frac{z^{2}}{2}},\\quad z\\in[0,\\infty)\n\\end{aligned}$$\n\n::: example\n(First passage time.) Let $(B_{t}:t\\geq0)$ be a standard brownian\nmotion. Consider the random variable:\n\n$$\\begin{aligned}\n\\tau & =\\min\\{t\\geq0:B_{t}\\geq1\\}\n\\end{aligned}$$\n\nThis is the first time that $B_{t}$ reaches $1$.\n\n\\(a\\) Draw a histogram for the distribution of $\\tau\\land10$ on the\ntime-interval $[0,10]$ using $10,000$ brownian motion paths on $[0,10]$\nwith discretization $0.01$.\n\n*The notation $\\tau\\land10$ means that if the path does not reach $1$ on\n$[0,10]$, then give the value $10$ to the stopping time.*\n\n\\(b\\) Estimate $\\mathbf{E}[\\tau\\land10]$.\n\n\\(c\\) What proportion of paths never reach $1$ in the time interval\n$[0,10]$?\n:::\n\n*Solution.*\n\nTo compute the expectation, we classify the hitting times of all paths\ninto $50$ bins. I simply did\n\n$\\texttt{frequency, bins = np.histogram(firstPassageTimes,bins=50,range=(0,10))}$\n\nand then computed\n\n$\\texttt{expectation=np.dot(frequency,bins[1:])/10000}$.\n\nThis expectation estimate on my machine is\n$\\mathbf{E}[\\tau\\land10]=4.34$ secs. There were approximately $2600$\npaths out of $10,000$ that did not reach $1$.\n\n## Problems and Puzzles\n\n::: {#exm-doobs-maximal-inequalities}\n\n### Doob's maximal inequalities. \nWe prove the following: Let $(M_{k}:k\\geq1)$ be positive submartingale for\nthe filtration $(\\mathcal{F}_{k}:k\\in\\mathbf{N})$. Then, for any\n$1\\leq p<\\infty$ and $a>0$\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\max_{k\\leq n}M_{k}>a\\right) & \\leq\\frac{1}{a^{p}}\\mathbf{E}[M_{n}^{p}]\n\\end{aligned}$$\n\n\\(a\\) Use Jensen's inequality to show that if $(M_{k}:k\\geq1)$ is a\npositive submartingale, then so is $(M_{k}^{p}:k\\geq1)$ for\n$1\\leq p<\\infty$. Conclude that it suffices to prove the statement for\n$p=1$.\n:::\n\n*Solution.*\n\nThe function $f(x)=x^{p}$ is convex. By conditional Jensen's inequality,\n\n$$\\begin{aligned}\n\\left(\\mathbf{E}[M_{k+1}|\\mathcal{F}_{k}]\\right)^{p} & \\leq\\mathbf{E}[M_{k}^{p}|\\mathcal{F}_{k}]\n\\end{aligned}$$\n\nThus,\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{k+1}^{p}|\\mathcal{F}_{k}] & \\geq\\left(\\mathbf{E}[M_{k+1}|\\mathcal{F}_{k}]\\right)^{p}\\geq M_{k}^{p}\n\\end{aligned}$$\n\nwhere the last inequality follows from the fact that $(M_{k}:k\\geq1)$ is\na positive submartingale, so\n$\\mathbf{E}[M_{k+1}|\\mathcal{F}_{k}]\\geq M_{k}$. Consequently,\n$(M_{k}^{p}:k\\geq1)$ is also a positive submartingale.\n\n\\(b\\) Consider the events\n\n$$\\begin{aligned}\nB_{k} & =\\bigcap_{j<k}\\{\\omega:M_{j}(\\omega)\\leq a\\}\\cap\\{\\omega:M_{k}(\\omega)>a\\}\n\\end{aligned}$$\n\nArgue that the $B_{k}$'s are disjoint and that\n$\\bigcup_{k\\leq n}B_{k}=\\{\\max_{k\\leq n}M_{k}>a\\}=B$.\n\n*Solution.*\n\nClearly, $B_{k}$ is the event that the first time to cross $a$ is $k$.\nIf $B_{k}$ occurs, $B_{k+1},B_{k+2},\\ldots$ fail to occur. Hence, all\n$B_{k}'s$ are pairwise disjoint. The event $\\bigcup_{k\\leq n}B_{k}$ is\nthe event that the random walk crosses $a$ at any time $k\\leq n$. Thus,\nthe running maximum of the Brownian motion at time $n$ exceeds $a$.\n\n\\(c\\) Show that\n\n$$\\begin{aligned}\n\\mathbf{E}[M_{n}]\\geq\\mathbf{E}[M_{n}\\mathbf{1}_{B}] & \\geq a\\sum_{k\\leq n}\\mathbb{P}(B_{k})=a\\mathbb{P}(B)\n\\end{aligned}$$\n\nby decomposing $B$ in $B_{k}$'s and by using the properties of\nexpectations, as well as the submartingale property.\n\n*Solution.*\n\nClearly, $M_{n}\\geq M_{n}\\mathbf{1}_{B}\\geq a\\mathbf{1}_{B}$. And\n$M_{n}$ is a positive random variable. By monotonicity of expectations,\n$\\mathbf{E}[M_{n}]\\geq\\mathbf{E}[M_{n}\\mathbf{1}_{B}]\\geq a\\mathbf{E}[\\mathbf{1}_{B}]=a\\mathbb{P}(B)=a\\sum_{k\\leq n}\\mathbb{P}(B_{k})$,\nwhere the last equality holds because the $B_{k}$'s are disjoint.\n\n\\(d\\) Argue that the inequality holds for continuous paths by\ndiscretizing time and using convergence theorems : If $(M_{t}:t\\geq0)$\nis a positive submartingale with continuous paths for the filtration\n$(\\mathcal{F}_{t}:t\\geq0)$, then for any $1\\leq p<\\infty$ and $a>0$:\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\max_{s\\leq t}M_{s}>a\\right) & \\leq\\frac{1}{a^{p}}\\mathbf{E}[M_{t}^{p}]\n\\end{aligned}$$\n\n*Solution.*\n\nLet $(M_{t}:t\\geq0)$ be a positive submartingale with continuous paths\nfor the filtration $(\\mathcal{F}_{t}:t\\geq0)$. Consider a sequence of\npartitions of the interval $[0,t]$ into $2^{r}$ subintervals :\n\n$$\\begin{aligned}\nD_{r} & =\\left\\{ \\frac{kt}{2^{r}}:k=0,1,2,\\ldots,2^{n}\\right\\} \n\\end{aligned}$$\n\nAnd consider a sequence of discrete positive sub-martingales:\n\n$$\\begin{aligned}\nM_{kt/2^{r}}^{(r)} & =M_{kt/2^{r}},\\quad k\\in\\mathbf{N},0\\leq k\\leq2^{r}\n\\end{aligned}$$\n\nNext, we define for $r=1,2,3,\\ldots$\n\n$$\\begin{aligned}\nA_{r} & =\\left\\{ \\sup_{s\\in D_{r}}|M_{s}^{(r)}|>a\\right\\} \n\\end{aligned}$$\n\nBy using the maximal inequality in discrete time, gives us:\n\n$$\\begin{aligned}\n\\mathbb{P}(A_{r})=\\mathbb{P}\\left\\{ \\sup_{s\\in D_{r}}|M_{s}^{(r)}|>a\\right\\}  & \\leq\\frac{1}{a^{p}}\\mathbf{E}\\left[\\left(M_{s}^{(r)}\\right)^{p}\\right]=\\frac{1}{a^{p}}\\mathbf{E}\\left[M_{t}^{p}\\right]\n\\end{aligned}$$\n\n$$\\begin{aligned}\n\\mathbb{P}\\left(\\max_{s\\leq t}M_{s}>a\\right) & =\\mathbb{P}\\left(\\bigcup_{r=1}^{\\infty}A_{r}\\right)\\\\\n & =\\lim_{r\\to\\infty}\\mathbb{P}\\left(A_{r}\\right)\\\\\n & \\left\\{ \\text{Continuity of probability measure}\\right\\} \\\\\n & \\leq\\lim_{r\\to\\infty}\\frac{1}{a^{p}}\\mathbf{E}\\left[M_{t}^{p}\\right]\n\\end{aligned}$$\n\n::: {#exm-simple-random-walk}\n\nLet $(S_n,n=0,1,2,\\ldots)$ be a simple random walk starting at $S_0 = 100$ with $S_n = S_0 + X_1 + X_2 + \\ldots + X_n$, where $P(X_i = 1) = p$ and $P(X_i = -1) = q = 1 - p$.\n\n(1) Prove that $M_n = \\left(\\frac{q}{p}\\right)^{S_n}$ is a martingale.\n\n(2) Let $\\tau = \\min\\{n \\geq 0 : S_n = 200 \\text{ or } S_n = 0\\}$. Prove that $\\mathbb{E}[\\tau] < \\infty$.\n\n(3) Find $P(S_\\tau = 200)$.\n:::\n\n*Proof*.\n\n(1) Clearly, $(S_n, n=0,1,2,\\ldots)$ is a martingale. For we have:\n\n$$\n\\begin{align*}\n\\mathbb{E}\\left[\\left.\\frac{M_{n+1}}{M_n}\\right|\\mathcal{F}_n\\right] &= \\mathbb{E}\\left[\\left.\\left(\\frac{q}{p}\\right)\\right|\\mathcal{F}_n\\right]\\\\\n&= p\\frac{q}{p} + q \\frac{p}{q}\\\\\n&=1\n\\end{align*}\n$$\n\nMultiplying both sides by $M_n$, we have the desired result:\n\n$$\n\\mathbb{E}[M_{n+1} | \\mathcal{F}_n] = M_n\n$$\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
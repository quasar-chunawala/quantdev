{
  "hash": "dfb2c83888040faad08944f9ffc17e1e",
  "result": {
    "markdown": "---\ntitle: Memory Barriers\nauthor: Quasar\ndate: '2025-03-08'\ncategories:\n  - C++\nimage: cpp.jpg\ntoc: true\ntoc-depth: 3\n---\n\n# C++ Atomics\n\n## Introduction to Atomic Operations\n\nAtomic operations are indivisible. An atomic operation is any operation that is **guaranteed to execute as a single transaction**. At a low-level, atomic operations are special hardware instructions.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\nConsider a shared variable `counter` initialized to `0`. The assembly instructions for incrementing this counter show that it requires multiple CPU instructions: load the value from memory into a register, add 1 to the register, and store the result back to memory. This multi-step process creates opportunities for race conditions in multi-threaded code.\n\nUsing atomic types and operations solves this problem by using special CPU instructions like `lock add` that guarantee the entire operation completes as a single, indivisible unit.\n\n```cpp\n// Incrementing a counter        \nint counter {0};\n\nint main(){\n    counter++;\n    return 0;\n}\n```\n\n## Generated Assembly - Non-Atomic\n\n```asm\ncounter:\n        .zero   4\nmain:\n        push    rbp\n        mov     rbp, rsp\n        mov     eax, DWORD PTR counter[rip]\n        add     eax, 1\n        mov     DWORD PTR counter[rip], eax\n        mov     eax, 0\n        pop     rbp\n        ret\n```\n\n## Atomic Increment\n\nUsing atomic types and operations:\n\n```cpp\n#include <atomic>\nstd::atomic<int> counter {0};\n\nint main(){\n    counter++;\n    return 0;\n}\n```\n\n**Generated Assembly:**\n\n```asm\nlock add DWORD PTR counter[rip], 1\n```\n\nAtomic operations allow threads to read, modify and write indivisibly and can also be used as synchronization primitives. Atomic operations must be provided by the CPU (as in the `lock add` instruction).\n\n## Operations on `std::atomic<T>`\n\n**Explicit reads and writes:**\n\n```cpp\nstd::atomic<T> x;\nT y = x.load();     // Same as T y = x;\nx.store(y);         // Same as x = y;\n```\n\n**Atomic exchange:**\n\n```cpp\nT z = x.exchange(y);    // Atomically: z = x; x = y;\n```\n\n`exchange` is an atomic swap - a read-modify-write done atomically. It reads the old value, replaces it with the new value and guarantees that nobody can get in there in between.\n\n## Compare-and-Swap (CAS)\n\n**Compare-and-swap (conditional exchange):**\n\n```cpp\nbool success = x.compare_exchange_strong(y,z);  // T& y\n// var.compare_exchange_strong(expected,desired);\n// If x == y, x = z and return true\n// Otherwise, set y = x and return false\n```\n\n## Why is CAS So Special?\n\nCompare-and-swap (CAS) is used in most lock-free algorithms. Consider this example of atomic increment with CAS. Pretty much every lock-free algorithm is centered around a loop like this. We want to increment `x`. First, we read the atomic value and store it in a local `x0`. We hope nobody got to `x` before us, that `x` hasn't changed. If that's `true`, we change it atomically to the desired value (which could be an increment, decrement, multiplication by 2, etc.). If nobody else changed `x`, we did our increment atomically. CAS returns `true` and the loop ends.\n\nIf somebody did change `x`, CAS fails and returns `false`. The changed value of `x` is updated in `x0`, so we don't have to read again. We continue to the next iteration and keep trying, until our compare-and-swap beats everyone else's and gets that increment in.\n\n```cpp\nstd::atomic<int> x{0};\nint x0 = x.load();     // [1] \nwhile(!x.compare_exchange_swap(x0, x0 + 1)){}  // [2]\n```\n\n## Additional Atomic Operations\n\n**For integer T:**\n\n```cpp\nstd::atomic<int> x;\nx.fetch_add(y);     // Same as x += y;\n```\n\n`fetch_add()` doesn't just add atomically. It increments atomically, but also returns the old value (the fetch part). So it returns the old value and adds the increment, all atomically.\n\nAlso available: `fetch_sub`, `fetch_and()`, `fetch_or()` and `fetch_xor()`.\n\n**Note:** If you have multiple atomic operations, their composition is not atomic.\n\n## Do Atomic Operations Wait on Each Other?\n\nAtomic operations are lock-free, maybe even wait-free. It doesn't mean they don't wait on each other. Atomic operations do wait for cache-line access. \n\n:::{.text-center}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows.meta --implicit-standalone\n\n\n\\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        \n\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n%uncomment if require: \\path (0,249); %set diagram left start at 0, and has height of 249\n\n%Rounded Rect [id:dp7226624209327404] \n\\draw  [fill={rgb, 255:red, 181; green, 212; blue, 244 }  ,fill opacity=1 ] (115,43.47) .. controls (115,33.82) and (122.82,26) .. (132.47,26) -- (505.03,26) .. controls (514.68,26) and (522.5,33.82) .. (522.5,43.47) -- (522.5,95.88) .. controls (522.5,105.52) and (514.68,113.34) .. (505.03,113.34) -- (132.47,113.34) .. controls (122.82,113.34) and (115,105.52) .. (115,95.88) -- cycle ;\n%Rounded Rect [id:dp404929071772297] \n\\draw  [fill={rgb, 255:red, 181; green, 212; blue, 244 }  ,fill opacity=1 ] (113,158.47) .. controls (113,148.82) and (120.82,141) .. (130.47,141) -- (503.03,141) .. controls (512.68,141) and (520.5,148.82) .. (520.5,158.47) -- (520.5,210.88) .. controls (520.5,220.52) and (512.68,228.34) .. (503.03,228.34) -- (130.47,228.34) .. controls (120.82,228.34) and (113,220.52) .. (113,210.88) -- cycle ;\n%Rounded Rect [id:dp8165659276958122] \n\\draw  [fill={rgb, 255:red, 251; green, 247; blue, 204 }  ,fill opacity=1 ] (151,62) .. controls (151,57.58) and (154.58,54) .. (159,54) -- (300.5,54) .. controls (304.92,54) and (308.5,57.58) .. (308.5,62) -- (308.5,86) .. controls (308.5,90.42) and (304.92,94) .. (300.5,94) -- (159,94) .. controls (154.58,94) and (151,90.42) .. (151,86) -- cycle ;\n%Rounded Rect [id:dp5545605907049965] \n\\draw  [fill={rgb, 255:red, 251; green, 247; blue, 204 }  ,fill opacity=1 ] (325,61) .. controls (325,56.58) and (328.58,53) .. (333,53) -- (474.5,53) .. controls (478.92,53) and (482.5,56.58) .. (482.5,61) -- (482.5,85) .. controls (482.5,89.42) and (478.92,93) .. (474.5,93) -- (333,93) .. controls (328.58,93) and (325,89.42) .. (325,85) -- cycle ;\n%Rounded Rect [id:dp9918614693760329] \n\\draw  [fill={rgb, 255:red, 251; green, 247; blue, 204 }  ,fill opacity=1 ] (144,176) .. controls (144,171.58) and (147.58,168) .. (152,168) -- (293.5,168) .. controls (297.92,168) and (301.5,171.58) .. (301.5,176) -- (301.5,200) .. controls (301.5,204.42) and (297.92,208) .. (293.5,208) -- (152,208) .. controls (147.58,208) and (144,204.42) .. (144,200) -- cycle ;\n%Rounded Rect [id:dp9032232500910276] \n\\draw  [fill={rgb, 255:red, 251; green, 247; blue, 204 }  ,fill opacity=1 ] (332,176) .. controls (332,171.58) and (335.58,168) .. (340,168) -- (481.5,168) .. controls (485.92,168) and (489.5,171.58) .. (489.5,176) -- (489.5,200) .. controls (489.5,204.42) and (485.92,208) .. (481.5,208) -- (340,208) .. controls (335.58,208) and (332,204.42) .. (332,200) -- cycle ;\n\n% Text Node\n\\draw (263,31) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{std::atomic<int> x;}};\n% Text Node\n\\draw (257,146) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{std::atomic<int> x[2];}};\n% Text Node\n\\draw (231.33,63.33) node   [align=left] {\\begin{minipage}[lt]{47.15pt}\\setlength\\topsep{0pt}\nThread-1\n\\end{minipage}};\n% Text Node\n\\draw (410,64) node   [align=left] {\\begin{minipage}[lt]{47.15pt}\\setlength\\topsep{0pt}\nThread-2\n\\end{minipage}};\n% Text Node\n\\draw (221.33,180) node   [align=left] {\\begin{minipage}[lt]{47.15pt}\\setlength\\topsep{0pt}\nThread-1\n\\end{minipage}};\n% Text Node\n\\draw (414,180) node   [align=left] {\\begin{minipage}[lt]{47.15pt}\\setlength\\topsep{0pt}\nThread-2\n\\end{minipage}};\n% Text Node\n\\draw (231.71,82) node   [align=left] {\\begin{minipage}[lt]{27.6pt}\\setlength\\topsep{0pt}\n++x;\n\\end{minipage}};\n% Text Node\n\\draw (410.38,80.67) node   [align=left] {\\begin{minipage}[lt]{27.6pt}\\setlength\\topsep{0pt}\n++x;\n\\end{minipage}};\n% Text Node\n\\draw (228.08,198) node   [align=left] {\\begin{minipage}[lt]{44.43pt}\\setlength\\topsep{0pt}\n++x[0];\n\\end{minipage}};\n% Text Node\n\\draw (419.58,196.67) node   [align=left] {\\begin{minipage}[lt]{43.75pt}\\setlength\\topsep{0pt}\n++x[1];\n\\end{minipage}};\n% Text Node\n\\draw (214.67,13.33) node   [align=left] {\\begin{minipage}[lt]{205.81pt}\\setlength\\topsep{0pt}\nAccessing shared variable\n\\end{minipage}};\n% Text Node\n\\draw (212.67,127.33) node   [align=left] {\\begin{minipage}[lt]{205.81pt}\\setlength\\topsep{0pt}\nAccessing Non-shared variable\n\\end{minipage}};\n\n\n\\end{tikzpicture}\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](index_files/figure-html/cell-3-output-1.svg){}\n:::\n:::\n\n\nFig. Functions - control flow\n:::\n\n2. **Accessing non-shared variables:**\n   - Thread-1: `++x[0];`\n   - Thread-2: `++x[1];`\n\n*Credit: C++ atomics, from basic to advanced. What do they really do? by Fedor Pikus, CppCon 2017*\n\n## Benchmark Results\n\n```\nBenchmark                                        Time         CPU\n------------------------------------------------------------------------\nBM_SharedAtomicIncrement/1/real_time         0.558 ms    0.015 ms\nBM_SharedAtomicIncrement/2/real_time          2.56 ms    0.044 ms\nBM_SharedAtomicIncrement/4/real_time          5.40 ms    0.096 ms\nBM_SharedAtomicIncrement/8/real_time          10.6 ms    0.218 ms\nBM_SharedAtomicIncrement/16/real_time         22.9 ms    0.401 ms\nBM_SharedAtomicIncrement/32/real_time         48.4 ms     1.14 ms\nBM_SeparateAtomicIncrement/1/real_time       0.556 ms    0.016 ms\nBM_SeparateAtomicIncrement/2/real_time        2.74 ms    0.033 ms\nBM_SeparateAtomicIncrement/4/real_time        5.74 ms    0.081 ms\nBM_SeparateAtomicIncrement/8/real_time        11.2 ms    0.244 ms\nBM_SeparateAtomicIncrement/16/real_time       23.9 ms    0.403 ms\nBM_SeparateAtomicIncrement/32/real_time       26.6 ms     1.23 ms\n```\n\n## Cache Line Contention\n\nCan we conclude that atomic operations don't wait on each other? Not necessarily! What's actually happening is that the two atomic operations are in the same cache-line. On x86, the whole cache line trickles up and down from main memory to the on-board CPU cache and back. Even if you want one variable from the cache line, the entire 64-byte chunk will go up and down. If two different CPUs want two different variables within the same cache-line, they need to wait, as if it was the same variable. You don't get lower granularity than 64-bytes on x86.\n\n*Credit: C++ atomics, from basic to advanced by Fedor Pikus*\n\n## False Sharing Test Results\n\n```\nBenchmark                            Time         CPU\n------------------------------------------------------------------------\nBM_Shared/1/real_time            0.561 ms    0.015 ms\nBM_Shared/2/real_time             3.25 ms    0.054 ms\nBM_Shared/4/real_time             6.07 ms    0.061 ms\nBM_Shared/8/real_time             10.6 ms    0.240 ms\nBM_Shared/16/real_time            22.9 ms    0.384 ms\nBM_Shared/32/real_time            51.4 ms    0.944 ms\nBM_Shared/64/real_time             103 ms     2.67 ms\nBM_FalseShared/1/real_time       0.578 ms    0.016 ms\nBM_FalseShared/2/real_time        2.98 ms    0.037 ms\nBM_FalseShared/4/real_time        6.18 ms    0.065 ms\nBM_FalseShared/8/real_time        10.6 ms    0.234 ms\nBM_FalseShared/16/real_time       13.5 ms    0.386 ms\nBM_FalseShared/32/real_time       26.7 ms     1.21 ms\nBM_FalseShared/64/real_time       37.0 ms     2.36 ms\n```\n\n## Non-Shared Results - Perfect Scaling\n\n```\nBM_NonShared/1/real_time         0.576 ms    0.015 ms\nBM_NonShared/2/real_time         0.591 ms    0.028 ms\nBM_NonShared/4/real_time         0.632 ms    0.051 ms\nBM_NonShared/8/real_time         0.598 ms    0.102 ms\nBM_NonShared/16/real_time         1.05 ms    0.225 ms\nBM_NonShared/32/real_time         1.81 ms    0.701 ms\nBM_NonShared/64/real_time         3.21 ms     1.28 ms\n```\n\n**Key Finding:** When variables are on different cache lines (NonShared), performance scales near-perfectly with thread count!\n\n## Atomic Operations Summary\n\nAtomic operations have to wait for cache line access. This is the **price of data sharing** without race conditions. Modifying different locations on the same cache line still incurs a run-time penalty, a phenomenon known as false sharing. To avoid false sharing, we should align per-thread data to separate cache lines. Atomic operations do wait on each other, particularly write operations. However, read-only operations can scale near-perfectly when properly designed.\n\n## Strong vs Weak Compare-and-Swap\n\n```cpp\n// Strong CAS\nx.compare_exchange_strong(old_x, new_x);\n/* \nif (x == old_x)\n{\n    x = new_x;\n    return true;\n}else{\n    old_x = x;\n    return false;\n}\n*/\n```\n\n`x.compare_exchange_weak(old_x, new_x)` is essentially the same thing, but can **spuriously fail** and return `false`, even if `x == old_x`.\n\n# Memory Barriers\n\n## Memory Barriers - Introduction\n\nMemory barriers go hand-in-hand with C++ atomics. Memory barriers control how changes to memory made by one CPU core become visible to other CPU cores. Without memory barriers, there is no guarantee of visibility whatsoever. Imagine you have two CPUs, both modifying a variable `x` in their on-chip caches. The main memory doesn't have to change at all! There is no guarantee that anybody can see anything.\n\nFor example, CPU-1 cache might have `x = 42`, CPU-2 cache might have `x = 17`, while Main Memory still shows `x = 0` (stale/unchanged). The problem is that each CPU sees its own value. Memory is unchanged, and there is no coherence between the cores.\n\n## Memory Barriers in C++\n\nC++ memory barriers are modifiers on atomic operations.\n\n**Example:**\n\n```cpp\nstd::atomic<int> x;\nx.store(1, std::memory_order_release);\n```\n\nThis implies that we have put a release memory barrier on that store.\n\n## No Barriers - `std::memory_order_relaxed`\n\nNo memory barrier means that we can reorder reads and writes any way we want. We have an atomic `x` variable and `a`, `b` and `c` are non-atomic variables. In the program order, we write to `a`, then `b`, then `c`, then `x`. However, the observed order could be anything. For example, one possible reordering might be: `c = 3`, `x.store(4)`, `a = 1`, `b = 2`.\n\nThe key point is that non-atomic variables (a, b, c) can be reordered freely. The atomic variable (x) with `memory_order_relaxed` also provides no ordering guarantees. The CPU and compiler can execute instructions in any order they choose.\n\n```cpp\nx.fetch_add(1, std::memory_order_relaxed);\n```\n\n## Acquire Barrier\n\nAn acquire barrier is a half-barrier that acts as a one-way gate. Nothing that was after the `load` can move in front of it, but anything that was before can move after. Acquire barrier guarantees that all memory operations scheduled after the barrier in the program order become visible after the barrier. This applies to **all operations** - not just all reads or all writes, but both reads and writes. Furthermore, this applies to **all operations**, not just operations on the atomic variable, but literally all memory operations. Reads and writes cannot be reordered from after to before the barrier.\n\nConsider the program order: `a=1` → `b=2` → `x.load() [ACQUIRE BARRIER]` → `c=3` → `d=4` → `e=5`. A possible observed order with reordering might be: `a=1` → `x.load() [ACQUIRE BARRIER]` → `b=2` → `e=5` → `c=3` → `d=4`. Operations before the barrier can reorder and move after the barrier, but operations after the barrier cannot move before it, though they can reorder among themselves.\n\n## Release Barrier\n\nA release barrier is the reverse of an acquire barrier. Nothing that was before the barrier can move after, but anything that is after can move in front of the `store`.\n\nConsider the program order: `a=1` → `b=2` → `c=3` → `x.store() [RELEASE BARRIER]` → `d=4` → `e=5`. A possible observed order with reordering might be: `b=2` → `a=1` → `e=5` → `c=3` → `x.store() [RELEASE BARRIER]` → `d=4`. Operations before the barrier cannot move after it, but they can reorder among themselves. Operations after the barrier can reorder and move before the barrier.\n\n## Acquire-Release Protocol\n\nAcquire and release barriers are often used together. Thread t1 writes atomic variable `x` with a `release` barrier. Thread t2 reads atomic variable `x` with an `acquire` barrier. On the acquire side, all memory reads done after the `acquire` barrier in t2 in program order have to be done after the barrier in actual execution order. On the release side, all memory writes done before the `release` barrier in t1 in program order have to be done before the barrier in actual execution order.\n\nThe result is that all memory writes that happen in t1 before the barrier become visible in thread t2 after the barrier. Thread 1 prepares data (does some writes) then **releases** (publishes) it by updating atomic variable `x`. Thread 2 **acquires** atomic variable `x` and the data is guaranteed to be visible. It's important to note that it has to be the same atomic variable `x` for this synchronization to work.\n\n## Acquire-Release Example\n\n**Thread 1:**\n\n```cpp\na = 1;\nb = 2;\nc = 3;\n// --- RELEASE BARRIER ---\nx.store(1, memory_order_release);  // Publishes data\nd = 4;\ne = 5;\n```\n\n**Thread 2:**\n\n```cpp\nf = 6;\ng = 7;\nval = x.load(memory_order_acquire);  // Acquires data\n// --- ACQUIRE BARRIER ---\nread a;  // Guaranteed to see a = 1\nread b;  // Guaranteed to see b = 2\nread c;  // Guaranteed to see c = 3\n```\n\n**Guarantee:** All writes before release in T1 are visible after acquire in T2. Must be the same atomic variable `x`.\n\n## References {.appendix}\n\n- **Compiler Explorer examples:**\n  - Thread-safe queue: [https://compiler-explorer.com/z/Gz1vMGcno](https://compiler-explorer.com/z/Gz1vMGcno)\n  - Cache line experiment: [https://compiler-explorer.com/z/YxWdYeGca](https://compiler-explorer.com/z/YxWdYeGca)\n\n- **Recommended talks:**\n  - \"C++ atomics, from basic to advanced\" by Fedor Pikus (CppCon 2017)\n  - Lock-free programming talks at CppCon\n\n- **Books:**\n  - \"C++ Concurrency in Action\" by Anthony Williams\n  - \"The Art of Multiprocessor Programming\" by Herlihy and Shavit\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
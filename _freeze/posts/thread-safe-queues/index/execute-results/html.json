{
  "hash": "bcde7ab3f3a9e24a8f41fb78ad09e76e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A thread-safe queue implementation\"\nauthor: \"Quasar\"\ndate: \"2025-02-23\"\ncategories: [C++]      \nimage: \"cpp.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n\n\n\n## Producer-consumer problem\n\nIn the *producer-consumer problem*, we have two classes of threads, producers and consumers and a buffer containing a fixed number of slots. A producer thread attempts to put something into the next empty buffer slot, a consumer thread attempts to take something out of the next occupied buffer slot. The synchronization conditions are that producers cannot proceed unless there are empty slots and consumers cannot proceed unless there are occupied slots. The problem occurs because of the different rates at which producers deposit and consumers exhaust data.\n\nThis is a classic, but frequently occurring synchronization problem. For example, the heart of the implementation of UNIX pipes is an instance of this problem.\n\n## Ring buffer\n\nConsider a single, fixed-size buffer as if it were connected end-to-end, such that the oldest entry is processed first. This is a circular FIFO queue.\n\nWhat do we use SPSC FIFO queues for? In the industry, you often have a pipeline of processes. For example, you have one thread reading from sockets, another thread that handles the messages from the sockets and maybe processes them and produces a result and a third thread writes a response to the network. Those can be connected by SPSC FIFO queues. There's a couple of advantages to this. All these advantages and disadvantages are subject to measurement, so always measure. It may improve the throughput over just a single thread doing all $3$ of these operations, in fact, I'll be surprised if it didn't. It also should improve the resiliency of the application to spikes in message traffic. Some of the disadvantages are that you have to manage 3 threads and it probably uses more memory, because each of the FIFO queues needs place to store its messages.\n\nWe all have come across circular FIFO queues. We usually have two cursors - `rear` and `front`. Items are pushed to the `rear` of the queue and popped off the `front` of the queue.\n\n![Circular FIFO Queue](circular_fifo_queue.jpg){fig-align=\"center\" width=40% height=40%}\n\nWhen we `push(42)` into the FIFO queue, the `rear` cursor is incremented and each time we `pop()`, the `front` cursor is incremented. When the `front` cursor and the `rear` cursor are no longer equal, the FIFO queue is no longer empty. Eventually, we push so many values in, that the FIFO queue fills up. At this point, the `rear` cursor is `capacity` greater than the `front` cursor.\n\nThe FIFO queue empty and queue full conditions use the remainder operator `%`. Division uses $20$ to $30$ cycles so it is a bit expensive. Another approach is to constrain the buffer size to an integral power of $2$, and use the bitwise `&` operator and that's a $1$ cycle operation. \n\n### Implementation notes\n\n\n```cpp\n#include <iostream>\n#include <queue>\n#include <thread>\n#include <array>\n#include <numeric>\n#include <memory>\n#include <condition_variable>\n#include <mutex>\n#include <shared_mutex>\n\nnamespace dev {\n    template<typename T>\n    class ring_buffer {\n    private:\n        enum {min_capacity = 128};\n        T* ring;\n        int m_front;\n        int m_rear;\n        int m_capacity;\n\n    public:\n        /* Default constructor*/\n        ring_buffer() \n            : m_front{0}\n            , m_rear{0}\n            , ring{nullptr}\n            , m_capacity{0}\n        { \n            ring = static_cast<T*>(operator new(min_capacity));\n            m_capacity = min_capacity;\n        }\n\n        ring_buffer(int capacity)\n            : m_front{ 0 }\n            , m_rear{ 0 }\n            , ring{ nullptr }\n            , m_capacity{ 0 }\n        {\n            ring = static_cast<T*>(operator new(capacity));\n            m_capacity = capacity;\n        }\n\n        /* Copy constructor - Perform a deep copy */\n        ring_buffer(const ring_buffer<T>& other)\n            : m_front{ 0 }\n            , m_rear{ 0 }\n            , ring{ nullptr }\n            , m_capacity{ 0 }\n        {\n            /* Allocation */\n            ring = static_cast<T*>(operator new(other.m_capacity));\n\n            m_capacity = other.m_capacity;\n\n            /* Construction */\n            for (int i{0}; i < other.size(); ++i)\n            {\n                new (&ring[i]) T(other[i]);\n            }\n        }\n\n        /* Swap */\n        friend void swap(ring_buffer<T>& lhs, ring_buffer<T>& rhs)\n        {\n            std::swap(lhs.m_front, rhs.m_front);\n            std::swap(lhs.m_rear, rhs.m_rear);\n            std::swap(lhs.m_capacity, rhs.m_capacity);\n            std::swap(lhs.ring, rhs.ring);\n        }\n\n        /* Copy assignment */\n        ring_buffer<T>& operator=(const ring_buffer<T>& other)\n        {\n            ring_buffer<T> temp{ other };  //Copy-construct\n            swap(*this, temp);\n            return *this;\n        }\n\n        T& front() {\n            if (empty())\n                throw std::exception(\"buffer is empty!\");\n\n            return ring[m_front % m_capacity];\n        }\n\n        T& back() {\n            if (empty())\n                throw std::exception(\"buffer is empty!\");\n\n            return ring[(m_rear - 1) % m_capacity];\n        }\n\n        T& operator[](int i) const {\n            if (empty())\n                throw std::exception(\"buffer is empty!\");\n\n            return ring[(m_front + i) % m_capacity];\n        }\n\n        bool empty() const {\n            return m_front == m_rear;\n        }\n\n        bool full() const {\n            return size() == capacity();\n        }\n\n        void push(const T& value) {\n            if (full())\n                throw std::exception(\"buffer is full!\");\n\n            new (&ring[m_rear % m_capacity]) T(value);\n            std::cout << \"\\n\" << \"pushed \" << value << \" to buffer\";\n            ++m_rear;\n        }\n\n        void push(T&& value) {\n            if (full())\n                throw std::exception(\"buffer is full!\");\n\n            new (&ring[m_rear % m_capacity]) T(std::move(value));\n            std::cout << \"\\n\" << \"pushed \" << value << \" to buffer\";\n            ++m_rear;\n        }\n\n        void pop() {\n            if (empty())\n                throw std::exception(\"buffer is empty!\");\n\n            T value = front();\n            ring[m_front % m_capacity].~T();\n            std::cout << \"\\n\" << \"popped \" << value << \" off buffer\";\n            ++m_front;\n        }\n\n        int capacity() const{\n            return m_capacity;\n        }\n\n        int size() const {\n            return (m_rear - m_front);\n        }\n\n        void print() {\n            for (int i{ 0 };i < size();++i) {\n                std::cout << \"\\n\" << \"ring[\" << i << \"] = \" << (*this)[i];\n            }\n        }\n    };\n}\n```\n\nRunning the above code-snippet, we find that there are several occassions when there are buffer overflows or underflows, and there are also data races. \n\n## Building a thread-safe queue using condition variables\n\nYou essentially have three groups of operations : those that query the state of the whole queue(`empty()` and `size()`), those that query the elements of the queue(`front()` and `back()`) and those that modify the queue (`push()`, `pop()` and `emplace()`). This is the same as we've seen for the stack container adapter, and we have the same issues regarding race conditions inherent in the interface. Consequently, we need to combine `front()` and `pop()` into a single function call, much as you combined `pop()` and `top()` for the stack. \n\n\n```cpp\nnamespace dev {\n    template<typename T>\n    class threadsafe_ring_buffer {\n    public:\n        threadsafe_ring_buffer() : m_ring_buffer{ring_buffer<T>()}\n        {}\n\n        threadsafe_ring_buffer(int capacity) : m_ring_buffer{ring_buffer<T>(capacity)}\n        {}\n\n        threadsafe_ring_buffer(const threadsafe_ring_buffer<T>& other)\n        {\n            std::shared_lock<std::shared_mutex> lck(mtx);\n            m_ring_buffer = other.m_ring_buffer;\n        }\n        threadsafe_ring_buffer<T>& operator=(const threadsafe_ring_buffer<T>& other) = delete;\n\n        void wait_and_push(T new_value) {\n            std::unique_lock<std::shared_mutex> lck(mtx);\n            queue_not_full_cond.wait(lck, [this]() {return !m_ring_buffer.full();});\n            m_ring_buffer.push(new_value);\n            queue_not_empty_cond.notify_one();\n        }\n\n        bool try_push(T new_value) {\n            std::unique_lock<std::shared_mutex> lck(mtx);\n            if (!m_ring_buffer.full())\n                return false;\n\n            m_ring_buffer.push(new_value);\n            return true;\n        }\n\n        void wait_and_pop(T& value) {\n            std::unique_lock<std::shared_mutex> lck(mtx);\n            queue_not_empty_cond.wait(lck, [this]() { return !m_ring_buffer.empty();});\n            value = m_ring_buffer.front();\n            m_ring_buffer.pop();\n            queue_not_full_cond.notify_one();\n        }\n\n        void try_pop(T& value) {\n            std::unique_lock<std::shared_mutex> lck(mtx);\n            if (m_ring_buffer.empty())\n                return false;\n\n            value = m_ring_buffer.front();\n            m_ring_buffer.pop();\n            return true;\n        }\n\n        std::shared_ptr<T> wait_and_pop() {\n            std::unique_lock<std::shared_mutex> lck(mtx);\n            queue_not_empty_cond.wait([this]() { return !empty(); });\n            std::shared_ptr<T> result{ std::make_shared<T>(m_ring_buffer.front()) };\n            m_ring_buffer.pop();\n            return result;\n        }\n\n        std::shared_ptr<T> try_pop() {\n            std::unique_lock<std::shared_mutex> lck(mtx);\n            if (m_ring_buffer.empty())\n                return nullptr;\n\n            std::shared_ptr<T> result{ std::make_shared<T>(m_ring_buffer.front()) };\n            m_ring_buffer.pop();\n            return result;\n        }\n\n        bool empty() const {\n            std::shared_lock<std::shared_mutex> lck(mtx);\n            return m_ring_buffer.empty();\n        }\n\n        bool full() const {\n            std::shared_lock<std::shared_mutex> lck(mtx);\n            return m_ring_buffer.full();\n        }\n\n        int size() const {\n            std::shared_lock<std::shared_mutex> lck(mtx);\n            return m_ring_buffer.size();\n        }\n\n        int capacity() const {\n            std::shared_lock<std::shared_mutex> lck(mtx);\n            return m_ring_buffer.capacity();\n        }\n\n    private:\n        ring_buffer<T> m_ring_buffer;\n        mutable std::shared_mutex mtx;\n        std::condition_variable_any queue_not_empty_cond;\n        std::condition_variable_any queue_not_full_cond;\n    };\n}\n\nint main()\n{\n    dev::threadsafe_ring_buffer<int> buffer(64);\n\n    std::thread producer(\n        [&]() {\n            for (int i{ 1 };i <= 1000;++i)\n            {\n                try {\n                    buffer.wait_and_push(i);\n                }\n                catch (std::exception e) {\n                    std::cout << \"\\n\" << \"buffer full!\";\n                }\n            }\n        }\n    );\n\n    std::thread consumer(\n        [&]() {\n            for (int i{ 1 };i <= 1000;++i)\n            {\n                try {\n                    int value;\n                    buffer.wait_and_pop(value);\n                    std::this_thread::sleep_for(std::chrono::microseconds(1));\n                }\n                catch (std::exception e) {\n                    std::cout << \"\\n\" << \"buffer empty!\";\n                }\n            }\n        }\n    );\n\n    producer.join();\n    consumer.join();\n    \n    std::cout << \"\\nFinished execution\";\n}\n```\n\n## Semaphores\n\nC++20 introduces new synchronization primitives to write multi-threaded applications . \n\nA **semaphore** is a counter that manages the numberof permits available for accessing a share resource. Semaphores can be classified into two main types:\n\n- **Binary Semaphore**. It has only $2$ states: $0$ and $1$. Event though a binary semaphore is conceptually like a mutex, there are some differences between a binary semaphore and a mutex, that we'll explore later.\n- **Counting Semaphore**. It can have a value greater than $1$ and is used to control access to a resource that has a limited number of instances.\n\nC++20 implements both binary and counting semaphores.\n\n### Binary Semaphores\n\nA binary semaphore is a synchronization primitive that can be used to control access to a shared resource. It has two states: $0$ and $1$. A semaphore with a value of $0$ indicates that the resource is unavailable, while a semaphore with a value of $1$ indicates that the resource is available.\n\nThe most significant difference between mutexes and semaphores is that threads that have acquired a mutex have exclusive ownership of it. Only the thread owning the mutex can release it. Semaphores can be signaled by any thread. A mutex is a locking mechanism for a critical section, whereas a semaphore is more like a signaling mechanism. For this reason, semaphores are commonly used for signaling rather than for mutual exlusion.\n\nIn C++20, `std::binary_semaphore` is an alias for the specialization of `std::counting_semaphore` with `LeastMaxValue` being $1$.\n\nBinary semaphores must be initialized with either $1$ or $0$ as follows:\n\n```cpp\nstd::binary_semaphore smphr1{ 0 };\nstd::binary_semaphore smphr2{ 1 };\n```\n\nIf the initial value is `0`, acquiring the semaphore will block the thread trying to acquire it, and before it can be acquired, it must be released by another thread. Acquiring the semaphore decreases the counter, and releasing it increases the counter.\n\n### Counting semaphores\n\nA counting semaphore allows access to a shared resource by more than one thread. The counter can be initialized to an arbitrary number, and it will be decreased every time a thread acquires the semaphore.\n\nWe can design a thread-safe queue using semaphores instead of condition variables to synchronize access to the queue.\n\n\n\n\n```{cpp .filename=queue.cpp}\n#include <iostream>\n#include <shared_mutex>\n#include <semaphore>\n\nnamespace dev {\n    /* \n    A queue implements a first-in-first-out data-structure allowing \n    enqueuing (adding) items to the rear and dequeuing(removing)\n    them from the front.\n\n    My implementation uses a circular queue with doubling - the simplest\n    and reasonably efficient choice.\n\n    The interface design conforms to the standard library std::queue<T>\n    specification.\n    */\n    template<typename T>\n    class queue {\n    private:\n        enum{min_capacity = 8};\n        T* m_ring_buffer;\n        int m_front;\n        int m_rear;\n        int m_capacity;\n\n    public:\n        using value_type = T;\n        using reference = T&;\n\n        /* Constructors */\n\n        // Default constructor\n        queue() : queue(min_capacity){}\n\n        // Parametrized Constructor\n        queue(int capacity)\n            : m_ring_buffer{ nullptr }\n            , m_front{ 0 }\n            , m_rear{ 0 }\n            , m_capacity{ 0 }\n        {\n            m_ring_buffer = static_cast<T*>(operator new(capacity * sizeof(T)));\n            m_capacity = capacity;\n        }\n\n        /* Destructor */\n        ~queue() {\n            clear();\n            operator delete(m_ring_buffer);\n        }\n\n        /* Copy constructor \n        * Perform a deep-copy of the contents of the queue\n        */\n        queue(const queue& other) \n            : queue(other.m_capacity) // Allocation step\n        {\n            /* Call the copy constructor of T \n            * placing it directly into the pre-allocated\n            * storage at memory address &m_ring_buffer[i]\n            */\n            for (int i{ 0 };i < other.size();++i) {\n                new (&m_ring_buffer[i]) T(other[i]);\n                ++m_rear;\n            }\n        }\n\n        /* Swap the contents of lhs and rhs member-by-member */\n        friend void swap(queue& lhs, queue& rhs) {\n            std::swap(lhs.m_ring_buffer, rhs.m_ring_buffer);\n            std::swap(lhs.m_front, rhs.m_front);\n            std::swap(lhs.m_rear, rhs.m_rear);\n            std::swap(lhs.m_capacity, rhs.m_capacity);\n        }\n\n        /* Copy-assignment */\n        queue& operator=(const queue& other) {\n            queue temp{ other };    // Copy-construct\n            swap(*this, temp);      // and swap idiom\n            return (*this);\n        }\n\n        /* Move constructor */\n        queue(queue&& other)\n            : m_ring_buffer{ std::move(other.m_ring_buffer) }\n            , m_front{ std::move(other.m_front) }\n            , m_rear{ std::move(other.m_rear) }\n            , m_capacity{ std::move(other.m_capacity) }\n        {\n            other.m_ring_buffer = nullptr;\n        }\n\n        /* Move assignment */\n        queue& operator=(queue&& other) {\n            queue temp{ std::move(other) };\n            std::swap(*this, temp);\n            return (*this);\n        }\n\n        /* Capacity*/\n\n        /* Checks whether the queue is empty */\n        bool empty()\n        {\n            return (m_rear == m_front);\n        }\n\n        bool full() {\n            return (m_rear == m_front + m_capacity);\n        }\n\n        /* Returns the number of elements in the queue*/\n        int size() const {\n            return (m_rear - m_front);\n        }\n\n        int capacity() const {\n            return m_capacity;\n        }\n\n        /* Modifiers */\n\n        /* Double the capacity of the queue */\n        void resize()\n        {\n            std::cout << \"\\n\" << \"Resizing the queue from \" << m_capacity \n                << \" to \" << 2 * m_capacity << \" elements.\";\n\n            // 1. Allocation step\n            T* new_array = static_cast<T*>(operator new(2 * m_capacity * sizeof(T)));\n\n            std::cout << \"\\n\" << \"Allocation complete.\";\n\n            // 2. Copy over the elements of the queue to the newly\n            // allocated storage.\n            int new_size = size();\n            for (int i{ 0 };i < new_size;++i) {\n                new (&new_array[i]) T(std::move(m_ring_buffer[(i + m_front) % m_capacity]));\n            }\n\n            std::cout << \"\\n\" << \"Copy-construction complete\";\n\n            // 3. Destroy the old array\n            clear();\n            operator delete(m_ring_buffer);\n\n            std::cout << \"\\n\" << \"Destruction of old array complete\";\n            \n            // Re-wire m_ring_buffer, set front, rear and capacity\n            m_ring_buffer = new_array;\n            m_front = 0;\n            m_rear = new_size;\n            m_capacity *= 2;\n        }\n\n        /* Push the given value to the end of the queue */\n        void push(const T& value) {\n            std::cout << \"\\n\" << \"Pushing \" << value << \" to the queue\";\n            if (full())\n                resize();\n            new (&m_ring_buffer[(m_rear % m_capacity)]) T(value);\n            std::cout << \"\\n\" << \"Pushed \" << value << \" to the queue\";\n            m_rear++;\n        }\n\n        void push(T&& value) {\n            std::cout << \"\\n\" << \"Pushing \" << value << \" to the queue\";\n            if (full())\n                resize();\n            new(&m_ring_buffer[(m_rear % m_capacity)]) T(std::move(value));\n            m_rear++;\n        }\n\n        /* Removes an element from the front of the queue */\n        void pop() {\n            m_ring_buffer[m_front % m_capacity].~T();\n            m_front++;\n        }\n\n        /* Element access */\n\n        T& operator[](int i) {\n            return m_ring_buffer[(m_front + i) % m_capacity];\n        }\n\n        T& operator[](int i) const {\n            return m_ring_buffer[(m_front + i) % m_capacity];\n        }\n\n        /* Returns a reference to the first element in the queue */\n        T& front() {\n            return m_ring_buffer[m_front % m_capacity];\n        }\n\n        /* Return a reference to the last element in the queue */\n        T& back() {\n            return m_ring_buffer[(m_rear - 1) % m_capacity];\n        }\n\n    private:\n        /* Helper function to clear the queue */\n        void clear() {\n            for (int i{ 0 }; i < size(); ++i) {\n                m_ring_buffer[(m_front + i) % m_capacity].~T();\n            }\n        }\n    };\n}\n\nint main()\n{\n    dev::queue<double> q;\n    std::cout << \"\\n\" << \"Queue capacity = \" << q.capacity();\n    \n    for (double i{ 1.0 };i <= 10.0;i++)\n        q.push(i);\n    \n    std::cout << \"\\n\" << \"Queue capacity = \" << q.capacity();\n}\n```\n\n\n\n\n[Play on Compiler Explorer](https://godbolt.org/z/7xbY4x5x3)\n\n## SPMC queues and coding up a `ThreadPool`\n\nA *thread-pool* is a group of pre-instantiated, idle threads which stand ready to be given work. These are preferred over instantiating new threads for each task whenever there are a large number of short tasks to be done rather than a small number of long ones. This prevents having to incur the overhead of creating a thread a large number of times and starvation of threads. \n\nThe `ThreadPool` class has a container for the worker threads as one of its member-variables. \n\nWhen a *thread-pool* is handed a `Task`, it is added to a SPMC queue. If a thread from the `ThreadPool` is idle, it can `pop()` the next task off the `TaskQueue` and executes it. Once the execution is complete, the thread hands itself back to the pool to be put into the container for reuse and until the `queue_not_empty_cond` condition is met, and the cycle repeats.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
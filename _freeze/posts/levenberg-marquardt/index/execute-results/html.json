{
  "hash": "b87fe009ecaf262f96fa3562db755f91",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"Levenberg-Marquardt Algorithm\"\nauthor: \"Quasar\"\ndate: \"2025-02-01\"\ncategories: [Numerical Methods]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\nengine: julia\nformat:\n    html:\n        code-tools: true\n        code-block-border-left: true\n        code-annotations: below\n        highlight-style: pygments\n---\n\n\n\n\n# The Levenberg Marquardt Algorithm\n\n## Algorithm Description\n\nThe Levenberg-Marquardt(LM) method consists of an iterative least-squares minimization of a function based on a modification of the Newton method. It's a super-intuitive algorithm and a generic implementation can be very quickly coded up. I state the problem formally before defining the algorithm. We'll use finite differences to approximate the first and second-order derivatives of the function. \n\nLet $\\mathbf{x}\\in\\mathbf{R}^n$ be the parameter vector to be optimized. We want to find the optimal $\\mathbf{x}^*$ that minimizes the scalar error function:\n\n$$\n\\begin{align*}\nF(\\mathbf{x}) = \\frac{1}{2}||\\mathbf{r}(\\mathbf{x})||^2 = \\frac{1}{2}\\mathbf{r}(\\mathbf{x})^T \\mathbf{r}(\\mathbf{x})\n\\end{align*}\n$$\n\nThe residual error function $\\mathbf{r}:\\mathbf{R}^n \\to \\mathbf{R}^m$ may sometimes include a comparison to reference or observed data. A very simple linear example would $\\mathbf{r}(\\mathbf{x}) = \\mathbf{b} - \\mathbf{Ax}$. However, in the following, I assume that $\\mathbf{r}(\\cdot)$ is any vector-valued function:\n\n$$\n\\begin{align*}\n\\mathbf{r}(\\mathbf{x}) = (r_1(\\mathbf{x}),f_2(\\mathbf{x}),\\ldots,r_m(\\mathbf{x}))\n\\end{align*}\n$$\n\nWe can define the Jacobian of the residual error functions as $m \\times n$ matrix with entries :\n\n$$\n\\mathbf{J}_{ij}(\\mathbf{x}) = \\frac{\\partial r_i}{\\partial x_j}(\\mathbf{x})\n$$\n\nWe can also define the Hessian of the residual error functions as the $n \\times n$ matrix with entries :\n\n$$\n\\begin{align*}\n\\mathbf{H}_{ij}(\\mathbf{x}) = \\frac{\\partial^2 r_i}{\\partial x_i \\partial x_j} (\\mathbf{x})\n\\end{align*}\n$$\n\nThe gradient of the scalar-valued function $F$, by the $uv$ product rule is:\n\n$$\n\\begin{align*}\n\\nabla F(\\mathbf{x}) = D\\mathbf{r}(\\mathbf{x}) \\mathbf{r}(\\mathbf{x}) = \\mathbf{J}(\\mathbf{x})\\cdot \\mathbf{r}(\\mathbf{x})\n\\end{align*}\n$$\n\nThe Hessian of the function $F$ is:\n\n$$\n\\begin{align*}\n\\nabla^2 F(\\mathbf{x}) &= D\\left\\{\\sum_{j=1}^{m} \\nabla r_j(\\mathbf{x}) \\cdot r_j(\\mathbf{x})\\right\\}\\\\\n&= \\sum_{j=1}^m \\nabla^2 r_j(\\mathbf{x}) r_j(\\mathbf{x}) + (\\nabla r_j(\\mathbf{x}))^2\n\\end{align*}\n$$\n\nIf the derivatives $\\nabla^2 r_j(\\mathbf{x})$ are small, they can be dropped and the Hessian in this case simply becomes:\n\n$$\n\\nabla^2 F(\\mathbf{x}) = \\nabla r(\\mathbf{x})^T \\nabla(r(\\mathbf{x})) = \\mathbf{J}(\\mathbf{x})^T \\cdot \\mathbf{J}(\\mathbf{x})\n$$\n\nThen, the LM method minimizes the following $2$nd-order Taylor's expansion of the actual error function:\n\n$$\nF(\\mathbf{x}^{(k)} + \\mathbf{h}) - F(\\mathbf{x}^{(k)}) = \\mathbf{h} \\nabla F(\\mathbf{x}^{(k)}) + \\frac{1}{2}\\mathbf{h}^T \\nabla^2 F(\\mathbf{x}^{(k)}) \\mathbf{h}\n$${#eq-error-function-to-be-minimized}\n\nDescent methods like gradient descent can place too much trust in their first- or second- order information, which can result in excessively large steps or premature convergence. \n\nSo, in LM, we add a penalty term \n\n$$ \\frac{1}{2} \\lambda^{(k)} \\mathbf{h}^T \\mathbf{h} = \\frac{1}{2} \\lambda^{(k)} ||\\mathbf{x} - \\mathbf{x}^{(k)}||^2$${#eq-penalty-for-the-level-of-distrust}\n\nto the above @eq-error-function-to-be-minimized, that we want to minimize. That's because, we don't want to go too far away from $\\mathbf{x}^{(k)}$. It's not because, we think the solution is not too far away. The actual solution could be far away. But, that's a question of trust. And $\\lambda^{(k)}$ essentially gives you your level of distrust. If $\\lambda^{(k)}$ is super-big, it means that you don't trust the model very much, or you trust it, but only if you are very close to $\\mathbf{x}^{(k)}$. When $\\lambda^{(k)}$ gets really small, it means you really trust your model. And you're gonna find that $\\mathbf{x}$ is going to very far from $\\mathbf{x}^{(k)}$. So, that's the gist. Putting together,\n\n$$\nE(\\mathbf{h}) = \\mathbf{h} \\nabla F(\\mathbf{x}^{(k)}) + \\frac{1}{2}\\mathbf{h}^T \\nabla^2 ( F(\\mathbf{x}^{(k)}) + \\lambda^{(k)} I )\\mathbf{h}\n$${#eq-expression-to-minimize}\n\nWe can just solve for the optimal step-size $\\mathbf{h}_{lm}$ analytically. Taking the first derivative with respect to the step-size $\\mathbf{h}$ and setting it equal to zero:\n\n$$\n\\nabla E(\\mathbf{h}) = \\nabla F(\\mathbf{x}^{(k)}) + \\mathbf{h}_{lm}( \\nabla^2 F(\\mathbf{x}^{(k)}) + \\lambda^{(k)}I) = 0\n$${#eq-first-derivative}\n\nConsequently, \n\n$$\n\\begin{align*}\n\\mathbf{h}_{lm} &= -(\\nabla^2 F(\\mathbf{x}^{(k)}) + \\lambda^{(k)}I)^{-1} \\nabla F(\\mathbf{x}^{(k)})\\\\\n&=-(\\mathbf{J}(\\mathbf{x}^{(k)})^T \\mathbf{J}(\\mathbf{x})^{(k)} + \\lambda^{(k)}I)^{-1} \\mathbf{J}(\\mathbf{x}^{(k)}) \\mathbf{r}(\\mathbf{x}^{(k)})\n\\end{align*}\n$${#eq-optimal-step-size}\n\nOur best estimate of the minima, is consequently:\n\n$$\n\\begin{align*}\n\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\mathbf{h}_{lm}\\\\\n&= \\mathbf{x}^{(k)} -(\\mathbf{J}(\\mathbf{x}^{(k)})^T \\mathbf{J}(\\mathbf{x})^{(k)} + \\lambda^{(k)}I)^{-1} \\mathbf{J}(\\mathbf{x}^{(k)}) \\mathbf{r}(\\mathbf{x}^{(k)})\n\\end{align*}\n$${#eq-lm-iteration}\n\n\n## Updating $\\lambda^{(k)}$\n\nA trust-region method, or restricted step method maintains a local model of the trust region. It depends on the success of the previous step. If the step $\\mathbf{h}_{lm}$ results in a decrease in $||F(\\mathbf{x})||^2$, then we reduce $\\lambda^{(k)}$, otherwise we increase the value of this parameter.\n\nSo, we can use the following update mechanism:\n\n- If $||F(\\mathbf{x}^{(k+1)})||^2$ < $||F(\\mathbf{x}^{(k)})||^2$, accept the new $x$ and reduce $\\lambda$\n\n$$ \\lambda^{(k+1)} = 0.8 \\lambda^{(k)}$$\n\n- otherwise, we increase the $\\lambda$ and do not update $\\mathbf{x}$:\n\n$$ \\lambda^{(k+1)} = 2 \\lambda^{k}, \\quad \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)}$$\n\n## Generic implementation in Julia\n\n\n\n\n::: {#2 .cell execution_count=1}\n``` {.julia .cell-code}\nusing Pkg\n\n```\n:::\n\n\n\n\n\n\n# References {.appendix}\n\n- [Levenberg Marquardt Iteration](https://www.youtube.com/watch?v=UQsOyMj9lnI), Professor Stephen Boyd, Stanford ENGR108\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
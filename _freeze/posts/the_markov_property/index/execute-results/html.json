{
  "hash": "b516977e4c91926ef4e628016c4243ec",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The Markov Property\"\nauthor: \"Quasar\"\ndate: \"2024-07-12\"\ncategories: [Stochastic Calculus]      \nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 3\n---\n\n\n\n\n## The Markov Property for Diffusions\n\nLet's start by exhibiting the Markov property of Brownian motion. To see this, consider $(\\mathcal{F}_t,t\\geq 0)$, the natural filtration of the Brownian motion $(B_t,t\\geq 0)$. Consider $g(B_t)$ for some time $t$ and bounded function $g$. (For example, $g$ could be an indicator function.) Consider also a random variable $W$ that is $\\mathcal{F}_s$ measurable for $s < t$. (For example, $W$ could be $B_s$ or $1_{B_s > 0}$.) Let's compute $\\mathbb{E}[g(B_t)W]$.\n\n\\begin{align*}\n\\mathbb{E}[g(B_t)W] &= \\mathbb{E}[\\mathbb{E}[Wg(B_t - B_s + B_s)|\\mathcal{F}_s]]\n\\end{align*}\n\nThe random variable $(B_t - B_s)$ follows a $\\mathcal{N}(0,t-s)$ distribution. By LOTUS,\n\n$$\n\\begin{align*}\n\\mathbb{E}[g(B_t)W] \n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)|\\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\\\\n&= \\{\\text{ Using the fact that }B_t - B_s \\perp B_s\\}\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]\\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\n$$\n\nBy Fubini's theorem, the integral and the expectation operator can be interchanged, and since $W$ is $\\mathcal{F}_s$ measurable, it follows from the definition of conditional expectations that:\n\n$$\n\\begin{align*}\n\\mathbb{E}[Wg(B_t)] = \\mathbb{E}[W\\mathbb{E}[g(B_t)|\\mathcal{F}_s]] = \\mathbb{E}\\left[W\\int_{\\mathbb{R}}g(y + B_s)]\\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\\right]\n\\end{align*}\n$$\n\nIt follows that:\n\n$$\n\\begin{align*}\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] = \\int_{\\mathbb{R}}g(y + B_s)]\\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\n$${#eq-conditional-expectation-of-function-brownian-motion}\n\nWe make two important observations. First, the right hand side is a function of $s,t$ and $B_s$ only (and not of the Brownian motion before time s). In particular, we have:\n\n$$\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] = \\mathbb{E}[g(B_t)|B_s]\n$$\n\nThis holds for any bounded function $g$. In particular, it holds for all indicator functions. This implies that the conditional distribution of $B_t$ given $\\mathcal{F}_s$ depends solely on $B_s$, and not on other values before time $s$. Second, the right-hand side is *time-homogenous* in the sense that it depends on the time difference $t-s$. \n\nWe have just shown that Brownian motion is a *time-homogenous Markov process*. \n\n::: {#def-markov-process}\n\n### Markov process.\n\nConsider a stochastic process $(X_t,t\\geq 0)$ and its natural filtration $(\\mathcal{F}_t,t\\geq 0)$. It is said to be a *Markov process* if and only if for any (bounded) function $g: \\mathbb{R} \\to \\mathbb{R}$, we have:\n\n$$\n\\mathbb{E}[g(X_t) | \\mathcal{F}_s] = \\mathbb{E}[g(X_t) | X_s], \\quad \\forall t \\geq 0, \\forall s \\leq t\n$$ {#eq-markov-process}\n\n:::\n\nThis implies that $\\mathbb{E}[g(X_t)|\\mathcal{F}_s]$ is an explicit function of $s$, $t$ and $X_s$. It is said to be *time-homogenous*, if it is a function of $t-s$ and $X_s$. Since the above holds for all bounded $g$, the conditional distribution of $X_t$ given $\\mathcal{F}_s$ is the same as the conditional distribution of $X_t$ given $X_s$. \n\nOne way to compute the conditional distribution of $X_t$ given $\\mathcal{F}_s$ is to compute the conditional MGF given $\\mathcal{F}_s$, that is:\n\n$$\n\\mathbb{E}[e^{a X_t}|\\mathcal{F}_s], \\quad a \\geq 0\n$$ {#eq-conditional-mgf-of-xt}\n\nThe process would be Markov, if the conditional MGF is an explicit function of $s$, $t$ and $X_s$. \n\n::: {#exm-brownian-motion-is-markov}\n\n(Brownian Motion is Markov) Let $(B_t,t\\geq 0)$ be a standard brownian motion. Our claim is that the brownian motion is a markov process.\n:::\n\n*Proof.*\n\nWe have:\n\n\\begin{align*}\n\\mathbb{E}[e^{a B_t}|\\mathcal{F}_s] &= \\mathbb{E}[e^{a (B_t - B_s + B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_s \\text{ is }\\mathcal{F}_s-\\text{ measurable }\\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_t - B_s \\perp \\mathcal{F}_s \\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}]\\\\\n&= e^{a B_s} e^{\\frac{1}{2}a^2(t-s)}\n\\end{align*}\n\nThis closes the proof. $\\blacksquare$\n\nAn equivalent (but more symmetric) way to express the Markov property is to say that *the future of the process is independent of the past, when conditioned on the present*. Concretely, this means that for any $r < s< t$, we have that $X_t$ is independent of $X_r$, when we condition on $X_s$.\n\nThe conditional distribution of $X_t$ given $X_s$ is well described using *transition probabilities*. We will more interested in a case well these probabilities admit a density $f_{X_t|X_s=x}(y)$. More precisely, for such a Markov process, we have:\n\n$$\n\\begin{align*}\n\\mathbb{E}[g(X_t)|X_s = x] &= \\int_{\\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\\\\n&=\\int_{\\mathbb{R}} g(y) p(y,t|x,s) dy\n\\end{align*}\n$$\n\nHere, we explicitly write the left-hand side as a function of space, that is, the position $X_s$, by fixing $X_s = x$. In words, the *transition probability density* $p(y,t|x,s)$ represents the probability density that starting from $X_s = x$ at time $s$, the process ends up at $X_t = y$ at time $t > s$. If the process is time-homogenous, this only depends on the time difference $(t-s)$ and we write $p(y,t|x,s)$. From @eq-conditional-expectation-of-function-brownian-motion, we can write:\n$$\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(u + x) \\frac{e^{-\\frac{u^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} du\n$$\n\nIn the above expression, the random variable $B_t - B_s$ takes some value $u \\in \\mathbb{R}$ and $B_s = x$ is fixed. Then, $B_t$ takes the value $u + x$. Let $y = u + x$. Then, $u = y - x$. Consequently, we may write:\n\n$$\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(y) \\frac{e^{-\\frac{(y-x)^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} dy\n$$\n\nSo, the transition density function for standard Brownian motion is:\n\n$$\np(y,t|x,0)= \\frac{e^{-\\frac{(y-x)^2}{2s}}}{\\sqrt{2\\pi s}}, \\quad s>0, x,y\\in\\mathbb{R}\n$$ {#eq-brownian-motion-transition-density-function}\n\nThis function is sometimes called the *heat kernel*, as it relates to the *heat equation*. \n\nThe Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process $(X_t,t\\geq 0)$ at different times. Consider the functions $f = \\mathbf{1}_A$ and $g = \\mathbf{1}_B$ from $\\mathbb{R} \\to \\mathbb{R}$, where $A$ and $B$ are two intervals in $\\mathbb{R}$. Let's compute $\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) = \\mathbb{E}[\\mathbf{1}_{A} \\mathbf{1}_{B}] = \\mathbb{E}[f(X_{t_1}) g(X_{t_2})]$ for $t_1 < t_2$. By the properties of conditional expectation and the Markov property, we have:\n\n\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\mathbb{E}[f(X_{t_1})g(X_{t_2})]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|\\mathcal{F}_{t_1}]]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|X_{t_1}]]\n\\end{align*}\n\nAssuming that the process is time-homogenous and admits a transition density $p(y,t|x,0)$ as for Brownian motion, this becomes:\n\n\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\int_{\\mathbb{R}} f(x_1) \\left(\\int_{\\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\\\\\n&= \\int_{A} \\left(\\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\n\\end{align*}\n\nThis easily generalizes to any finite-dimensional distribution of $(X_t, t\\geq 0)$.\n\n::: {#exm-markov-versus-martingale}\n\n(Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift $(X_t, t \\geq 0)$, where $X_t = \\sigma B_t + \\mu t$. Conversely, take $Y_t = \\int_0^t X_s dB_s$, where $X_s = \\int_0^s B_u dB_u$. The integrand $X_s$ depends on whole Brownian motion path upto time $s$ and not just on $B_s$. \n:::\n\n::: {#nte-functions-of-markov .callout-tip}\n\n### Functions of Markov Processes\n\nIt might be tempting to think that if $(X_t,t\\geq 0)$ is a Markov process, then the process defined by $Y_t = f(X_t)$ for some reasonable function $f$ is also Markov. Indeed, one could hope to write for an arbitrary bounded function $g$:\n\n$$\n\\begin{align*}\n\\mathbb{E}[g(Y_t)|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{X}_s] \n\\end{align*}\n$$ {#eq-functions-of-markov-process}\n\nby using the Markov property of $(X_t,t\\geq 0)$. The flaw in this reasoning is that the Markov property should hold for the natural fitration $(\\mathcal{F}_t^Y,t\\geq 0)$ of the process $(Y_t,t\\geq 0)$ and not the one of $(X_t,t\\geq 0)$, $(\\mathcal{F}_t^X,t\\geq 0)$. It might be that the filtration $(\\mathcal{F}_t^Y,t\\geq 0)$ has less information that $(\\mathcal{F}_t^X,t\\geq 0)$, especially, if the function $f$ is not one-to-one. For example, if $f(x)=x^2$, then $\\mathcal{F}_t^Y$ has less information than $\\mathcal{F}_t^X$ as we cannot recover the sign of $X_t$ knowing $Y_t$. In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when $f$ is not one-to-one. \n:::\n\nIt turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.\n\n::: {#thm-diffusions-are-markov-processes}\n\n### Diffusions are Markov processes. \n\nLet $(B_t,t\\geq 0)$ be a standard Brownian motion. Let $\\mu : \\mathbb{R} \\to \\mathbb{R}$ and $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ be differentiable functions with bounded derivatives on $[0,T]$. Then, the diffusion with the SDE \n\n$$\ndX_t = \\mu(X_t) dt + \\sigma(X_t)dB_t, \\quad X_0 = x_0\n$$\n\ndefines a time-homogenous markov process on $[0,T]$. \n:::\n\nAn analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments. \n\n*Proof.*\n\nBy the [existence and uniqueness theorem](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes), this stochastic initial value problem(SIVP) defines a unique continous adapted process $(X_t,t\\leq T)$. Let $(\\mathcal{F}_t^X,t\\geq 0)$ be the natural filtration of $(X_t,t\\leq T)$. For a fixed $t > 0$, consider the process $W_s = B_{t+s} - B_t, s \\geq 0$. Let $(\\mathcal{F}_t,t \\geq 0)$ be the natural filtration of $(B_t,t \\geq 0)$. It turns out that the process $(W_s,s \\geq 0)$ is a standard brownian motion independent of $\\mathcal{F}_t$ (@exr-shifted-brownian-motion). For $s \\geq 0$, we consider the SDE:\n\n$$\ndY_s = \\mu (Y_s) ds + \\sigma(Y_s) dW_s, \\quad Y_0 = X_t\n$$\n\nAgain by the [existence and uniqueness theorem](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes), there exists a unique solution to the SIVP that is adapted to the natural filtration of $W$. Note that, the shifted process $(X_{t+s},s\\geq 0)$ is *the* solution to this SIVP since:\n\n\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{t}^{t+s}\\mu(X_u) du + \\int_{t}^{t+s}\\sigma(X_u) dB_u\n\\end{align*}\n\nPerform a change of variable $v = u - t$. Then, $dv = du$, $dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v$. So,\n\n\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{0}^{s}\\mu(X_{t+v}) dv + \\int_{0}^{s}\\sigma(X_{t+v}) dW_v\n\\end{align*}\n\nLet $Y_v= X_{t+v}$, $Y_0 = X_t$. Then,\n\n\\begin{align*}\nY_s &= Y_0 + \\int_{0}^{s}\\mu(Y_v) dv + \\int_{0}^{s}\\sigma(Y_v) dW_v\n\\end{align*}\n\nThus, we conclude that for any interval $A$:\n\n$$\n\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(Y_s \\in A | \\mathcal{F}_t^X)\n$$\n\nBut, since $(Y_s,s \\geq 0)$ depends on $\\mathcal{F}_t^X$ only through $X_t$ (because $(W_s,s \\geq 0)$ is independent of $\\mathcal{F}_t$), we conclude that $\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(X_{t+s} \\in A|X_t)$, so $(X_t,t \\geq 0)$ is a time-homogenous markov process. $\\blacksquare$\n\n## The Strong Markov Property\n\nThe Doob's Optional Stopping theorem extended some properties of martingales to stopping times. The Markov property can also be extended to stopping times for certain processes. These processes are called *strong Markov processes*. \n\nWe know, that the sigma-algebra $\\mathcal{F}_t$ represents the set of all observable events upto time $t$. What is the sigma-algebra of observable events at a random stopping time $\\tau$? \n\n::: {#def-sigma-algebra-of-the-past}\n\n### $\\sigma$-algebra of $\\tau$-past\n\nLet $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\geq 0},\\mathbb{P})$ be a filtered probability space. The sigma-algebra at the stopping time $\\tau$ is then:\n\n$$\n\\mathcal{F}_{\\tau} = \\{A \\in \\mathcal{F}_\\infty : A \\cap \\{\\tau \\leq t\\} \\in \\mathcal{F}_t, \\forall t \\geq 0 \\}\n$$ {#eq-sigma-algebra-of-the-past}\n\n:::\n\nIn words, an event $A$ is in $\\mathcal{F}_\\tau$, if we can determine if $A$ and $\\{\\tau \\leq t\\}$ both occurred or not based on the information $\\mathcal{F}_t$ known at any arbitrary time $t$. You should be able to tell the value of the random variable $\\mathbf{1}_A \\cdot \\mathbf{1}_{\\{\\tau \\leq t\\}}$ given $\\mathcal{F}_t$ for any arbitrary time $t \\geq 0$.\n\nFor example, if $\\tau < \\infty$, the event $\\{B_\\tau > 0\\}$ is in $\\mathcal{F}_\\tau$. However, the event $\\{B_1 > 0\\}$ is not in $\\mathcal{F}_\\tau$ in general, since $A \\cap \\{\\tau \\leq t\\}$ is not in $\\mathcal{F}_t$ for $t < 1$. Roughly speaking, a random variable that is $\\mathcal{F}_\\tau$-measurable should be thought of as an explicit function of $X_\\tau$. With this new object, we are ready to define the *strong markov property*.\n\n::: {#def-strong-markov-property}\n\n### Strong Markov Property\n\nLet $(X_t,t\\geq 0)$ be a stochastic process and let $(\\mathcal{F}_t,t\\geq 0)$ be its natural filtration. The process $(X_t,t\\geq 0)$ is said to be *strong markov* if for any stopping time $\\tau$ for the filtration of the process and any bounded function $g$:\n\n$$\n\\mathbb{E}[g(X_{t+\\tau})|\\mathcal{F}_\\tau] = \\mathbb{E}[g(X_{t+\\tau})|X_\\tau]\n$$\n\n:::\n\nThis means that $X_{t+\\tau}$ depends on $\\mathcal{F}_\\tau$ solely through $X_\\tau$ (whenever $\\tau < \\infty$). It turns out that Brownian motion is a strong markov process. In fact a stronger statement holds which generalizes @exr-shifted-brownian-motion. \n\n::: {#thm-shifted-brownian-motion-about-a-stopping-time}\n\nLet $\\tau$ be a stopping time for the filtration of the Brownian motion $(B_t,t\\geq 0)$ such that $\\tau < \\infty$. Then, the process:\n\n$$\n(B_{t+\\tau} - B_{\\tau},t\\geq 0)\n$$\n\nis a standard brownian motion independent of $\\mathcal{F}_\\tau$.\n:::\n\n::: {#exm-brownian-motion-is-strong-markov}\n\n(Brownian motion is strong Markov) To see this, let's compute the conditional MGF as in @eq-conditional-mgf-of-xt. We have:\n\n$$\n\\begin{align*}\n\\mathbb{E}[e^{aB_{t+\\tau}}|\\mathcal{F}_\\tau] &= \\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau + B_\\tau)}|\\mathcal{F}_\\tau]\\\\\n&= e^{aB_\\tau} \\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau)}|\\mathcal{F}_\\tau]\\\\\n& \\{ B_\\tau \\text{ is }\\mathcal{F}_\\tau-\\text{measurable }\\}\\\\\n&= e^{aB_\\tau}\\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau)}]\\\\\n& \\{ (B_{t+\\tau} - B_\\tau) \\perp \\mathcal{F}_\\tau\\}\\\\\n&= e^{aB_\\tau}e^{\\frac{1}{2}a^2 t}\\\\\n\\end{align*}\n$$\n\nThus, the conditional MGF is an explicit function of $B_\\tau$ and $t$. This proves the proposition. $\\blacksquare$\n:::\n\n*Proof* of @thm-shifted-brownian-motion-about-a-stopping-time.\n\nWe first consider for fixed $n$ the discrete valued stopping time:\n\n$$\n\\tau_n = \\frac{k + 1}{2^n}, \\quad \\text{ if } \\frac{k}{2^n} \\leq \\tau < \\frac{k+1}{2^n}, k\\in \\mathbb{N}\n$$\n\nIn other words, if $\\tau$ occurs in the interval $[\\frac{k}{2^n},\\frac{k+1}{2^n})$, we stop at the next dyadic $\\frac{k+1}{2^n}$. By construction $\\tau_n$ depends only on the process in the past. Consider the process $W_t = B_{t + \\tau_n} - B_{\\tau_n}, t \\geq 0$. We show it is a standard brownian motion independent of $\\tau_n$. This is feasible as we can decompose over the discrete values taken by $\\tau_n$. More, precisely, take $E \\in \\mathcal{F}_{\\tau_n}$, and some generic event $\\{W_t \\in A\\}$ for the process $W$. Then, by decomposing over the values of $\\tau_n$, we have:\n\n$$\n\\begin{align*}\n\\mathbb{P}(\\{W_t \\in A\\} \\cap E) &= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{W_t \\in A\\} \\cap E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\\\\\n&= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\} \\cap E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\\\\\n&= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\}\\right) \\times \\mathbb{P}\\left( E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\n\\end{align*}\n$$\n\nsince $(B_{t+k/2^n} - B_{k/2^n})$ is independent of $\\mathcal{F}_{k/2^n}$ by @exr-shifted-brownian-motion and since $E \\cap \\{\\tau_n = \\frac{k}{2^n}\\} \\in \\mathcal{F}_{k/2^n}$ by definition of stopping time. But, given $\\{\\tau_n = k/2^n\\}$, the event $\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\}$ is the same as $\\{B_t \\in A\\} = \\{W_t \\in A\\}$, since this process is now a standard brownian motion. Thus, $\\mathbb{P}\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\} = \\mathbb{P}\\{B_t \\in A\\} = \\mathbb{P}\\{W_t \\in A\\}$, dropping the dependence on $k$. The sum over $k$ then yields:\n\n$$\n\\mathbb{P}\\left(\\{W_t \\in A\\}\\cap E\\right) = \\mathbb{P}(W_t \\in A) \\mathbb{P}(E)\n$$\n\nas claimed. The extension to $\\tau$ is done by using continuity of paths. We have:\n\n$$\n\\lim_{n \\to \\infty} B_{t + \\tau_n} - B_{\\tau_n} = B_{t+\\tau} - B_{\\tau} \\text{ almost surely}\n$$\n\nNote, that this only uses right continuity! Moreover, this implies that $B_{t+\\tau} - B_\\tau$ is independent of $\\mathcal{F}_{\\tau_n}$ for all $n$. Again by (right-)continuity this extends to independence of $\\mathcal{F}_\\tau$. The limiting distribution of the process is obtained by looking at the finite dimensional distributions of the increments of $B_{t+\\tau_n} - B_{\\tau_n}$ for a finite number of $t$'s and taking the limit as above. $\\blacksquare$\n\nMost diffusions also enjoy the strong markov property, as long as the functions $\\sigma$ and $\\mu$ encoding the volatility and drift are nice enough. This is the case for the diffusions we have considered. \n\n::: {#thm-most-diffusions-are-strong-markov}\n\n### Most diffusions are strong markov\n\nConsider a diffusion $(X_t,t\\leq T)$ as as in @thm-diffusions-are-markov-processes. Then, the diffusion has strong markov property. \n:::\n\nThe proof follows the line of the one of @thm-diffusions-are-markov-processes\n\n*Proof.*\n\nConsider the time-homogenous diffusion:\n\n$$\ndX_t = \\mu(X_t)dt + \\sigma(X_t)dB_t\n$$\n\nBy the [existence and uniqueness theorem](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes), this SIVP defines a unique continuous adapted process $(X_t,t \\geq 0)$. Let $\\mathfrak{F}=(\\mathcal{F}_t^X,t \\geq 0)$ be the natural filtration of $(X_t, t\\leq T)$. Let $\\tau$ be a stopping time for the filtration $\\mathfrak{F}$ and consider the process $W_t = B_{t+\\tau} - B_\\tau$. From @thm-shifted-brownian-motion-about-a-stopping-time, we know that the process $(W_t,t\\geq 0)$ is a standard brownian motion independent $\\mathcal{F}_\\tau$. For $s \\geq 0$, we consider the SDE:\n\n$$\ndY_s = \\mu(Y_s)ds + \\sigma(Y_s)dW_s, \\quad Y_0 = X_\\tau\n$$ {#eq-diffusion-of-Y}\n\nAgain by the [existence and uniqueness theorem](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes), there exists a unique solution to the SIVP that is adapted to the natural filtration of $W$. We claim that $(X_{s+\\tau},s \\geq 0)$ is the solution to this equation, since:\n\n$$\nX_{s+\\tau} = X_\\tau + \\int_\\tau^{s+\\tau} \\mu(X_u)du + \\int_{\\tau}^{s+\\tau} \\sigma(X_u)dB_u\n$$\n\nPerform a change of variable $v = u - \\tau$. Then, the limits of integration bare, $v = 0$ and $v = s$. And $dv = du$. \n\n$dB_u  \\approx B_{u_2} - B_{u_1} = B(v_1 + \\tau) - B(v_2 + \\tau) = W(v_2) - W(v_1) =dW_v$. \n\n$$\nX_{s+\\tau} = X_\\tau + \\int_0^{s} \\mu(X_{v+\\tau})dv + \\int_{0}^{s} \\sigma(X_{v+\\tau})dW_v\n$$\n\nIf we let $Y_0 = X_\\tau$, $Y_v = X_{v+\\tau}$, we recover the dynamics of $(Y_v,v \\geq 0)$ in @eq-diffusion-of-Y. So, $(X_{s+\\tau},s\\geq 0)$ is the solution to the SIVP in @eq-diffusion-of-Y. Thus, we conclude for any interval $A$:\n\n$$\n\\mathbb{P}(X_{s+\\tau} \\in A | \\mathcal{F}_\\tau^X) = \\mathbb{P}(Y_v \\in A| \\mathcal{F}_\\tau^X)\n$$\n\nBut, since $(Y_v,v\\geq 0)$ depends on $\\mathcal{F}_\\tau^X$ only through $X_\\tau$, we conclude that $\\mathbb{P}(X_{s + \\tau} \\in A | \\mathcal{F}_\\tau^X) = \\mathbb{P}(X_{s + \\tau} \\in A| X_\\tau)$. Consequently, $(X_t,t \\geq 0)$ is a strong-markov process. $\\blacksquare$\n\n::: {#nte-extension-of-optional-sampling .callout-tip}\n\n### Extension of optional sampling\n\nConsider a continuous martingale $(M_t, t\\leq T)$ for a filtration $(\\mathcal{F}_t, t\\geq 0)$ and a stopping time $\\tau$ for the same filtration. Suppose we would like to compute for some $T$:\n\n$$\n\\mathbb{E}[M_T \\mathbf{1}_{\\{\\tau \\leq T\\}}]\n$$\n\nIt would be tempting to condition on $\\mathcal{F}_\\tau$ and write $\\mathbb{E}[M_T |\\mathcal{F}_\\tau] = M_\\tau$ on the event $\\{\\tau \\leq T\\}$. We would then conclude that:\n\n$$\n\\mathbb{E}[M_T 1_{\\{\\tau \\leq T\\}}] = \\mathbb{E}[1_{\\{\\tau \\leq T\\}} \\mathbb{E}[M_T|\\mathcal{F}_\\tau] ] = \\mathbb{E}[M_\\tau 1_{\\{\\tau \\leq T\\}}]\n$$\n\nIn some sense, we have extended the martingale property to stopping times. This property can be proved under reasonable assumptions on $(M_t,t\\leq T)$ (for example, if it is positive). Indeed, it suffices to approximate $\\tau$ by discrete valued stopping time $\\tau_n$ as in the proof of @thm-shifted-brownian-motion-about-a-stopping-time. One can then apply martingale property at a fixed time. \n:::\n\n## The Heat Equation\n\nWe look at more detail on how PDEs come up when computing quantities related to Markov processes.\n\n::: {#exm-heat-equation-and-brownian-motion}\n\n(Heat Equation and Brownian motion) Let $f(t,x)$ be a function of time and space. The heat equation in $1+1$-dimension (one dimension of time, one dimension of space) is the PDE:\n\n$$\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\n\\end{align*}\n$$ {#eq-heat-equation-in-2d}\n\nIn $1+d$ (one dimension of time, $d$ dimensions of space), the heat equation is:\n\n$$\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{1}{2}\\nabla^2 f\n\\end{align*}\n$$ {#eq-heat-equation-in-d-plus-one-dims}\n\nwhere $\\nabla^2$ is the Laplacian operator. \n\nThe solutions to these PDEs can be expressed as an expectation over Brownian motion paths. \n\nLet $(X_t,t \\geq 0)$ be a brownian motion and let $f(t,x)$ represent the PDF of $X_t$. Then (as we will see) satisfies the PDE:\n\n$$\n\\begin{align*}\n\\partial_{t}f(t,x) = \\frac{1}{2}\\partial_x^2 f(t,x)\n\\end{align*}\n$$\n\nSuppose the Brownian motion is at $y$ in the present. Suppose *the present* is a specific time $t_1$. The joint probability density that $\\{X_{t}=x,X_{t_1}=y\\}$ is:\n\n$$\n\\begin{align*}\np(x, t, y, t_1) = f(y,t_1) \\cdot p(x,t|y,t_1)\n\\end{align*}\n$$\n\nwhere $p(x,t|y,t_1)$ is the transition probability density function. \n\nSince $f(t,x)$ is the density of $X_t$ and since we have a formula for the joint density of $X_{t_1}$ and $X_t$, we can view $f(t,x)$ as the marginal of the joint density. You find the marginal density by integrating out the variables you are not interested in, $X_{t_1}$ in this case. In the abstract, this is:\n\n$$\n\\begin{align*}\nf(t,x) &= \\int_{-\\infty}^{\\infty} p(x, t, y, t_1)dy\\\\\n&= \\int_{-\\infty}^{\\infty} f(t_1,y) p(x,t_2 | y,t_1) dy\n\\end{align*}\n$$\n\nOur claim is that $f$ indeed satisfies the PDE (@eq-heat-equation-in-2d). \n\nThe gaussian transition probability density function (heat kernel) $p(x,t|y,0)$ is given by:\n\n$$\np(x,t|y,0) = \\frac{1}{\\sqrt{2\\pi t}}\\exp\\left(-\\frac{(x-y)^2}{2t}\\right)\n$$\n\nDifferentiating $p$ with respect to $t$, we have:\n\n$$\n\\begin{align*}\n\\frac{\\partial}{\\partial t} p(x,t|y,0) &= \\frac{\\sqrt{2\\pi t} \\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{\\partial}{\\partial t}\\left(-\\frac{(x-y)^2}{2t}\\right) - \\exp\\left(-\\frac{(x-y)^2}{2t}\\right)\\sqrt{2\\pi}\\left(\\frac{1}{2\\sqrt{t}}\\right)}{2\\pi t}\\\\\n&=\\sqrt{2\\pi}\\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{\\frac{(x-y)^2}{2t^{3/2}} - \\frac{t}{2t^{3/2}}}{2\\pi t}\\\\\n&= \\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{(x-y)^2 - t}{\\sqrt{2\\pi} (2t^{5/2}) }\n\\end{align*}\n$$ {#eq-partial-with-respect-to-time}\n\nDifferentiating $p$ with respect to $x$, we have:\n\n$$\n\\begin{align*}\n\\frac{\\partial }{\\partial x} p(x,t|y,0) &= \\frac{1}{\\sqrt{2\\pi t}}\\exp\\left[-\\frac{(x-y)^2}{2t}\\right]\\frac{\\partial}{\\partial x}\\left(-\\frac{(x-y)^2}{2t}\\right)\\\\\n&= \\frac{1}{\\sqrt{2\\pi t}} \\cdot \\left(-\\frac{1}{\\cancel{2} t}\\right) \\exp\\left[-\\frac{(x-y)^2}{2t}\\right] \\cdot \\cancel{2}(x-y)\\\\\n&= -\\frac{1}{t\\sqrt{2\\pi t}} (x-y)\\exp\\left[-\\frac{(x-y)^2}{2t}\\right]\n\\end{align*}\n$$ {#eq-first-derivative-with-respect-to-space}\n\nDifferentiating again with respect to space, we have:\n\n$$\n\\begin{align*}\n\\frac{\\partial^2}{\\partial x^2} p(x,t|y,0) &= -\\frac{1}{t\\sqrt{2\\pi t}} \\left[\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} + (x-y)\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\}\\left(-\\frac{2(x-y)}{2y}\\right)\\right]\\\\\n&=-\\frac{1}{t\\sqrt{2\\pi t}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\left[1 - \\frac{(x-y)^2}{t}\\right]\\\\\n&=\\frac{1}{t\\sqrt{2\\pi t}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\left[\\frac{(x-y)^2 - t}{t}\\right]\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\cdot \\frac{(x-y)^2 - t}{t^{5/2}}\n\\end{align*}\n$$ {#eq-second-derivative-with-respect-to-space}\n\nFrom @eq-partial-with-respect-to-time and @eq-second-derivative-with-respect-to-space, it follows that:\n\n$$\n\\frac{\\partial}{\\partial t} p(x,t|y,0) = \\frac{1}{2}\\frac{\\partial ^2}{\\partial x^2} p(x,t|y,0)\n$$\n\nThus,\n\n$$\n\\begin{align*}\n\\frac{\\partial}{\\partial t} \\int_{-\\infty}^\\infty g(y) p(x,t|y,0)dy &= \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} \\int_{-\\infty}^\\infty g(y) p(x,t|y,0)dy \\\\\n\\frac{\\partial }{\\partial t}f(t,x) &= \\frac{1}{2}\\frac{\\partial^2 }{\\partial x^2} f(t,x)\n\\end{align*}\n$$\n\n:::\n\n## Solution to the heat PDE as an expectation over Brownian-motion paths\n\nConsider again the heat PDE:\n\n$$\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} = \\frac{1}{2}\\frac{\\partial^2 f }{\\partial x^2}\n\\end{align*}\n$$\n\nwith the initial condition \n\n$$\nf(0,x) = g(x)\n$$\n\n\n## Robert Brown's erratic motion of pollen\n\nIn the summer of 1827, the Scottish botanist Robert Brown observed that microscopic pollen grains suspended in water move in an erratic, highly irregular, zigzag pattern. It was only in 1905, that Albert Einstein could provide a satisfactory explanation of Brownian motion. He asserted that Brownian motion originates in the continual bombardment of the pollen grains by the molecules of the surrounding water. As a result of continual collisions, the particles themselves had the same kinetic energy as the water molecules. Thus, he showed that Brownian motion provided a solution (in a certain sense) to Fourier's famous heat equation\n\n$$\n\\frac{\\partial u}{\\partial t}(t,x) = \\kappa \\frac{\\partial^2 u}{\\partial x^2}(t,x)\n$$\n\n### Albert Einstein's proof of the existence of Brownian motion\n\nWe now summarize Einstein's original 1905 argument. Let's say that we are interested in the motion along the horizontal $x$-axis. Let's say we drop brownian particles in a liquid. Let $f(t,x)$ represent the number of particles per unit volume (density) at position $x$ at time $t$. So, the number of particles in a small interval $I=[x,x+dx]$ of width $dx$ will be $f(t,x)dx$. \n\nNow, as time progresses, the number of particles in this interval $I$ will change. The brownian particles will zig-zag upon bombardment by the molecules of the liquid. Some particles will move out of the interval $I$, while other particles will move in. \n\nLet's consider a timestep of length $\\tau$. Einstein's probabilistic approach was to model the distance travelled by the particles or displacement of the particles as a random variable $\\Delta$. To determine how many particles end up in the interval $I$, we start with the area to the right of the interval $I$.\n\nThe density of particles at $x+\\Delta$ is $f(t,x+\\Delta)$; the number of particles in a small interval of length $dx$ is $f(t,x+\\Delta)dx$. If we represent the probability density of the displacement by $\\phi(\\Delta)$, then the number of particles at $x+\\Delta$ that will move to $x$ will be $dx \\cdot f(t,x+\\Delta)\\phi(\\Delta)$. We can apply the same logic to the left hand side. The number of particles at $x - \\Delta$ that will move to $x$ will be $dx \\cdot f(t,x-\\Delta)\\phi(-\\Delta)$. Assume that $\\phi(\\Delta) = \\phi(-\\Delta)$.\n\nNow, if we integrate these movements across the real line, then we get the number of particles at $x$ at a short time later $t + \\tau$. \n\n$$\nf(t+ \\tau,x) dx = dx \\int_{-\\infty}^{\\infty} f(t,x+\\Delta) \\phi(\\Delta) d\\Delta\n$$\n\nNow, we can get rid of $dx$.\n\n$$\nf(t+ \\tau,x) = \\int_{-\\infty}^{\\infty} f(t,x+\\Delta) \\phi(\\Delta) d\\Delta\n$$ {#eq-expression-for-density-at-later-time}\n\nThe Taylor's series expansion of $f(t+\\tau,x)$ centered at $t$ (holding $x$ constant) is:\n\n$$\nf(t + \\tau,x) = f(t,x) + \\frac{\\partial f}{\\partial t}\\tau + O(\\tau^2)\n$$\n\n\nThe Taylor's series expansion of $f(t,x+\\Delta)$ centered at $x$ (holding $t$ constant) is:\n\n$$\nf(t,x+\\Delta) = f(t,x) + \\frac{\\partial f}{\\partial x}\\Delta + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\Delta^2 + O(\\Delta^3)\n$$\n\nWe can now substitute these into @eq-expression-for-density-at-later-time to get:\n\n$$\n\\begin{align*}\nf(t,x) + \\frac{\\partial f}{\\partial t}\\tau &= \\int_{-\\infty}^{\\infty}\\left(f(t,x) + \\frac{\\partial f}{\\partial x}\\Delta + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\Delta^2\\right) \\phi(\\Delta)d\\Delta\\\\\n&= f(t,x) \\int_{-\\infty}^{\\infty} \\phi(\\Delta)d\\Delta \\\\\n&+ \\frac{\\partial f} {\\partial x} \\int_{-\\infty}^{\\infty} \\Delta \\phi(\\Delta)d\\Delta \\\\\n&+ \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta\n\\end{align*}\n$$\n\nNow, since the probability distribution of displacement $\\phi(\\cdot)$ is symmetric around the origin, the second term is zero. And we know, that if we integrate the density over $\\mathbb{R}$, we should get one, so the first term equals one. So, we get:\n\n$$\nf(t,x) + \\frac{\\partial f}{\\partial t}\\tau = f(t,x) + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta\n$$\n\nNow, we can cancel the $f$ on both sides and then shift $\\tau$ to the right hand side:\n\n$$\n\\frac{\\partial f}{\\partial t} =  \\left(\\frac{1}{2\\tau} \\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta \\right)\\frac{\\partial^2 f}{\\partial x^2}\n$$\n\nDefine $D:= \\left(\\frac{1}{2\\tau} \\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta \\right)$. Then, we have: \n\n$$\n\\frac{\\partial f}{\\partial t} =  D\\frac{\\partial^2 f}{\\partial x^2}\n$$\n\nThe microscopic interpretation of the diffusion coefficient is, that its just the average of the squared displacements. The larger the $D$, the faster the brownian particles move.\n\n## Kolmogorov's Backward Equation\n\nThink of $y$ and $t$ as being current values and $y'$ and $t'$ being future values. The transition probability density function $p(y',t'|y,t)$ of a diffusion satisfies two equations - one involving derivatives with respect to a future state and time ($y'$ and $t'$) called *forward equation* and the other involving derivatives with respect to the current state and current time ($y$ and $t$) called the *backward equation*. These two equations are parabolic partial differential equations not dissimilar to the Black-Scholes equation. \n\n::: {#thm-backward-equation-with-initial-value}\n\n### Backward equation with initial value\n\nLet $(X_t,t\\geq 0)$ be a diffusion in $\\mathbb{R}$ with the SDE:\n\n$$\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t) dt\n$$ \n\nLet $g\\in C^2(\\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value \n\n$$\n\\begin{align*}\n\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(x)\\frac{\\partial f}{\\partial x}\\\\\nf(0,x) &= g(x)\n\\end{align*}\n$$ {#eq-backward-equation}\n\nhas the representation:\n\n$$\nf(t,x) = \\mathbb{E}[g(X_t)|X_0 = x]\n$$\n\n:::\n\n*Proof.*\n\n**Step 1.** \nLet's fix $t$ and consider the function of space $h(x)=f(t,x)=\\mathbb{E}[g(X_t)|X_0=x]$. Applying Ito's formula to $h$, we have:\n\n\\begin{align}\ndh(X_s) &= h'(X_s) dX_s + \\frac{1}{2}h''(X_s) (dX_s)^2\\\\\n&= h'(X_s) (\\sigma(X_s)dB_s + \\mu(X_s) ds) + \\frac{\\sigma(X_s)^2}{2}h''(X_s)ds\\\\\n&= \\sigma(X_s)h'(X_s)dB_s + \\left(\\frac{\\sigma(X_s)^2}{2}h''(X_s) + \\mu(X_s)h'(X_s)\\right)ds\n\\end{align}\n\nIn the integral form this is:\n\n\\begin{align*}\nh(X_s) - h(X_0) &= \\int_0^s \\sigma(X_u)h'(X_u)dB_u \\\\\n&+ \\int_0^s \\left(\\frac{\\sigma(X_u)^2}{2}h''(X_u) + \\mu(X_u)h'(X_u)\\right)du \\tag{1}\n\\end{align*}\n\n**Step 2.** Take expectations on both sides, divide by $s$ and let $s \\to 0$. We are interested in taking the derivative with respect to $s$ at $s_0=0$.\n\nThe expectation of the first term on the right hand side is zero, by the properties of the Ito integral. \n\nThe integrand of the second term (RHS) is a conditional expectation $\\mathbb{E}[\\xi(X_u)|X_0 = x]$, it is an average at time $u$, of the paths of the process starting at initial position $X_0 = x$, so it is a function of $u$ and $x$. So, $\\mathbb{E}[\\xi(X_u)|X_0 = x] = p(u,x)$. Suppressing the argument $x$, we have the representation: \n\n\\begin{align}\n\\int_0^s p(u) du\n\\end{align}\n\nRecall that, if $p$ is a continuous function, then it is Riemann integrable. Further, since integration and differentiation are inverse operations, there exists a unique antiderivative $P$ given by\n\n$$\nP(s) = \\int_{0}^{s}p(u)du\n$$\n\nsatisfying $P'(0) = p(0)$. \n\nBy the definition of the derivative:\n\n$$P'(0) = \\lim_{s \\to 0} \\frac{P(s) - P(0)}{s} = \\lim_{s\\to 0} \\frac{P(s)}{s} = p(0) \\quad \\{ P(0)=0 \\text{ by definition }\\}$$\n\nThus, we have:\n\n$$\np(0,x) = \\mathbb{E}[\\xi(X_0)|X_0 = x] = \\frac{\\sigma(x)^2}{2} h''(x) + \\mu(x)h'(x)\n$$\n\n\n**Step 3.** As for the left-hand side, we have:\n\n$$\n\\lim_{s \\to 0} \\frac{\\mathbb{E}[h(X_s)|X_0 = x] - h(X_0)}{s} = \\lim_{s \\to 0} \\frac{\\mathbb{E}[h(X_s)|X_0 = x] - f(t,x)}{s} \n$$\n\nTo prove that this limit is $\\frac{\\partial f}{\\partial t}(t,x)$, it remains to show that $\\mathbb{E}[h(X_s)|X_0 = x]=\\mathbb{E}[g(X_{t+s})|X_0 = x]=f(t+s,x)$. \n\nTo see this, note that $h(X_s) = \\mathbb{E}[g(X_{t+s})|X_s]$. We deduce:\n\n\\begin{align*}\n\\mathbb{E}[h(X_s)|X_0 = x] &= \\mathbb{E}[\\mathbb{E}[g(X_{t+s})|X_s]|X_0 = x]\\\\\n&= \\mathbb{E}[\\mathbb{E}[g(X_{t+s})|\\mathcal{F}_s]|X_0 = x]\\\\\n& \\{ (X_t,t\\geq 0) \\text{ is Markov }\\} \\\\\n&= \\mathbb{E}[g(X_{t+s})|X_0 = x]\\\\\n& \\{ \\text{ Tower property }\\} \\\\\n&= f(t+s,x)\n\\end{align*}\n\nThis closes the proof. $\\blacksquare$\n\nThe backward equation (@eq-backward-equation) can be conveniently written in terms of *the generator of the diffusion*.\n\n::: {#def-generator-of-the-diffusion}\n\n### Generator of a diffusion\n\nThe generator of a diffusion with SDE $dX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt$ is the differential operator acting on functions of space defined by :\n\n$$\nA = \\frac{\\sigma(x)^2}{2}\\frac{\\partial }{\\partial x^2} + \\mu(x)\\frac{\\partial}{\\partial x}\n$$\n\n:::\n\nWith this notation, the backward equation for the function $f(t,x)$ takes the form:\n\n$$\n\\frac{\\partial f}{\\partial x}(t,x) = Af(t,x)\n$$\n\nwhere it is understood that $A$ acts only on the space variable. @thm-backward-equation-with-initial-value gives a nice interpretation of the generator: it quantifies how much the function $f(t,x) = \\mathbb{E}[g(X_t)|X_0 = x]$ changes in a small time interval.\n\n## The heat equation as a special case of the Backward equation\n\nLet $(B_t,t\\geq 0)$ be a standard brownian motion. Then, the generator is:\n\n$$\nA = \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,x)\n$$\n\nThen, by @thm-backward-equation-with-initial-value, the solution of the heat PDE\n\n$$\n\\begin{align*}\n\\frac{\\partial f}{\\partial t}(t,x) = Af(x) = \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,x)\n\\end{align*}\n$$\n\nwith initial value $f(0,x)=g(x)$ has the stochastic representation:\n\n$$f(t,x) = \\mathbb{E}[g(B_t)|B_0 = x]$$\n\nIt can be represented as an average of $g(B_t)$ over all Brownian motion paths starting at the location $x$.\n\n::: {#exm-generator-of-the-ornstein-uhlenbeck-process}\n\n(Generator of the Ornstein Uhlenbeck Process) The SDE of the Ornstein-Uhlenbeck process is:\n\n$$\ndX_t = dB_t - X_t dt\n$$\n\nThis means that its generator is:\n\n$$\nA = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} - x \\frac{\\partial}{\\partial x}\n$$\n\n:::\n\n::: {#exm-generator-of-geometric-brownian-motion}\n\n(Generator of Geometric Brownian Motion) Recall that the geometric Brownian motion \n\n$$\nS_t = S_0 \\exp(\\sigma B_t + \\mu t)\n$$\n\nsatisfies the SDE:\n\n$$\ndS_t = \\sigma S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right) S_t dt\n$$\n\nIn particular, the generator of geometric Brownian motion is :\n\n$$\nA = \\frac{\\sigma^2 x^2}{2} x \\frac{\\partial^2}{\\partial x^2} + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\frac{\\partial}{\\partial x}\n$$\n:::\n\nFor applications, in particular in mathematical finance, it is important to solve the backward equation with terminal value instead of with initial value. The reversal of time causes the appearance of an extra minus sign in the equation.\n\n::: {#thm-backward-equation-with-terminal-value}\n\n### Backward equation with terminal value\n\nLet $(X_t,t\\leq T)$ be a diffusion with the dynamics:\n\n$$\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt\n$$\n\nLet $g\\in C^2(\\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with terminal value at time $T$\n\n$$\n\\begin{align*}\n-\\frac{\\partial f}{\\partial t} &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(x)\\frac{\\partial f}{\\partial x}\\\\\nf(T,x) &= g(x)\n\\end{align*}\n$$ {#eq-backward-equation-with-terminal-value}\n\nhas the representation:\n\n$$\nf(t,x) = \\mathbb{E}[g(X_T)|X_t = x]\n$$\n:::\n\n\n::: {#nte-functions-of-markov .callout-tip}\n### Backward equation with terminal value appears in the martingale condition\n\nOne way to construct a martingale for the filtration $(\\mathcal{F}_t,t\\geq 0)$ is to take \n\n$$\nM_t = \\mathbb{E}[Y | \\mathcal{F}_t]\n$$\n\nwhere $Y$ is some integrable random variable. The martingale property then follows from the tower property of the conditional expectation. In the setup of @thm-backward-equation-with-terminal-value, the random variable $Y$ is $g(X_T)$. By the Markov property of diffusion, we therefore have:\n\n$$\nf(t,X_t) = \\mathbb{E}[g(X_T)|X_t] = \\mathbb{E}[g(X_T)|\\mathcal{F}_t] \n$$\n\nIn other words, the solution to the backward equation with terminal value evaluated at $X_t = x$ yields a martingale for the natural filtration of the process. This is a different point of view on the procedure we have used many times now: To get a martingale of the form $f(t,X_t)$, apply the Ito's formula to $f(t,X_t)$ and set the $dt$ term to zero. The PDE we obtain is the backward equation with terminal value. In fact, the proof of the theorem takes this exact route.\n:::\n\n*Proof.*\n\nConsider $f(t,X_t)$ and apply Ito's formula.\n\n$$\n\\begin{align*}\ndf(t,X_t) &= \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial x}dX_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2} dX_t \\cdot dX_t\\\\\n&= \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial x}(\\sigma(X_t) dB_t + \\mu(X_t)dt) + \\frac{\\sigma(X_t)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} dt\\\\\n&= \\sigma(X_t) dB_t + \\left(\\frac{\\partial f}{\\partial t} + \\frac{\\sigma(X_t)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(X_t)\\frac{\\partial f}{\\partial x}\\right)dt\n\\end{align*}\n$$\n\nSince $f(t,x)$ is a solution to the equation, we get that the $dt$ term is $0$ and $f(t,X_t)$ is a martingale for the Brownian filtration (and thus also for the natural filtration of the diffusion, which contains less information). In particular we have:\n\n$$\nf(t,X_t) = \\mathbb{E}[f(T,X_T)|\\mathcal{F}_t] = \\mathbb{E}[g(X_T)|\\mathcal{F}_t]\n$$\n\nSince $(X_t,t\\leq T)$ is a Markov process, we finally get:\n\n$$\nf(t,x) = \\mathbb{E}[g(X_T)|X_t = x]\n$$\n\n::: {#exm-martingales-of-geometric-brownian-motion}\n\n(Martingales of geometric Brownian motion) Let $(S_t, \\geq 0)$ be a geometric brownian motion with SDE:\n\n$$\ndS_t = \\sigma S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)dt\n$$\n\nAs we saw in @exm-generator-of-geometric-brownian-motion, its generator is:\n\n$$\nA = \\frac{\\sigma^2 x^2}{2}\\frac{\\partial^2}{\\partial x^2} + x\\left(\\mu+\\frac{\\sigma^2}{2}\\right)\\frac{\\partial}{\\partial x}\n$$\n\nIn view of @thm-backward-equation-with-terminal-value, if $f(t,x)$ satisfies the PDE \n\n$$\n\\frac{\\partial f}{\\partial t} + \\frac{\\sigma^2 x^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + x\\left(\\mu+\\frac{\\sigma^2}{2}\\right)\\frac{\\partial f}{\\partial x}\n$$\n\nthen processes of the form $f(t,S_t)$ will be martingales for the natural filtration. \n:::\n\n## Kolmogorov's forward equation\n\nThe companion equation to the backward equation is the *Kolmogorov forward equation* or *forward equation*. It is also known as the *Fokker-Planck* equation from its physics origin. The equation is very useful as it is satisfied by the transition density function $p(y',t'|y,t)$ of a time-homogenous diffusion. It involves the *adjoint of the generator*. \n\n::: {#def-adjoint-of-the-generator}\n\n### Adjoint of the generator\n\nThe adjoint $A^*$ of the generator of a diffusion $(X_t,t\\geq 0)$ with SDE:\n\n$$\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t)dt\n$$\n\nis the differential operator acting on a function of space $f(x)$ as follows:\n\n$$\nA^*f(x) = \\frac{1}{2}\\frac{\\partial^2 }{\\partial x^2} \\frac{\\sigma(x)^2}{2} f(x) - \\frac{\\partial }{\\partial x}\\mu(x)f(x)\n$$ {#eq-adjoint-of-the-generator-of-a-diffusion}\n:::\n\nNote the differences with the generator in @def-generator-of-the-diffusion: there is an extra minus sign and the derivatives also act on the volatility and the drift.\n\n::: {#exm-the-generator-brownian-motion-is-self-adjoint}\n\n(The generator of Brownian motion is self-adjoint) In the case of standard brownian motion, it is easy to check that:\n\n$$\nA^* = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} \n$$\n\nand \n\n$$\nA^* = \\frac{1}{2}\\nabla^2\n$$\n\nin the multivariate case. In other words, the generator and its adjoint are the same. In this case, the operator is *self-adjoint*. \n:::\n\n::: {#exm-the-adjoint-for-geometric-brownian-motion}\nWe see that the adjoint of the generator acting on $f(x)$ for geometric Brownian motion is:\n\n$$\nA^*f(x) = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} (\\sigma^2 x^2 f(x)) - \\frac{\\partial}{\\partial x} \\left(\\left(\\mu + \\frac{\\sigma^2}{2}\\right) x f(x)\\right)\n$$\n\nUsing the product rule in differentiating we get:\n\n$$\nA^*[f(x)] = \\frac{\\sigma^2}{2}\\left(2x f(x) + x^2 f''(x)\\right) - \\left(\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\left(f(x) + x f'(x)\\right)\\right)\n$$\n\n:::\n\n::: {#exm-adjoint-for-the-ornstein-uhlenbeck-process}\n\nThe generator for the Ornstein-Uhlenbeck process was given in @exm-generator-of-the-ornstein-uhlenbeck-process. The adjoint acting on $f$ is therefore:\n\n$$\n\\begin{align*}\nA^*f(x) &= \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}(f(x)) - \\frac{\\partial}{\\partial x}(- x f(x))\\\\\n&= \\frac{f''(x)}{2} + (f(x)+xf'(x))\n\\end{align*}\n$$\n:::\n\nThe forward equation takes the following form for a function $f(t,x)$ of time and space:\n\n$$\n\\frac{\\partial f}{\\partial t} = A^* f\n$$ {#eq-forward-equation}\n\nFor brownian motion, since $A^* = A$, the backward and forward equations are the same. As advertised earlier, the forward equation is satisfied by the transition $p_t(y',t'|y,t)$ of a diffusion. Before showing this in general, we verify it in the Brownian case.\n\n::: {#exm-the-heat-kernel-as-the-solution-of-the-forward-equation}\nRecall that the transition probability density $p(y,t|x,0)$ for Brownian motion, or heat kernel, is:\n\n$$\np(y,t|x,0) = \\frac{e^{-\\frac{(y-x)^2}{2}}}{\\sqrt{2\\pi t}}\n$$\n\nHere, the space variable will be $y$ and $x$ will be fixed. The relevant function is thus $f(t,y) = p(y,t|x,0)$. The adjoint operator acting on the space variable $y$ is $A^* = A = \\frac{1}{2}\\frac{\\partial^2}{\\partial y^2}$. The relevant time and space derivatives are given by @eq-partial-with-respect-to-time and @eq-second-derivative-with-respect-to-space. \n\nWe conclude that $f(t,y)=p(y,t|x,0)$ is a solution of the forward equation. \n:::\n\nWhere does the form of the adjoint operator @eq-adjoint-of-the-generator-of-a-diffusion come from? In some sense, the adjoint operator plays a role similar to that of the transpose of a matrix in linear algebra. The adjoint acts on the function on the left. To see this, consider two functions $f,g$ of space on which the generator $A$ of a diffusion is well-defined. In particular, let's assume that the functions are zero outside an interval. Consider the quantity \n\n$$\n\\int_{\\mathbb{R}}g(x)A(f(x))dx = \\int_{\\mathbb{R}} g(x)\\left(\\frac{\\sigma(x)^2 }{2}f''(x) + \\mu(x)f'(x)\\right)dx\n$$\n\nThis quantity can represent for example the average of $Af(x)$ over some PDF $g(x)$. In the above, $A$ acts on the function on the right. To make the operator act on $g$, we integrate by parts. This gives for the second term:\n\n$$\n\\int_{\\mathbb{R}} g(x)\\mu(x)f'(x)dx = g(x)\\mu(x)f(x)\\Bigg|_{-\\infty}^{\\infty}-\\int_{\\mathbb{R}}f(x)\\frac{d}{dx}(g(x)\\mu(x))dx\n$$\n\nThe boundary term $g(x)f(x)\\mu(x)\\Bigg|_{-\\infty}^\\infty$ is $0$ by the assumptions on $f,g$. This term on $\\sigma$ is obtained by integrating by parts twice:\n\n$$\n\\begin{align*}\n\\int_{\\mathbb{R}} g(x) \\frac{\\sigma(x)^2}{2}f''(x)dx &= g(x) \\frac{\\sigma(x)^2}{2}f'(x)\\Bigg|_{-\\infty}^{\\infty} - \\int_{\\mathbb{R}}\\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right) f'(x)dx\\\\\n-\\int_{\\mathbb{R}} \\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f'(x)dx &= -\\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f(x) \\Bigg|_{-\\infty}^{\\infty} + \\int_{\\mathbb{R}}\\frac{d^2}{dx^2}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f(x)dx\n\\end{align*}\n$$\n\nThus,\n\n$$\n\\begin{align*}\n\\int_{\\mathbb{R}}g(x) Af(x)dx &= \\int_{\\mathbb{R}}\\left(\\frac{1}{2}\\frac{d^2}{dx^2}(g(x) \\sigma(x)^2) - \\frac{d}{dx}(g(x)\\mu(x))\\right)f(x)dx\\\\\n&= \\int_{\\mathbb{R}}(A^*g(x))f(x)dx\n\\end{align*}\n$$ {#eq-relation-between-generator-and-adjoint}\n\n::: {#thm-forward-equation-and-transition-probability}\n\n### Forward equation and transition probability \n\nLet $(X_t,t\\geq 0)$ be a diffusion with SDE:\n\n$$\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t)dt, \\quad X_0 = x_0\n$$\n\nLet $p(x,t|x_0,0)$ be the transition probability density function for a fixed $x_0$. Then, the function $f(t,y) = p(y,t|x_0,0)$ is a solution of the PDE \n\n\n$$\n\\frac{\\partial f}{\\partial t} = A^* f\n$$\n\nwhere $A^*$ is the adjoint of $A$. \n::: \n\n*Proof.*\n\nLet $h(x)$ be some arbitrary function of space that is $0$ outside an interval. We compute :\n\n$$\n\\frac{1}{\\epsilon}(\\mathbb{E}[h(X_{t+\\epsilon}) - \\mathbb{E}[h(X_t)]])\n$$\n\ntwo different ways and take the limit as $\\epsilon \\to 0$. \n\nOn one hand, we have by the definition of the transition density \n\n$$\n\\frac{1}{\\epsilon}\\left(\\mathbb{E}[h(X_{t+\\epsilon})]-\\mathbb{E}[h(X_t)]\\right) = \\int_{\\mathbb{R}}\\frac{1}{\\epsilon}(p(x,t+\\epsilon|x,0) - p(x,t|x_0,0))h(x)dx\n$$\n\nBy taking the limit $\\epsilon \\to 0$ inside the integral (assuming this is fine), we get:\n\n$$\n\\int_{\\mathbb{R}} \\frac{\\partial}{\\partial t}p(x,t|x_0,0)h(x)dx\n$$ {#eq-fwd-equation-partial-wrt-time}\n\nOn the other hand, Ito's formula implies \n\n$$\n\\begin{align*}\ndh(X_s) &= \\frac{\\partial h}{\\partial x} dX_s + \\frac{1}{2} \\frac{\\partial^2 h}{\\partial x^2} (dX_s)^2\\\\\n&= \\frac{\\partial h}{\\partial x} (\\sigma(X_s) dB_s + \\mu(X_s)ds) + \\frac{1}{2} \\frac{\\partial^2 h}{\\partial x^2} (\\sigma(X_s)^2 ds)\\\\\n&= \\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s + \\left(\\mu(X_s) \\frac{\\partial h}{\\partial x} + \\frac{\\sigma(X_s)^2}{2}\\frac{\\partial^2 h}{\\partial x^2}\\right)ds\\\\\nh(X_{t+\\epsilon}) - h(X_t) &= \\int_{t}^{t+\\epsilon}\\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s + \\int_{t}^{t+\\epsilon}(Ah(x))ds\\\\\n\\mathbb{E}[h(X_{t+\\epsilon})] - \\mathbb{E}[h(X_t)] &= \\underbrace{\\mathbb{E}\\left[\\int_{t}^{t+\\epsilon}\\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s\\right]}_{0} + \\int_{t}^{t+\\epsilon}\\mathbb{E}[Ah(X_s)]ds\n\\end{align*}\n$$\n\nDividing by $\\epsilon$ and taking the limit as $\\epsilon \\to 0$, we have:\n\n$$\n\\begin{align*}\n\\lim_{\\epsilon \\to 0} \\frac{1}{\\epsilon} (\\mathbb{E}[h(X_{t+\\epsilon})] - \\mathbb{E}[h(X_t)]) &= \\mathbb{E}[Ah(X_t)]\\\\\n&= \\int_{\\mathbb{R}} p(x,t|x_0,0) Ah(x) dx\n\\end{align*}\n$$\n\nThis can be written using @eq-relation-between-generator-and-adjoint as,\n\n$$\n\\int_{\\mathbb{R}}(A^* p(x,t|x_0,0)) h(x) dx\n$$\n\nSince $h$ is arbitrary, we conclude that:\n\n$$\n\\frac{\\partial}{\\partial t}p(x,t|x_0,0) = A^* p(x,t|x_0,0)\n$$ {#eq-forward-equation}\n\n::: {#exm-forward-equation-and-invariant-probability}\n\n(Forward equation and invariant probability.) The Ornstein-Uhlenbeck process converges to a stationary distribution as noted in the [example](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#exm-ornstein-uhlenbeck-process) here. For example, for the SDE of the form\n\n$$\ndX_t = -X_t dt + dB_t\n$$\n\nwith $X_0$ a Gaussian of mean $0$ and variance $1/2$, the PDF of $X_t$, is, for all $t$ is:\n\n$$\nf(x) = \\frac{1}{\\sqrt{\\pi}} e^{-x^2}\n$$ {#eq-pdf-of-OU-process}\n\nThis *invariant distribution* can be seen from the point of view of the forward equation. Indeed since the PDF is constant in time, the forward equation simply becomes:\n\n$$\nA^* f = 0\n$$ {#eq-forward-equation-of-ou-process}\n:::\n\n::: {#exm-smoluchowski-equation}\n\nThe SDE of the Ornstein-Uhlenbeck process can be generated as follows. Consider $V(x)$, a smooth function of space such that $\\int_{\\mathbb{R}} e^{-2V(x)}dx<\\infty$. The *Smoluchowski* equation is the SDE of the form:\n\n$$ \ndX_t = dB_t - V'(X_t) dt\n$$ {#eq-smoluchowski-equation}\n\nThe SDE can be interpreted as follows: $X_t$ represents the position of a particle on $\\mathbb{R}$. The position varies due to the Brownian fluctuations and also due to a force $V'(X_t)$ that depends on the position. The function $V(x)$ should then be thought of as the potential with which the particle moves, since the force (field) is the (negative) derivative of the potential function in Newtonian physics. The generator of this diffusion is:\n\n$$\nA = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} - V'(x)\\frac{\\partial}{\\partial x}\n$$\n\nThis diffusion admits an invariant distribution :\n\n$$\nf(x) = Ce^{-2V(x)}\n$$\n\nwhere $C$ is such that $\\int_{\\mathbb{R}}f(x)dx = 1$.\n:::\n\n## The Feynman-Kac Formula\n\nWe saw in @exm-heat-equation-and-brownian-motion that the solution of the heat equation:\n\n$$\n\\frac{\\partial f}{\\partial t} = \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\n$$\n\ncan be represented as an average over Brownian paths. This representation was extended to diffusions in theorem @thm-backward-equation-with-initial-value where the second derivative in the equation is replaced by the generator of the corresponding diffusion. How robust is this representation? In other words, is it possible to slightly change the PDE and still get a stochastic representation representation for the solution? The answer to this question is yes, when a term of the form $r(x)f(t,x)$ is added to the equation, where $r(x)$ is a well-behaved function of space (for example, piecewise continuous). The stochastic representation of the PDE in this case bears the name *Feynman-Kac* formula, making a fruitful collaboration between the physicist [Richard Feynman](https://en.wikipedia.org/wiki/Richard_Feynman) and the mathematician [Mark Kac](https://en.wikipedia.org/wiki/Mark_Kac). By the way, you pronounce \"Kac\" as \"cats\". His name is Polish. People who immigrated from Poland before him spelled their names as \"Katz\". The case when $r(x)$ is linear will be important in the applications to mathematical finance, where it represents the contribution of the interest rate. \n\n::: {#thm-initial-value-problem}\n\n### Initial Value Problem \n\nLet $(X_t,t\\geq 0)$ be a diffusion in $\\mathbb{R}$ with the SDE:\n\n$$\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt\n$$\n\nLet $g\\in C^2(\\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value \n\n$$\n\\begin{align*}\n\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,x) + \\mu(x)\\frac{\\partial f}{\\partial x}(t,x) - r(x)f(x)\\\\\nf(0,x) &= g(x)\n\\end{align*}\n$$ {#eq-initial-value-problem}\n\nhas the stochastic representation:\n\n$$\nf(t,x) = \\mathbb{E}\\left[g(X_t)\\exp\\left(-\\int_0^t r(X_s) ds\\right)\\Bigg| X_0 = x\\right]\n$$\n:::\n\n*Proof.*\n\nThe proof is again based on Ito's formula. For a fixed $t$, we consider the process:\n\n$$\nM_s = f(t-s, X_s) \\exp\\left(-\\int_0^s r(X_u) du\\right), \\quad s \\leq t\n$$\n\nWrite $Z_s = \\exp\\left(-\\int_0^s r(X_u) du\\right)$ and $V_s = f(t-s,X_s)$. A direct application of Ito's formula yields:\n\nLet $R_s = -\\int_0^s r(X_u) du$. So, $dR_t = r(X_t) dt$. $(R_t,t\\geq 0)$ is a random variable, because $r(X_s)$ depends on how $(X_s, s \\leq t)$ evolves, it is *stochastic*, but for very small intervals of time $r(X_s)$ is a constant, and hence the process $(R_t,t\\geq 0)$ is said to be locally deterministic.\n\n$$\n\\begin{align*}\nZ_s &= e^{-R_s}\\\\\ndZ_s &= -e^{-R_s} dR_s + \\frac{1}{2}e^{R_s} (dR_s)^2\\\\\n&= -Z_s r(X_s) ds\n\\end{align*}\n$$\n\nand \n\n$$\n\\begin{align*}\ndV_s &= \\frac{\\partial}{\\partial s}f(t-s, X_s)ds + \\frac{\\partial}{\\partial x}f(t-s, X_s)dX_s + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}f(t-s,X_s)(dX_s)^2\\\\\n&= -f_s ds + f_x (\\sigma(X_s)dB_s + \\mu(X_s)ds) + \\frac{1}{2}f_{xx} \\sigma(X_s)^2 ds \\\\\n&= \\sigma(X_s) f_x dB_s + \\\\\n&+ \\left\\{-f_s + \\mu(X_s)f_x + \\frac{\\sigma(X_s)^2}{2}f_{xx}\\right\\}ds\n\\end{align*}\n$$\n\nRecall that $t$ is fixed here, and we differentiate with respect to $s$ in time. Since $f(t,x)$ is a solution of the PDE, we can write the second equation as:\n\n$$\ndV_s = \\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds\n$$\n\nNow, by Ito's product rule, we finally have:\n\n$$\n\\begin{align*}\ndM_s &= V_s dZ_s + Z_s dV_s + dZ_s dV_s\\\\\n&= -f(t-s,X_s)Z_s r(X_s) ds + Z_s (\\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds) + 0\\\\\n&= \\sigma(X_s)Z_s f_x dB_s\n\\end{align*}\n$$\n\nThis proves that $(M_s, s \\leq t)$ is a martingale. We conclude that:\n\n$$\n\\mathbb{E}[M_t] = \\mathbb{E}[M_0]\n$$\n\nUsing the definition of $M_t$, this yields:\n\n$$\n\\mathbb{E}[M_t] = \\mathbb{E}\\left[f(0,X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\\right] = \\mathbb{E}\\left[g(X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\\right] = \\mathbb{E}[M_0] = f(t,x)\n$$\n\nThis proves the theorem. $\\blacksquare$\n\nAs for the backward equation, it is natural to consider the terminal value problem for the same PDE.\n\n::: {#thm-terminal-value-problem}\n\n### Terminal Value Problem\n\nLet $(X_t,t \\leq T)$ be a diffusion in $\\mathbb{R}$ with the SDE:\n\n$$\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t) dt\n$$\n\nLet $g\\in C^2(\\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value \n\n$$\n\\begin{align*}\n-\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,x) + \\mu(x)\\frac{\\partial f}{\\partial x}(t,x) - r(x)f(t,x)\\\\\nf(T,x) &= g(x)\n\\end{align*}\n$$\n\nhas the stochastic representation :\n\n$$\nf(t,x) = \\mathbb{E}\\left[g(X_T)\\exp\\left(-\\int_t^T r(X_u) du\\right)\\Bigg|X_t = x\\right]\n$$\n:::\n\n*Proof.*\n\nThe proof is similar by considering instead\n\n$$\nM_t = f(t,X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\n$$\n\n:::{#prp-generalized-feynman-kac}\n\n### Generalized version. \nLet $(X_t,t\\leq T)$ be a diffusion in $\\mathbb{R}$ with the SDE:\n\n$$\ndX_t = \\sigma(t,X_t) dB_t + \\mu(t,X_t)dt\n$$\n\nLet $V\\in C^2(\\mathbb{R})$ be the payout function. Then, the solution of the PDE \n\n$$\n\\begin{align*}\n\\left(\\frac{\\partial}{\\partial t} + \\mu(t,x)\\frac{\\partial}{\\partial x} + \\frac{1}{2}\\sigma^2(t,x)\\frac{\\partial^2}{\\partial x^2}\\right)f = r(t,x)f(t,x) + B(t,x)\n\\end{align*}\n$$\n\nwith the boundary condition:\n\n$$\nf(T,x) = V(x)\n$$\n\nhas the stochastic representation:\n\n$$\nf(t,x)=\\mathbb{E}_t\\left[\\exp\\left(-\\int_t^T r(u,X_u) du\\right)V(X_T)\\right] - \\mathbb{E}_t\\left[\\int_{t}^T \\exp\\left(-\\int_t^s r(u,X_u) du\\right)B(s,X_s)ds\\right]\n$$\n\n:::\n\n*Proof*\n\nFor brevity, we drop the space coordinate and write $X(t,W_t) as X_t$. \n\nDefine $Z_s = \\exp\\left(-\\int_t^s r_u du\\right)$. Consider the process\n\n$$\nY(s) = Z_s f(s,X_s) - \\int_t^s Z_s B_s ds\n$$\n\nBy Ito's product rule:\n\n$$\n\\begin{align*}\ndY_s &= dZ_s f + Z_s df + dZ_s df - Z_s B_s ds\n\\end{align*}\n$$\n\nSince $dZ_s df = O(dt dt)$ it can be dropped. We have:\n\n$$\n\\begin{align*}\ndY_s &= -r_s Z_s f ds + Z_s \\left(f_s ds + f_x dX_s + \\frac{1}{2}f_{xx}(dX_s)^2\\right) - Z_s B_s ds\\\\\n&= -r_s Z_s f ds + Z_s \\left[f_s ds + f_x (\\mu ds + \\sigma dW_s) + \\frac{1}{2}\\sigma^2f_{xx}ds\\right] - Z_s B_s ds \\\\\n&= -r_s Z_s f ds + Z_s \\left[\\left(f_s + \\mu f_x  + \\frac{1}{2}\\sigma^2f_{xx}\\right)ds + \\sigma f_x dW_s \\right]  - Z_s B_s ds\n\\end{align*}\n$$\n\nWe can substitute the term in the round brackets $\\left(f_s + \\mu f_x  + \\frac{1}{2}\\sigma^2f_{xx}\\right) = r_s f + B_s$, since $f$ satisfies the PDE. So, we have:\n\n$$\n\\begin{align*}\ndY_s \n&= -r_s Z_s f ds + Z_s \\left[\\left(r_s f + B_s\\right)ds + \\sigma f_x dW_s \\right]  - Z_s B_s ds\\\\\n&= Z_s \\sigma f_x dW_s\n\\end{align*}\n$$\n\nSo, the process $(Y_s,s\\leq T)$ is a martingale and $\\mathbb{E}_t[Y_T] = Y_t$. Consequently, we have:\n\n\n\n## Numerical Projects\n\n### Temperature of a rod \n\nConsider the initial function $g(x) = 1 - |x|$ for $|x| \\leq 1$ and $0$ if $|x| > 1$. This function may represent the temperature of a rod at time $0$. \n\n(a) Approximate the solution $f(t,x)$ to the heat equation at time $t=0.25$ at every $0.01$ in $x$ using the representation @eq-stochastic-representation-of-the-solution-to-the-heat-equation. Use a sample of $100$ paths for each $x$.\n\n*Solution.*\n\n::: {#451137f9 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\n\n# initial temperature of the rod as a function of position x\ndef g(x):\n    if x >= -1.0 and x <= 1.0:\n        return 1.0 - np.abs(x)\n    else:\n        return 0.0\n\n\n# helper function to generate brownian paths starting at B_0 = x_0\ndef brownian_paths(num_paths, step_size, t_0, t_n, x_0):\n    num_steps = int((t_n - t_0) / step_size)\n    db_t = np.sqrt(step_size) * np.random.standard_normal(size=(num_paths, num_steps))\n    db_t = np.concatenate([np.full((num_paths, 1), x_0), db_t], axis=1)\n    b_t = np.cumsum(db_t, axis=1)\n    return b_t\n\n\nx = np.linspace(-5.0, 5.0, 1001)  # space variable\nt = np.linspace(0.0, 1.0, 101)  # time variable\n```\n:::\n\n\nNow, let's use the data from the problem to compute the specific space average.\n\n::: {#e6a02796 .cell execution_count=2}\n``` {.python .cell-code}\n# generate sample paths\npaths = brownian_paths(num_paths=100, step_size=0.01, t_0=0.0, t_n=1.0, x_0=0.0)\n\n# look at the value of B(t) at t=0.25 and average them\n```\n:::\n\n\n## Exercises\n\n::: {#exr-shifted-brownian-motion}\n\n(Shifted Brownian Motion) Let $(B_t,t\\geq 0)$ be a standard brownian motion. Fix $t > 0$. Show that the process $(W_s,s \\geq 0)$ with $W_s = B_{t+s} - B_t$ is a standard brownian motion independent of $\\mathcal{F}_t$.\n:::\n\n*Solution*.\n\nAt $s = 0$, $W(0) = B(t) - B(t) = 0$. \n\nConsider any arbitrary times $t_1 < t_2$. We have:\n\n\\begin{align*}\nW(t_2) - W(t_1) &= (B(t + t_2) - B(t)) - (B(t + t_1) - B(t))\\\\\n&= B(t + t_2) - B(t + t_1)\n\\end{align*}\n\nNow, $B(t + t_2) - B(t + t_1) \\sim \\mathcal{N}(0,t_2 - t_1)$. So, $W(t_2) - W(t_1)$ is a Gaussian random variable with mean $0$ and variance $t_2 - t_1$.\n\nFinally, consider any finite set of times $0=t_0 < t_1 < t_2 < \\ldots < t_n = T$. Then, $t < t + t_1 < t + t_2 < \\ldots < t + t_n$. We have that, $B(t + t_1) - B(t)$, $B(t + t_2) - B(t + t_1)$, $B(t + t_3) - B(t + t_2)$, $\\ldots$, $B(t+T) - B(t+t_{n-1})$ are independent random variables. Consequently, $W(t_1) - W(0)$, $W(t_2) - W(t_1)$, $W(t_3) - W(t_2)$, $\\ldots$, $W(t_n) - W(t_{n-1})$ are independent random variables. So, $(W_s,s\\geq 0)$ is a standard brownian motion.\n\nAlso, we have:\n\n\\begin{align*}\n\\mathbb{E}[W(s)|\\mathcal{F}_t] &= \\mathbb{E}[B(t + s) - B(t)|\\mathcal{F}_t]\\\\\n& \\{ B(t+s) - B(t) \\perp \\mathcal{F}_t \\}\\\\\n&= \\mathbb{E}[B(t + s) - B(t)]\\\\\n&= \\mathbb{E}[W(s)]\n\\end{align*}\n\nThus, $W(s)$ is independent of $\\mathcal{F}_t$, it does not depend upon the information available upto time $t$.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
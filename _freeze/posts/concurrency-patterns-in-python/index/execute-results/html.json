{
  "hash": "7c0c9aa5e77f7670f145a89f524f53e8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Concurrency features in Python\"\nauthor: \"Quasar\"\ndate: \"2025-02-28\"\ncategories: [Python]      \nimage: \"cpp.jpg\"\ntoc: true\ntoc-depth: 3\nformat:\n    html:\n        code-tools: true\n        code-block-border-left: true\n        code-annotations: below\n        highlight-style: pygments\n---\n\n\n\n\n## Basics\n\nIn single-core processors, the machine can only perform one task at a time, but can switch between many tasks many times per second. By doing a bit of one task and then a bit of another and so on, it appears that the tasks are happening concureently. This is called *task switching*. Because the task switches are so fast, it provides an illusion of concurrency to both the user and the applications.\n\nOn a single-core maching doing task switching, chunks from each task are interleaved. But, they are also spaced out a bit; in order to do the interleaving, the operating system has to perform a *context switch* every time it changes from one task to another, and this takes time. In order to perform a context switch, the OS has to save the CPU state and the instruction pointer for the currently running task, work out which task to switch to, and reload the CPU state for the task being switched to.\n\nMulti-core processors are genuinely capable of running more than one task in parallel. This is called *hardware concurrency*. \n\n### Throughput and Latency \n\nThe rate of doing work (operations per second) is called *throughput*. The response time it takes for a system to process a request is called *latency*.\n\n### Synchronous vs Asynchronous\n\nSynchronous execution is sequential. \n\n::: {#9cf37ead .cell execution_count=1}\n``` {.python .cell-code}\ndef foo():\n    print(f\"Inside foo.\")\n\ndef main():\n    print(f\"Starting work.\")\n    foo()\n    print(f\"Finishing work.\")\n\nmain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting work.\nInside foo.\nFinishing work.\n```\n:::\n:::\n\n\nIn the `main()` code-path, the call to `foo()` is a blocking call, the execution jumps to `foo()` and `main()` resumes when `foo()` returns.\n\nAsynchronous(or *async*) execution refers to execution that doesn't block when invoking subroutines. It is a *fire-and-forget* technique. Any work package runs separately from the main application thread and notifies the calling thread of its completion, failure or progress. \n\nUsually, such methods return an entity called `future` or `promise` that is the representation of an in-progress computation. The calling thread can query for the status of the computation via the returned future or promise and retrieve the result once completed. \n\nAnother pattern is to pass a callback function to the asynchronous functional call, which is invoked with the results when the asynchronous function is done processing. \n\nAsynchronous programming is an execllent choice for applications that do extensive network or disk I/O and spend most of their time waiting.\n\n### I/O bound vs CPU bound\n\n#### CPU bound\n\nPrograms that are compute-intensive are called CPU bound programs. This could involve numerical optimizations, Monte-Carlo simulations, data-crunching etc.\n\n#### I/O bound\n\nI/O bound programs spend most of their time doing network or main memory and file I/O operations. Since the CPU and main memory are separate, a bus exists between the two to transfer bits. Similarly, data needs to moved from the NIC to CPU/memory. Even though these physical distances are small, the time taken to transfer the data can waste a few thousand CPU cycles. This is why I/O bound programs show relatively lower CPU utilization than CPU bound programs.\n\n### Data race-conditions and thread safety\n\nThe most common cause of bugs in concurrent code is a *race-condition*. \n\n::: {#2450509a .cell execution_count=2}\n``` {.python .cell-code}\nimport concurrent.futures\nimport logging\nimport time\nimport concurrent\nimport threading\n\nclass Account:\n    def __init__(self):\n        self.value = 0\n\n    @property\n    def value(self):\n        return self._value\n    \n    @value.setter\n    def value(self, x):\n        self._value = x\n    \n    def credit(self, name : str, amount : float):\n        logging.info(\"Thread %s: starting update\", name)\n        \n        # ----- Critical section -----\n        local_copy = self.value     \n        local_copy += amount\n        time.sleep(0.1)\n        self.value = local_copy\n        # ----- End of critical section -----\n\n        logging.info(\"Thread %s: finishing update\", name)\n\nif __name__ == \"__main__\":\n    format = \"%(asctime)s: %(message)s\"\n    logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n    account = Account()\n    logging.info(\"Testing update. Starting value is %d.\", account.value)\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n        for index in range(2):\n            executor.submit(account.credit, index, 100)\n\n    logging.info(\"Testing update. Ending value is %d\", account.value)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n20:10:57: Testing update. Starting value is 0.\n20:10:57: Thread 0: starting update\n20:10:57: Thread 1: starting update\n20:10:57: Thread 0: finishing update\n20:10:57: Thread 1: finishing update\n20:10:57: Testing update. Ending value is 100\n```\n:::\n:::\n\n\nThe above logic can be made thread-safe by fencing off the critical section using a mutex and enforcing that only a single thread can enter at a time.\n\n### Deadlocks\n\nImagine that you have a toy that comes in two parts, and you need both parts to play with it - a toy drum and a drumstick, for example. Now, imagine that you ave two small children, both of whom like playing with it. If one of them gets both the drum and the drumstick, that child can merrily play the drum until titing of it. If the other child wants to play, they have wait, however sad that makes them. Now, imagine one child  has the drum and other has the drumstick. They're stuck, unless one decides to be nice and let the other play, each will hold on to whatver they have and demand that they be given the other piece, so neither gets to play. This is a deadlock. \n\nImagine two threads arguing over locks on mutexes: each of a pair of threads needs to lock both of a pair of mutexes to perform some operation, and each thread has one mutex and is waiting for the other. Neither thread can proceed, because each is waiting for the other to release its mutex. This scenario is called *deadlock*.\n\n::: {#57eec4b6 .cell execution_count=3}\n``` {.python .cell-code}\nimport threading\nimport concurrent\nimport time\n\nif __name__ == \"__main__\":\n    drum = threading.Lock()\n    drumstick = threading.Lock()\n\n    def child1_plays_drums():\n        print(f\"\\nChild-1 waiting for drums\")\n        drum.acquire()\n        print(f\"\\nChild-1 acquired drums\")\n        print(f\"\\nChild-1 waiting for drumstick\")\n        drumstick.acquire()\n        print(f\"\\nChild-1 is playing drums\")\n\n    def child2_plays_drums():\n        print(f\"\\nChild-2 waiting for drumstick\")\n        drumstick.acquire()\n        print(f\"\\nChild-2 acquired drumstick\")\n        print(f\"\\nChild-2 waiting for drums\")\n        drum.acquire()\n        print(f\"\\nChild-2 acquired drums\")\n        print(f\"\\nChild-2 is playing drums\")\n\n    t1 = threading.Thread(target=child1_plays_drums)\n    t2 = threading.Thread(target=child2_plays_drums)\n    \n    t1.start()\n    t2.start()\n\n    time.sleep(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nChild-1 waiting for drums\n\nChild-1 acquired drums\n\nChild-1 waiting for drumstick\n\nChild-1 is playing drums\n\nChild-2 waiting for drumstick\n```\n:::\n:::\n\n\n### Mutexes and Semaphores\n\nA mutex is an programming construct that allows only a single thread to access a shared resource or critical section. Once a thread acquires a mutex, all other threads attempting to acquire the same mutex are blocked until the thread releases the mutex.\n\nA semaphore on the hand is used to limit access to a collection of resources. Think of semaphore as having a limited number of permits to give out. If a semaphore has given out all the permits it has, then any new thread that comes along requesting a permit will be blocked till an earlier thread with a permit returns it to the semaphore. A protoypical example is a `ConnectionPool` that hands out database connects to requesting threads.  \n\nA semaphore with a single permit is called a *binary semaphore*. Semaphores can also be used for signaling among threads. This is an important distinction as it allows threads to cooperatively work towards completing a task. A mutex on the other hand, is strictly limted to serializing access to shared data among competing threads.\n\n#### When can a semaphore masquerade as a mutex?\n\nA semaphore can potentially act as a mutex if the number of permits it can give is at most $1$. However, the most important difference is that, the thread that calls `acquire()` on a mutex must subsequently `release()` the mutex. A mutex is *owned* by the thread acquiring it, upto the point the owning thread releases it. Whilst, in the case of a binary semaphore, different threads can call `acquire()` and `release()` on the semaphore. \n\n### Semaphore for signaling\n\nAnother distinction between a semaphore and a mutex is that semaphores can be used for signaling amongst threads. For example, in case of the classical [producer-consumer problem](https://quantdev.blog/posts/thread-safe-queues/), the producer thread can signal the consumer thread by incrementing the semaphore count to indicate to the consumer thread to read items from the queue. Threads can coordinate tasks using semaphores. A mutex, in contrast, only guards access to shared data. \n\n## `threading` module\n\nData-parallelism can be achieved using multi-threading.\n\n::: {#30071e85 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport threading\nimport typing\n\ndef accumulate(a : np.array, idx : int):\n    result = np.sum(a)\n    print(f\"\\nSum of the subarray {idx} = {result}\")\n\nif __name__ == \"__main__\":\n    data = np.random.rand(1000000)\n    num_chunks = 4\n    chunk_size = int(len(data) / num_chunks)\n    num_threads = num_chunks\n\n    threads = []\n    for i in range(num_threads):\n        start = i * chunk_size\n        end = start + chunk_size\n        thread = threading.Thread(target=accumulate(data[start:end], i))\n        threads.append(thread)\n\n    for t in threads:\n        t.start()\n\n    for t in threads:\n        t.join()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSum of the subarray 0 = 125051.8697129235\n\nSum of the subarray 1 = 125153.90423648663\n\nSum of the subarray 2 = 124940.40373623975\n\nSum of the subarray 3 = 124764.81648104054\n```\n:::\n:::\n\n\nAnother way to create threads is subclassing the `threading.Thread` class.\n\n::: {#57350cf2 .cell execution_count=5}\n``` {.python .cell-code}\nfrom threading import Thread\nfrom threading import current_thread\n\nclass MyTask(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"subClassThread\", args=(2,3))\n\n    def run(self):\n        print(f\"{current_thread().name} is executing\")\n\nmyTask = MyTask()\nmyTask.start()  # start the thread\nmyTask.join()   # wait for the thread to complete\n```\n:::\n\n\nThe important caveats to remember when subclassing `Thread` class are:\n\n- We can only override the `run()` method and the constructor of the `Thread` class.\n- `Thread.__init__()` must be invoked if the subclass chooses to override the constructor.\n\n### Daemon Thread\n\n*Daemon* threads are background threads. When the `main` thread is about to exit, it cycles through all regular non-daemon threads and waits for them to complete. In the implementation of the `threading` module, the [`_shutdown()`](https://github.com/python/cpython/blob/df5cdc11123a35065bbf1636251447d0bfe789a5/Lib/threading.py#L1263) method iterates through non-daemon threads and invokes `join()` on each of them. `join()` is a blocking call, which returns when a thread's work package is complete.\n\n::: {#efe45ea3 .cell execution_count=6}\n``` {.python .cell-code}\nimport threading\nimport time\n\ndef daemon_task():\n    while(True):\n        print(f\"Executing daemon task\")\n        time.sleep(1)\n    print(f\"Completed daemon task\")\n\nif __name__ == \"__main__\":\n    daemon_thread = threading.Thread(\n        target=daemon_task,\n        name=\"daemon thread\",\n        daemon=True\n    )\n\n    daemon_thread.start()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExecuting daemon task\n```\n:::\n:::\n\n\n### Implementation of a thread-safe LIFO stack\n\n::: {#9a36f2d6 .cell execution_count=7}\n``` {.python .cell-code}\nimport threading\nimport time\nfrom typing import Any, Optional\n\nclass StackFull(Exception):\n    pass\n\nclass StackEmpty(Exception):\n    pass\n    \nclass Stack:\n    def __init__(self, maxsize : int = None):\n        self._mutex = threading.RLock()\n        self.maxsize = maxsize\n        self._data = list()\n\n    @property\n    def maxsize(self):\n        with self._mutex:\n            value = self._maxsize\n\n        return value\n\n    @maxsize.setter\n    def maxsize(self, value : int):\n        with self._mutex:\n            self._maxsize = value\n\n    def size(self) -> int:\n        with self._mutex:\n            size = len(self._data)\n        \n        return size\n\n    def empty(self) -> bool:\n        with self._mutex:\n            isEmpty = len(self._data) == 0\n        \n        return isEmpty\n\n    def full(self) -> bool:\n        with self._mutex:\n            if(self.maxsize is not None):\n                isFull = len(self._data) == self.maxsize\n            else:\n                isFull = False\n        \n        return isFull\n\n    def put(\n        self,\n        item : Any, \n        block : bool = True, \n        timeout : float = -1\n    ) -> None:\n        self._mutex.acquire(blocking=True,timeout=timeout)\n        print(f\"\\nPushing item {item} to the stack\")\n        if self.full():\n            print(\"Stack full!\")\n            self._mutex.release()\n            raise StackFull(\"Stack full!\")\n        \n        self._data.append(item)\n        print(f\"\\nPush complete\")\n        print(f\"stack : {self._data}\")\n        self._mutex.release()\n    \n    def put_nowait(self, item:Any):\n        self.put(item, block=False)\n\n    def get(self, block : bool = True, timeout : float = -1) -> Any:\n        self._mutex.acquire(blocking=block, timeout=timeout)\n        print(f\"\\nPopping from the stack\")\n        if self.empty():\n            print(\"Stack empty!\")\n            self._mutex.release()\n            raise StackEmpty(\"Stack empty!\")\n        \n        value = self._data[self.size() - 1]\n        del self._data[self.size() - 1]\n        print(f\"\\nPopped item {value} from the stack\")\n        print(f\"stack : {self._data}\")\n        self._mutex.release()\n\n        return value\n\n    def get_no_wait(self):\n        return self.get(block=False)\n\n    def top(self) -> Any:\n        self._mutex.acquire()\n        if self.empty():\n            self._mutex.release()\n            print(\"Stack empty!\")\n            raise StackEmpty(\"Stack empty!\")  \n\n        value = self._data[self.size() - 1]\n        self._mutex.release()\n        return value\n\ndef push_thread(stack : Stack):\n    \n    for i in range(10):\n        try:\n            stack.put(i)\n            time.sleep(0.1)\n        except Exception:\n            pass\n\ndef pop_thread(stack: Stack):\n    for i in range(10):\n        try:\n            item = stack.get()\n            time.sleep(0.12)\n        except Exception:\n            pass\n\nif __name__ == \"__main__\":\n    stack = Stack()\n    \n    t1 = threading.Thread(target=push_thread, args=(stack,))\n    t2 = threading.Thread(target=pop_thread, args=(stack,))\n    \n    t1.start()\n    t2.start()\n\n    t1.join()\n    t2.join()\n    \n    print(\"main() thread finished.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPushing item 0 to the stack\n\nPush complete\nstack : [0]\n\nPopping from the stack\n\nPopped item 0 from the stack\nstack : []\n\nPushing item 1 to the stack\n\nPush complete\nstack : [1]\n\nPopping from the stack\n\nPopped item 1 from the stack\nstack : []\n\nPushing item 2 to the stack\n\nPush complete\nstack : [2]\n\nPopping from the stack\n\nPopped item 2 from the stack\nstack : []\n\nPushing item 3 to the stack\n\nPush complete\nstack : [3]\n\nPopping from the stack\n\nPopped item 3 from the stack\nstack : []\n\nPushing item 4 to the stack\n\nPush complete\nstack : [4]\n\nPopping from the stack\n\nPopped item 4 from the stack\nstack : []\n\nPushing item 5 to the stack\n\nPush complete\nstack : [5]\n\nPopping from the stack\n\nPopped item 5 from the stack\nstack : []\n\nPushing item 6 to the stack\n\nPush complete\nstack : [6]\n\nPushing item 7 to the stack\n\nPush complete\nstack : [6, 7]\n\nPopping from the stack\n\nPopped item 7 from the stack\nstack : [6]\n\nPushing item 8 to the stack\n\nPush complete\nstack : [6, 8]\n\nPopping from the stack\n\nPopped item 8 from the stack\nstack : [6]\n\nPushing item 9 to the stack\n\nPush complete\nstack : [6, 9]\n\nPopping from the stack\n\nPopped item 9 from the stack\nstack : [6]\n\nPopping from the stack\n\nPopped item 6 from the stack\nstack : []\nmain() thread finished.\n```\n:::\n:::\n\n\nIn the above implementation, I used `RLock` - a reentrant lock. If a thread acquires a `RLock` object, it can choose to reacquire it as many times as possible. It is implicit to call `release()` as many times as `lock()` was called.\n\n### Condition variables\n\nWe looked at various ways of protecting the data that's shared between threads. But, sometimes we don't just need to protect the data, we also need to synchronize actions on separate threads. One thread might need to wait for another thread to complete a task before the first thread can complete its own. In general, its common to want a thread to wait for a specific event to happen or a condition to be `true`. Although it would be possible to do this by periodically checking a *task-complete* flag or something like that, it is far from ideal. The need to synchronize operations between threads like this is a common scenario and the python standard standard library provides facilities to handle it, in the form of *condition variables* and *futures*.  \n\nWe can create a condition variable by passing in a lock. The two important methods of a condition variables are:\n\n- `wait()` - invoked by a thread to wait(block) on a test condition to be satisfied\n- `notify()` - invoked by the waited-for thread when it finishes its task and wants to inform the waiting threads to proceed.\n\n### Implementation of a thread-safe SPSC bounded ring-buffer\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
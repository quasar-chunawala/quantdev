{
  "hash": "dd433c8d8d040983bdd53ec05e1b1cbe",
  "result": {
    "markdown": "---\ntitle: Pymalloc - Down the rabbit hole\nauthor: Quasar\ndate: '2025-11-28'\ncategories:\n  - Python\nimage: python.jpg\ntoc: true\ntoc-depth: 3\n---\n\n# Introduction \n\nCPython is one of the many implementations of the Python runtime written in C code. There are other implementations such as PyPy, Jython. In this blog post, I summarize how object memory is alllocated and freed and how CPython manages memory leaks.\n\nPython is a dynamically-typed language. The size of variables can't be calculated at compile-time. Most of Python's core types are dynamically sized. The `list` type can be of any size, a `dict` can have any number of keys, and even `int` is dynamic. The user never has to specify the size of these types. Names in Python can be reused for values of different types.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\na_value = 1\na_value = \"Now, I'm a string\"\na_value = [\"Now\", \"I\", \"am\", \"a\", \"list\"]\n```\n:::\n\n\nTo overcome these constraints, CPython relies heavily on dynamic memory allocation, but adds safety rails to automate the freeing of memory using the garbage collection and reference counting algorithms.\n\nInstead of the Python developer having to allocate memory, Python object memory is allocated automatically by a single, unified API. This design requires that the entire CPython standard library and core modules(written in C) use this API.\n\n# Allocation Domains\n\n- The raw domain is used for allocation from the system heap and large, or non-object related memory.\n\n- The object domain is used for the allocation of all Python object related memory.\n\n- The PyMem domain is the same as `PYMEM_DOMAIN_OBJ`. this exists for legacy purposes.\n\nEach domain implements the same interface of frunctions:\n\n- `_Alloc(size_t size)` allocates memory of `size` bytes and returns a pointer.\n\n- `_Calloc(size_t nelem, size_t el_size)` allocates `nelem` elements each of size `el_size` and returns a pointer.\n\n- `_Realloc(void* ptr, size_t new_size)` reallocates memory of size `new_size`.\n\n- `_Free(void* ptr)` frees memory at `ptr` back to the heap.\n\nThe `PyMemAllocatorDomain` enumeration represents the three domains in CPython as `PYMEM_DOMAIN_RAW`, `PYMEM_DOMAIN_OBJ` and `PYMEM_DOMAIN_MEM`.\n\nCPython uses $2$ memory allocators:\n\n- `malloc` is the operating system allocator for the raw memory domain.\n- `pymalloc` is the CPython allocator the PyMem and object domains.\n\n# The CPython memory allocator\n\nThe CPython memory alloctor sit on top of the OS memory allocator and has an algorithm for allocation. \n\n- Most of the memory allocation requests are small and of a fixed size, because `PyObject` is $16$ bytes, `PyASCIIObject` is $42$ bytes, `PyCompactUnicodeObject` is $72$ bytes.\n\n- The `pymalloc` allocator allocates memory blocks only upto $256$ KB. Anything larger is sent to the OS allocator.\n\n- `pymalloc` uses the GIL instead of the system thread-safety check.\n\nTo help clarify this situation, you can imagine a sports stadium, home of CPython FC, as an analogy. To help manage crowds, CPython FC has implemented a system breaking the stadium up into sections A to E, each with seating in rows $1$ to $40$. \n\nAt the front of the stadium, rows $1$ to $10$ are the roomier premium seats, with $80$ seats in each row. At the back rows $31$ to $40$ are the conomy seats with $150$ seats per row.\n\nThe Python memory allocation algorithm has similar characteristics:\n\n- Just like the stadium as seats, the `pymalloc` algorithm has memory blocks.\n- Just like the seats can be premium, regular, or economy, memory blocks are all of range of fixed sizes. You can't bring your desk chair!\n- Just like seats of the same size are put into rows, blocks of the same size are put into pools.\n\nA central register keeps a record of where blocks are and the number of blocks available in a pool, just as the stadium allocates seating. When a row in the stadium is full, the next row is used. When a pool of blocks is full, the next pool is used. Pools are groups into arenas, just like the stadium groups the rows into sections.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n%load_ext itikz\n```\n:::\n\n\nThere are several advantages to this strategy:\n\n- The algorithm is more performant for CPython's main use case: small, short-lived objects.\n- The algorithm uses the GIL instead of system thread-lock detection.\n- The algorithm uses memory mapping `mmap()` syscall instead of heap allocations. \n\nThe requested memory is always matched to a block size. Blocks of the same size are all put into the same **pool** of memory. Pools are grouped into arenas.\n\n## Blocks, Pools and Arenas\n\n`pymalloc` allocates large memory regions called arenas from the system, then divides them into pools. \n\n### Arenas\n\nArenas are allocated from the system heap using `mmap` when available otherwise `malloc`. CPython arenas are $256$-KB on 32-bit systems and $1$ MB on 64-bit systems. Each arena contains multiple pools. \n\n```cpp\n#ifdef USE_LARGE_ARENAS\n#define ARENA_BITS              20                    /* 1 MiB */\n#else\n#define ARENA_BITS              18                    /* 256 KiB */\n#endif\n#define ARENA_SIZE              (1 << ARENA_BITS)\n#define ARENA_SIZE_MASK         (ARENA_SIZE - 1)\n```\n\nArenas are scattered in the main memory. Each arena is a contiguous chunk of memory. CPython maintains a doubly linked list `usable_arenas` that tracks all the arena objects, that have atleast one pool available for allocation. The `usable_arenas` list is maintained in ascending order of each arena's `nfreepools` (number of free pools). This sorting ensures that allocations preferentially use arenas with fewest free pools.\n\nOur main book-keeping data structure of interest is the `arena_object`. The `usable_arenas` doubly linked list strings together `arena_object` nodes . These contain meta-information of each arena  with `prevarena` and `nextarena` pointers. Each of these nodes have a `address` field which is a pointer to the actual contiguous block of memory ($256$KB / $1$MB) returned by `malloc`.\n\n```cpp\n/* Record keeping for arenas. */\nstruct arena_object {\n    /* The address of the arena, as returned by malloc.  Note that 0\n     * will never be returned by a successful malloc, and is used\n     * here to mark an arena_object that doesn't correspond to an\n     * allocated arena.\n     */\n    uintptr_t address;\n\n    /* Pool-aligned pointer to the next pool to be carved off. */\n    pymem_block* pool_address;\n\n    /* The number of available pools in the arena:  free pools + never-\n     * allocated pools.\n     */\n    uint nfreepools;\n\n    /* The total number of pools in the arena, whether or not available. */\n    uint ntotalpools;\n\n    /* Singly-linked list of available pools. */\n    struct pool_header* freepools;\n    /* Whenever this arena_object is not associated with an allocated\n     * arena, the nextarena member is used to link all unassociated\n     * arena_objects in the singly-linked `unused_arena_objects` list.\n     * The prevarena member is unused in this case.\n     *\n     * When this arena_object is associated with an allocated arena\n     * with at least one available pool, both members are used in the\n     * doubly-linked `usable_arenas` list, which is maintained in\n     * increasing order of `nfreepools` values.\n\n     *\n     * Else this arena_object is associated with an allocated arena\n     * all of whose pools are in use.  `nextarena` and `prevarena`\n     * are both meaningless in this case.\n     */\n\n    struct arena_object* nextarena;\n    struct arena_object* prevarena;\n};\n```\n\nEach `arena_object` node also contains a  field `freepools` of the type `pool_header*` which points to the head of a linked list which tracks available pools in the arena.\n\nWhen an arena's `nfreepools` reaches zero (all pools allocated), it's removed from `usable_arenas` linked list. Conversely, when a pool becomes empty and is added to an arena's freepools, the arena may be added back to `usable_arenas` if it was previously full. \n\n### Pools\n\nPools are $4$KB subdivisions (or $16$ Kb on large systems) of arenas. Normally, the size of the pool is equal to the size of a memory page. Limiting pool to the fixed size of blocks helps with fragmentation. Each pool manages blocks of single size class and can be in three states:\n\n- Used : Partially allocated blocks.\n- Full : All blocks allocated.\n- Empty : All blocks available. \n\n```cpp\n/*\n * Size of the pools used for small blocks.  Must be a power of 2.\n */\n\n#ifdef USE_LARGE_POOLS\n#define POOL_BITS               14                  /* 16 KiB */\n#else\n#define POOL_BITS               12                  /* 4 KiB */\n#endif\n#define POOL_SIZE               (1 << POOL_BITS)\n```\n\nEach pool has a special header structure:\n\n```cpp\n/* Pool for small blocks. */\nstruct pool_header {\n\n    union { pymem_block *_padding;\n            uint count; } ref;          /* number of allocated blocks    */\n\n    pymem_block *freeblock;             /* pool's free list head         */\n\n    struct pool_header *nextpool;       /* next pool of this size class  */\n\n    struct pool_header *prevpool;       /* previous pool of this size class                             */\n\n    uint arenaindex;                    /* index into arenas of base adr */\n\n    uint szidx;                         /* block size class index        */\n\n    uint nextoffset;                    /* bytes to virgin block         */\n\n    uint maxnextoffset;                 /* largest valid nextoffset      */\n\n};\n```\n\nPools of the same sized blocks are linked together using doubly linked list (the `nextpool` and `prevpool` fields). The `szidx` field keeps the size class index, whereas `ref.count` keeps the number of used blocks. The `arenaindex` stores the number of an arena in which this pool was created.\n\nIn order to efficient manage pools, a pool table called `usedpools` is maintained. A pool table is an array of pointers. It is segmented by the size class index `i`. For an index `i`, `usedpools[i]` points to the head of linked list of all partial used pools of the same size class `i`. Each node of this linked list is of type `pool_header`. \n\nIf a full pool has a block freed, then the pool is put back in the used state. The newly freed pool is added to the front of the approapriate `usedpools[]` list, so the next allocation for its size class will use the freed block. \n\nOn transition to empty, a pool is unlinked from its `usedpools[]` list and added to the front of the arena's singly linked `freepools` list.\n\n### Blocks\n\nBlocks are the actual allocated units, with sizes ranging from $8$ to $512$ bytes in $8$-byte increments. \n\n| Request in bytes | Size of the allocated block | Size class index |\n|----------|----------|----------|\n| 1-8  | 8   | 0   |\n| 9-16 | 16 | 1 |\n| 17-24| 24 | 2 |\n| ... | ... | ... |\n|505-512| 512 | 63 |\n\nOn a 64-bit system, since an arena is $1$ MB, there will always be $64$ pools each of size $16$KB. \n\nWithin a pool, blocks of fixed size can be allocated and freed. Available blocks within a pool are listed in the singly linked list `freeblock`. Whenever a block is freed, its inserted at the front of the `freeblock` list. When a pool is initialized, only the first two blocks are linked within the `freeblock` list.\n\n```{mermaid}\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffffff', 'primaryBorderColor': '#333333', 'background': '#ffffff', 'mainBkg': '#ffffff', 'clusterBkg': '#ffffff', 'clusterBorder': '#333333', 'tertiaryBkgColor': '#ffffff', 'tertiaryBorderColor': '#333333'}}}%%\ngraph LR\n    subgraph usable_arenas [\"usable_arenas: Doubly-Linked Arena List\"]\n        A1[\"arena_object #1<br/>nfreepools: 3<br/>address: 0x1000\"]\n        A2[\"arena_object #2<br/>nfreepools: 5<br/>address: 0x2000\"]\n        A3[\"arena_object #3<br/>nfreepools: 1<br/>address: 0x3000\"]\n        \n        A1 -->|nextarena| A2\n        A2 -->|prevarena| A1\n        A2 -->|nextarena| A3\n        A3 -->|prevarena| A2\n    end\n    \n    subgraph freepools_A1 [\"freepools: Singly-Linked Pool List\"]\n        P1[\"pool_header<br/>szidx: 5 (40B)<br/>count: 2<br/>state: Used\"]\n        P2[\"pool_header<br/>szidx: 10 (80B)<br/>count: 0<br/>state: Empty\"]\n        P3[\"pool_header<br/>szidx: 3 (24B)<br/>count: 5<br/>state: Used\"]\n        \n        P1 -->|nextpool| P2\n        P2 -->|nextpool| P3\n        P3 -->|nextpool| None1[\"NULL\"]\n    end\n    \n    subgraph freeblock_P1 [\"freeblock: Singly-Linked Block List\"]\n        B1[\"Block<br/>addr: 0x1200<br/>size: 40B<br/>FREE\"]\n        B2[\"Block<br/>addr: 0x1228<br/>size: 40B<br/>FREE\"]\n        B3[\"Block<br/>addr: 0x1250<br/>size: 40B<br/>ALLOCATED\"]\n        B4[\"Block<br/>addr: 0x1278<br/>size: 40B<br/>FREE\"]\n        \n        B1 -->|next| B2\n        B2 -->|next| B3\n        B3 -.->|skip| B4\n        B4 -->|next| None2[\"NULL\"]\n    end\n    \n    A1 -->|freepools| P1\n    P1 -->|freeblock| B1\n    \n    style A1 fill:#64B5F6,stroke:#1976D2,stroke-width:2px,color:#000\n    style A2 fill:#64B5F6,stroke:#1976D2,stroke-width:2px,color:#000\n    style A3 fill:#64B5F6,stroke:#1976D2,stroke-width:2px,color:#000\n    \n    style P1 fill:#81C784,stroke:#2E7D32,stroke-width:2px,color:#000\n    style P2 fill:#81C784,stroke:#2E7D32,stroke-width:2px,color:#000\n    style P3 fill:#81C784,stroke:#2E7D32,stroke-width:2px,color:#000\n    \n    style B1 fill:#FFB74D,stroke:#F57C00,stroke-width:2px,color:#000\n    style B2 fill:#FFB74D,stroke:#F57C00,stroke-width:2px,color:#000\n    style B3 fill:#EF9A9A,stroke:#C62828,stroke-width:2px,color:#000\n    style B4 fill:#FFB74D,stroke:#F57C00,stroke-width:2px,color:#000\n    \n    style None1 fill:#EEEEEE,stroke:#666,stroke-width:1px,color:#000\n    style None2 fill:#EEEEEE,stroke:#666,stroke-width:1px,color:#000\n    \n    style usable_arenas fill:#ffffff,stroke:#1976D2,stroke-width:2px,color:#000\n    style freepools_A1 fill:#ffffff,stroke:#2E7D32,stroke-width:2px,color:#000\n    style freeblock_P1 fill:#ffffff,stroke:#F57C00,stroke-width:2px,color:#000\n```\n\n\n### Block allocation API\n\n \n\n# CPython interning\n\nThe general rule is that objects are allocated on assignment. Variables just point to objects. There is an exception to this general rule. This is python runtime implementation specific. Often commonly used objects are preallocated and are shared instead of costly new allocations. This is mainly done for performance optimization. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nx = 255\ny = 255\nprint(x is y, x == y)\nx = 1024\ny = 1024\nprint(x is y, x == y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue True\nFalse True\n```\n:::\n:::\n\n\n`int`s in the range $[-5,257)$, empty tuples and all empty or single length strings are preallocated. \n\n## `string` interning explained\n\nString interning is a method of storing only one copy of each distinct string value, which must be immutable. In Python3, we have a function `sys.intern()` and if we use this function, we can enter a string in the table of interned strings. We get a reference to the interned string. \n\nSo, we can gain a little extra performance on dictionary lookup (key comparisons after hashing can be done by a pointer compare instead of a string compare). \n\nNames used in programs are automatically interned. Dictionaries used to hold module, class or instance attributes have interned keys. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sys import intern\na, b = \"strin\", \"string\"\nprint(f\"{a + 'g' is b}\")    # Returns false\nintern(a + 'g') is intern(b) # returns True\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFalse\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nTrue\n```\n:::\n:::\n\n\n# Mutable containers memory allocationn strategy\n\nThere are different mutable containers in Python - `list`s, `set`s and `dict`s. Behind the scenes, there is a strategy for allocating these containers. \n\nA good strategy will plan for growth and shrinkage. It will slightly overallocate memory needed by the container, to leave room for growth. Each time we append to a `list`, we won't have to reallocate the memory. We also have to remember that sometimes we have to shrink the memory for a mutable container. This can help reduce the number of expensive function calls for instance to `realloc` or `memcpy`.\n\n# List allocation strategy\n\nLists are stored as fixed length array of pointers. So, we just point to objects. By design, we overallocate for list growth by append.\n\n- The capacity growth pattern is roughly $4,8,16,25,35,46,\\ldots$\n\n\nIf we put something at the end of the list, these operations are cheap. But, if we put something in the middle or the beginning, we'll have to copy/shift the elements in memory to perform this operation. \n\nOn my 64-bit machine, with Python 3.13.7, the list allocation size is:\n\n- 64 bits : `56 + 8 * length`\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport sys\nl = []\nfor i in range(17):\n    l.append(i+1)\n    print(f\"List len = {len(l)}, size = {sys.getsizeof(l)} bytes\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList len = 1, size = 88 bytes\nList len = 2, size = 88 bytes\nList len = 3, size = 88 bytes\nList len = 4, size = 88 bytes\nList len = 5, size = 120 bytes\nList len = 6, size = 120 bytes\nList len = 7, size = 120 bytes\nList len = 8, size = 120 bytes\nList len = 9, size = 184 bytes\nList len = 10, size = 184 bytes\nList len = 11, size = 184 bytes\nList len = 12, size = 184 bytes\nList len = 13, size = 184 bytes\nList len = 14, size = 184 bytes\nList len = 15, size = 184 bytes\nList len = 16, size = 184 bytes\nList len = 17, size = 248 bytes\n```\n:::\n:::\n\n\nThere is a fixed overhead of $56$ bytes (the object header, type pointer, reference count ) etc.\n\nFor each size tier, the calculation is approximately:\n\n- 88 bytes = 56 + 32(4 slots)\n- 120 bytes = 56 + 64(8 slots)\n- 184 bytes = 56 + 128(16 slots)\n- 248 bytes =56 + 192(24 slots)\n\nWe shrink when the list size goes below $1/2$ of the allocated storage. This is why appending to Python lists is $O(1)$ amortized time - most appends just fill unused capacity, and only occasional appends trigger a reallocation. \n\n# Overallocation of dictionaries and sets\n\nThese are represented as fixed-length hash tables. We overallocate when we reach $2/3$rd of the capacity of a dictionary of set. For small dictionaries or sets, we quadruple the capacity else we double the capacity. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
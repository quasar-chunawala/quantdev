---
title: "The Markov Property"
author: "Quasar"
date: "2024-07-12"
categories: [Stochastic Calculus]      
image: "image.jpg"
toc: true
toc-depth: 3
jupyter: python3
---

## The Markov Property for Diffusions

Let's start by exhibiting the Markov property of Brownian motion. To see this, consider $(\mathcal{F}_t,t\geq 0)$, the natural filtration of the Brownian motion $(B_t,t\geq 0)$. Consider $g(B_t)$ for some time $t$ and bounded function $g$. (For example, $g$ could be an indicator function.) Consider also a random variable $W$ that is $\mathcal{F}_s$ measurable for $s < t$. (For example, $W$ could be $B_s$ or $1_{B_s > 0}$.) Let's compute $\mathbb{E}[g(B_t)W]$.

```{python}
%load_ext itikz
```

\begin{align*}
\mathbb{E}[g(B_t)W] &= \mathbb{E}[\mathbb{E}[Wg(B_t - B_s + B_s)|\mathcal{F}_s]]
\end{align*}

The random variable $(B_t - B_s)$ follows a $\mathcal{N}(0,t-s)$ distribution. By LOTUS,

$$
\begin{align*}
\mathbb{E}[g(B_t)W] 
&= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)|\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\
&= \{\text{ Using the fact that }B_t - B_s \perp B_s\}\\
&= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\
&= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy
\end{align*}
$$

By Fubini's theorem, the integral and the expectation operator can be interchanged, and since $W$ is $\mathcal{F}_s$ measurable, it follows from the definition of conditional expectations that:

$$
\begin{align*}
\mathbb{E}[Wg(B_t)] = \mathbb{E}[W\mathbb{E}[g(B_t)|\mathcal{F}_s]] = \mathbb{E}\left[W\int_{\mathbb{R}}g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy\right]
\end{align*}
$$

It follows that:

$$
\begin{align*}
\mathbb{E}[g(B_t)|\mathcal{F}_s] = \int_{\mathbb{R}}g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy
\end{align*}
$${#eq-conditional-expectation-of-function-brownian-motion}

We make two important observations. First, the right hand side is a function of $s,t$ and $B_s$ only (and not of the Brownian motion before time s). In particular, we have:

$$
\mathbb{E}[g(B_t)|\mathcal{F}_s] = \mathbb{E}[g(B_t)|B_s]
$$

This holds for any bounded function $g$. In particular, it holds for all indicator functions. This implies that the conditional distribution of $B_t$ given $\mathcal{F}_s$ depends solely on $B_s$, and not on other values before time $s$. Second, the right-hand side is *time-homogenous* in the sense that it depends on the time difference $t-s$. 

We have just shown that Brownian motion is a *time-homogenous Markov process*. 

::: {#def-markov-process}

### Markov process.

Consider a stochastic process $(X_t,t\geq 0)$ and its natural filtration $(\mathcal{F}_t,t\geq 0)$. It is said to be a *Markov process* if and only if for any (bounded) function $g: \mathbb{R} \to \mathbb{R}$, we have:

$$
\mathbb{E}[g(X_t) | \mathcal{F}_s] = \mathbb{E}[g(X_t) | X_s], \quad \forall t \geq 0, \forall s \leq t
$$ {#eq-markov-process}

:::

This implies that $\mathbb{E}[g(X_t)|\mathcal{F}_s]$ is an explicit function of $s$, $t$ and $X_s$. It is said to be *time-homogenous*, if it is a function of $t-s$ and $X_s$. Since the above holds for all bounded $g$, the conditional distribution of $X_t$ given $\mathcal{F}_s$ is the same as the conditional distribution of $X_t$ given $X_s$. 

One way to compute the conditional distribution of $X_t$ given $\mathcal{F}_s$ is to compute the conditional MGF given $\mathcal{F}_s$, that is:

$$
\mathbb{E}[e^{a X_t}|\mathcal{F}_s], \quad a \geq 0
$$ {#eq-conditional-mgf-of-xt}

The process would be Markov, if the conditional MGF is an explicit function of $s$, $t$ and $X_s$. 

::: {#exm-brownian-motion-is-markov}

(Brownian Motion is Markov) Let $(B_t,t\geq 0)$ be a standard brownian motion. Our claim is that the brownian motion is a markov process.
:::

*Proof.*

We have:

\begin{align*}
\mathbb{E}[e^{a B_t}|\mathcal{F}_s] &= \mathbb{E}[e^{a (B_t - B_s + B_s)}|\mathcal{F}_s]\\
& \{ \text{ since }B_s \text{ is }\mathcal{F}_s-\text{ measurable }\}\\
&= e^{a B_s} \mathbb{E}[e^{a (B_t - B_s)}|\mathcal{F}_s]\\
& \{ \text{ since }B_t - B_s \perp \mathcal{F}_s \}\\
&= e^{a B_s} \mathbb{E}[e^{a (B_t - B_s)}]\\
&= e^{a B_s} e^{\frac{1}{2}a^2(t-s)}
\end{align*}

This closes the proof. $\blacksquare$

An equivalent (but more symmetric) way to express the Markov property is to say that *the future of the process is independent of the past, when conditioned on the present*. Concretely, this means that for any $r < s< t$, we have that $X_t$ is independent of $X_r$, when we condition on $X_s$.

The conditional distribution of $X_t$ given $X_s$ is well described using *transition probabilities*. We will more interested in a case well these probabilities admit a density $f_{X_t|X_s=x}(y)$. More precisely, for such a Markov process, we have:

$$
\begin{align*}
\mathbb{E}[g(X_t)|X_s = x] &= \int_{\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\
&=\int_{\mathbb{R}} g(y) p(y,t|x,s) dy
\end{align*}
$$

Here, we explicitly write the left-hand side as a function of space, that is, the position $X_s$, by fixing $X_s = x$. In words, the *transition probability density* $p(y,t|x,s)$ represents the probability density that starting from $X_s = x$ at time $s$, the process ends up at $X_t = y$ at time $t > s$. If the process is time-homogenous, this only depends on the time difference $(t-s)$ and we write $p(y,t|x,s)$. From @eq-conditional-expectation-of-function-brownian-motion, we can write:
$$
\mathbb{E}[g(B_t)|B_s = x] = \int_{\mathbb{R}} g(u + x) \frac{e^{-\frac{u^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} du
$$

In the above expression, the random variable $B_t - B_s$ takes some value $u \in \mathbb{R}$ and $B_s = x$ is fixed. Then, $B_t$ takes the value $u + x$. Let $y = u + x$. Then, $u = y - x$. Consequently, we may write:

$$
\mathbb{E}[g(B_t)|B_s = x] = \int_{\mathbb{R}} g(y) \frac{e^{-\frac{(y-x)^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} dy
$$

So, the transition density function for standard Brownian motion is:

$$
p(y,t|x,0)= \frac{e^{-\frac{(y-x)^2}{2s}}}{\sqrt{2\pi s}}, \quad s>0, x,y\in\mathbb{R}
$$ {#eq-brownian-motion-transition-density-function}

This function is sometimes called the *heat kernel*, as it relates to the *heat equation*. 

The Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process $(X_t,t\geq 0)$ at different times. Consider the functions $f = \mathbf{1}_A$ and $g = \mathbf{1}_B$ from $\mathbb{R} \to \mathbb{R}$, where $A$ and $B$ are two intervals in $\mathbb{R}$. Let's compute $\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) = \mathbb{E}[\mathbf{1}_{A} \mathbf{1}_{B}] = \mathbb{E}[f(X_{t_1}) g(X_{t_2})]$ for $t_1 < t_2$. By the properties of conditional expectation and the Markov property, we have:

\begin{align*}
\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &= \mathbb{E}[f(X_{t_1})g(X_{t_2})]\\
&= \mathbb{E}[f(X_{t_1})\mathbb{E}[g(X_{t_2})|\mathcal{F}_{t_1}]]\\
&= \mathbb{E}[f(X_{t_1})\mathbb{E}[g(X_{t_2})|X_{t_1}]]
\end{align*}

Assuming that the process is time-homogenous and admits a transition density $p(y,t|x,0)$ as for Brownian motion, this becomes:

\begin{align*}
\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &= \int_{\mathbb{R}} f(x_1) \left(\int_{\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \right) p(x_1,t_1|x_0,0) dx_1\\
&= \int_{A} \left(\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \right) p(x_1,t_1|x_0,0) dx_1
\end{align*}

This easily generalizes to any finite-dimensional distribution of $(X_t, t\geq 0)$.

::: {#exm-markov-versus-martingale}

(Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift $(X_t, t \geq 0)$, where $X_t = \sigma B_t + \mu t$. Conversely, take $Y_t = \int_0^t X_s dB_s$, where $X_s = \int_0^s B_u dB_u$. The integrand $X_s$ depends on whole Brownian motion path upto time $s$ and not just on $B_s$. 
:::

::: {#nte-functions-of-markov .callout-tip}

### Functions of Markov Processes

It might be tempting to think that if $(X_t,t\geq 0)$ is a Markov process, then the process defined by $Y_t = f(X_t)$ for some reasonable function $f$ is also Markov. Indeed, one could hope to write for an arbitrary bounded function $g$:

$$
\begin{align*}
\mathbb{E}[g(Y_t)|\mathcal{F}_s] = \mathbb{E}[g(f(X_t))|\mathcal{F}_s] = \mathbb{E}[g(f(X_t))|\mathcal{X}_s] 
\end{align*}
$$ {#eq-functions-of-markov-process}

by using the Markov property of $(X_t,t\geq 0)$. The flaw in this reasoning is that the Markov property should hold for the natural fitration $(\mathcal{F}_t^Y,t\geq 0)$ of the process $(Y_t,t\geq 0)$ and not the one of $(X_t,t\geq 0)$, $(\mathcal{F}_t^X,t\geq 0)$. It might be that the filtration $(\mathcal{F}_t^Y,t\geq 0)$ has less information that $(\mathcal{F}_t^X,t\geq 0)$, especially, if the function $f$ is not one-to-one. For example, if $f(x)=x^2$, then $\mathcal{F}_t^Y$ has less information than $\mathcal{F}_t^X$ as we cannot recover the sign of $X_t$ knowing $Y_t$. In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when $f$ is not one-to-one. 
:::

It turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.

::: {#thm-diffusions-are-markov-processes}

### Diffusions are Markov processes. 

Let $(B_t,t\geq 0)$ be a standard Brownian motion. Let $\mu : \mathbb{R} \to \mathbb{R}$ and $\sigma: \mathbb{R} \to \mathbb{R}$ be differentiable functions with bounded derivatives on $[0,T]$. Then, the diffusion with the SDE 

$$
dX_t = \mu(X_t) dt + \sigma(X_t)dB_t, \quad X_0 = x_0
$$

defines a time-homogenous markov process on $[0,T]$. 
:::

An analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments. 

*Proof.*

By the [existence and uniqueness theorem](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes), this stochastic initial value problem(SIVP) defines a unique continous adapted process $(X_t,t\leq T)$. Let $(\mathcal{F}_t^X,t\geq 0)$ be the natural filtration of $(X_t,t\leq T)$. For a fixed $t > 0$, consider the process $W_s = B_{t+s} - B_t, s \geq 0$. Let $(\mathcal{F}_t,t \geq 0)$ be the natural filtration of $(B_t,t \geq 0)$. It turns out that the process $(W_s,s \geq 0)$ is a standard brownian motion independent of $\mathcal{F}_t$ (@exr-shifted-brownian-motion). For $s \geq 0$, we consider the SDE:

$$
dY_s = \mu (Y_s) ds + \sigma(Y_s) dW_s, \quad Y_0 = X_t
$$

Again by the [existence and uniqueness theorem](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes), there exists a unique solution to the SIVP that is adapted to the natural filtration of $W$. Note that, the shifted process $(X_{t+s},s\geq 0)$ is *the* solution to this SIVP since:

\begin{align*}
X_{t+s} &= X_{t} + \int_{t}^{t+s}\mu(X_u) du + \int_{t}^{t+s}\sigma(X_u) dB_u
\end{align*}

Perform a change of variable $v = u - t$. Then, $dv = du$, $dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v$. So,

\begin{align*}
X_{t+s} &= X_{t} + \int_{0}^{s}\mu(X_{t+v}) dv + \int_{0}^{s}\sigma(X_{t+v}) dW_v
\end{align*}

Let $Y_v= X_{t+v}$, $Y_0 = X_t$. Then,

\begin{align*}
Y_s &= Y_0 + \int_{0}^{s}\mu(Y_v) dv + \int_{0}^{s}\sigma(Y_v) dW_v
\end{align*}

Thus, we conclude that for any interval $A$:

$$
\mathbb{P}(X_{t+s} \in A|\mathcal{F}_t^X) = \mathbb{P}(Y_s \in A | \mathcal{F}_t^X)
$$

But, since $(Y_s,s \geq 0)$ depends on $\mathcal{F}_t^X$ only through $X_t$ (because $(W_s,s \geq 0)$ is independent of $\mathcal{F}_t$), we conclude that $\mathbb{P}(X_{t+s} \in A|\mathcal{F}_t^X) = \mathbb{P}(X_{t+s} \in A|X_t)$, so $(X_t,t \geq 0)$ is a time-homogenous markov process. $\blacksquare$

## The Strong Markov Property

The Doob's Optional Stopping theorem extended some properties of martingales to stopping times. The Markov property can also be extended to stopping times for certain processes. These processes are called *strong Markov processes*. 

We know, that the sigma-algebra $\mathcal{F}_t$ represents the set of all observable events upto time $t$. What is the sigma-algebra of observable events at a random stopping time $\tau$? 

::: {#def-sigma-algebra-of-the-past}

### $\sigma$-algebra of $\tau$-past

Let $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\geq 0},\mathbb{P})$ be a filtered probability space. The sigma-algebra at the stopping time $\tau$ is then:

$$
\mathcal{F}_{\tau} = \{A \in \mathcal{F}_\infty : A \cap \{\tau \leq t\} \in \mathcal{F}_t, \forall t \geq 0 \}
$$ {#eq-sigma-algebra-of-the-past}

:::

In words, an event $A$ is in $\mathcal{F}_\tau$, if we can determine if $A$ and $\{\tau \leq t\}$ both occurred or not based on the information $\mathcal{F}_t$ known at any arbitrary time $t$. You should be able to tell the value of the random variable $\mathbf{1}_A \cdot \mathbf{1}_{\{\tau \leq t\}}$ given $\mathcal{F}_t$ for any arbitrary time $t \geq 0$.

For example, if $\tau < \infty$, the event $\{B_\tau > 0\}$ is in $\mathcal{F}_\tau$. However, the event $\{B_1 > 0\}$ is not in $\mathcal{F}_\tau$ in general, since $A \cap \{\tau \leq t\}$ is not in $\mathcal{F}_t$ for $t < 1$. Roughly speaking, a random variable that is $\mathcal{F}_\tau$-measurable should be thought of as an explicit function of $X_\tau$. With this new object, we are ready to define the *strong markov property*.

::: {#def-strong-markov-property}

### Strong Markov Property

Let $(X_t,t\geq 0)$ be a stochastic process and let $(\mathcal{F}_t,t\geq 0)$ be its natural filtration. The process $(X_t,t\geq 0)$ is said to be *strong markov* if for any stopping time $\tau$ for the filtration of the process and any bounded function $g$:

$$
\mathbb{E}[g(X_{t+\tau})|\mathcal{F}_\tau] = \mathbb{E}[g(X_{t+\tau})|X_\tau]
$$

:::

This means that $X_{t+\tau}$ depends on $\mathcal{F}_\tau$ solely through $X_\tau$ (whenever $\tau < \infty$). It turns out that Brownian motion is a strong markov process. In fact a stronger statement holds which generalizes @exr-shifted-brownian-motion. 

::: {#thm-shifted-brownian-motion-about-a-stopping-time}

Let $\tau$ be a stopping time for the filtration of the Brownian motion $(B_t,t\geq 0)$ such that $\tau < \infty$. Then, the process:

$$
(B_{t+\tau} - B_{\tau},t\geq 0)
$$

is a standard brownian motion independent of $\mathcal{F}_\tau$.
:::

::: {#exm-brownian-motion-is-strong-markov}

(Brownian motion is strong Markov) To see this, let's compute the conditional MGF as in @eq-conditional-mgf-of-xt. We have:

$$
\begin{align*}
\mathbb{E}[e^{aB_{t+\tau}}|\mathcal{F}_\tau] &= \mathbb{E}[e^{a(B_{t+\tau} - B_\tau + B_\tau)}|\mathcal{F}_\tau]\\
&= e^{aB_\tau} \mathbb{E}[e^{a(B_{t+\tau} - B_\tau)}|\mathcal{F}_\tau]\\
& \{ B_\tau \text{ is }\mathcal{F}_\tau-\text{measurable }\}\\
&= e^{aB_\tau}\mathbb{E}[e^{a(B_{t+\tau} - B_\tau)}]\\
& \{ (B_{t+\tau} - B_\tau) \perp \mathcal{F}_\tau\}\\
&= e^{aB_\tau}e^{\frac{1}{2}a^2 t}\\
\end{align*}
$$

Thus, the conditional MGF is an explicit function of $B_\tau$ and $t$. This proves the proposition. $\blacksquare$
:::

*Proof* of @thm-shifted-brownian-motion-about-a-stopping-time.

We first consider for fixed $n$ the discrete valued stopping time:

$$
\tau_n = \frac{k + 1}{2^n}, \quad \text{ if } \frac{k}{2^n} \leq \tau < \frac{k+1}{2^n}, k\in \mathbb{N}
$$

In other words, if $\tau$ occurs in the interval $[\frac{k}{2^n},\frac{k+1}{2^n})$, we stop at the next dyadic $\frac{k+1}{2^n}$. By construction $\tau_n$ depends only on the process in the past. Consider the process $W_t = B_{t + \tau_n} - B_{\tau_n}, t \geq 0$. We show it is a standard brownian motion independent of $\tau_n$. This is feasible as we can decompose over the discrete values taken by $\tau_n$. More, precisely, take $E \in \mathcal{F}_{\tau_n}$, and some generic event $\{W_t \in A\}$ for the process $W$. Then, by decomposing over the values of $\tau_n$, we have:

$$
\begin{align*}
\mathbb{P}(\{W_t \in A\} \cap E) &= \sum_{k=0}^\infty \mathbb{P}\left(\{W_t \in A\} \cap E \cap \{\tau_n = \frac{k}{2^n}\}\right)\\
&= \sum_{k=0}^\infty \mathbb{P}\left(\{(B_{t+k/2^n} - B_{k/2^n}) \in A\} \cap E \cap \{\tau_n = \frac{k}{2^n}\}\right)\\
&= \sum_{k=0}^\infty \mathbb{P}\left(\{(B_{t+k/2^n} - B_{k/2^n}) \in A\}\right) \times \mathbb{P}\left( E \cap \{\tau_n = \frac{k}{2^n}\}\right)
\end{align*}
$$

since $(B_{t+k/2^n} - B_{k/2^n})$ is independent of $\mathcal{F}_{k/2^n}$ by @exr-shifted-brownian-motion and since $E \cap \{\tau_n = \frac{k}{2^n}\} \in \mathcal{F}_{k/2^n}$ by definition of stopping time. But, given $\{\tau_n = k/2^n\}$, the event $\{(B_{t+k/2^n} - B_{k/2^n}) \in A\}$ is the same as $\{B_t \in A\} = \{W_t \in A\}$, since this process is now a standard brownian motion. Thus, $\mathbb{P}\{(B_{t+k/2^n} - B_{k/2^n}) \in A\} = \mathbb{P}\{B_t \in A\} = \mathbb{P}\{W_t \in A\}$, dropping the dependence on $k$. The sum over $k$ then yields:

$$
\mathbb{P}\left(\{W_t \in A\}\cap E\right) = \mathbb{P}(W_t \in A) \mathbb{P}(E)
$$

as claimed. The extension to $\tau$ is done by using continuity of paths. We have:

$$
\lim_{n \to \infty} B_{t + \tau_n} - B_{\tau_n} = B_{t+\tau} - B_{\tau} \text{ almost surely}
$$

Note, that this only uses right continuity! Moreover, this implies that $B_{t+\tau} - B_\tau$ is independent of $\mathcal{F}_{\tau_n}$ for all $n$. Again by (right-)continuity this extends to independence of $\mathcal{F}_\tau$. The limiting distribution of the process is obtained by looking at the finite dimensional distributions of the increments of $B_{t+\tau_n} - B_{\tau_n}$ for a finite number of $t$'s and taking the limit as above. $\blacksquare$

Most diffusions also enjoy the strong markov property, as long as the functions $\sigma$ and $\mu$ encoding the volatility and drift are nice enough. This is the case for the diffusions we have considered. 

::: {#thm-most-diffusions-are-strong-markov}

### Most diffusions are strong markov

Consider a diffusion $(X_t,t\leq T)$ as as in @thm-diffusions-are-markov-processes. Then, the diffusion has strong markov property. 
:::

The proof follows the line of the one of @thm-diffusions-are-markov-processes

*Proof.*

Consider the time-homogenous diffusion:

$$
dX_t = \mu(X_t)dt + \sigma(X_t)dB_t
$$

By the [existence and uniqueness theorem](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes), this SIVP defines a unique continuous adapted process $(X_t,t \geq 0)$. Let $\mathfrak{F}=(\mathcal{F}_t^X,t \geq 0)$ be the natural filtration of $(X_t, t\leq T)$. Let $\tau$ be a stopping time for the filtration $\mathfrak{F}$ and consider the process $W_t = B_{t+\tau} - B_\tau$. From @thm-shifted-brownian-motion-about-a-stopping-time, we know that the process $(W_t,t\geq 0)$ is a standard brownian motion independent $\mathcal{F}_\tau$. For $s \geq 0$, we consider the SDE:

$$
dY_s = \mu(Y_s)ds + \sigma(Y_s)dW_s, \quad Y_0 = X_\tau
$$ {#eq-diffusion-of-Y}

Again by the [existence and uniqueness theorem](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes), there exists a unique solution to the SIVP that is adapted to the natural filtration of $W$. We claim that $(X_{s+\tau},s \geq 0)$ is the solution to this equation, since:

$$
X_{s+\tau} = X_\tau + \int_\tau^{s+\tau} \mu(X_u)du + \int_{\tau}^{s+\tau} \sigma(X_u)dB_u
$$

Perform a change of variable $v = u - \tau$. Then, the limits of integration bare, $v = 0$ and $v = s$. And $dv = du$. 

$dB_u  \approx B_{u_2} - B_{u_1} = B(v_1 + \tau) - B(v_2 + \tau) = W(v_2) - W(v_1) =dW_v$. 

$$
X_{s+\tau} = X_\tau + \int_0^{s} \mu(X_{v+\tau})dv + \int_{0}^{s} \sigma(X_{v+\tau})dW_v
$$

If we let $Y_0 = X_\tau$, $Y_v = X_{v+\tau}$, we recover the dynamics of $(Y_v,v \geq 0)$ in @eq-diffusion-of-Y. So, $(X_{s+\tau},s\geq 0)$ is the solution to the SIVP in @eq-diffusion-of-Y. Thus, we conclude for any interval $A$:

$$
\mathbb{P}(X_{s+\tau} \in A | \mathcal{F}_\tau^X) = \mathbb{P}(Y_v \in A| \mathcal{F}_\tau^X)
$$

But, since $(Y_v,v\geq 0)$ depends on $\mathcal{F}_\tau^X$ only through $X_\tau$, we conclude that $\mathbb{P}(X_{s + \tau} \in A | \mathcal{F}_\tau^X) = \mathbb{P}(X_{s + \tau} \in A| X_\tau)$. Consequently, $(X_t,t \geq 0)$ is a strong-markov process. $\blacksquare$

::: {#nte-extension-of-optional-sampling .callout-tip}

### Extension of optional sampling

Consider a continuous martingale $(M_t, t\leq T)$ for a filtration $(\mathcal{F}_t, t\geq 0)$ and a stopping time $\tau$ for the same filtration. Suppose we would like to compute for some $T$:

$$
\mathbb{E}[M_T \mathbf{1}_{\{\tau \leq T\}}]
$$

It would be tempting to condition on $\mathcal{F}_\tau$ and write $\mathbb{E}[M_T |\mathcal{F}_\tau] = M_\tau$ on the event $\{\tau \leq T\}$. We would then conclude that:

$$
\mathbb{E}[M_T 1_{\{\tau \leq T\}}] = \mathbb{E}[1_{\{\tau \leq T\}} \mathbb{E}[M_T|\mathcal{F}_\tau] ] = \mathbb{E}[M_\tau 1_{\{\tau \leq T\}}]
$$

In some sense, we have extended the martingale property to stopping times. This property can be proved under reasonable assumptions on $(M_t,t\leq T)$ (for example, if it is positive). Indeed, it suffices to approximate $\tau$ by discrete valued stopping time $\tau_n$ as in the proof of @thm-shifted-brownian-motion-about-a-stopping-time. One can then apply martingale property at a fixed time. 
:::

## The Heat Equation

We look at more detail on how PDEs come up when computing quantities related to Markov processes.
    
Consider a homogenous rod of length $L$ lying along the interval $[0,L]$ on the $x$-axis. We focus on a little segment of this rod $[x,x+\Delta x]$. Suppose the temperature measurement of this little segment at time $t$ is $u(x,t)$ degrees. In general, the temperature $u$ of the rod varies with space $x$ and time $t$.
        
The heat energy content in this little segment at time $t$ is :
$$E(x,x+\Delta x, t) \approx s \cdot u(x,t) \cdot (\rho \Delta x)$$ 

where $s$ is the specific heat constant, $\rho$ is the mass density $\text{kg}\cdot m^{-1}$.

\textbf{Fourier}'s law of heat conduction quantifies the idea that heat flows from warmer to colder regions and states that the (rightward) heat flux density (the flow of heat energy per unit area per unit time, SI units $Js^{-1}m^{-2}$ at any point is:

$$\phi(x,t) = -K_0 u_x(x,t)$$

where $K_0$ is the thermal conductivity of the rode. The negative sign shows that the heat flows from higher temperature regions to colder temperature regions.

Appealing to the law of conservation of energy:

$$\underbrace{\frac{\partial}{\partial t}(u(x,t) \cdot s \cdot (\rho \Delta x))}_{\text{Heat flux through segment}} = \underbrace{(-K_0u_x(x,t))}_{\text{Flux in}} - \underbrace{(-K_0u_x(x+\Delta x, t))}_{\text{Flux out}}$$

Dividing throughout by $\Delta x$, we have:

$$\frac{\partial u}{\partial t} = \frac{K_0}{s\rho}\frac{u_x(x+\Delta x,t) - u_x(x,t)}{\Delta x}$$
    
Letting $\Delta x \to 0$, we get:

$$
\begin{aligned}
\boxed{\frac{\partial u}{\partial t} = c^2\frac{\partial^2 u}{\partial x^2}}
\end{aligned}
$${#eq-diffusion-of-Y}

where $c^2 = \frac{K_0}{s\rho}$ is called the \textit{thermal diffusivity}.

## Deriving the diffusion equation using the Einstein's approach

We now summarize Einstein's original 1905 argument. Let's say that we are interested in the motion along the horizontal $x$-axis. Let's say we drop brownian particles in a liquid. Let $f(t,x)$ represent the number of particles per unit volume (density) at position $x$ at time $t$. So, the number of particles in a small interval $I=[x,x+dx]$ of width $dx$ will be $f(t,x)dx$. 

```{python}
# | code-fold: true
# | code-summary: "Show the code"
%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone 
%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1.5,xscale=1.5]
%Straight Lines [id:da6446978261676468] 
\draw    (140.67,160) -- (400.5,160.17) ;
%Straight Lines [id:da5099895315117877] 
\draw    (400.5,160.17) -- (400.5,210.17) ;
%Straight Lines [id:da29223877192666536] 
\draw    (140.5,209.5) -- (477.17,210.16) ;
\draw [shift={(480.17,210.17)}, rotate = 180.11] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
%Straight Lines [id:da7176453962138043] 
\draw    (140.5,209.5) -- (140.18,93.17) ;
\draw [shift={(140.17,90.17)}, rotate = 89.84] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
%Shape: Circle [id:dp9172058642707208] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (155,176.08) .. controls (155,170.33) and (159.66,165.67) .. (165.42,165.67) .. controls (171.17,165.67) and (175.83,170.33) .. (175.83,176.08) .. controls (175.83,181.84) and (171.17,186.5) .. (165.42,186.5) .. controls (159.66,186.5) and (155,181.84) .. (155,176.08) -- cycle ;
%Shape: Circle [id:dp41708018361647703] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (175,196.08) .. controls (175,190.33) and (179.66,185.67) .. (185.42,185.67) .. controls (191.17,185.67) and (195.83,190.33) .. (195.83,196.08) .. controls (195.83,201.84) and (191.17,206.5) .. (185.42,206.5) .. controls (179.66,206.5) and (175,201.84) .. (175,196.08) -- cycle ;
%Shape: Circle [id:dp7781895801877037] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (185.67,174.42) .. controls (185.67,168.66) and (190.33,164) .. (196.08,164) .. controls (201.84,164) and (206.5,168.66) .. (206.5,174.42) .. controls (206.5,180.17) and (201.84,184.83) .. (196.08,184.83) .. controls (190.33,184.83) and (185.67,180.17) .. (185.67,174.42) -- cycle ;
%Shape: Circle [id:dp24637416264324552] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (213.33,173.42) .. controls (213.33,167.66) and (218,163) .. (223.75,163) .. controls (229.5,163) and (234.17,167.66) .. (234.17,173.42) .. controls (234.17,179.17) and (229.5,183.83) .. (223.75,183.83) .. controls (218,183.83) and (213.33,179.17) .. (213.33,173.42) -- cycle ;
%Shape: Circle [id:dp48466517127845876] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (267.67,194.75) .. controls (267.67,189) and (272.33,184.33) .. (278.08,184.33) .. controls (283.84,184.33) and (288.5,189) .. (288.5,194.75) .. controls (288.5,200.5) and (283.84,205.17) .. (278.08,205.17) .. controls (272.33,205.17) and (267.67,200.5) .. (267.67,194.75) -- cycle ;
%Shape: Circle [id:dp2216365065410234] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (253.67,172.42) .. controls (253.67,166.66) and (258.33,162) .. (264.08,162) .. controls (269.84,162) and (274.5,166.66) .. (274.5,172.42) .. controls (274.5,178.17) and (269.84,182.83) .. (264.08,182.83) .. controls (258.33,182.83) and (253.67,178.17) .. (253.67,172.42) -- cycle ;
%Shape: Circle [id:dp315977950945718] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (314.33,175.08) .. controls (314.33,169.33) and (319,164.67) .. (324.75,164.67) .. controls (330.5,164.67) and (335.17,169.33) .. (335.17,175.08) .. controls (335.17,180.84) and (330.5,185.5) .. (324.75,185.5) .. controls (319,185.5) and (314.33,180.84) .. (314.33,175.08) -- cycle ;
%Shape: Circle [id:dp43596515241859735] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (347.33,174.75) .. controls (347.33,169) and (352,164.33) .. (357.75,164.33) .. controls (363.5,164.33) and (368.17,169) .. (368.17,174.75) .. controls (368.17,180.5) and (363.5,185.17) .. (357.75,185.17) .. controls (352,185.17) and (347.33,180.5) .. (347.33,174.75) -- cycle ;
%Shape: Circle [id:dp9605369285734118] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (372,195.42) .. controls (372,189.66) and (376.66,185) .. (382.42,185) .. controls (388.17,185) and (392.83,189.66) .. (392.83,195.42) .. controls (392.83,201.17) and (388.17,205.83) .. (382.42,205.83) .. controls (376.66,205.83) and (372,201.17) .. (372,195.42) -- cycle ;
%Straight Lines [id:da31603620824605405] 
\draw    (213.33,239.68) -- (240.5,239.82) ;
\draw [shift={(243.5,239.83)}, rotate = 180.29] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw [shift={(210.33,239.67)}, rotate = 0.29] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
%Shape: Rectangle [id:dp2322723029099315] 
\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ] (210,140.5) -- (242.83,140.5) -- (242.83,230.17) -- (210,230.17) -- cycle ;
%Shape: Circle [id:dp21415420334871738] 
\draw  [draw opacity=0][fill={rgb, 255:red, 184; green, 233; blue, 134 }  ,fill opacity=1 ] (222,197.75) .. controls (222,192) and (226.66,187.33) .. (232.42,187.33) .. controls (238.17,187.33) and (242.83,192) .. (242.83,197.75) .. controls (242.83,203.5) and (238.17,208.17) .. (232.42,208.17) .. controls (226.66,208.17) and (222,203.5) .. (222,197.75) -- cycle ;

% Text Node
\draw (209.33,211.07) node [anchor=north west][inner sep=0.75pt]    {$x$};
% Text Node
\draw (215,243.07) node [anchor=north west][inner sep=0.75pt]    {$dx$};
% Text Node
\draw (186,120.4) node [anchor=north west][inner sep=0.75pt]    {$f( x,t)$};

\end{tikzpicture}

```

Now, as time progresses, the number of particles in this interval $I$ will change. The brownian particles will zig-zag upon bombardment by the molecules of the liquid. Some particles will move out of the interval $I$, while other particles will move in. 

Let's consider a timestep of length $\tau$. Einstein's probabilistic approach was to model the distance travelled by the particles or displacement of the particles as a random variable $\Delta$. To determine how many particles end up in the interval $I$, we start with the area to the right of the interval $I$.

The density of particles at $x+\Delta$ is $f(t,x+\Delta)$; the number of particles in a small interval of length $dx$ is $f(t,x+\Delta)dx$. If we represent the probability density of the displacement by $\phi(\Delta)$, then the number of particles at $x+\Delta$ that will move to $x$ will be $dx \cdot f(t,x+\Delta)\phi(\Delta)$. We can apply the same logic to the left hand side. The number of particles at $x - \Delta$ that will move to $x$ will be $dx \cdot f(t,x-\Delta)\phi(-\Delta)$. Assume that $\phi(\Delta) = \phi(-\Delta)$.

Now, if we integrate these movements across the real line, then we get the number of particles at $x$ at a short time later $t + \tau$. 

$$
f(t+ \tau,x) dx = dx \int_{-\infty}^{\infty} f(t,x+\Delta) \phi(\Delta) d\Delta
$$

Now, we can get rid of $dx$.

$$
f(t+ \tau,x) = \int_{-\infty}^{\infty} f(t,x+\Delta) \phi(\Delta) d\Delta
$$ {#eq-expression-for-density-at-later-time}

The Taylor's series expansion of $f(t+\tau,x)$ centered at $t$ (holding $x$ constant) is:

$$
f(t + \tau,x) = f(t,x) + \frac{\partial f}{\partial t}\tau + O(\tau^2)
$$


The Taylor's series expansion of $f(t,x+\Delta)$ centered at $x$ (holding $t$ constant) is:

$$
f(t,x+\Delta) = f(t,x) + \frac{\partial f}{\partial x}\Delta + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\Delta^2 + O(\Delta^3)
$$

We can now substitute these into @eq-expression-for-density-at-later-time to get:

$$
\begin{align*}
f(t,x) + \frac{\partial f}{\partial t}\tau &= \int_{-\infty}^{\infty}\left(f(t,x) + \frac{\partial f}{\partial x}\Delta + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\Delta^2\right) \phi(\Delta)d\Delta\\
&= f(t,x) \int_{-\infty}^{\infty} \phi(\Delta)d\Delta \\
&+ \frac{\partial f} {\partial x} \int_{-\infty}^{\infty} \Delta \phi(\Delta)d\Delta \\
&+ \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta
\end{align*}
$$

Now, since the probability distribution of displacement $\phi(\cdot)$ is symmetric around the origin, the second term is zero. And we know, that if we integrate the density over $\mathbb{R}$, we should get one, so the first term equals one. So, we get:

$$
f(t,x) + \frac{\partial f}{\partial t}\tau = f(t,x) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta
$$

Now, we can cancel the $f$ on both sides and then shift $\tau$ to the right hand side:

$$
\frac{\partial f}{\partial t} =  \left(\frac{1}{2\tau} \int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta \right)\frac{\partial^2 f}{\partial x^2}
$$

Define $D:= \left(\frac{1}{2\tau} \int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta \right)$. Then, we have: 

$$
\frac{\partial f}{\partial t} =  D\frac{\partial^2 f}{\partial x^2}
$$

The microscopic interpretation of the diffusion coefficient is, that its just the average of the squared displacements. The larger the $D$, the faster the brownian particles move.

## Kolmogorov's Backward Equation

Think of $y$ and $t$ as being current values and $y'$ and $t'$ being future values. The transition probability density function $p(y',t'|y,t)$ of a diffusion satisfies two equations - one involving derivatives with respect to a future state and time ($y'$ and $t'$) called *forward equation* and the other involving derivatives with respect to the current state and current time ($y$ and $t$) called the *backward equation*. These two equations are parabolic partial differential equations not dissimilar to the Black-Scholes equation. 

::: {#thm-backward-equation-with-initial-value}

### Backward equation with initial value

Let $(X_t,t\geq 0)$ be a diffusion in $\mathbb{R}$ with the SDE:

$$
dX_t = \sigma(X_t)dB_t + \mu(X_t) dt
$$ 

Let $g\in C^2(\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value 

$$
\begin{align*}
\frac{\partial f}{\partial t}(t,x) &= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(x)\frac{\partial f}{\partial x}\\
f(0,x) &= g(x)
\end{align*}
$$ {#eq-backward-equation}

has the representation:

$$
f(t,x) = \mathbb{E}[g(X_t)|X_0 = x]
$$

:::

*Proof.*

**Step 1.** 
Let's fix $t$ and consider the function of space $h(x)=f(t,x)=\mathbb{E}[g(X_t)|X_0=x]$. We will show that, $f$  satisfies the PDE @eq-backward-equation. Applying Ito's formula to $h$, we have:

\begin{align}
dh(X_s) &= h'(X_s) dX_s + \frac{1}{2}h''(X_s) (dX_s)^2\\
&= h'(X_s) (\sigma(X_s)dB_s + \mu(X_s) ds) + \frac{\sigma(X_s)^2}{2}h''(X_s)ds\\
&= \sigma(X_s)h'(X_s)dB_s + \left(\frac{\sigma(X_s)^2}{2}h''(X_s) + \mu(X_s)h'(X_s)\right)ds
\end{align}

In the integral form this is:

\begin{align*}
h(X_s) - h(X_0) &= \int_0^s \sigma(X_u)h'(X_u)dB_u \\
&+ \int_0^s \left(\frac{\sigma(X_u)^2}{2}h''(X_u) + \mu(X_u)h'(X_u)\right)du 
\end{align*}

**Step 2.** Take expectations on both sides, divide by $s$ and let $s \to 0$. We are interested in taking the derivative with respect to $s$ at $s_0=0$.

$$
\begin{align*}
\mathbb{E}\left[\frac{h(X_s) - h(X_0)}{s}\rvert X_0 = x \right] &= \frac{1}{s}\mathbb{E}\left[\int_0^s \sigma(X_u)h'(X_u)dB_u | X_0 = x\right] \\
&+ \frac{1}{s}\int_{0}^{s}\mathbb{E}\left[\frac{\sigma(X_u)^2}{2}h''(X_u) + \mu(X_u)h'(X_u)|X_0 = x\right]du
\end{align*}
$$

The expectation of the first term on the right hand side is zero, by the properties of the Ito integral. 

The integrand of the second term (RHS) is a conditional expectation $\mathbb{E}[\xi(X_u)|X_0 = x]$, it is an average at time $u$, of the paths of the process starting at initial position $X_0 = x$, so it is a function of $u$ and $x$. Let $\mathbb{E}[\xi(X_u)|X_0 = x] = \gamma(u,x)$. Suppressing the argument $x$, we have the representation: 

\begin{align}
\int_0^s \gamma(u) du
\end{align}

Recall that, if $\gamma$ is a continuous function, then it is Riemann integrable. Further, since integration and differentiation are inverse operations, there exists a unique antiderivative $\Gamma$ given by

$$
\Gamma(s) = \int_{0}^{s}\gamma(u)du
$$

satisfying $\Gamma'(0) = \gamma(0)$. 

By the definition of the derivative:

$$\Gamma'(0) = \lim_{s \to 0} \frac{\Gamma(s) - \Gamma(0)}{s} = \lim_{s\to 0} \frac{\Gamma(s)}{s} = \gamma(0) \quad \{ \Gamma(0)=0 \text{ by definition }\}$$

Thus, we have:

$$
\gamma(0,x) = \mathbb{E}[\xi(X_0)|X_0 = x] = \frac{\sigma(x)^2}{2} h''(x) + \mu(x)h'(x)
$$

This is the desired RHS.

**Step 3.** As for the left-hand side, we have:

$$
\lim_{s \to 0} \frac{\mathbb{E}[h(X_s)|X_0 = x] - h(X_0)}{s} = \lim_{s \to 0} \frac{\mathbb{E}[h(X_s)|X_0 = x] - f(t,x)}{s} 
$$

To prove that this limit is $\frac{\partial f}{\partial t}(t,x)$, it remains to show that $\mathbb{E}[h(X_s)|X_0 = x]=\mathbb{E}[g(X_{t+s})|X_0 = x]=f(t+s,x)$. 

To see this, note that $h(X_s) = \mathbb{E}[g(X_{t+s})|X_s]$. We deduce:

\begin{align*}
\mathbb{E}[h(X_s)|X_0 = x] &= \mathbb{E}[\mathbb{E}[g(X_{t+s})|X_s]|X_0 = x]\\
&= \mathbb{E}[\mathbb{E}[g(X_{t+s})|\mathcal{F}_s]|X_0 = x]\\
& \{ (X_t,t\geq 0) \text{ is Markov }\} \\
&= \mathbb{E}[g(X_{t+s})|X_0 = x]\\
& \{ \text{ Tower property }\} \\
&= f(t+s,x)
\end{align*}

So, the LHS is $\frac{\partial f}{\partial t}(t,x)$. This closes the proof. $\blacksquare$

The backward equation (@eq-backward-equation) can be conveniently written in terms of *the generator of the diffusion*.

::: {#def-generator-of-the-diffusion}

### Generator of a diffusion

The generator of a diffusion with SDE $dX_t = \sigma(X_t) dB_t + \mu(X_t)dt$ is the differential operator acting on functions of space defined by :

$$
A = \frac{\sigma(x)^2}{2}\frac{\partial }{\partial x^2} + \mu(x)\frac{\partial}{\partial x}
$$

:::

With this notation, the backward equation for the function $f(t,x)$ takes the form:

$$
\frac{\partial f}{\partial t}(t,x) = Af(t,x)
$$

where it is understood that $A$ acts only on the space variable. @thm-backward-equation-with-initial-value gives a nice interpretation of the generator: it quantifies how much the function $f(t,x) = \mathbb{E}[g(X_t)|X_0 = x]$ changes in a small time interval.

### The heat equation as a special case of the Backward equation

::: {#exm-heat-pde-as-special-case-of-brownian-motion}

Let $(B_t,t\geq 0)$ be a standard brownian motion. Then, the generator is:

$$
A = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}(t,x)
$$

Then, by @thm-backward-equation-with-initial-value, the solution of the heat PDE

$$
\begin{align*}
\frac{\partial f}{\partial t}(t,x) = Af(x) = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}(t,x)
\end{align*}
$$

with initial value $f(0,x)=g(x)$ has the stochastic representation:

$$f(t,x) = \mathbb{E}[g(B_t)|B_0 = x]$$

It can be represented as an average of $g(B_t)$ over all Brownian motion paths starting at the location $x$.

:::

###  Other examples

::: {#exm-generator-of-the-ornstein-uhlenbeck-process}

(Generator of the Ornstein Uhlenbeck Process) The SDE of the Ornstein-Uhlenbeck process is:

$$
dX_t = dB_t - X_t dt
$$

This means that its generator is:

$$
A = \frac{1}{2}\frac{\partial^2}{\partial x^2} - x \frac{\partial}{\partial x}
$$

:::

::: {#exm-generator-of-geometric-brownian-motion}

(Generator of Geometric Brownian Motion) Recall that the geometric Brownian motion 

$$
S_t = S_0 \exp(\sigma B_t + \mu t)
$$

satisfies the SDE:

$$
dS_t = \sigma S_t dB_t + \left(\mu + \frac{\sigma^2}{2}\right) S_t dt
$$

In particular, the generator of geometric Brownian motion is :

$$
A = \frac{\sigma^2 x^2}{2} x \frac{\partial^2}{\partial x^2} + \left(\mu + \frac{\sigma^2}{2}\right)\frac{\partial}{\partial x}
$$
:::

For applications, in particular in mathematical finance, it is important to solve the backward equation with terminal value instead of with initial value. The reversal of time causes the appearance of an extra minus sign in the equation.

###  Backward equation with terminal value

::: {#thm-backward-equation-with-terminal-value}


Let $(X_t,t\leq T)$ be a diffusion with the dynamics:

$$
dX_t = \sigma(X_t) dB_t + \mu(X_t)dt
$$

Let $g\in C^2(\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with terminal value at time $T$

$$
\begin{align*}
-\frac{\partial f}{\partial t} &= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(x)\frac{\partial f}{\partial x}\\
f(T,x) &= g(x)
\end{align*}
$$ {#eq-backward-equation-with-terminal-value}

has the representation:

$$
f(t,x) = \mathbb{E}[g(X_T)|X_t = x]
$$
:::


::: {#nte-functions-of-markov .callout-tip}
### Backward equation with terminal value appears in the martingale condition

One way to construct a martingale for the filtration $(\mathcal{F}_t,t\geq 0)$ is to take 

$$
M_t = \mathbb{E}[Y | \mathcal{F}_t]
$$

where $Y$ is some integrable random variable. The martingale property then follows from the tower property of the conditional expectation. In the setup of @thm-backward-equation-with-terminal-value, the random variable $Y$ is $g(X_T)$. By the Markov property of diffusion, we therefore have:

$$
f(t,X_t) = \mathbb{E}[g(X_T)|X_t] = \mathbb{E}[g(X_T)|\mathcal{F}_t] 
$$

In other words, the solution to the backward equation with terminal value evaluated at $X_t = x$ yields a martingale for the natural filtration of the process. This is a different point of view on the procedure we have used many times now: To get a martingale of the form $f(t,X_t)$, apply the Ito's formula to $f(t,X_t)$ and set the $dt$ term to zero. The PDE we obtain is the backward equation with terminal value. In fact, the proof of the theorem takes this exact route.
:::

*Proof.*

Consider $f(t,X_t)$ and apply Ito's formula.

$$
\begin{align*}
df(t,X_t) &= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x}dX_t + \frac{1}{2}\frac{\partial^2 f}{\partial x^2} dX_t \cdot dX_t\\
&= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x}(\sigma(X_t) dB_t + \mu(X_t)dt) + \frac{\sigma(X_t)^2}{2}\frac{\partial^2 f}{\partial x^2} dt\\
&= \sigma(X_t) dB_t + \left(\frac{\partial f}{\partial t} + \frac{\sigma(X_t)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(X_t)\frac{\partial f}{\partial x}\right)dt
\end{align*}
$$

Since $f(t,x)$ is a solution to the equation, we get that the $dt$ term is $0$ and $f(t,X_t)$ is a martingale for the Brownian filtration (and thus also for the natural filtration of the diffusion, which contains less information). In particular we have:

$$
f(t,X_t) = \mathbb{E}[f(T,X_T)|\mathcal{F}_t] = \mathbb{E}[g(X_T)|\mathcal{F}_t]
$$

Since $(X_t,t\leq T)$ is a Markov process, we finally get:

$$
f(t,x) = \mathbb{E}[g(X_T)|X_t = x]
$$

::: {#exm-martingales-of-geometric-brownian-motion}

(Martingales of geometric Brownian motion) Let $(S_t, \geq 0)$ be a geometric brownian motion with SDE:

$$
dS_t = \sigma S_t dB_t + \left(\mu + \frac{\sigma^2}{2}\right)dt
$$

As we saw in @exm-generator-of-geometric-brownian-motion, its generator is:

$$
A = \frac{\sigma^2 x^2}{2}\frac{\partial^2}{\partial x^2} + x\left(\mu+\frac{\sigma^2}{2}\right)\frac{\partial}{\partial x}
$$

In view of @thm-backward-equation-with-terminal-value, if $f(t,x)$ satisfies the PDE 

$$
\frac{\partial f}{\partial t} + \frac{\sigma^2 x^2}{2}\frac{\partial^2 f}{\partial x^2} + x\left(\mu+\frac{\sigma^2}{2}\right)\frac{\partial f}{\partial x}
$$

then processes of the form $f(t,S_t)$ will be martingales for the natural filtration. 
:::

## Kolmogorov's forward equation

The companion equation to the backward equation is the *Kolmogorov forward equation* or *forward equation*. It is also known as the *Fokker-Planck* equation from its physics origin. The equation is very useful as it is satisfied by the transition density function $p(y',t'|y,t)$ of a time-homogenous diffusion. It involves the *adjoint of the generator*. 

::: {#def-adjoint-of-the-generator}

### Adjoint of the generator

The adjoint $A^*$ of the generator of a diffusion $(X_t,t\geq 0)$ with SDE:

$$
dX_t = \sigma(X_t)dB_t + \mu(X_t)dt
$$

is the differential operator acting on a function of space $f(x)$ as follows:

$$
A^*f(x) = \frac{1}{2}\frac{\partial^2 }{\partial x^2} \sigma(x)^2 f(x) - \frac{\partial }{\partial x}\mu(x)f(x)
$$ {#eq-adjoint-of-the-generator-of-a-diffusion}
:::

Note the differences with the generator in @def-generator-of-the-diffusion: there is an extra minus sign and the derivatives also act on the volatility and the drift.

::: {#exm-the-generator-brownian-motion-is-self-adjoint}

(The generator of Brownian motion is self-adjoint) In the case of standard brownian motion, it is easy to check that:

$$
A^* = \frac{1}{2}\frac{\partial^2}{\partial x^2} 
$$

and 

$$
A^* = \frac{1}{2}\nabla^2
$$

in the multivariate case. In other words, the generator and its adjoint are the same. In this case, the operator is *self-adjoint*. 
:::

::: {#exm-the-adjoint-for-geometric-brownian-motion}
We see that the adjoint of the generator acting on $f(x)$ for geometric Brownian motion is:

$$
A^*f(x) = \frac{1}{2}\frac{\partial^2}{\partial x^2} (\sigma^2 x^2 f(x)) - \frac{\partial}{\partial x} \left(\left(\mu + \frac{\sigma^2}{2}\right) x f(x)\right)
$$

Using the product rule in differentiating we get:

$$
A^*[f(x)] = \frac{\sigma^2}{2}\left(2x f(x) + x^2 f''(x)\right) - \left(\left(\mu + \frac{\sigma^2}{2}\right)\left(f(x) + x f'(x)\right)\right)
$$

:::

::: {#exm-adjoint-for-the-ornstein-uhlenbeck-process}

The generator for the Ornstein-Uhlenbeck process was given in @exm-generator-of-the-ornstein-uhlenbeck-process. The adjoint acting on $f$ is therefore:

$$
\begin{align*}
A^*f(x) &= \frac{1}{2}\frac{\partial^2}{\partial x^2}(f(x)) - \frac{\partial}{\partial x}(- x f(x))\\
&= \frac{f''(x)}{2} + (f(x)+xf'(x))
\end{align*}
$$
:::

The forward equation takes the following form for a function $f(t,x)$ of time and space:

$$
\frac{\partial f}{\partial t} = A^* f
$$ {#eq-forward-equation}

For brownian motion, since $A^* = A$, the backward and forward equations are the same. As advertised earlier, the forward equation is satisfied by the transition $p_t(y',t'|y,t)$ of a diffusion. Before showing this in general, we verify it in the Brownian case.

::: {#exm-the-heat-kernel-as-the-solution-of-the-forward-equation}
(The Heat Kernel as the solution of the forward equation) Recall that the transition probability density $p(y,t|x,0)$ for Brownian motion, or heat kernel, is:

$$
p(y,t|x,0) = \frac{e^{-\frac{(y-x)^2}{2t}}}{\sqrt{2\pi t}}
$$

Here, the space variable will be $y$ and $x$ will be fixed. The relevant function is thus $f(t,y) = p(y,t|x,0)$. The adjoint operator acting on the space variable $y$ is $A^* = A = \frac{1}{2}\frac{\partial^2}{\partial y^2}$. The relevant time and space derivatives are given by:

$$
\begin{align*}
\frac{\partial f}{\partial t} & =\frac{\partial }{\partial t}\left(\frac{1}{\sqrt{2\pi t}}\exp\left( -\frac{( y-x)^{2}}{2t}\right)\right)\\
 & =\frac{\sqrt{2\pi t} \cdot \frac{\partial }{\partial t}\left(\exp\left( -\frac{( y-x)^{2}}{2t}\right)\right) -\exp\left( -\frac{( y-x)^{2}}{2t}\right) \cdot \frac{\partial }{\partial t}\left(\sqrt{2\pi t}\right)}{\left(\sqrt{2\pi t}\right)^{2}}\\
 & =\frac{\sqrt{2\pi t} \cdot \exp\left( -\frac{( y-x)^{2}}{2t}\right) \cdot \frac{\partial }{\partial t}\left( -\frac{( y-x)^{2}}{2t}\right) -\exp\left( -\frac{( y-x)^{2}}{2t}\right)\frac{\sqrt{2\pi }}{2\sqrt{t}}}{\left(\sqrt{2\pi t}\right)^{2}}\\
 & =\exp\left( -\frac{( y-x)^{2}}{2t}\right)\frac{\sqrt{2\pi t} \cdot \left( -\frac{( y-x)^{2}}{2}\right) \cdot \left( -\frac{1}{t^{2}}\right) -\frac{\sqrt{2\pi }}{2\sqrt{t}}}{\left(\sqrt{2\pi t}\right)^{2}}\\
 & =\sqrt{2\pi }\exp\left( -\frac{( y-x)^{2}}{2t}\right)\frac{\frac{( y-x)^{2}}{2t^{3/2}} -\frac{1}{2t^{1/2}}}{\left(\sqrt{2\pi }\right)^{2} \cdot t}\\
 & =\frac{1}{2t^{1/2}} \cdot \frac{1}{\sqrt{2\pi }}\exp\left( -\frac{( y-x)^{2}}{2t}\right)\frac{\frac{( y-x)^{2} -t}{t}}{t}\\
 & =\frac{1}{\sqrt{2\pi }}\exp\left( -\frac{( y-x)^{2}}{2t}\right)\frac{( y-x)^{2} -t}{2t^{5/2}}\\
 & =\frac{( y-x)^{2} -t}{2t^{2}} f
\end{align*}
$$

Also:

$$
\begin{align*}
\frac{\partial f}{\partial y} & =\frac{\partial }{\partial y}\left(\frac{1}{\sqrt{2\pi t}}\exp\left( -\frac{( y-x)^{2}}{2t}\right)\right)\\
 & =\frac{1}{\sqrt{2\pi t}}\exp\left( -\frac{( y-x)^{2}}{2t}\right) \cdot \frac{\partial }{\partial x}\left( -\frac{( y-x)^{2}}{2t}\right)\\
 & =-\frac{1}{\sqrt{2\pi }} \cdot \frac{1}{2t^{3/2}} \cdot \exp\left( -\frac{( y-x)^{2}}{2t}\right) 2( y-x)\\
 & =-\frac{1}{\sqrt{2\pi }} \cdot \frac{1}{t^{3/2}} \cdot \exp\left( -\frac{( y-x)^{2}}{2t}\right)( y-x)\\
 & =-\frac{1}{t}( y-x) f
\end{align*}
$$

and

$$
\begin{align*}
A^{*} f=\frac{1}{2}\frac{\partial ^{2} f}{\partial y^{2}} & =\frac{1}{2}\left( -\frac{1}{t}\right)\frac{\partial }{\partial y}( yf-xf)\\
 & =-\frac{1}{2t}\left( f+y\frac{\partial f}{\partial y} -x\frac{\partial f}{\partial y}\right)\\
 & =-\frac{1}{2t}\left( f+( y-x)\frac{\partial f}{\partial y}\right)\\
 & =-\frac{1}{2t}\left( f-\frac{( y-x)^{2}}{t} f\right)\\
 & =\frac{( y-x)^{2} -t}{2t^{2}} f
\end{align*}
$$


We conclude that $f(t,y)=p(y,t|x,0)$ is a solution of the forward equation in the standard Brownian motion case. $\blacksquare$
:::

Where does the form of the adjoint operator @eq-adjoint-of-the-generator-of-a-diffusion come from? In some sense, the adjoint operator plays a role similar to that of the transpose of a matrix in linear algebra. The adjoint acts on the function on the left. To see this, consider two functions $f,g$ of space on which the generator $A$ of a diffusion is well-defined. In particular, let's assume that the functions are zero outside an interval. Consider the quantity 

$$
\int_{\mathbb{R}}g(x)A(f(x))dx = \int_{\mathbb{R}} g(x)\left(\frac{\sigma(x)^2 }{2}f''(x) + \mu(x)f'(x)\right)dx
$$

This quantity can represent for example the average of $Af(x)$ over some PDF $g(x)$. In the above, $A$ acts on the function on the right. To make the operator act on $g$, we integrate by parts. This gives for the second term:

$$
\int_{\mathbb{R}} g(x)\mu(x)f'(x)dx = g(x)\mu(x)f(x)\Bigg|_{-\infty}^{\infty}-\int_{\mathbb{R}}f(x)\frac{d}{dx}(g(x)\mu(x))dx
$$

The boundary term $g(x)f(x)\mu(x)\Bigg|_{-\infty}^\infty$ is $0$ by the assumptions on $f,g$. This term on $\sigma$ is obtained by integrating by parts twice:

$$
\begin{align*}
\int_{\mathbb{R}} g(x) \frac{\sigma(x)^2}{2}f''(x)dx &= g(x) \frac{\sigma(x)^2}{2}f'(x)\Bigg|_{-\infty}^{\infty} - \int_{\mathbb{R}}\frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right) f'(x)dx\\
-\int_{\mathbb{R}} \frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right)f'(x)dx &= -\frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right)f(x) \Bigg|_{-\infty}^{\infty} + \int_{\mathbb{R}}\frac{d^2}{dx^2}\left(g(x) \frac{\sigma(x)^2}{2}\right)f(x)dx
\end{align*}
$$

Thus,

$$
\begin{align*}
\int_{\mathbb{R}}g(x) Af(x)dx &= \int_{\mathbb{R}}\left(\frac{1}{2}\frac{d^2}{dx^2}(g(x) \sigma(x)^2) - \frac{d}{dx}(g(x)\mu(x))\right)f(x)dx\\
&= \int_{\mathbb{R}}(A^*g(x))f(x)dx
\end{align*}
$$ {#eq-relation-between-generator-and-adjoint}

::: {#thm-forward-equation-and-transition-probability}

### Forward equation and transition probability 

Let $(X_t,t\geq 0)$ be a diffusion with SDE:

$$
dX_t = \sigma(X_t)dB_t + \mu(X_t)dt, \quad X_0 = x_0
$$

Let $p(x,t|x_0,0)$ be the transition probability density function for a fixed $x_0$. Then, the function $f(t,y) = p(y,t|x_0,0)$ is a solution of the PDE 


$$
\frac{\partial f}{\partial t} = A^* f
$$

where $A^*$ is the adjoint of $A$. 
::: 

*Proof.*

Let $h(x)$ be some arbitrary function of space that is $0$ outside an interval. We compute :

$$
\frac{1}{\epsilon}(\mathbb{E}[h(X_{t+\epsilon}) - \mathbb{E}[h(X_t)]])
$$

two different ways and take the limit as $\epsilon \to 0$. 

On one hand, we have by the definition of the transition density 

$$
\frac{1}{\epsilon}\left(\mathbb{E}[h(X_{t+\epsilon})]-\mathbb{E}[h(X_t)]\right) = \int_{\mathbb{R}}\frac{1}{\epsilon}(p(x,t+\epsilon|x,0) - p(x,t|x_0,0))h(x)dx
$$

By taking the limit $\epsilon \to 0$ inside the integral (assuming this is fine), we get:

$$
\int_{\mathbb{R}} \frac{\partial}{\partial t}p(x,t|x_0,0)h(x)dx
$$ {#eq-fwd-equation-partial-wrt-time}

On the other hand, Ito's formula implies 

$$
\begin{align*}
dh(X_s) &= \frac{\partial h}{\partial x} dX_s + \frac{1}{2} \frac{\partial^2 h}{\partial x^2} (dX_s)^2\\
&= \frac{\partial h}{\partial x} (\sigma(X_s) dB_s + \mu(X_s)ds) + \frac{1}{2} \frac{\partial^2 h}{\partial x^2} (\sigma(X_s)^2 ds)\\
&= \sigma(X_s)\frac{\partial h}{\partial x} dB_s + \left(\mu(X_s) \frac{\partial h}{\partial x} + \frac{\sigma(X_s)^2}{2}\frac{\partial^2 h}{\partial x^2}\right)ds\\
h(X_{t+\epsilon}) - h(X_t) &= \int_{t}^{t+\epsilon}\sigma(X_s)\frac{\partial h}{\partial x} dB_s + \int_{t}^{t+\epsilon}(Ah(x))ds\\
\mathbb{E}[h(X_{t+\epsilon})] - \mathbb{E}[h(X_t)] &= \underbrace{\mathbb{E}\left[\int_{t}^{t+\epsilon}\sigma(X_s)\frac{\partial h}{\partial x} dB_s\right]}_{0} + \int_{t}^{t+\epsilon}\mathbb{E}[Ah(X_s)]ds
\end{align*}
$$

Dividing by $\epsilon$ and taking the limit as $\epsilon \to 0$, we have:

$$
\begin{align*}
\lim_{\epsilon \to 0} \frac{1}{\epsilon} (\mathbb{E}[h(X_{t+\epsilon})] - \mathbb{E}[h(X_t)]) &= \mathbb{E}[Ah(X_t)]\\
&= \int_{\mathbb{R}} p(x,t|x_0,0) Ah(x) dx
\end{align*}
$$

This can be written using @eq-relation-between-generator-and-adjoint as,

$$
\int_{\mathbb{R}}(A^* p(x,t|x_0,0)) h(x) dx
$$

Since $h$ is arbitrary, we conclude that:

$$
\frac{\partial}{\partial t}p(x,t|x_0,0) = A^* p(x,t|x_0,0)
$$ {#eq-forward-equation}

::: {#exm-forward-equation-and-invariant-probability}

(Forward equation and invariant probability.) The Ornstein-Uhlenbeck process converges to a stationary distribution as noted in the [example](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#exm-ornstein-uhlenbeck-process) here. For example, for the SDE of the form

$$
dX_t = -X_t dt + dB_t
$$

with $X_0$ a Gaussian of mean $0$ and variance $1/2$, the PDF of $X_t$, is, for all $t$ is:

$$
f(x) = \frac{1}{\sqrt{\pi}} e^{-x^2}
$$ {#eq-pdf-of-OU-process}

This *invariant distribution* can be seen from the point of view of the forward equation. Indeed since the PDF is constant in time, the forward equation simply becomes:

$$
A^* f = 0
$$ {#eq-forward-equation-of-ou-process}
:::

::: {#exm-smoluchowski-equation}

The SDE of the Ornstein-Uhlenbeck process can be generated as follows. Consider $V(x)$, a smooth function of space such that $\int_{\mathbb{R}} e^{-2V(x)}dx<\infty$. The *Smoluchowski* equation is the SDE of the form:

$$ 
dX_t = dB_t - V'(X_t) dt
$$ {#eq-smoluchowski-equation}

The SDE can be interpreted as follows: $X_t$ represents the position of a particle on $\mathbb{R}$. The position varies due to the Brownian fluctuations and also due to a force $V'(X_t)$ that depends on the position. The function $V(x)$ should then be thought of as the potential with which the particle moves, since the force (field) is the (negative) derivative of the potential function in Newtonian physics. The generator of this diffusion is:

$$
A = \frac{1}{2}\frac{\partial^2}{\partial x^2} - V'(x)\frac{\partial}{\partial x}
$$

This diffusion admits an invariant distribution :

$$
f(x) = Ce^{-2V(x)}
$$

where $C$ is such that $\int_{\mathbb{R}}f(x)dx = 1$.
:::

## The Feynman-Kac Formula

We saw in @exm-heat-equation-and-brownian-motion that the solution of the heat equation:

$$
\frac{\partial f}{\partial t} = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}
$$

can be represented as an average over Brownian paths. This representation was extended to diffusions in theorem @thm-backward-equation-with-initial-value where the second derivative in the equation is replaced by the generator of the corresponding diffusion. How robust is this representation? In other words, is it possible to slightly change the PDE and still get a stochastic representation representation for the solution? The answer to this question is yes, when a term of the form $r(x)f(t,x)$ is added to the equation, where $r(x)$ is a well-behaved function of space (for example, piecewise continuous). The stochastic representation of the PDE in this case bears the name *Feynman-Kac* formula, making a fruitful collaboration between the physicist [Richard Feynman](https://en.wikipedia.org/wiki/Richard_Feynman) and the mathematician [Mark Kac](https://en.wikipedia.org/wiki/Mark_Kac). By the way, you pronounce "Kac" as "cats". His name is Polish. People who immigrated from Poland before him spelled their names as "Katz". The case when $r(x)$ is linear will be important in the applications to mathematical finance, where it represents the contribution of the interest rate. 

::: {#thm-initial-value-problem}

### Initial Value Problem 

Let $(X_t,t\geq 0)$ be a diffusion in $\mathbb{R}$ with the SDE:

$$
dX_t = \sigma(X_t) dB_t + \mu(X_t)dt
$$

Let $g\in C^2(\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value 

$$
\begin{align*}
\frac{\partial f}{\partial t}(t,x) &= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2}(t,x) + \mu(x)\frac{\partial f}{\partial x}(t,x) - r(x)f(x)\\
f(0,x) &= g(x)
\end{align*}
$$ {#eq-initial-value-problem}

has the stochastic representation:

$$
f(t,x) = \mathbb{E}\left[g(X_t)\exp\left(-\int_0^t r(X_s) ds\right)\Bigg| X_0 = x\right]
$$
:::

*Proof.*

The proof is again based on Ito's formula. For a fixed $t$, we consider the process:

$$
M_s = f(t-s, X_s) \exp\left(-\int_0^s r(X_u) du\right), \quad s \leq t
$$

Write $Z_s = \exp\left(-\int_0^s r(X_u) du\right)$ and $V_s = f(t-s,X_s)$. A direct application of Ito's formula yields:

Let $R_s = -\int_0^s r(X_u) du$. So, $dR_t = r(X_t) dt$. $(R_t,t\geq 0)$ is a random variable, because $r(X_s)$ depends on how $(X_s, s \leq t)$ evolves, it is *stochastic*, but for very small intervals of time $r(X_s)$ is a constant, and hence the process $(R_t,t\geq 0)$ is said to be locally deterministic.

$$
\begin{align*}
Z_s &= e^{-R_s}\\
dZ_s &= -e^{-R_s} dR_s + \frac{1}{2}e^{R_s} (dR_s)^2\\
&= -Z_s r(X_s) ds
\end{align*}
$$

and 

$$
\begin{align*}
dV_s &= \frac{\partial}{\partial s}f(t-s, X_s)ds + \frac{\partial}{\partial x}f(t-s, X_s)dX_s + \frac{1}{2}\frac{\partial^2}{\partial x^2}f(t-s,X_s)(dX_s)^2\\
&= -f_s ds + f_x (\sigma(X_s)dB_s + \mu(X_s)ds) + \frac{1}{2}f_{xx} \sigma(X_s)^2 ds \\
&= \sigma(X_s) f_x dB_s + \\
&+ \left\{-f_s + \mu(X_s)f_x + \frac{\sigma(X_s)^2}{2}f_{xx}\right\}ds
\end{align*}
$$

Recall that $t$ is fixed here, and we differentiate with respect to $s$ in time. Since $f(t,x)$ is a solution of the PDE, we can write the second equation as:

$$
dV_s = \sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds
$$

Now, by Ito's product rule, we finally have:

$$
\begin{align*}
dM_s &= V_s dZ_s + Z_s dV_s + dZ_s dV_s\\
&= -f(t-s,X_s)Z_s r(X_s) ds + Z_s (\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds) + 0\\
&= \sigma(X_s)Z_s f_x dB_s
\end{align*}
$$

This proves that $(M_s, s \leq t)$ is a martingale. We conclude that:

$$
\mathbb{E}[M_t] = \mathbb{E}[M_0]
$$

Using the definition of $M_t$, this yields:

$$
\mathbb{E}[M_t] = \mathbb{E}\left[f(0,X_t)\exp\left(-\int_0^t r(X_u) du\right)\right] = \mathbb{E}\left[g(X_t)\exp\left(-\int_0^t r(X_u) du\right)\right] = \mathbb{E}[M_0] = f(t,x)
$$

This proves the theorem. $\blacksquare$

As for the backward equation, it is natural to consider the terminal value problem for the same PDE.

::: {#thm-terminal-value-problem}

### Terminal Value Problem

Let $(X_t,t \leq T)$ be a diffusion in $\mathbb{R}$ with the SDE:

$$
dX_t = \sigma(X_t) dB_t + \mu(X_t) dt
$$

Let $g\in C^2(\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value 

$$
\begin{align*}
-\frac{\partial f}{\partial t}(t,x) &= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2}(t,x) + \mu(x)\frac{\partial f}{\partial x}(t,x) - r(x)f(t,x)\\
f(T,x) &= g(x)
\end{align*}
$$

has the stochastic representation :

$$
f(t,x) = \mathbb{E}\left[g(X_T)\exp\left(-\int_t^T r(X_u) du\right)\Bigg|X_t = x\right]
$$
:::

*Proof.*

The proof is similar by considering instead

$$
M_t = f(t,X_t)\exp\left(-\int_0^t r(X_u) du\right)
$$

:::{#thm-generalized-feynman-kac}

### Generalized version. 

Let $V\in C^2(\mathbb{R})$ be the payout function. Then, the solution to the PDE 

$$
\begin{align*}
\left(\frac{\partial}{\partial t} + \mu(t,x)\frac{\partial}{\partial x} + \frac{1}{2}\sigma^2(t,x)\frac{\partial^2}{\partial x^2}\right)f = r(t,x)f(t,x) + B(t,x)
\end{align*}
$$

with the boundary condition:

$$
f(T,x) = V(x)
$$

has the stochastic representation:

$$
f(t,x)=\mathbb{E}_t\left[\exp\left(-\int_t^T r(u,X_u) du\right)V(X_T)\right] - \mathbb{E}_t\left[\int_{t}^T \exp\left(-\int_t^s r(u,X_u) du\right)B(s,X_s)ds\right]
$$


where $(X_t,t\leq T)$ is a diffusion in $\mathbb{R}$ with the dynamics :

$$
dX_t = \sigma(t,X_t) dB_t + \mu(t,X_t)dt
$$
:::

*Proof*

For brevity, I drop the space coordinates in the below derivations.

Define $Z_s = \exp\left(-\int_t^s r_u du\right)$. Consider the process

$$
Y(s) = Z_s f(s,X_s) - \int_t^s Z_s B_s ds
$$

By Ito's product rule:

$$
\begin{align*}
dY_s &= dZ_s f + Z_s df + dZ_s df - Z_s B_s ds
\end{align*}
$$

Since $dZ_s df = O(dt dt)$ it can be dropped. We have:

$$
\begin{align*}
dY_s &= -r_s Z_s f ds + Z_s \left(f_s ds + f_x dX_s + \frac{1}{2}f_{xx}(dX_s)^2\right) - Z_s B_s ds\\
&= -r_s Z_s f ds + Z_s \left[f_s ds + f_x (\mu ds + \sigma dW_s) + \frac{1}{2}\sigma^2f_{xx}ds\right] - Z_s B_s ds \\
&= -r_s Z_s f ds + Z_s \left[\left(f_s + \mu f_x  + \frac{1}{2}\sigma^2f_{xx}\right)ds + \sigma f_x dW_s \right]  - Z_s B_s ds
\end{align*}
$$

We can substitute the term in the round brackets $\left(f_s + \mu f_x  + \frac{1}{2}\sigma^2f_{xx}\right) = r_s f + B_s$, since $f$ satisfies the PDE. So, we have:

$$
\begin{align*}
dY_s 
&= -r_s Z_s f ds + Z_s \left[\left(r_s f + B_s\right)ds + \sigma f_x dW_s \right]  - Z_s B_s ds\\
&= Z_s \sigma f_x dW_s
\end{align*}
$$

So, the process $(Y_s,s\leq T)$ is a martingale. Integrating the above equation from $t$ to $T$, we have:

$$
Y(T) - Y(t) = \int_t^T Z_s \sigma f_x dW_s
$$

Upon taking expectations, conditioned on $X_t = x$ and observing that the RHS is an Ito integral, which has zero expectation, it follows that:

$$
\mathbb{E}_t[Y_T|X_t = x] =  \mathbb{E}_t[Y_t|X_t = x]
$$

On the right hand side, $Y_t = f(t,X_t) =f(t,x)$. It follows that:

$$
\begin{align*}
f(t,x) &= \mathbb{E}_t\left[Z_T f(T,X_T) - \int_{t}^T Z_s B_s ds \right]\\
&= \mathbb{E}_t\left[Z_T V(X_T)\right] - \mathbb{E}_t\left[\int_{t}^T Z_s B_s ds \right]
\end{align*}
$$

This closes the proof. $\blacksquare$

## Exercises

::: {#exr-shifted-brownian-motion}

(Shifted Brownian Motion) Let $(B_t,t\geq 0)$ be a standard brownian motion. Fix $t > 0$. Show that the process $(W_s,s \geq 0)$ with $W_s = B_{t+s} - B_t$ is a standard brownian motion independent of $\mathcal{F}_t$.
:::

*Solution*.

At $s = 0$, $W(0) = B(t) - B(t) = 0$. 

Consider any arbitrary times $t_1 < t_2$. We have:

\begin{align*}
W(t_2) - W(t_1) &= (B(t + t_2) - B(t)) - (B(t + t_1) - B(t))\\
&= B(t + t_2) - B(t + t_1)
\end{align*}

Now, $B(t + t_2) - B(t + t_1) \sim \mathcal{N}(0,t_2 - t_1)$. So, $W(t_2) - W(t_1)$ is a Gaussian random variable with mean $0$ and variance $t_2 - t_1$.

Finally, consider any finite set of times $0=t_0 < t_1 < t_2 < \ldots < t_n = T$. Then, $t < t + t_1 < t + t_2 < \ldots < t + t_n$. We have that, $B(t + t_1) - B(t)$, $B(t + t_2) - B(t + t_1)$, $B(t + t_3) - B(t + t_2)$, $\ldots$, $B(t+T) - B(t+t_{n-1})$ are independent random variables. Consequently, $W(t_1) - W(0)$, $W(t_2) - W(t_1)$, $W(t_3) - W(t_2)$, $\ldots$, $W(t_n) - W(t_{n-1})$ are independent random variables. So, $(W_s,s\geq 0)$ is a standard brownian motion.

Also, we have:

\begin{align*}
\mathbb{E}[W(s)|\mathcal{F}_t] &= \mathbb{E}[B(t + s) - B(t)|\mathcal{F}_t]\\
& \{ B(t+s) - B(t) \perp \mathcal{F}_t \}\\
&= \mathbb{E}[B(t + s) - B(t)]\\
&= \mathbb{E}[W(s)]
\end{align*}

Thus, $W(s)$ is independent of $\mathcal{F}_t$, it does not depend upon the information available upto time $t$.


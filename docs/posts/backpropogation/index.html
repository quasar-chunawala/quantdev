<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-06-05">

<title>quantdev.blog - Backpropogation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../.././symbol.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sell_side_quant_critical_path.html" rel="" target="">
 <span class="menu-text">Sell-side Quant</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../roadmap.html" rel="" target="">
 <span class="menu-text">C++ Roadmap</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://patreon.com/u59411143?utm_medium=unknown&amp;utm_source=join_link&amp;utm_campaign=creatorshare_creator&amp;utm_content=copyLink" rel="" target=""><i class="bi bi-patreon" role="img">
</i> 
 <span class="menu-text">Become a patreon</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/quasar-chunawala" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://linkedin.com/in/quasar-chunawala" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Backpropogation</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 5, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#calculating-the-network-error-with-loss" id="toc-calculating-the-network-error-with-loss" class="nav-link active" data-scroll-target="#calculating-the-network-error-with-loss">Calculating the network error with Loss</a>
  <ul class="collapse">
  <li><a href="#logit-vector" id="toc-logit-vector" class="nav-link" data-scroll-target="#logit-vector">Logit vector</a></li>
  <li><a href="#entropy-cross-entropy-and-kl-divergence" id="toc-entropy-cross-entropy-and-kl-divergence" class="nav-link" data-scroll-target="#entropy-cross-entropy-and-kl-divergence">Entropy, Cross-Entropy and KL-Divergence</a></li>
  <li><a href="#categorical-cross-entropy-loss-function" id="toc-categorical-cross-entropy-loss-function" class="nav-link" data-scroll-target="#categorical-cross-entropy-loss-function">Categorical cross-entropy loss function</a></li>
  </ul></li>
  <li><a href="#categorical-cross-entropy-loss-class" id="toc-categorical-cross-entropy-loss-class" class="nav-link" data-scroll-target="#categorical-cross-entropy-loss-class">Categorical Cross-Entropy Loss Class</a></li>
  <li><a href="#backpropogation" id="toc-backpropogation" class="nav-link" data-scroll-target="#backpropogation">Backpropogation</a></li>
  <li><a href="#backprop-for-a-single-neuron---a-python-implementation" id="toc-backprop-for-a-single-neuron---a-python-implementation" class="nav-link" data-scroll-target="#backprop-for-a-single-neuron---a-python-implementation">Backprop for a single neuron - a python implementation</a></li>
  <li><a href="#backprop-for-a-layer-of-neurons" id="toc-backprop-for-a-layer-of-neurons" class="nav-link" data-scroll-target="#backprop-for-a-layer-of-neurons">Backprop for a layer of neurons</a>
  <ul class="collapse">
  <li><a href="#gradient-of-the-loss-with-respect-to-haty" id="toc-gradient-of-the-loss-with-respect-to-haty" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-haty">Gradient of the loss with respect to <span class="math inline">\(\hat{y}\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-a" id="toc-gradient-of-the-loss-with-respect-to-a" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-a">Gradient of the loss with respect to <span class="math inline">\(a\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-z" id="toc-gradient-of-the-loss-with-respect-to-z" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-z">Gradient of the loss with respect to <span class="math inline">\(z\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-weights-w" id="toc-gradient-of-the-loss-with-respect-to-weights-w" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-weights-w">Gradient of the loss with respect to weights <span class="math inline">\(W\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-the-biases-b" id="toc-gradient-of-the-loss-with-respect-to-the-biases-b" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-the-biases-b">Gradient of the loss with respect to the biases <span class="math inline">\(b\)</span></a></li>
  <li><a href="#naive-python-implementation" id="toc-naive-python-implementation" class="nav-link" data-scroll-target="#naive-python-implementation">Naive Python implementation</a></li>
  </ul></li>
  <li><a href="#backprop-with-a-batch-of-inputs" id="toc-backprop-with-a-batch-of-inputs" class="nav-link" data-scroll-target="#backprop-with-a-batch-of-inputs">Backprop with a batch of inputs</a>
  <ul class="collapse">
  <li><a href="#gradient-of-the-loss-with-respect-to-weights-w-1" id="toc-gradient-of-the-loss-with-respect-to-weights-w-1" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-weights-w-1">Gradient of the loss with respect to weights <span class="math inline">\(w\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-the-biases-b-1" id="toc-gradient-of-the-loss-with-respect-to-the-biases-b-1" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-the-biases-b-1">Gradient of the loss with respect to the biases <span class="math inline">\(b\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-the-inputs" id="toc-gradient-of-the-loss-with-respect-to-the-inputs" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-the-inputs">Gradient of the loss with respect to the inputs</a></li>
  </ul></li>
  <li><a href="#adding-backward-to-denselayer" id="toc-adding-backward-to-denselayer" class="nav-link" data-scroll-target="#adding-backward-to-denselayer">Adding <code>backward()</code> to <code>DenseLayer</code></a></li>
  <li><a href="#categorical-cross-entropy-loss-derivative" id="toc-categorical-cross-entropy-loss-derivative" class="nav-link" data-scroll-target="#categorical-cross-entropy-loss-derivative">Categorical cross-entropy loss derivative</a>
  <ul class="collapse">
  <li><a href="#adding-backward-to-categoricalcrossentropyloss" id="toc-adding-backward-to-categoricalcrossentropyloss" class="nav-link" data-scroll-target="#adding-backward-to-categoricalcrossentropyloss">Adding <code>backward()</code> to <code>CategoricalCrossEntropyLoss</code></a></li>
  </ul></li>
  <li><a href="#softmax-activation-function-derivative" id="toc-softmax-activation-function-derivative" class="nav-link" data-scroll-target="#softmax-activation-function-derivative">Softmax Activation function derivative</a></li>
  <li><a href="#softmax-backward-implementation" id="toc-softmax-backward-implementation" class="nav-link" data-scroll-target="#softmax-backward-implementation">Softmax <code>backward()</code> implementation</a></li>
  <li><a href="#categorical-cross-entropy-loss-and-softmax-activation-function-derivative" id="toc-categorical-cross-entropy-loss-and-softmax-activation-function-derivative" class="nav-link" data-scroll-target="#categorical-cross-entropy-loss-and-softmax-activation-function-derivative">Categorical cross-entropy loss and softmax activation function derivative</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="calculating-the-network-error-with-loss" class="level2">
<h2 class="anchored" data-anchor-id="calculating-the-network-error-with-loss">Calculating the network error with Loss</h2>
<p>With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate the error in our model. The <em>loss function</em> also referred to as the <em>cost function</em> quantifies the error.</p>
<section id="logit-vector" class="level3">
<h3 class="anchored" data-anchor-id="logit-vector">Logit vector</h3>
<p>Let <span class="math inline">\(\vec{l} = \mathbf{w}\cdot \mathbf{x} + \mathbf{b}\)</span> be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the <strong>logit vector</strong> in machine learning literature.</p>
</section>
<section id="entropy-cross-entropy-and-kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="entropy-cross-entropy-and-kl-divergence">Entropy, Cross-Entropy and KL-Divergence</h3>
<p>Let <span class="math inline">\(X\)</span> be a random variable with possible outcomes <span class="math inline">\(\mathcal{X}\)</span>. Let <span class="math inline">\(P\)</span> be the true probability distribution of <span class="math inline">\(X\)</span> with probability mass function <span class="math inline">\(p(x)\)</span>. Let <span class="math inline">\(Q\)</span> be an approximating distribution with probability mass function <span class="math inline">\(q(x)\)</span>.</p>
<p><em>Definition</em>. The entropy of <span class="math inline">\(P\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
H(P) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log p(x)
\end{align*}\]</span></p>
<p>In information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm <span class="math inline">\(\log p(x)\)</span>, <em>we concentrate on the order of the surprise</em>. Entropy, then, is an expectation over the uncertainties or the <em>expected surprise</em>.</p>
<p><em>Definition</em>. The cross-entropy of <span class="math inline">\(Q\)</span> relative to <span class="math inline">\(P\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
H(P,Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log q(x)
\end{align*}\]</span></p>
<p><em>Definition</em>. For discrete distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> defined on the sample space <span class="math inline">\(\mathcal{X}\)</span>, the <em>Kullback-Leibler(KL) divergence</em> (or relative entropy) from <span class="math inline">\(Q\)</span> to <span class="math inline">\(P\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
D_{KL}(P||Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)}
\end{align*}\]</span></p>
<p>Intuitively, it is the expected excess surprise from using <span class="math inline">\(Q\)</span> as a model instead of <span class="math inline">\(P\)</span>, when the actual distribution is <span class="math inline">\(P\)</span>. Note that, <span class="math inline">\(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)</span>, so it is not symmetric and hence it is not a norm.</p>
</section>
<section id="categorical-cross-entropy-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="categorical-cross-entropy-loss-function">Categorical cross-entropy loss function</h3>
<p>We are going to work on a multi-class classification problem.</p>
<p>For any input <span class="math inline">\(\mathbf{x}_i\)</span>, the target vector <span class="math inline">\(\mathbf{y}_i\)</span> could be specified using <em>one-hot</em> encoding or an integer in the range <code>[0,numClasses)</code>.</p>
<p>Let’s say, we have <code>numClasses = 3</code>.</p>
<p>In one-hot encoding, the target vector <code>y_true</code> is an array like <code>[1, 0, 0]</code>, <code>[0, 1, 0]</code>, or <code>[0, 0, 1]</code>. The category/class is determined by the index which is <strong>hot</strong>. For example, if <code>y_true</code> equals <code>[0, 1, 0]</code>, then the sample belongs to class <span class="math inline">\(1\)</span>, whilst if <code>y_true</code> equals <code>[0, 0, 1]</code>, the sample belongs to class <span class="math inline">\(2\)</span>.</p>
<p>In integer encoding, the target vector <code>y_true</code> is an integer. For example, if <code>y_true</code> equals <span class="math inline">\(1\)</span>, the sample belongs to class <span class="math inline">\(1\)</span>, whilst if <code>y_true</code> equals <span class="math inline">\(2\)</span>, the sample belongs to class <span class="math inline">\(2\)</span>.</p>
<p>The <code>categorical_crossentropy</code> is defined as:</p>
<p><span class="math display">\[\begin{align*}
L_i = -\sum_{j} y_{i,j} \log(\hat{y}_{i,j})
\end{align*}\]</span></p>
<p>Assume that we have a softmax output <span class="math inline">\(\hat{\mathbf{y}}_i\)</span>, <code>[0.7, 0.1, 0.2]</code> and target vector <span class="math inline">\(\mathbf{y}_i\)</span> <code>[1, 0, 0]</code>. Then, we can compute the categorical cross entropy loss as:</p>
<p><span class="math display">\[\begin{align*}
-\left(1\cdot \log (0.7) + 0 \cdot \log (0.1) + 0 \cdot \log(0.2)\right) = 0.35667494
\end{align*}\]</span></p>
<p>Let’s that we have a batch of <span class="math inline">\(3\)</span> samples. Additionally, suppose the target <code>y_true</code> is integer encoded. After running through the softmax activation function, the network’s output layer yields:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>load_ext itikz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb2-4"><a href="#cb2-4"></a>    [</span>
<span id="cb2-5"><a href="#cb2-5"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb2-6"><a href="#cb2-6"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb2-7"><a href="#cb2-7"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb2-8"><a href="#cb2-8"></a>    ]</span>
<span id="cb2-9"><a href="#cb2-9"></a>)</span>
<span id="cb2-10"><a href="#cb2-10"></a></span>
<span id="cb2-11"><a href="#cb2-11"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="cf">for</span> targ_index, distribution <span class="kw">in</span> <span class="bu">zip</span>(y_true,y_pred):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="bu">print</span>(distribution[targ_index])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.7
0.5
0.08</code></pre>
</div>
</div>
<p>This can be simplified.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="bu">print</span>(y_pred[[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>],y_true])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.7  0.5  0.08]</code></pre>
</div>
</div>
<p><code>numpy</code> lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="bu">print</span>(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.7  0.5  0.08]</code></pre>
</div>
</div>
<p>The categorical cross-entropy loss for each of the samples is:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="bu">print</span>(<span class="op">-</span>np.log(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.35667494 0.69314718 2.52572864]</code></pre>
</div>
</div>
<p>Finally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>neg_log <span class="op">=</span> <span class="op">-</span>np.log(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true])</span>
<span id="cb11-2"><a href="#cb11-2"></a>average_loss <span class="op">=</span> np.mean(neg_log)</span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="bu">print</span>(average_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.191850256268978</code></pre>
</div>
</div>
<p>In the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If <code>y_true.shape</code> has <span class="math inline">\(2\)</span> dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if <code>y_true</code> is a list, that is <code>y_true.shape</code> has <span class="math inline">\(1\)</span> dimension, then it means, we have <em>sparse labels</em>/integer encoding.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb13-4"><a href="#cb13-4"></a>    [</span>
<span id="cb13-5"><a href="#cb13-5"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb13-6"><a href="#cb13-6"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb13-7"><a href="#cb13-7"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb13-8"><a href="#cb13-8"></a>    ]</span>
<span id="cb13-9"><a href="#cb13-9"></a>)</span>
<span id="cb13-10"><a href="#cb13-10"></a></span>
<span id="cb13-11"><a href="#cb13-11"></a>y_true <span class="op">=</span> np.array(</span>
<span id="cb13-12"><a href="#cb13-12"></a>    [</span>
<span id="cb13-13"><a href="#cb13-13"></a>        [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb13-14"><a href="#cb13-14"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb13-15"><a href="#cb13-15"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb13-16"><a href="#cb13-16"></a>    ]</span>
<span id="cb13-17"><a href="#cb13-17"></a>)</span>
<span id="cb13-18"><a href="#cb13-18"></a></span>
<span id="cb13-19"><a href="#cb13-19"></a>correct_confidences <span class="op">=</span> np.array([])</span>
<span id="cb13-20"><a href="#cb13-20"></a></span>
<span id="cb13-21"><a href="#cb13-21"></a><span class="co"># If categorical labels</span></span>
<span id="cb13-22"><a href="#cb13-22"></a><span class="cf">if</span>(<span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">1</span>):</span>
<span id="cb13-23"><a href="#cb13-23"></a>    correct_confidences <span class="op">=</span> y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb13-24"><a href="#cb13-24"></a><span class="cf">elif</span>(<span class="bu">len</span>(y_pred.shape)<span class="op">==</span><span class="dv">2</span>):</span>
<span id="cb13-25"><a href="#cb13-25"></a>    correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-26"><a href="#cb13-26"></a></span>
<span id="cb13-27"><a href="#cb13-27"></a>neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb13-28"><a href="#cb13-28"></a>average_loss <span class="op">=</span> np.mean(neg_log)</span>
<span id="cb13-29"><a href="#cb13-29"></a><span class="bu">print</span>(average_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.191850256268978</code></pre>
</div>
</div>
<p>If the neural network output <code>y_pred</code> for some reason is the vector <code>[1, 0, 0]</code>, this would result in <code>numpy.log</code> function returning a negative infinity. To avoid such situations, it’s safer to apply a ceil and floor to <code>y_pred</code>.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span><span class="op">-</span>epsilon)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="categorical-cross-entropy-loss-class" class="level2">
<h2 class="anchored" data-anchor-id="categorical-cross-entropy-loss-class">Categorical Cross-Entropy Loss Class</h2>
<p>I first create an abstract base class <code>Loss</code>. Every <code>Loss</code> object exposes the <code>calculate</code> method which in turn calls <code>Loss</code> object’s forward method to compute the log-loss for each sample and then takes an average of the sample losses.</p>
<p><code>CategoricalCrossEntropyLoss</code> class is a child class of <code>Loss</code> and provides an implementation of the <code>forward</code> method.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="im">import</span> nnfs</span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="im">from</span> nnfs.datasets <span class="im">import</span> spiral_data</span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="im">from</span> abc <span class="im">import</span> abstractmethod</span>
<span id="cb16-5"><a href="#cb16-5"></a></span>
<span id="cb16-6"><a href="#cb16-6"></a></span>
<span id="cb16-7"><a href="#cb16-7"></a><span class="co"># Abstract base class for losses</span></span>
<span id="cb16-8"><a href="#cb16-8"></a><span class="kw">class</span> Loss:</span>
<span id="cb16-9"><a href="#cb16-9"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb16-10"><a href="#cb16-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb16-11"><a href="#cb16-11"></a>        <span class="cf">pass</span></span>
<span id="cb16-12"><a href="#cb16-12"></a></span>
<span id="cb16-13"><a href="#cb16-13"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb16-14"><a href="#cb16-14"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb16-15"><a href="#cb16-15"></a>        <span class="cf">pass</span></span>
<span id="cb16-16"><a href="#cb16-16"></a></span>
<span id="cb16-17"><a href="#cb16-17"></a>    <span class="co"># Calculates the data and regularization losses</span></span>
<span id="cb16-18"><a href="#cb16-18"></a>    <span class="co"># given model output and ground truth values</span></span>
<span id="cb16-19"><a href="#cb16-19"></a>    <span class="kw">def</span> calculate(<span class="va">self</span>, output, y):</span>
<span id="cb16-20"><a href="#cb16-20"></a></span>
<span id="cb16-21"><a href="#cb16-21"></a>        <span class="co"># Calculate the sample losses</span></span>
<span id="cb16-22"><a href="#cb16-22"></a>        sample_losses <span class="op">=</span> <span class="va">self</span>.forward(output, y)</span>
<span id="cb16-23"><a href="#cb16-23"></a></span>
<span id="cb16-24"><a href="#cb16-24"></a>        <span class="co"># Calculate the mean loss</span></span>
<span id="cb16-25"><a href="#cb16-25"></a>        data_loss <span class="op">=</span> np.mean(sample_losses)</span>
<span id="cb16-26"><a href="#cb16-26"></a></span>
<span id="cb16-27"><a href="#cb16-27"></a>        <span class="co"># Return loss</span></span>
<span id="cb16-28"><a href="#cb16-28"></a>        <span class="cf">return</span> data_loss</span>
<span id="cb16-29"><a href="#cb16-29"></a></span>
<span id="cb16-30"><a href="#cb16-30"></a></span>
<span id="cb16-31"><a href="#cb16-31"></a><span class="co"># Cross-Entropy loss</span></span>
<span id="cb16-32"><a href="#cb16-32"></a><span class="kw">class</span> CategoricalCrossEntropyLoss(Loss):</span>
<span id="cb16-33"><a href="#cb16-33"></a></span>
<span id="cb16-34"><a href="#cb16-34"></a>    <span class="co"># Forward pass</span></span>
<span id="cb16-35"><a href="#cb16-35"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb16-36"><a href="#cb16-36"></a>        num_samples <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb16-37"><a href="#cb16-37"></a></span>
<span id="cb16-38"><a href="#cb16-38"></a>        <span class="co"># Clip data to prevent division by 0</span></span>
<span id="cb16-39"><a href="#cb16-39"></a>        <span class="co"># Clip both sides to not drag mean towards any value</span></span>
<span id="cb16-40"><a href="#cb16-40"></a>        epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb16-41"><a href="#cb16-41"></a>        y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span> <span class="op">-</span> epsilon)</span>
<span id="cb16-42"><a href="#cb16-42"></a></span>
<span id="cb16-43"><a href="#cb16-43"></a>        <span class="co"># If categorical labels</span></span>
<span id="cb16-44"><a href="#cb16-44"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb16-45"><a href="#cb16-45"></a>            correct_confidences <span class="op">=</span> y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb16-46"><a href="#cb16-46"></a>        <span class="co"># else if one-hot encoding</span></span>
<span id="cb16-47"><a href="#cb16-47"></a>        <span class="cf">elif</span> <span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb16-48"><a href="#cb16-48"></a>            correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-49"><a href="#cb16-49"></a></span>
<span id="cb16-50"><a href="#cb16-50"></a>        neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb16-51"><a href="#cb16-51"></a>        <span class="cf">return</span> neg_log</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using the manual created outputs and targets, we have:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb17-2"><a href="#cb17-2"></a>    [</span>
<span id="cb17-3"><a href="#cb17-3"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb17-4"><a href="#cb17-4"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb17-5"><a href="#cb17-5"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb17-6"><a href="#cb17-6"></a>    ]</span>
<span id="cb17-7"><a href="#cb17-7"></a>)</span>
<span id="cb17-8"><a href="#cb17-8"></a></span>
<span id="cb17-9"><a href="#cb17-9"></a>y_true <span class="op">=</span> np.array(</span>
<span id="cb17-10"><a href="#cb17-10"></a>    [</span>
<span id="cb17-11"><a href="#cb17-11"></a>        [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb17-12"><a href="#cb17-12"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb17-13"><a href="#cb17-13"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb17-14"><a href="#cb17-14"></a>    ]</span>
<span id="cb17-15"><a href="#cb17-15"></a>)</span>
<span id="cb17-16"><a href="#cb17-16"></a></span>
<span id="cb17-17"><a href="#cb17-17"></a>loss_function <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb17-18"><a href="#cb17-18"></a>loss <span class="op">=</span> loss_function.calculate(y_pred, y_true)</span>
<span id="cb17-19"><a href="#cb17-19"></a><span class="bu">print</span>(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.191850256268978</code></pre>
</div>
</div>
</section>
<section id="backpropogation" class="level2">
<h2 class="anchored" data-anchor-id="backpropogation">Backpropogation</h2>
<p>Backpropogation consists going backwards along the edges and passing along gradients. We are going to chop up a neuron into it’s elementary operations and draw a computational graph. Each node in the graph receives an upstream gradient. The goal is pass on the correct downstream gradient.</p>
<p>Each node has a <em>local gradient</em> - the gradient of it’s output with respect to it’s input. Consider a node receiving an input <span class="math inline">\(z\)</span> and producing an output <span class="math inline">\(h=f(z)\)</span>. Then, we have:</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb19-2"><a href="#cb19-2"></a>\begin{tikzpicture}</span>
<span id="cb19-3"><a href="#cb19-3"></a>    \node [circle,minimum size<span class="op">=</span><span class="dv">40</span><span class="er">mm</span>,draw] (f) at (<span class="dv">0</span>,<span class="dv">0</span>) {\huge $f$}<span class="op">;</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>    \node [blue] (localgrad) at (<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>) {\huge $\frac{\partial h}{\partial z}$}<span class="op">;</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>    \node [blue] (lgrad) at (<span class="fl">0.0</span>,<span class="dv">1</span>) {\large Local gradient}<span class="op">;</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="fl">1.80</span>,<span class="dv">1</span>) <span class="op">--</span> node [above,midway] {\huge $h$} (<span class="dv">5</span>,<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node [below,midway] {\huge $\frac{\partial s}{\partial h}$} (<span class="fl">1.80</span>,<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-8"><a href="#cb19-8"></a>    \node [] (upgrad) at (<span class="fl">4.0</span>,<span class="op">-</span><span class="dv">3</span>) {\huge Upstream gradient}<span class="op">;</span></span>
<span id="cb19-9"><a href="#cb19-9"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="op">-</span><span class="dv">5</span>,<span class="dv">1</span>) <span class="op">--</span> node [above,midway] {\huge $z$} (<span class="op">-</span><span class="fl">1.80</span>,<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="op">-</span><span class="fl">1.80</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node [below,midway] {\huge $\frac{\partial s}{\partial z} <span class="op">=</span> \frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial z}$} (<span class="op">-</span><span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-11"><a href="#cb19-11"></a>    \node [] (downgrad) at (<span class="op">-</span><span class="fl">4.0</span>,<span class="op">-</span><span class="dv">3</span>) {\huge Downstream gradient}<span class="op">;</span></span>
<span id="cb19-12"><a href="#cb19-12"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<p><img src="index_files/figure-html/cell-13-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>The downstream gradient <span class="math inline">\(\frac{\partial s}{\partial z}\)</span> equals the upstream graient <span class="math inline">\(\frac{\partial s}{\partial h}\)</span> times the local gradient <span class="math inline">\(\frac{\partial h}{\partial z}\)</span>.</p>
<p>What about nodes with multiple inputs? Say that, <span class="math inline">\(h=f(x,y)\)</span>. Multiple inputs imply multiple local gradients.</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb20-2"><a href="#cb20-2"></a>\begin{tikzpicture}[x<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,y<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,yscale<span class="op">=-</span><span class="dv">1</span>,scale<span class="op">=</span><span class="fl">1.75</span>]</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="op">%</span>uncomment <span class="cf">if</span> require: \path (<span class="dv">0</span>,<span class="dv">216</span>)<span class="op">;</span> <span class="op">%</span><span class="bu">set</span> diagram left start at <span class="dv">0</span>, <span class="kw">and</span> has height of <span class="dv">216</span></span>
<span id="cb20-4"><a href="#cb20-4"></a></span>
<span id="cb20-5"><a href="#cb20-5"></a><span class="op">%</span>Shape: Circle [<span class="bu">id</span>:dp08328772161506959] </span>
<span id="cb20-6"><a href="#cb20-6"></a>\draw   (<span class="fl">302.75</span>,<span class="fl">83.38</span>) .. controls (<span class="fl">302.75</span>,<span class="fl">53.62</span>) <span class="kw">and</span> (<span class="fl">326.87</span>,<span class="fl">29.5</span>) .. (<span class="fl">356.63</span>,<span class="fl">29.5</span>) .. controls (<span class="fl">386.38</span>,<span class="fl">29.5</span>) <span class="kw">and</span> (<span class="fl">410.5</span>,<span class="fl">53.62</span>) .. (<span class="fl">410.5</span>,<span class="fl">83.38</span>) .. controls (<span class="fl">410.5</span>,<span class="fl">113.13</span>) <span class="kw">and</span> (<span class="fl">386.38</span>,<span class="fl">137.25</span>) .. (<span class="fl">356.63</span>,<span class="fl">137.25</span>) .. controls (<span class="fl">326.87</span>,<span class="fl">137.25</span>) <span class="kw">and</span> (<span class="fl">302.75</span>,<span class="fl">113.13</span>) .. (<span class="fl">302.75</span>,<span class="fl">83.38</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb20-7"><a href="#cb20-7"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da2730189357413113] </span>
<span id="cb20-8"><a href="#cb20-8"></a>\draw    (<span class="dv">406</span>,<span class="fl">59.38</span>) <span class="op">--</span> (<span class="fl">513.5</span>,<span class="fl">59.74</span>) <span class="op">;</span></span>
<span id="cb20-9"><a href="#cb20-9"></a>\draw [shift<span class="op">=</span>{(<span class="fl">515.5</span>,<span class="fl">59.75</span>)}, rotate <span class="op">=</span> <span class="fl">180.2</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-10"><a href="#cb20-10"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da21080101466010737] </span>
<span id="cb20-11"><a href="#cb20-11"></a>\draw    (<span class="dv">515</span>,<span class="fl">110.75</span>) <span class="op">--</span> (<span class="dv">405</span>,<span class="fl">110.26</span>) <span class="op">;</span></span>
<span id="cb20-12"><a href="#cb20-12"></a>\draw [shift<span class="op">=</span>{(<span class="dv">403</span>,<span class="fl">110.25</span>)}, rotate <span class="op">=</span> <span class="fl">0.26</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-13"><a href="#cb20-13"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da05192158713361961] </span>
<span id="cb20-14"><a href="#cb20-14"></a>\draw    (<span class="dv">209</span>,<span class="fl">1.75</span>) <span class="op">--</span> (<span class="fl">309.71</span>,<span class="fl">51.37</span>) <span class="op">;</span></span>
<span id="cb20-15"><a href="#cb20-15"></a>\draw [shift<span class="op">=</span>{(<span class="fl">311.5</span>,<span class="fl">52.25</span>)}, rotate <span class="op">=</span> <span class="fl">206.23</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-16"><a href="#cb20-16"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da3568530309648137] </span>
<span id="cb20-17"><a href="#cb20-17"></a>\draw    (<span class="dv">305</span>,<span class="fl">68.25</span>) <span class="op">--</span> (<span class="fl">204.31</span>,<span class="fl">20.61</span>) <span class="op">;</span></span>
<span id="cb20-18"><a href="#cb20-18"></a>\draw [shift<span class="op">=</span>{(<span class="fl">202.5</span>,<span class="fl">19.75</span>)}, rotate <span class="op">=</span> <span class="fl">25.32</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-19"><a href="#cb20-19"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da4437541566257528] </span>
<span id="cb20-20"><a href="#cb20-20"></a>\draw    (<span class="dv">205</span>,<span class="fl">167.25</span>) <span class="op">--</span> (<span class="fl">311.2</span>,<span class="fl">116.12</span>) <span class="op">;</span></span>
<span id="cb20-21"><a href="#cb20-21"></a>\draw [shift<span class="op">=</span>{(<span class="dv">313</span>,<span class="fl">115.25</span>)}, rotate <span class="op">=</span> <span class="fl">154.29</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-22"><a href="#cb20-22"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da2672766038605987] </span>
<span id="cb20-23"><a href="#cb20-23"></a>\draw    (<span class="fl">304.5</span>,<span class="fl">101.75</span>) <span class="op">--</span> (<span class="fl">205.82</span>,<span class="fl">146.92</span>) <span class="op">;</span></span>
<span id="cb20-24"><a href="#cb20-24"></a>\draw [shift<span class="op">=</span>{(<span class="dv">204</span>,<span class="fl">147.75</span>)}, rotate <span class="op">=</span> <span class="fl">335.41</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-25"><a href="#cb20-25"></a></span>
<span id="cb20-26"><a href="#cb20-26"></a><span class="op">%</span> Text Node</span>
<span id="cb20-27"><a href="#cb20-27"></a>\draw (<span class="dv">352</span>,<span class="fl">76.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $f$}<span class="op">;</span></span>
<span id="cb20-28"><a href="#cb20-28"></a><span class="op">%</span> Text Node</span>
<span id="cb20-29"><a href="#cb20-29"></a>\draw (<span class="fl">318.5</span>,<span class="fl">44.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial h}{\partial x}$}<span class="op">;</span></span>
<span id="cb20-30"><a href="#cb20-30"></a><span class="op">%</span> Text Node</span>
<span id="cb20-31"><a href="#cb20-31"></a>\draw (<span class="fl">318.5</span>,<span class="fl">88.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">36</span><span class="op">;</span> blue, <span class="dv">255</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial h}{\partial y}$}<span class="op">;</span></span>
<span id="cb20-32"><a href="#cb20-32"></a><span class="op">%</span> Text Node</span>
<span id="cb20-33"><a href="#cb20-33"></a>\draw (<span class="fl">258.5</span>,<span class="fl">7.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $x$}<span class="op">;</span></span>
<span id="cb20-34"><a href="#cb20-34"></a><span class="op">%</span> Text Node</span>
<span id="cb20-35"><a href="#cb20-35"></a>\draw (<span class="dv">264</span>,<span class="fl">136.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $y$}<span class="op">;</span></span>
<span id="cb20-36"><a href="#cb20-36"></a><span class="op">%</span> Text Node</span>
<span id="cb20-37"><a href="#cb20-37"></a>\draw (<span class="fl">151.5</span>,<span class="fl">96.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial y} <span class="op">=</span>\frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial y}$}<span class="op">;</span></span>
<span id="cb20-38"><a href="#cb20-38"></a><span class="op">%</span> Text Node</span>
<span id="cb20-39"><a href="#cb20-39"></a>\draw (<span class="dv">150</span>,<span class="fl">33.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial x} <span class="op">=</span>\frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial x}$}<span class="op">;</span></span>
<span id="cb20-40"><a href="#cb20-40"></a><span class="op">%</span> Text Node</span>
<span id="cb20-41"><a href="#cb20-41"></a>\draw (<span class="fl">322.5</span>,<span class="fl">4.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $h<span class="op">=</span>f(x,y)$}<span class="op">;</span></span>
<span id="cb20-42"><a href="#cb20-42"></a><span class="op">%</span> Text Node</span>
<span id="cb20-43"><a href="#cb20-43"></a>\draw (<span class="fl">449.5</span>,<span class="fl">39.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $h$}<span class="op">;</span></span>
<span id="cb20-44"><a href="#cb20-44"></a><span class="op">%</span> Text Node</span>
<span id="cb20-45"><a href="#cb20-45"></a>\draw (<span class="fl">451.5</span>,<span class="fl">112.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $\frac{\partial s}{\partial h}$}<span class="op">;</span></span>
<span id="cb20-46"><a href="#cb20-46"></a><span class="op">%</span> Text Node</span>
<span id="cb20-47"><a href="#cb20-47"></a>\draw (<span class="fl">164.5</span>,<span class="fl">172.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $ \begin{array}{l}</span>
<span id="cb20-48"><a href="#cb20-48"></a>Downstream\ \<span class="op">\</span></span>
<span id="cb20-49"><a href="#cb20-49"></a>gradients</span>
<span id="cb20-50"><a href="#cb20-50"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb20-51"><a href="#cb20-51"></a><span class="op">%</span> Text Node</span>
<span id="cb20-52"><a href="#cb20-52"></a>\draw (<span class="fl">430.5</span>,<span class="fl">175.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $ \begin{array}{l}</span>
<span id="cb20-53"><a href="#cb20-53"></a>Upstream\ \<span class="op">\</span></span>
<span id="cb20-54"><a href="#cb20-54"></a>gradients</span>
<span id="cb20-55"><a href="#cb20-55"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb20-56"><a href="#cb20-56"></a><span class="op">%</span> Text Node</span>
<span id="cb20-57"><a href="#cb20-57"></a>\draw (<span class="fl">318.5</span>,<span class="fl">173.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">3</span><span class="op">;</span> green, <span class="dv">50</span><span class="op">;</span> blue, <span class="dv">255</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $ \begin{array}{l}</span>
<span id="cb20-58"><a href="#cb20-58"></a>Local\ \<span class="op">\</span></span>
<span id="cb20-59"><a href="#cb20-59"></a>gradients</span>
<span id="cb20-60"><a href="#cb20-60"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb20-61"><a href="#cb20-61"></a></span>
<span id="cb20-62"><a href="#cb20-62"></a></span>
<span id="cb20-63"><a href="#cb20-63"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<p><img src="index_files/figure-html/cell-14-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Let’s start with a simple forward pass with <span class="math inline">\(1\)</span> neuron. Let’s say, we have the following input vector, weights and bias:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>x <span class="op">=</span> [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>]  <span class="co"># input values</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>w <span class="op">=</span> [<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>] <span class="co"># weights</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>b <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="co"># Forward pass</span></span>
<span id="cb21-6"><a href="#cb21-6"></a>z <span class="op">=</span> np.dot(x,w) <span class="op">+</span> b</span>
<span id="cb21-7"><a href="#cb21-7"></a></span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="co"># ReLU Activation function</span></span>
<span id="cb21-9"><a href="#cb21-9"></a>y <span class="op">=</span> <span class="bu">max</span>(z, <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="15">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb22-2"><a href="#cb22-2"></a>\begin{tikzpicture}</span>
<span id="cb22-3"><a href="#cb22-3"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb22-4"><a href="#cb22-4"></a>{</span>
<span id="cb22-5"><a href="#cb22-5"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb22-6"><a href="#cb22-6"></a>}</span>
<span id="cb22-7"><a href="#cb22-7"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb22-8"><a href="#cb22-8"></a>{</span>
<span id="cb22-9"><a href="#cb22-9"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb22-10"><a href="#cb22-10"></a>}</span>
<span id="cb22-11"><a href="#cb22-11"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb22-12"><a href="#cb22-12"></a>{</span>
<span id="cb22-13"><a href="#cb22-13"></a>    \node[circle, </span>
<span id="cb22-14"><a href="#cb22-14"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb22-15"><a href="#cb22-15"></a>        draw,</span>
<span id="cb22-16"><a href="#cb22-16"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb22-17"><a href="#cb22-17"></a>        </span>
<span id="cb22-18"><a href="#cb22-18"></a>}</span>
<span id="cb22-19"><a href="#cb22-19"></a></span>
<span id="cb22-20"><a href="#cb22-20"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb22-21"><a href="#cb22-21"></a></span>
<span id="cb22-22"><a href="#cb22-22"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb22-23"><a href="#cb22-23"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb22-24"><a href="#cb22-24"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb22-25"><a href="#cb22-25"></a></span>
<span id="cb22-26"><a href="#cb22-26"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb22-27"><a href="#cb22-27"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb22-28"><a href="#cb22-28"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb22-29"><a href="#cb22-29"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb22-30"><a href="#cb22-30"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb22-31"><a href="#cb22-31"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb22-32"><a href="#cb22-32"></a></span>
<span id="cb22-33"><a href="#cb22-33"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb22-34"><a href="#cb22-34"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb22-35"><a href="#cb22-35"></a></span>
<span id="cb22-36"><a href="#cb22-36"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb22-37"><a href="#cb22-37"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb22-38"><a href="#cb22-38"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb22-39"><a href="#cb22-39"></a></span>
<span id="cb22-40"><a href="#cb22-40"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb22-41"><a href="#cb22-41"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb22-42"><a href="#cb22-42"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb22-43"><a href="#cb22-43"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="15">
<p><img src="index_files/figure-html/cell-16-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>The ReLU function <span class="math inline">\(f(x)=\max(x,0)\)</span> is differentiable everywhere except at <span class="math inline">\(x = 0\)</span>. We define <span class="math inline">\(f'(x)\)</span> as:</p>
<p><span class="math display">\[\begin{align*}
f'(x) =
\begin{cases}
1 &amp; x &gt; 0 \\
0 &amp; \text{otherwise}
\end{cases}
\end{align*}\]</span></p>
<p>In Python, we write:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>relu_dz <span class="op">=</span> (<span class="fl">1.</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The input to the ReLU function is <span class="math inline">\(6.00\)</span>, so the derivative equals <span class="math inline">\(1.00\)</span>. We multiply this local gradient by the upstream gradient to calculate the downstream gradient.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2"></a></span>
<span id="cb24-3"><a href="#cb24-3"></a>x <span class="op">=</span> [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>]  <span class="co"># input values</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>w <span class="op">=</span> [<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>]  <span class="co"># weights</span></span>
<span id="cb24-5"><a href="#cb24-5"></a>b <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb24-6"><a href="#cb24-6"></a></span>
<span id="cb24-7"><a href="#cb24-7"></a><span class="co"># Forward pass</span></span>
<span id="cb24-8"><a href="#cb24-8"></a>z <span class="op">=</span> np.dot(x, w) <span class="op">+</span> b</span>
<span id="cb24-9"><a href="#cb24-9"></a></span>
<span id="cb24-10"><a href="#cb24-10"></a><span class="co"># ReLU Activation function</span></span>
<span id="cb24-11"><a href="#cb24-11"></a>y <span class="op">=</span> <span class="bu">max</span>(z, <span class="dv">0</span>)</span>
<span id="cb24-12"><a href="#cb24-12"></a></span>
<span id="cb24-13"><a href="#cb24-13"></a><span class="co"># Backward pass</span></span>
<span id="cb24-14"><a href="#cb24-14"></a><span class="co"># Upstream gradient</span></span>
<span id="cb24-15"><a href="#cb24-15"></a>ds_drelu <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb24-16"><a href="#cb24-16"></a></span>
<span id="cb24-17"><a href="#cb24-17"></a><span class="co"># Derivative of the ReLU and the chain rule</span></span>
<span id="cb24-18"><a href="#cb24-18"></a>drelu_dz <span class="op">=</span> <span class="fl">1.0</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb24-19"><a href="#cb24-19"></a>ds_dz <span class="op">=</span> ds_drelu <span class="op">*</span> drelu_dz</span>
<span id="cb24-20"><a href="#cb24-20"></a><span class="bu">print</span>(ds_dz)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.0</code></pre>
</div>
</div>
<p>The results with the derivative of the ReLU function and chain rule look as follows:</p>
<div class="cell" data-execution_count="18">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb26-2"><a href="#cb26-2"></a>\begin{tikzpicture}</span>
<span id="cb26-3"><a href="#cb26-3"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb26-4"><a href="#cb26-4"></a>{</span>
<span id="cb26-5"><a href="#cb26-5"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb26-6"><a href="#cb26-6"></a>}</span>
<span id="cb26-7"><a href="#cb26-7"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb26-8"><a href="#cb26-8"></a>{</span>
<span id="cb26-9"><a href="#cb26-9"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb26-10"><a href="#cb26-10"></a>}</span>
<span id="cb26-11"><a href="#cb26-11"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb26-12"><a href="#cb26-12"></a>{</span>
<span id="cb26-13"><a href="#cb26-13"></a>    \node[circle, </span>
<span id="cb26-14"><a href="#cb26-14"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb26-15"><a href="#cb26-15"></a>        draw,</span>
<span id="cb26-16"><a href="#cb26-16"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb26-17"><a href="#cb26-17"></a>        </span>
<span id="cb26-18"><a href="#cb26-18"></a>}</span>
<span id="cb26-19"><a href="#cb26-19"></a></span>
<span id="cb26-20"><a href="#cb26-20"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb26-21"><a href="#cb26-21"></a></span>
<span id="cb26-22"><a href="#cb26-22"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb26-23"><a href="#cb26-23"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb26-24"><a href="#cb26-24"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb26-25"><a href="#cb26-25"></a></span>
<span id="cb26-26"><a href="#cb26-26"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb26-27"><a href="#cb26-27"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb26-28"><a href="#cb26-28"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb26-29"><a href="#cb26-29"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb26-30"><a href="#cb26-30"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb26-31"><a href="#cb26-31"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb26-32"><a href="#cb26-32"></a></span>
<span id="cb26-33"><a href="#cb26-33"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb26-34"><a href="#cb26-34"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb26-35"><a href="#cb26-35"></a></span>
<span id="cb26-36"><a href="#cb26-36"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb26-37"><a href="#cb26-37"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb26-38"><a href="#cb26-38"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb26-39"><a href="#cb26-39"></a></span>
<span id="cb26-40"><a href="#cb26-40"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb26-41"><a href="#cb26-41"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb26-42"><a href="#cb26-42"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb26-43"><a href="#cb26-43"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb26-44"><a href="#cb26-44"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="18">
<p><img src="index_files/figure-html/cell-19-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Moving backward through our neural network, consider the add function <span class="math inline">\(f(x,y,z)=x + y + z\)</span>. The partial derivatives <span class="math inline">\(\frac{\partial f}{\partial x}\)</span>, <span class="math inline">\(\frac{\partial f}{\partial y}\)</span> and <span class="math inline">\(\frac{\partial f}{\partial z}\)</span> are all equal to <span class="math inline">\(1\)</span>. So, the <strong>add gate</strong> always takes on the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># Local gradients for the + function</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>dz_dw0x0 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-3"><a href="#cb27-3"></a>dz_dw1x1 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-4"><a href="#cb27-4"></a>dz_dw2x2 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-5"><a href="#cb27-5"></a>dz_db <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-6"><a href="#cb27-6"></a></span>
<span id="cb27-7"><a href="#cb27-7"></a><span class="co"># Calculate the downstream gradients</span></span>
<span id="cb27-8"><a href="#cb27-8"></a>ds_dw0x0 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw0x0</span>
<span id="cb27-9"><a href="#cb27-9"></a>ds_dw1x1 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw1x1</span>
<span id="cb27-10"><a href="#cb27-10"></a>ds_dw2x2 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw2x2</span>
<span id="cb27-11"><a href="#cb27-11"></a>ds_db <span class="op">=</span> ds_dz <span class="op">*</span> dz_db</span>
<span id="cb27-12"><a href="#cb27-12"></a><span class="bu">print</span>(ds_dw0x0, ds_dw1x1, ds_dw2x2, ds_db)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.0 1.0 1.0 1.0</code></pre>
</div>
</div>
<p>We can update the computation graph as:</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb29-2"><a href="#cb29-2"></a>\begin{tikzpicture}</span>
<span id="cb29-3"><a href="#cb29-3"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb29-4"><a href="#cb29-4"></a>{</span>
<span id="cb29-5"><a href="#cb29-5"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb29-6"><a href="#cb29-6"></a>}</span>
<span id="cb29-7"><a href="#cb29-7"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb29-8"><a href="#cb29-8"></a>{</span>
<span id="cb29-9"><a href="#cb29-9"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb29-10"><a href="#cb29-10"></a>}</span>
<span id="cb29-11"><a href="#cb29-11"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb29-12"><a href="#cb29-12"></a>{</span>
<span id="cb29-13"><a href="#cb29-13"></a>    \node[circle, </span>
<span id="cb29-14"><a href="#cb29-14"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb29-15"><a href="#cb29-15"></a>        draw,</span>
<span id="cb29-16"><a href="#cb29-16"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb29-17"><a href="#cb29-17"></a>        </span>
<span id="cb29-18"><a href="#cb29-18"></a>}</span>
<span id="cb29-19"><a href="#cb29-19"></a></span>
<span id="cb29-20"><a href="#cb29-20"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb29-21"><a href="#cb29-21"></a></span>
<span id="cb29-22"><a href="#cb29-22"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb29-23"><a href="#cb29-23"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb29-24"><a href="#cb29-24"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb29-25"><a href="#cb29-25"></a></span>
<span id="cb29-26"><a href="#cb29-26"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb29-27"><a href="#cb29-27"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb29-28"><a href="#cb29-28"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb29-29"><a href="#cb29-29"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb29-30"><a href="#cb29-30"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb29-31"><a href="#cb29-31"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb29-32"><a href="#cb29-32"></a></span>
<span id="cb29-33"><a href="#cb29-33"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb29-34"><a href="#cb29-34"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb29-35"><a href="#cb29-35"></a></span>
<span id="cb29-36"><a href="#cb29-36"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb29-37"><a href="#cb29-37"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb29-38"><a href="#cb29-38"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb29-39"><a href="#cb29-39"></a></span>
<span id="cb29-40"><a href="#cb29-40"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb29-41"><a href="#cb29-41"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb29-42"><a href="#cb29-42"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-43"><a href="#cb29-43"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-44"><a href="#cb29-44"></a>\node [red] (C) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">3.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-45"><a href="#cb29-45"></a>\node [red] (D) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-46"><a href="#cb29-46"></a>\node [red] (E) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">7.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-47"><a href="#cb29-47"></a>\node [red] (f) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">12.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-48"><a href="#cb29-48"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="20">
<p><img src="index_files/figure-html/cell-21-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Now, consider the production function <span class="math inline">\(f(x,y) = x * y\)</span>. The gradients of <span class="math inline">\(f\)</span> are <span class="math inline">\(\frac{\partial f}{\partial x} = y\)</span>, <span class="math inline">\(\frac{\partial f}{\partial y} = x\)</span>. The <strong>multiply gate</strong> is therefore a little less easy to interpret. Its local gradients are the input values, except switched and this is multiplied by the upstream gradient.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Local gradients for the * function</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>dw0x0_dx0 <span class="op">=</span> w[<span class="dv">0</span>]</span>
<span id="cb30-3"><a href="#cb30-3"></a>dw0x0_dw0 <span class="op">=</span> x[<span class="dv">0</span>]</span>
<span id="cb30-4"><a href="#cb30-4"></a>dw1x1_dx1 <span class="op">=</span> w[<span class="dv">1</span>]</span>
<span id="cb30-5"><a href="#cb30-5"></a>dw1x1_dw1 <span class="op">=</span> x[<span class="dv">1</span>]</span>
<span id="cb30-6"><a href="#cb30-6"></a>dw2x2_dx2 <span class="op">=</span> w[<span class="dv">2</span>]</span>
<span id="cb30-7"><a href="#cb30-7"></a>dw2x2_dw2 <span class="op">=</span> x[<span class="dv">2</span>]</span>
<span id="cb30-8"><a href="#cb30-8"></a></span>
<span id="cb30-9"><a href="#cb30-9"></a><span class="co"># Calculate the downstream gradients</span></span>
<span id="cb30-10"><a href="#cb30-10"></a>ds_dx0 <span class="op">=</span> ds_dw0x0 <span class="op">*</span> dw0x0_dx0</span>
<span id="cb30-11"><a href="#cb30-11"></a>ds_dw0 <span class="op">=</span> ds_dw0x0 <span class="op">*</span> dw0x0_dw0</span>
<span id="cb30-12"><a href="#cb30-12"></a>ds_dx1 <span class="op">=</span> ds_dw1x1 <span class="op">*</span> dw1x1_dx1</span>
<span id="cb30-13"><a href="#cb30-13"></a>ds_dw1 <span class="op">=</span> ds_dw1x1 <span class="op">*</span> dw1x1_dw1</span>
<span id="cb30-14"><a href="#cb30-14"></a>ds_dx2 <span class="op">=</span> ds_dw2x2 <span class="op">*</span> dw2x2_dx2</span>
<span id="cb30-15"><a href="#cb30-15"></a>ds_dw2 <span class="op">=</span> ds_dw2x2 <span class="op">*</span> dw2x2_dw2</span>
<span id="cb30-16"><a href="#cb30-16"></a></span>
<span id="cb30-17"><a href="#cb30-17"></a><span class="bu">print</span>(ds_dx0, ds_dw0, ds_dx1, ds_dw1, ds_dx2, ds_dw2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-3.0 1.0 -1.0 -2.0 2.0 3.0</code></pre>
</div>
</div>
<p>We can update the computation graph as follows:</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb32-2"><a href="#cb32-2"></a>\begin{tikzpicture}</span>
<span id="cb32-3"><a href="#cb32-3"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb32-4"><a href="#cb32-4"></a>{</span>
<span id="cb32-5"><a href="#cb32-5"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb32-6"><a href="#cb32-6"></a>}</span>
<span id="cb32-7"><a href="#cb32-7"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb32-8"><a href="#cb32-8"></a>{</span>
<span id="cb32-9"><a href="#cb32-9"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb32-10"><a href="#cb32-10"></a>}</span>
<span id="cb32-11"><a href="#cb32-11"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb32-12"><a href="#cb32-12"></a>{</span>
<span id="cb32-13"><a href="#cb32-13"></a>    \node[circle, </span>
<span id="cb32-14"><a href="#cb32-14"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb32-15"><a href="#cb32-15"></a>        draw,</span>
<span id="cb32-16"><a href="#cb32-16"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb32-17"><a href="#cb32-17"></a>        </span>
<span id="cb32-18"><a href="#cb32-18"></a>}</span>
<span id="cb32-19"><a href="#cb32-19"></a></span>
<span id="cb32-20"><a href="#cb32-20"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb32-21"><a href="#cb32-21"></a></span>
<span id="cb32-22"><a href="#cb32-22"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb32-23"><a href="#cb32-23"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb32-24"><a href="#cb32-24"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb32-25"><a href="#cb32-25"></a></span>
<span id="cb32-26"><a href="#cb32-26"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb32-27"><a href="#cb32-27"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb32-28"><a href="#cb32-28"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb32-29"><a href="#cb32-29"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb32-30"><a href="#cb32-30"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb32-31"><a href="#cb32-31"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb32-32"><a href="#cb32-32"></a></span>
<span id="cb32-33"><a href="#cb32-33"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb32-34"><a href="#cb32-34"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb32-35"><a href="#cb32-35"></a></span>
<span id="cb32-36"><a href="#cb32-36"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb32-37"><a href="#cb32-37"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb32-38"><a href="#cb32-38"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb32-39"><a href="#cb32-39"></a></span>
<span id="cb32-40"><a href="#cb32-40"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb32-41"><a href="#cb32-41"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb32-42"><a href="#cb32-42"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-43"><a href="#cb32-43"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-44"><a href="#cb32-44"></a>\node [red] (C) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">3.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-45"><a href="#cb32-45"></a>\node [red] (D) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-46"><a href="#cb32-46"></a>\node [red] (E) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">7.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-47"><a href="#cb32-47"></a>\node [red] (F) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">12.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-48"><a href="#cb32-48"></a>\node [red] (G) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">0.75</span>) {\large $<span class="op">-</span><span class="fl">3.0</span>$}<span class="op">;</span></span>
<span id="cb32-49"><a href="#cb32-49"></a>\node [red] (H) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">2</span>) {\large $<span class="fl">1.0</span>$}<span class="op">;</span></span>
<span id="cb32-50"><a href="#cb32-50"></a>\node [red] (I) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">4.75</span>) {\large $<span class="op">-</span><span class="fl">1.0</span>$}<span class="op">;</span></span>
<span id="cb32-51"><a href="#cb32-51"></a>\node [red] (J) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">6</span>) {\large $<span class="op">-</span><span class="fl">2.0</span>$}<span class="op">;</span></span>
<span id="cb32-52"><a href="#cb32-52"></a>\node [red] (K) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">8.75</span>) {\large $<span class="fl">2.0</span>$}<span class="op">;</span></span>
<span id="cb32-53"><a href="#cb32-53"></a>\node [red] (L) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">10</span>) {\large $<span class="fl">3.0</span>$}<span class="op">;</span></span>
<span id="cb32-54"><a href="#cb32-54"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<p><img src="index_files/figure-html/cell-23-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Gradients sum at outward branches. Consider the following computation graph:</p>
<div class="cell" data-execution_count="23">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb33-2"><a href="#cb33-2"></a>\begin{tikzpicture}[x<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,y<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,yscale<span class="op">=-</span><span class="dv">1</span>,xscale<span class="op">=</span><span class="dv">1</span>]</span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="op">%</span>uncomment <span class="cf">if</span> require: \path (<span class="dv">0</span>,<span class="dv">211</span>)<span class="op">;</span> <span class="op">%</span><span class="bu">set</span> diagram left start at <span class="dv">0</span>, <span class="kw">and</span> has height of <span class="dv">211</span></span>
<span id="cb33-4"><a href="#cb33-4"></a></span>
<span id="cb33-5"><a href="#cb33-5"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp4612472925724298] </span>
<span id="cb33-6"><a href="#cb33-6"></a>\draw   (<span class="fl">444.62</span>,<span class="dv">95</span>) .. controls (<span class="fl">444.62</span>,<span class="fl">81.19</span>) <span class="kw">and</span> (<span class="fl">455.38</span>,<span class="dv">70</span>) .. (<span class="fl">468.64</span>,<span class="dv">70</span>) .. controls (<span class="fl">481.91</span>,<span class="dv">70</span>) <span class="kw">and</span> (<span class="fl">492.66</span>,<span class="fl">81.19</span>) .. (<span class="fl">492.66</span>,<span class="dv">95</span>) .. controls (<span class="fl">492.66</span>,<span class="fl">108.81</span>) <span class="kw">and</span> (<span class="fl">481.91</span>,<span class="dv">120</span>) .. (<span class="fl">468.64</span>,<span class="dv">120</span>) .. controls (<span class="fl">455.38</span>,<span class="dv">120</span>) <span class="kw">and</span> (<span class="fl">444.62</span>,<span class="fl">108.81</span>) .. (<span class="fl">444.62</span>,<span class="dv">95</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-7"><a href="#cb33-7"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp4844626229099638] </span>
<span id="cb33-8"><a href="#cb33-8"></a>\draw   (<span class="fl">299.33</span>,<span class="fl">31.5</span>) .. controls (<span class="fl">299.33</span>,<span class="fl">17.69</span>) <span class="kw">and</span> (<span class="fl">310.08</span>,<span class="fl">6.5</span>) .. (<span class="fl">323.35</span>,<span class="fl">6.5</span>) .. controls (<span class="fl">336.61</span>,<span class="fl">6.5</span>) <span class="kw">and</span> (<span class="fl">347.37</span>,<span class="fl">17.69</span>) .. (<span class="fl">347.37</span>,<span class="fl">31.5</span>) .. controls (<span class="fl">347.37</span>,<span class="fl">45.31</span>) <span class="kw">and</span> (<span class="fl">336.61</span>,<span class="fl">56.5</span>) .. (<span class="fl">323.35</span>,<span class="fl">56.5</span>) .. controls (<span class="fl">310.08</span>,<span class="fl">56.5</span>) <span class="kw">and</span> (<span class="fl">299.33</span>,<span class="fl">45.31</span>) .. (<span class="fl">299.33</span>,<span class="fl">31.5</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-9"><a href="#cb33-9"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp2271780920027553] </span>
<span id="cb33-10"><a href="#cb33-10"></a>\draw   (<span class="fl">303.25</span>,<span class="fl">94.7</span>) .. controls (<span class="fl">303.25</span>,<span class="fl">80.89</span>) <span class="kw">and</span> (<span class="dv">314</span>,<span class="fl">69.7</span>) .. (<span class="fl">327.27</span>,<span class="fl">69.7</span>) .. controls (<span class="fl">340.53</span>,<span class="fl">69.7</span>) <span class="kw">and</span> (<span class="fl">351.29</span>,<span class="fl">80.89</span>) .. (<span class="fl">351.29</span>,<span class="fl">94.7</span>) .. controls (<span class="fl">351.29</span>,<span class="fl">108.51</span>) <span class="kw">and</span> (<span class="fl">340.53</span>,<span class="fl">119.7</span>) .. (<span class="fl">327.27</span>,<span class="fl">119.7</span>) .. controls (<span class="dv">314</span>,<span class="fl">119.7</span>) <span class="kw">and</span> (<span class="fl">303.25</span>,<span class="fl">108.51</span>) .. (<span class="fl">303.25</span>,<span class="fl">94.7</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-11"><a href="#cb33-11"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp150108609534231] </span>
<span id="cb33-12"><a href="#cb33-12"></a>\draw   (<span class="fl">299.25</span>,<span class="fl">167.7</span>) .. controls (<span class="fl">299.25</span>,<span class="fl">153.89</span>) <span class="kw">and</span> (<span class="dv">310</span>,<span class="fl">142.7</span>) .. (<span class="fl">323.27</span>,<span class="fl">142.7</span>) .. controls (<span class="fl">336.53</span>,<span class="fl">142.7</span>) <span class="kw">and</span> (<span class="fl">347.29</span>,<span class="fl">153.89</span>) .. (<span class="fl">347.29</span>,<span class="fl">167.7</span>) .. controls (<span class="fl">347.29</span>,<span class="fl">181.51</span>) <span class="kw">and</span> (<span class="fl">336.53</span>,<span class="fl">192.7</span>) .. (<span class="fl">323.27</span>,<span class="fl">192.7</span>) .. controls (<span class="dv">310</span>,<span class="fl">192.7</span>) <span class="kw">and</span> (<span class="fl">299.25</span>,<span class="fl">181.51</span>) .. (<span class="fl">299.25</span>,<span class="fl">167.7</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-13"><a href="#cb33-13"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da7844123205705824] </span>
<span id="cb33-14"><a href="#cb33-14"></a>\draw    (<span class="fl">347.37</span>,<span class="fl">31.5</span>) <span class="op">--</span> (<span class="fl">450.04</span>,<span class="fl">76.06</span>) <span class="op">;</span></span>
<span id="cb33-15"><a href="#cb33-15"></a>\draw [shift<span class="op">=</span>{(<span class="fl">452.79</span>,<span class="fl">77.25</span>)}, rotate <span class="op">=</span> <span class="fl">203.46</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-16"><a href="#cb33-16"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da814168086414518] </span>
<span id="cb33-17"><a href="#cb33-17"></a>\draw    (<span class="fl">351.29</span>,<span class="fl">94.7</span>) <span class="op">--</span> (<span class="fl">441.62</span>,<span class="fl">94.99</span>) <span class="op">;</span></span>
<span id="cb33-18"><a href="#cb33-18"></a>\draw [shift<span class="op">=</span>{(<span class="fl">444.62</span>,<span class="dv">95</span>)}, rotate <span class="op">=</span> <span class="fl">180.18</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-19"><a href="#cb33-19"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da7411937688169676] </span>
<span id="cb33-20"><a href="#cb33-20"></a>\draw    (<span class="fl">347.29</span>,<span class="fl">167.7</span>) <span class="op">--</span> (<span class="fl">446.35</span>,<span class="fl">110.75</span>) <span class="op">;</span></span>
<span id="cb33-21"><a href="#cb33-21"></a>\draw [shift<span class="op">=</span>{(<span class="fl">448.95</span>,<span class="fl">109.25</span>)}, rotate <span class="op">=</span> <span class="fl">150.1</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-22"><a href="#cb33-22"></a><span class="op">%</span>Shape: Circle [<span class="bu">id</span>:dp515320046458885] </span>
<span id="cb33-23"><a href="#cb33-23"></a>\draw   (<span class="dv">163</span>,<span class="dv">96</span>) .. controls (<span class="dv">163</span>,<span class="fl">82.19</span>) <span class="kw">and</span> (<span class="fl">174.19</span>,<span class="dv">71</span>) .. (<span class="dv">188</span>,<span class="dv">71</span>) .. controls (<span class="fl">201.81</span>,<span class="dv">71</span>) <span class="kw">and</span> (<span class="dv">213</span>,<span class="fl">82.19</span>) .. (<span class="dv">213</span>,<span class="dv">96</span>) .. controls (<span class="dv">213</span>,<span class="fl">109.81</span>) <span class="kw">and</span> (<span class="fl">201.81</span>,<span class="dv">121</span>) .. (<span class="dv">188</span>,<span class="dv">121</span>) .. controls (<span class="fl">174.19</span>,<span class="dv">121</span>) <span class="kw">and</span> (<span class="dv">163</span>,<span class="fl">109.81</span>) .. (<span class="dv">163</span>,<span class="dv">96</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-24"><a href="#cb33-24"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da6219161786925074] </span>
<span id="cb33-25"><a href="#cb33-25"></a>\draw    (<span class="fl">492.66</span>,<span class="dv">95</span>) <span class="op">--</span> (<span class="dv">567</span>,<span class="fl">94.52</span>) <span class="op">;</span></span>
<span id="cb33-26"><a href="#cb33-26"></a>\draw [shift<span class="op">=</span>{(<span class="dv">570</span>,<span class="fl">94.5</span>)}, rotate <span class="op">=</span> <span class="fl">179.63</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-27"><a href="#cb33-27"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da5694521418691749] </span>
<span id="cb33-28"><a href="#cb33-28"></a>\draw    (<span class="fl">84.5</span>,<span class="fl">95.75</span>) <span class="op">--</span> (<span class="dv">160</span>,<span class="fl">95.99</span>) <span class="op">;</span></span>
<span id="cb33-29"><a href="#cb33-29"></a>\draw [shift<span class="op">=</span>{(<span class="dv">163</span>,<span class="dv">96</span>)}, rotate <span class="op">=</span> <span class="fl">180.18</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">8.04</span>,<span class="op">-</span><span class="fl">3.86</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">8.04</span>,<span class="fl">3.86</span>) <span class="op">--</span> (<span class="fl">5.34</span>,<span class="dv">0</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-30"><a href="#cb33-30"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da08990804845355682] </span>
<span id="cb33-31"><a href="#cb33-31"></a>\draw    (<span class="fl">210.69</span>,<span class="fl">85.5</span>) <span class="op">--</span> (<span class="fl">296.86</span>,<span class="fl">31.4</span>) <span class="op">;</span></span>
<span id="cb33-32"><a href="#cb33-32"></a>\draw [shift<span class="op">=</span>{(<span class="fl">299.4</span>,<span class="fl">29.8</span>)}, rotate <span class="op">=</span> <span class="fl">147.88</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-33"><a href="#cb33-33"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da1505672958459916] </span>
<span id="cb33-34"><a href="#cb33-34"></a>\draw    (<span class="fl">212.61</span>,<span class="dv">96</span>) <span class="op">--</span> (<span class="fl">300.4</span>,<span class="fl">95.03</span>) <span class="op">;</span></span>
<span id="cb33-35"><a href="#cb33-35"></a>\draw [shift<span class="op">=</span>{(<span class="fl">303.4</span>,<span class="dv">95</span>)}, rotate <span class="op">=</span> <span class="fl">179.37</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-36"><a href="#cb33-36"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da23258128449735227] </span>
<span id="cb33-37"><a href="#cb33-37"></a>\draw    (<span class="dv">203</span>,<span class="fl">116.5</span>) <span class="op">--</span> (<span class="fl">296.36</span>,<span class="fl">167.17</span>) <span class="op">;</span></span>
<span id="cb33-38"><a href="#cb33-38"></a>\draw [shift<span class="op">=</span>{(<span class="dv">299</span>,<span class="fl">168.6</span>)}, rotate <span class="op">=</span> <span class="fl">208.49</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-39"><a href="#cb33-39"></a></span>
<span id="cb33-40"><a href="#cb33-40"></a><span class="op">%</span> Text Node</span>
<span id="cb33-41"><a href="#cb33-41"></a>\draw (<span class="fl">464.08</span>,<span class="fl">84.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $s$}<span class="op">;</span></span>
<span id="cb33-42"><a href="#cb33-42"></a><span class="op">%</span> Text Node</span>
<span id="cb33-43"><a href="#cb33-43"></a>\draw (<span class="fl">317.25</span>,<span class="fl">18.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">1</span>}$}<span class="op">;</span></span>
<span id="cb33-44"><a href="#cb33-44"></a><span class="op">%</span> Text Node</span>
<span id="cb33-45"><a href="#cb33-45"></a>\draw (<span class="fl">321.65</span>,<span class="fl">82.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">2</span>}$}<span class="op">;</span></span>
<span id="cb33-46"><a href="#cb33-46"></a><span class="op">%</span> Text Node</span>
<span id="cb33-47"><a href="#cb33-47"></a>\draw (<span class="fl">317.65</span>,<span class="fl">155.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">3</span>}$}<span class="op">;</span></span>
<span id="cb33-48"><a href="#cb33-48"></a><span class="op">%</span> Text Node</span>
<span id="cb33-49"><a href="#cb33-49"></a>\draw (<span class="fl">365.04</span>,<span class="fl">44.2</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">1</span>}}$}<span class="op">;</span></span>
<span id="cb33-50"><a href="#cb33-50"></a><span class="op">%</span> Text Node</span>
<span id="cb33-51"><a href="#cb33-51"></a>\draw (<span class="fl">365.52</span>,<span class="fl">94.3</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">2</span>}}$}<span class="op">;</span></span>
<span id="cb33-52"><a href="#cb33-52"></a><span class="op">%</span> Text Node</span>
<span id="cb33-53"><a href="#cb33-53"></a>\draw (<span class="fl">366.72</span>,<span class="dv">154</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">3</span>}}$}<span class="op">;</span></span>
<span id="cb33-54"><a href="#cb33-54"></a><span class="op">%</span> Text Node</span>
<span id="cb33-55"><a href="#cb33-55"></a>\draw (<span class="fl">183.5</span>,<span class="fl">85.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $a$}<span class="op">;</span></span>
<span id="cb33-56"><a href="#cb33-56"></a><span class="op">%</span> Text Node</span>
<span id="cb33-57"><a href="#cb33-57"></a>\draw (<span class="fl">304.78</span>,<span class="fl">21.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">1</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-58"><a href="#cb33-58"></a><span class="op">%</span> Text Node</span>
<span id="cb33-59"><a href="#cb33-59"></a>\draw (<span class="fl">305.82</span>,<span class="fl">84.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">2</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-60"><a href="#cb33-60"></a><span class="op">%</span> Text Node</span>
<span id="cb33-61"><a href="#cb33-61"></a>\draw (<span class="fl">303.26</span>,<span class="fl">156.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">3</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-62"><a href="#cb33-62"></a><span class="op">%</span> Text Node</span>
<span id="cb33-63"><a href="#cb33-63"></a>\draw (<span class="fl">251.38</span>,<span class="fl">53.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">1</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">1</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-64"><a href="#cb33-64"></a><span class="op">%</span> Text Node</span>
<span id="cb33-65"><a href="#cb33-65"></a>\draw (<span class="fl">249.38</span>,<span class="fl">99.8</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">2</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">2</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-66"><a href="#cb33-66"></a><span class="op">%</span> Text Node</span>
<span id="cb33-67"><a href="#cb33-67"></a>\draw (<span class="fl">245.78</span>,<span class="fl">165.8</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">3</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">3</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-68"><a href="#cb33-68"></a></span>
<span id="cb33-69"><a href="#cb33-69"></a></span>
<span id="cb33-70"><a href="#cb33-70"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<p><img src="index_files/figure-html/cell-24-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>The upstream gradient for the node <span class="math inline">\(a\)</span> is <span class="math inline">\(\frac{ds}{da}\)</span>. By the law of total derivatives:</p>
<p><span class="math display">\[\begin{align*}
\frac{ds}{da} = \frac{\partial s}{\partial z^1} \cdot \frac{\partial z^1}{\partial a} + \frac{\partial s}{\partial z^2} \cdot \frac{\partial z^2}{\partial a} + \frac{\partial s}{\partial z^3} \cdot \frac{\partial z^3}{\partial a}
\end{align*}\]</span></p>
</section>
<section id="backprop-for-a-single-neuron---a-python-implementation" class="level2">
<h2 class="anchored" data-anchor-id="backprop-for-a-single-neuron---a-python-implementation">Backprop for a single neuron - a python implementation</h2>
<p>We can write a naive implementation for the backprop algorithm for a single neuron.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-2"><a href="#cb34-2"></a></span>
<span id="cb34-3"><a href="#cb34-3"></a>weights <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>])</span>
<span id="cb34-4"><a href="#cb34-4"></a>bias <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb34-5"><a href="#cb34-5"></a>inputs <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>])</span>
<span id="cb34-6"><a href="#cb34-6"></a>target_output <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb34-7"><a href="#cb34-7"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb34-8"><a href="#cb34-8"></a></span>
<span id="cb34-9"><a href="#cb34-9"></a></span>
<span id="cb34-10"><a href="#cb34-10"></a><span class="kw">def</span> relu(x):</span>
<span id="cb34-11"><a href="#cb34-11"></a>    <span class="cf">return</span> np.maximum(x, <span class="dv">0</span>)</span>
<span id="cb34-12"><a href="#cb34-12"></a></span>
<span id="cb34-13"><a href="#cb34-13"></a></span>
<span id="cb34-14"><a href="#cb34-14"></a><span class="kw">def</span> relu_derivative(x):</span>
<span id="cb34-15"><a href="#cb34-15"></a>    <span class="cf">return</span> np.where(x <span class="op">&gt;</span> <span class="dv">0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb34-16"><a href="#cb34-16"></a></span>
<span id="cb34-17"><a href="#cb34-17"></a></span>
<span id="cb34-18"><a href="#cb34-18"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb34-19"><a href="#cb34-19"></a>    <span class="co"># Forward pass</span></span>
<span id="cb34-20"><a href="#cb34-20"></a>    z <span class="op">=</span> np.dot(weights, inputs) <span class="op">+</span> bias</span>
<span id="cb34-21"><a href="#cb34-21"></a>    a <span class="op">=</span> relu(z)</span>
<span id="cb34-22"><a href="#cb34-22"></a>    loss <span class="op">=</span> (a <span class="op">-</span> target_output) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb34-23"><a href="#cb34-23"></a></span>
<span id="cb34-24"><a href="#cb34-24"></a>    <span class="co"># Backward pass</span></span>
<span id="cb34-25"><a href="#cb34-25"></a>    dloss_da <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (a <span class="op">-</span> target_output)</span>
<span id="cb34-26"><a href="#cb34-26"></a>    dloss_dz <span class="op">=</span> dloss_da <span class="op">*</span> relu_derivative(z)</span>
<span id="cb34-27"><a href="#cb34-27"></a>    dz_dx <span class="op">=</span> weights</span>
<span id="cb34-28"><a href="#cb34-28"></a>    dz_dw <span class="op">=</span> inputs</span>
<span id="cb34-29"><a href="#cb34-29"></a>    dz_db <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb34-30"><a href="#cb34-30"></a>    dloss_dx <span class="op">=</span> dloss_dz <span class="op">*</span> dz_dx</span>
<span id="cb34-31"><a href="#cb34-31"></a>    dloss_dw <span class="op">=</span> dloss_dz <span class="op">*</span> dz_dw</span>
<span id="cb34-32"><a href="#cb34-32"></a>    dloss_db <span class="op">=</span> dloss_dz <span class="op">*</span> dz_db</span>
<span id="cb34-33"><a href="#cb34-33"></a></span>
<span id="cb34-34"><a href="#cb34-34"></a>    <span class="co"># Update the weights and bias</span></span>
<span id="cb34-35"><a href="#cb34-35"></a>    weights <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_dw</span>
<span id="cb34-36"><a href="#cb34-36"></a>    bias <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_db</span>
<span id="cb34-37"><a href="#cb34-37"></a></span>
<span id="cb34-38"><a href="#cb34-38"></a>    <span class="co"># print the loss for this iteration</span></span>
<span id="cb34-39"><a href="#cb34-39"></a>    <span class="cf">if</span> (<span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb34-40"><a href="#cb34-40"></a>        <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span><span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-41"><a href="#cb34-41"></a></span>
<span id="cb34-42"><a href="#cb34-42"></a><span class="bu">print</span>(<span class="st">"Final weights : "</span>, weights)</span>
<span id="cb34-43"><a href="#cb34-43"></a><span class="bu">print</span>(<span class="st">"Final bias : "</span>, bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 10, loss: 20.80624545154949
Iteration 20, loss: 11.314318574097976
Iteration 30, loss: 6.152662434665503
Iteration 40, loss: 3.345783025909011
Iteration 50, loss: 1.8194178821496518
Iteration 60, loss: 0.9893891517327431
Iteration 70, loss: 0.5380242236653578
Iteration 80, loss: 0.29257452918677535
Iteration 90, loss: 0.1591003738562249
Iteration 100, loss: 0.08651788326054576
Iteration 110, loss: 0.04704793547908108
Iteration 120, loss: 0.025584401159906914
Iteration 130, loss: 0.013912652617925996
Iteration 140, loss: 0.007565621788733219
Iteration 150, loss: 0.004114142329436494
Iteration 160, loss: 0.00223724732474303
Iteration 170, loss: 0.0012166024389232565
Iteration 180, loss: 0.0006615815238773228
Iteration 190, loss: 0.0003597642900693548
Iteration 200, loss: 0.00019563778572677352
Final weights :  [-3.3990955  -0.20180899  0.80271349]
Final bias :  0.6009044964039992</code></pre>
</div>
</div>
</section>
<section id="backprop-for-a-layer-of-neurons" class="level2">
<h2 class="anchored" data-anchor-id="backprop-for-a-layer-of-neurons">Backprop for a layer of neurons</h2>
<p>We are now in a position to write a naive implementation of the backprop algorithm for a layer of neurons.</p>
<p>A neural network with a single hidden layer is shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="backprop.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">backprop</figcaption>
</figure>
</div>
<p>Let <span class="math inline">\(\mathcal{L}\)</span> be a loss function of a neural network to minimize. Let <span class="math inline">\(x \in \mathbf{R}^{d_0}\)</span> be a single sample(input). Let <span class="math inline">\(d_{l}\)</span> be number of neurons(inputs) in layer <span class="math inline">\(l\)</span>. In our example, <span class="math inline">\(x \in \mathbf{R}^4\)</span>.</p>
<p>Let’s derive expressions for all the derivatives we want to compute.</p>
<section id="gradient-of-the-loss-with-respect-to-haty" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-haty">Gradient of the loss with respect to <span class="math inline">\(\hat{y}\)</span></h3>
<p>The gradient of the loss function <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(\hat{y}\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial \hat{y}} &amp;= 2*(\hat{y} - y)
\end{align*}\]</span></p>
</section>
<section id="gradient-of-the-loss-with-respect-to-a" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-a">Gradient of the loss with respect to <span class="math inline">\(a\)</span></h3>
<p>The gradient of <span class="math inline">\(\hat{y}\)</span> with respect to <span class="math inline">\(a_1, a_2, a_3\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \hat{y}}{\partial a} &amp;= \left[\frac{\partial \hat{y}}{\partial a_1}, \frac{\partial \hat{y}}{\partial a_2}, \frac{\partial \hat{y}}{\partial a_3}\right] = [1, 1, 1]
\end{align*}\]</span></p>
<p>So, by chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial a} &amp;= \left[\frac{\partial \mathcal{L}}{\partial a_1}, \frac{\partial \mathcal{L}}{\partial a_2}, \frac{\partial \mathcal{L}}{\partial a_3}\right] \\
&amp;=\left[\frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_1}, \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_2}, \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_3}\right] \\
&amp;= \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a}
\end{align*}\]</span></p>
<p>This vector has the shape <code>[1,layer_width]</code>. In this example, it’s dimensions are <code>(1,3)</code>.</p>
</section>
<section id="gradient-of-the-loss-with-respect-to-z" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-z">Gradient of the loss with respect to <span class="math inline">\(z\)</span></h3>
<p>In our example, <span class="math inline">\(a_1 = max(z_1,0)\)</span>, <span class="math inline">\(a_2 = max(z_2,0)\)</span> and <span class="math inline">\(a_3 = max(z_3,0)\)</span>. Consequently, the derivative:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial a}{\partial z} &amp;= \left[\frac{\partial a_1}{\partial z_1}, \frac{\partial a_2}{\partial z_2}, \frac{\partial a_3}{\partial z_3}\right]\\
&amp;= \left[1_{(z_1 &gt; 0)}, 1_{(z_2 &gt; 0)}, 1_{(z_3 &gt; 0)}\right]
\end{align*}\]</span></p>
<p>Again this vector has shape <code>[1,layer_width]</code>, which in our example equals <code>(1,3)</code>.</p>
<p>By the chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial z} &amp;= \left[\frac{\partial \mathcal{L}}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1}, \frac{\partial \mathcal{L}}{\partial a_2} \cdot \frac{\partial a_2}{\partial z_2}, \frac{\partial \mathcal{L}}{\partial a_3} \cdot \frac{\partial a_3}{\partial z_3}\right]\\
&amp;= \frac{\partial \mathcal{L}}{\partial a} \odot \frac{\partial \mathcal{a}}{\partial z}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> denotes the element wise product of the two vectors. The gradient of the loss with respect to <span class="math inline">\(z\)</span>, is also a vector of shape <code>[1,layer_width]</code>.</p>
</section>
<section id="gradient-of-the-loss-with-respect-to-weights-w" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-weights-w">Gradient of the loss with respect to weights <span class="math inline">\(W\)</span></h3>
<p>Since</p>
<p><span class="math display">\[\begin{align*}
z_1 &amp;= w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + b_1 \\
z_2 &amp;= w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + b_2 \\
z_3 &amp;= w_{31}x_1 + w_{32}x_2 + w_{23}x_3 + w_{24}x_4 + b_3
\end{align*}\]</span></p>
<p>it follows that: <span class="math display">\[\begin{align*}
\frac{\partial z_i}{\partial w_{ij}} = x_j
\end{align*}\]</span></p>
<p>Now,</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{ij}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i} \cdot \frac{\partial z_i}{\partial w_{ij}} \\
&amp;= \frac{\partial \mathcal{L}}{\partial z_i} \cdot x_j
\end{align*}\]</span></p>
<p>In other words:</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial w_{11}} \\
\frac{\partial \mathcal{L}}{\partial w_{12}} \\
\frac{\partial \mathcal{L}}{\partial w_{13}} \\
\frac{\partial \mathcal{L}}{\partial w_{14}}
\end{bmatrix}
&amp;= \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{11}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{12}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{13}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{14}}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_1\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_2\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_3\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_4
\end{bmatrix}
\end{align*}\]</span></p>
<p>Putting this together, we define the jacobian matrix <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial W}\)</span> as:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial W}&amp;=\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial w_{11}} &amp; \frac{\partial \mathcal{L}}{\partial w_{21}} &amp; \frac{\partial \mathcal{L}}{\partial w_{31}} &amp; \frac{\partial \mathcal{L}}{\partial w_{41}} \\
\frac{\partial \mathcal{L}}{\partial w_{12}} &amp; \frac{\partial \mathcal{L}}{\partial w_{22}} &amp; \frac{\partial \mathcal{L}}{\partial w_{32}} &amp; \frac{\partial \mathcal{L}}{\partial w_{42}} \\
\frac{\partial \mathcal{L}}{\partial w_{13}} &amp; \frac{\partial \mathcal{L}}{\partial w_{23}} &amp; \frac{\partial \mathcal{L}}{\partial w_{33}} &amp; \frac{\partial \mathcal{L}}{\partial w_{43}} \\
\frac{\partial \mathcal{L}}{\partial w_{14}} &amp; \frac{\partial \mathcal{L}}{\partial w_{24}} &amp; \frac{\partial \mathcal{L}}{\partial w_{34}} &amp; \frac{\partial \mathcal{L}}{\partial w_{44}} \\
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{11}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{21}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{31}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{12}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{22}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{32}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{13}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{23}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{33}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{14}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{24}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{34}}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_1 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_1 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_1 \\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_2 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_2 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_2\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_3 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_3 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_3\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_4 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_4 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_4
\end{bmatrix}\\
&amp;= \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix} \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} &amp; \frac{\partial \mathcal{L}}{\partial z_2} &amp; \frac{\partial \mathcal{L}}{\partial z_3}
\end{bmatrix} \\
&amp;= X^T \cdot \frac{\partial \mathcal{L}}{\partial z}
\end{align*}\]</span></p>
<p>The dimensions of <span class="math inline">\(X^T\)</span> and <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial z}\)</span> are <code>[input_size,1]</code> and <code>[1,layer_width]</code> respectively. Therefore, <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial W}\)</span> will be of dimensions <code>[input_size,layer_width]</code>. In our example this equals <code>(4,3)</code>.</p>
<p>The first column of <span class="math inline">\(X^T \cdot \frac{\partial \mathcal{L}}{\partial z}\)</span> gives the derivative with respect to the first neuron’s weights, the second column gives the derivative with respect to the second neuron’s weights and so forth.</p>
</section>
<section id="gradient-of-the-loss-with-respect-to-the-biases-b" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-the-biases-b">Gradient of the loss with respect to the biases <span class="math inline">\(b\)</span></h3>
<p>Since</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial z}{\partial b} &amp;= \left[\frac{\partial z_1}{\partial b_1}, \frac{\partial z_2}{\partial b_2}, \frac{\partial z_3}{\partial b_3}\right]\\
&amp;= [1,1,1]
\end{align*}\]</span></p>
<p>It follows that:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial b} &amp;= \left[\frac{\partial \mathcal{L}}{\partial b_1}, \frac{\partial \mathcal{L}}{\partial b_2}, \frac{\partial \mathcal{L}}{\partial b_3}\right]\\
&amp;= \left[\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial b_1}, \frac{\partial \mathcal{L}}{\partial b_2} \cdot \frac{\partial z_2}{\partial b_21}, \frac{\partial \mathcal{L}}{\partial b_3}\cdot \cdot \frac{\partial z_3}{\partial b_3}\right]\\
&amp;=\left[\frac{\partial \mathcal{L}}{\partial z_1} \cdot 1, \frac{\partial \mathcal{L}}{\partial b_2} \cdot 1, \frac{\partial \mathcal{L}}{\partial b_3}\cdot \cdot 1\right]\\
&amp;= \frac{\partial \mathcal{L}}{\partial z}
\end{align*}\]</span></p>
</section>
<section id="naive-python-implementation" class="level3">
<h3 class="anchored" data-anchor-id="naive-python-implementation">Naive Python implementation</h3>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2"></a></span>
<span id="cb36-3"><a href="#cb36-3"></a>inputs <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb36-4"><a href="#cb36-4"></a>weights <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>], [<span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>], [<span class="fl">0.9</span>, <span class="fl">1.0</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span>]])</span>
<span id="cb36-5"><a href="#cb36-5"></a></span>
<span id="cb36-6"><a href="#cb36-6"></a>biases <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>])</span>
<span id="cb36-7"><a href="#cb36-7"></a></span>
<span id="cb36-8"><a href="#cb36-8"></a><span class="co"># Learning rate</span></span>
<span id="cb36-9"><a href="#cb36-9"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb36-10"><a href="#cb36-10"></a></span>
<span id="cb36-11"><a href="#cb36-11"></a></span>
<span id="cb36-12"><a href="#cb36-12"></a><span class="co"># ReLU Activation function and its derivative</span></span>
<span id="cb36-13"><a href="#cb36-13"></a><span class="kw">def</span> relu(x):</span>
<span id="cb36-14"><a href="#cb36-14"></a>    <span class="cf">return</span> np.maximum(x, <span class="dv">0</span>)</span>
<span id="cb36-15"><a href="#cb36-15"></a></span>
<span id="cb36-16"><a href="#cb36-16"></a></span>
<span id="cb36-17"><a href="#cb36-17"></a><span class="kw">def</span> relu_derivative(z):</span>
<span id="cb36-18"><a href="#cb36-18"></a>    <span class="cf">return</span> np.where(z <span class="op">&gt;</span> <span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb36-19"><a href="#cb36-19"></a></span>
<span id="cb36-20"><a href="#cb36-20"></a></span>
<span id="cb36-21"><a href="#cb36-21"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb36-22"><a href="#cb36-22"></a>    <span class="co"># Forward pass</span></span>
<span id="cb36-23"><a href="#cb36-23"></a>    z <span class="op">=</span> np.dot(weights, inputs) <span class="op">+</span> biases</span>
<span id="cb36-24"><a href="#cb36-24"></a>    a <span class="op">=</span> relu(z)</span>
<span id="cb36-25"><a href="#cb36-25"></a>    y_pred <span class="op">=</span> np.<span class="bu">sum</span>(a)</span>
<span id="cb36-26"><a href="#cb36-26"></a>    y_true <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb36-27"><a href="#cb36-27"></a>    loss <span class="op">=</span> (y_pred <span class="op">-</span> y_true) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb36-28"><a href="#cb36-28"></a></span>
<span id="cb36-29"><a href="#cb36-29"></a>    <span class="co"># Backward pass</span></span>
<span id="cb36-30"><a href="#cb36-30"></a>    <span class="co"># Gradient of loss with respect to y_pred</span></span>
<span id="cb36-31"><a href="#cb36-31"></a>    dloss_dy <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (y_pred <span class="op">-</span> y_true)</span>
<span id="cb36-32"><a href="#cb36-32"></a></span>
<span id="cb36-33"><a href="#cb36-33"></a>    <span class="co"># Gradient of y_pred with respect to a</span></span>
<span id="cb36-34"><a href="#cb36-34"></a>    dy_da <span class="op">=</span> np.ones_like(a)</span>
<span id="cb36-35"><a href="#cb36-35"></a></span>
<span id="cb36-36"><a href="#cb36-36"></a>    <span class="co"># Gradient of the activation function with respect to z</span></span>
<span id="cb36-37"><a href="#cb36-37"></a>    da_dz <span class="op">=</span> relu_derivative(z)</span>
<span id="cb36-38"><a href="#cb36-38"></a></span>
<span id="cb36-39"><a href="#cb36-39"></a>    <span class="co"># Gradient of z with respect to the weights</span></span>
<span id="cb36-40"><a href="#cb36-40"></a>    dz_dw <span class="op">=</span> inputs</span>
<span id="cb36-41"><a href="#cb36-41"></a></span>
<span id="cb36-42"><a href="#cb36-42"></a>    <span class="co"># Gradient of z with respect to inputs</span></span>
<span id="cb36-43"><a href="#cb36-43"></a>    dz_dx <span class="op">=</span> weights</span>
<span id="cb36-44"><a href="#cb36-44"></a></span>
<span id="cb36-45"><a href="#cb36-45"></a>    <span class="co"># Gradient of loss with respect to a</span></span>
<span id="cb36-46"><a href="#cb36-46"></a>    dloss_da <span class="op">=</span> dloss_dy <span class="op">*</span> dy_da</span>
<span id="cb36-47"><a href="#cb36-47"></a></span>
<span id="cb36-48"><a href="#cb36-48"></a>    <span class="co"># Gradient of loss with respect to z</span></span>
<span id="cb36-49"><a href="#cb36-49"></a>    dloss_dz <span class="op">=</span> dloss_da <span class="op">*</span> da_dz</span>
<span id="cb36-50"><a href="#cb36-50"></a></span>
<span id="cb36-51"><a href="#cb36-51"></a>    <span class="co"># Gradient of loss with respect to the weights</span></span>
<span id="cb36-52"><a href="#cb36-52"></a>    dloss_dw <span class="op">=</span> np.outer(dloss_dz, dz_dw)</span>
<span id="cb36-53"><a href="#cb36-53"></a></span>
<span id="cb36-54"><a href="#cb36-54"></a>    <span class="co"># Gradient of loss with respect to biases</span></span>
<span id="cb36-55"><a href="#cb36-55"></a>    dloss_db <span class="op">=</span> dloss_dz</span>
<span id="cb36-56"><a href="#cb36-56"></a></span>
<span id="cb36-57"><a href="#cb36-57"></a>    weights <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_dw</span>
<span id="cb36-58"><a href="#cb36-58"></a>    biases <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_db</span>
<span id="cb36-59"><a href="#cb36-59"></a></span>
<span id="cb36-60"><a href="#cb36-60"></a>    <span class="cf">if</span> (<span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">20</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb36-61"><a href="#cb36-61"></a>        <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span><span class="bu">iter</span><span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-62"><a href="#cb36-62"></a></span>
<span id="cb36-63"><a href="#cb36-63"></a><span class="bu">print</span>(<span class="st">"Final weights : "</span>, weights)</span>
<span id="cb36-64"><a href="#cb36-64"></a><span class="bu">print</span>(<span class="st">"Final bias : "</span>, biases)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 20, loss = 6.057433318678514
Iteration 40, loss = 0.4681684867419663
Iteration 60, loss = 0.03618392815029436
Iteration 80, loss = 0.0027965928794077364
Iteration 100, loss = 0.00021614380010564146
Iteration 120, loss = 1.670537841532316e-05
Iteration 140, loss = 1.2911296454618448e-06
Iteration 160, loss = 9.978916489916474e-08
Iteration 180, loss = 7.712531012091791e-09
Iteration 200, loss = 5.96088109107831e-10
Final weights :  [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]
 [ 0.25975286  0.11950572 -0.02074143 -0.16098857]
 [ 0.53548461  0.27096922  0.00645383 -0.25806156]]
Final bias :  [-0.00698895 -0.04024714 -0.06451539]</code></pre>
</div>
</div>
</section>
</section>
<section id="backprop-with-a-batch-of-inputs" class="level2">
<h2 class="anchored" data-anchor-id="backprop-with-a-batch-of-inputs">Backprop with a batch of inputs</h2>
<p>Let <span class="math inline">\(x\)</span> be a batch of inputs of dimensions <code>[batch_size,input_size]</code>. Consider</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>x <span class="op">=</span> np.array(</span>
<span id="cb38-2"><a href="#cb38-2"></a>    [</span>
<span id="cb38-3"><a href="#cb38-3"></a>        [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">2.5</span>],</span>
<span id="cb38-4"><a href="#cb38-4"></a>        [<span class="dv">2</span>, <span class="dv">5</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb38-5"><a href="#cb38-5"></a>        [<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">2.7</span>, <span class="fl">3.3</span>, <span class="op">-</span><span class="fl">0.8</span>]</span>
<span id="cb38-6"><a href="#cb38-6"></a>    ]</span>
<span id="cb38-7"><a href="#cb38-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>of shape <code>(3,4)</code>. Each sample will give one loss. Hence, the total loss <span class="math inline">\(\mathcal{L} = L_1 + L_2 + L_3\)</span>.</p>
<section id="gradient-of-the-loss-with-respect-to-weights-w-1" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-weights-w-1">Gradient of the loss with respect to weights <span class="math inline">\(w\)</span></h3>
<p>I am going to denote use the following convention for the <span class="math inline">\(z\)</span>’s:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}[c|ccc]
\text{} &amp; \text{Neuron}-1 &amp; \text{Neuron}-2 &amp; \text{Neuron}-3\\
\hline
\text{Sample}-1 &amp; z_{11} &amp; z_{12} &amp; z_{13} \\
\text{Sample}-2 &amp; z_{21} &amp; z_{22} &amp; z_{23} \\
\text{Sample}-3 &amp; z_{31} &amp; z_{32} &amp; z_{33} \\
\text{Sample}-4 &amp; z_{41} &amp; z_{42} &amp; z_{43}
\end{array}
\end{align*}\]</span></p>
<p>In this case <span class="math inline">\(\frac{d\mathcal{L}}{dz}\)</span> will be a matrix of partial derivatives of shape <code>[batch_size,layer_width]</code>.</p>
<p>I can write:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{11}} &amp;= \frac{\partial L_1}{\partial w_{11}} + \frac{\partial L_2}{\partial w_{11}} + \frac{\partial L_3}{\partial w_{11}} \\
&amp;= \frac{\partial L_1}{\partial z_{11}}\cdot \frac{\partial z_{11}}{\partial w_{11}} + \frac{\partial L_2}{\partial z_{21}}\cdot\frac{\partial z_{21}}{\partial w_{11}} + \frac{\partial L_3}{\partial z_{31}} \cdot \frac{\partial z_{31}}{\partial w_{11}}\\
&amp;=\frac{\partial L_1}{\partial z_{11}}\cdot x_{11} + \frac{\partial L_2}{\partial z_{21}}\cdot x_{21} + \frac{\partial L_3}{\partial z_{31}} \cdot x_{31}
\end{align*}\]</span></p>
<p>If you work out the derivatives of the loss function with respect to each of the weights, you would find:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial W} &amp;= X^T \cdot \frac{\partial \mathcal{L}}{\partial z}
\end{align*}\]</span></p>
<p><code>X.T</code> has shape <code>[input_size,batch_size]</code> and <code>dloss_dz</code> has shape <code>[batch_size,layer_width]</code>, so the matrix product will have dimensions <code>[input_size,layer_width]</code>.</p>
</section>
<section id="gradient-of-the-loss-with-respect-to-the-biases-b-1" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-the-biases-b-1">Gradient of the loss with respect to the biases <span class="math inline">\(b\)</span></h3>
<p>Consider again:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial b_1} &amp;= \frac{\partial L}{\partial z_{11}} \cdot \frac{\partial z_{11}}{\partial b_1} + \frac{\partial L}{\partial z_{21}} \cdot \frac{\partial z_{21}}{\partial b_1} + \frac{\partial L}{\partial z_{31}} \cdot \frac{\partial z_{31}}{\partial b_1} \\
&amp;= \frac{\partial L}{\partial z_{11}} \cdot 1 + \frac{\partial L}{\partial z_{21}} \cdot 1 + \frac{\partial L}{\partial z_{31}} \cdot 1
\end{align*}\]</span></p>
<p>So, to find the partial derivative of the loss with respect to <span class="math inline">\(b_1\)</span>, we will just look at the partial derivatives of the loss with respect to the first neuron and then add them up.</p>
<p>In python, we would write this as</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>dloss_dbiases <span class="op">=</span> np.<span class="bu">sum</span>(dloss_dz, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="gradient-of-the-loss-with-respect-to-the-inputs" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-the-inputs">Gradient of the loss with respect to the inputs</h3>
<p>The gradients of the loss with respect to the weights in the layer <span class="math inline">\(l\)</span>, require the gradients of the loss with respect to the inputs in layer <span class="math inline">\(l+1\)</span>. It’s easy to see that:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial w_{11}^{(l)}} &amp;= \frac{\partial L}{\partial x_1^{(l+1)}}\cdot \frac{\partial x_1^{(l+1)}}{\partial z_{1}^{l}} \cdot \frac{\partial z_1^{(l)}}{\partial w_{11}^{(l)}}
\end{align*}\]</span></p>
<p>What is <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_1}\)</span>, <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_2}\)</span>, <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_3}\)</span> and <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_4}\)</span>?</p>
<p>By the chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial x_1} &amp;= \frac{\partial L}{\partial z_1}\cdot \frac{\partial z_1}{\partial x_1} +  \frac{\partial L}{\partial z_2}\cdot \frac{\partial z_2}{\partial x_1} +  \frac{\partial L}{\partial z_3}\cdot \frac{\partial z_3}{\partial x_1} \\
&amp;= \frac{\partial L}{\partial z_1}\cdot w_{11} +  \frac{\partial L}{\partial z_2}\cdot w_{21} +  \frac{\partial L}{\partial z_3}\cdot w_{31}
\end{align*}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial x_1} &amp; \frac{\partial \mathcal{L}}{\partial x_2} &amp; \frac{\partial \mathcal{L}}{\partial x_3} &amp; \frac{\partial \mathcal{L}}{\partial x_4}
\end{bmatrix} &amp;=
\begin{bmatrix}
\frac{\partial L}{\partial z_1} &amp; \frac{\partial L}{\partial z_2} &amp; \frac{\partial L}{\partial z_3}
\end{bmatrix}
\begin{bmatrix}
w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14}\\
w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24}\\
w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34}
\end{bmatrix}\\
\frac{\partial \mathcal{L}}{\partial x} &amp;= \frac{\partial L}{\partial z} \cdot W
\end{align*}\]</span></p>
<p>What if we have a batch of input data of 3 examples? In such case, <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial z}\)</span> will have shape <code>(3,3)</code> and <span class="math inline">\(W\)</span> will have shape <code>(3,4)</code>. So, we can multiply them and the result would be <code>(3,4)</code>.</p>
</section>
</section>
<section id="adding-backward-to-denselayer" class="level2">
<h2 class="anchored" data-anchor-id="adding-backward-to-denselayer">Adding <code>backward()</code> to <code>DenseLayer</code></h2>
<p>We will now add backward pass code to the <code>DenseLayer</code> and <code>ReLUActivation</code> classes.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="im">from</span> nnfs.datasets <span class="im">import</span> spiral_data</span>
<span id="cb40-2"><a href="#cb40-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb40-3"><a href="#cb40-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-4"><a href="#cb40-4"></a><span class="im">import</span> nnfs</span>
<span id="cb40-5"><a href="#cb40-5"></a></span>
<span id="cb40-6"><a href="#cb40-6"></a>nnfs.init()</span>
<span id="cb40-7"><a href="#cb40-7"></a></span>
<span id="cb40-8"><a href="#cb40-8"></a></span>
<span id="cb40-9"><a href="#cb40-9"></a><span class="kw">class</span> DenseLayer:</span>
<span id="cb40-10"><a href="#cb40-10"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_inputs, n_neurons):</span>
<span id="cb40-11"><a href="#cb40-11"></a>        <span class="va">self</span>.width <span class="op">=</span> n_neurons</span>
<span id="cb40-12"><a href="#cb40-12"></a>        <span class="co"># Weight vectors per neuron</span></span>
<span id="cb40-13"><a href="#cb40-13"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.array(</span>
<span id="cb40-14"><a href="#cb40-14"></a>            [[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>], [<span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>], [<span class="fl">0.9</span>, <span class="fl">1.0</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span>]]</span>
<span id="cb40-15"><a href="#cb40-15"></a>        )</span>
<span id="cb40-16"><a href="#cb40-16"></a>        <span class="va">self</span>.biases <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>])</span>
<span id="cb40-17"><a href="#cb40-17"></a></span>
<span id="cb40-18"><a href="#cb40-18"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb40-19"><a href="#cb40-19"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb40-20"><a href="#cb40-20"></a>        <span class="va">self</span>.output <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights.T) <span class="op">+</span> <span class="va">self</span>.biases</span>
<span id="cb40-21"><a href="#cb40-21"></a></span>
<span id="cb40-22"><a href="#cb40-22"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_dz):</span>
<span id="cb40-23"><a href="#cb40-23"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> dloss_dz</span>
<span id="cb40-24"><a href="#cb40-24"></a>        <span class="va">self</span>.dz_dweights <span class="op">=</span> <span class="va">self</span>.inputs</span>
<span id="cb40-25"><a href="#cb40-25"></a>        <span class="va">self</span>.dz_dbiases <span class="op">=</span> np.ones_like(<span class="va">self</span>.inputs)</span>
<span id="cb40-26"><a href="#cb40-26"></a>        <span class="va">self</span>.dz_dinputs <span class="op">=</span> <span class="va">self</span>.weights</span>
<span id="cb40-27"><a href="#cb40-27"></a>        <span class="va">self</span>.dloss_dweights <span class="op">=</span> np.dot(<span class="va">self</span>.inputs.T, <span class="va">self</span>.dloss_dz).T</span>
<span id="cb40-28"><a href="#cb40-28"></a>        <span class="va">self</span>.dloss_dbiases <span class="op">=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.dloss_dz, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-29"><a href="#cb40-29"></a>        <span class="va">self</span>.dloss_dinputs <span class="op">=</span> np.dot(<span class="va">self</span>.dloss_dz, <span class="va">self</span>.dz_dinputs)</span>
<span id="cb40-30"><a href="#cb40-30"></a></span>
<span id="cb40-31"><a href="#cb40-31"></a></span>
<span id="cb40-32"><a href="#cb40-32"></a><span class="kw">class</span> ReLUActivation:</span>
<span id="cb40-33"><a href="#cb40-33"></a></span>
<span id="cb40-34"><a href="#cb40-34"></a>    <span class="co"># Forward pass</span></span>
<span id="cb40-35"><a href="#cb40-35"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb40-36"><a href="#cb40-36"></a>        <span class="co"># Calculate output values from the inputs</span></span>
<span id="cb40-37"><a href="#cb40-37"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb40-38"><a href="#cb40-38"></a>        <span class="va">self</span>.output <span class="op">=</span> np.maximum(<span class="dv">0</span>, inputs)</span>
<span id="cb40-39"><a href="#cb40-39"></a></span>
<span id="cb40-40"><a href="#cb40-40"></a>    <span class="co"># Backward pass</span></span>
<span id="cb40-41"><a href="#cb40-41"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_da):</span>
<span id="cb40-42"><a href="#cb40-42"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> dloss_da</span>
<span id="cb40-43"><a href="#cb40-43"></a>        <span class="va">self</span>.da_dz <span class="op">=</span> np.where(<span class="va">self</span>.inputs <span class="op">&gt;</span> <span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb40-44"><a href="#cb40-44"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> <span class="va">self</span>.dloss_da <span class="op">*</span> <span class="va">self</span>.da_dz</span>
<span id="cb40-45"><a href="#cb40-45"></a></span>
<span id="cb40-46"><a href="#cb40-46"></a></span>
<span id="cb40-47"><a href="#cb40-47"></a><span class="co"># Create dataset</span></span>
<span id="cb40-48"><a href="#cb40-48"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">2.5</span>], [<span class="dv">2</span>, <span class="dv">5</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">2.7</span>, <span class="fl">3.3</span>, <span class="op">-</span><span class="fl">0.8</span>]])</span>
<span id="cb40-49"><a href="#cb40-49"></a></span>
<span id="cb40-50"><a href="#cb40-50"></a><span class="co"># Create a dense layer with 4 input features and 3 output values</span></span>
<span id="cb40-51"><a href="#cb40-51"></a>dense1 <span class="op">=</span> DenseLayer(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb40-52"><a href="#cb40-52"></a>relu <span class="op">=</span> ReLUActivation()</span>
<span id="cb40-53"><a href="#cb40-53"></a></span>
<span id="cb40-54"><a href="#cb40-54"></a><span class="co"># Perform a forward pass of our training data through this layer</span></span>
<span id="cb40-55"><a href="#cb40-55"></a>dense1.forward(X)</span>
<span id="cb40-56"><a href="#cb40-56"></a>relu.forward(dense1.output)</span>
<span id="cb40-57"><a href="#cb40-57"></a></span>
<span id="cb40-58"><a href="#cb40-58"></a><span class="co"># Calculate loss</span></span>
<span id="cb40-59"><a href="#cb40-59"></a>y_pred <span class="op">=</span> np.<span class="bu">sum</span>(relu.output)</span>
<span id="cb40-60"><a href="#cb40-60"></a>y_true <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb40-61"><a href="#cb40-61"></a>loss <span class="op">=</span> (y_pred <span class="op">-</span> y_true) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb40-62"><a href="#cb40-62"></a></span>
<span id="cb40-63"><a href="#cb40-63"></a><span class="co"># Gradient of the loss with respect to y</span></span>
<span id="cb40-64"><a href="#cb40-64"></a>dloss_dy <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (y_pred <span class="op">-</span> y_true)</span>
<span id="cb40-65"><a href="#cb40-65"></a>dy_da <span class="op">=</span> np.ones_like(relu.output)</span>
<span id="cb40-66"><a href="#cb40-66"></a>dloss_da <span class="op">=</span> dloss_dy <span class="op">*</span> dy_da</span>
<span id="cb40-67"><a href="#cb40-67"></a></span>
<span id="cb40-68"><a href="#cb40-68"></a>relu.backward(dloss_da)</span>
<span id="cb40-69"><a href="#cb40-69"></a>dense1.backward(relu.dloss_dz)</span>
<span id="cb40-70"><a href="#cb40-70"></a><span class="bu">print</span>(<span class="ss">f"dloss_dweights = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dweights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-71"><a href="#cb40-71"></a><span class="bu">print</span>(<span class="ss">f"dloss_dbiases = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dbiases<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-72"><a href="#cb40-72"></a><span class="bu">print</span>(<span class="ss">f"dloss_dinputs = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dinputs<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dloss_dweights = [[124.560005 805.48804  440.112    307.24802 ]
 [124.560005 805.48804  440.112    307.24802 ]
 [124.560005 805.48804  440.112    307.24802 ]]
dloss_dbiases = [[249.12000303 249.12000303 249.12000303]]
dloss_dinputs = [[124.560005 149.472    174.384    199.296   ]
 [124.560005 149.472    174.384    199.296   ]
 [124.560005 149.472    174.384    199.296   ]]</code></pre>
</div>
</div>
</section>
<section id="categorical-cross-entropy-loss-derivative" class="level2">
<h2 class="anchored" data-anchor-id="categorical-cross-entropy-loss-derivative">Categorical cross-entropy loss derivative</h2>
<p>The cross-entropy loss of the <span class="math inline">\(i\)</span>-th sample is given by:</p>
<p><span class="math display">\[\begin{align*}
L_i = -\sum_k y_{ik}log(\hat{y}_ik)
\end{align*}\]</span></p>
<p>Differentiating with respect to <span class="math inline">\(\hat{y}_{ij}\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L_i}{\partial \hat{y}_{ij}} &amp;= -\frac{\partial}{\partial \hat{y}_{ik}} \left[\sum_k y_{ik}\log (\hat{y}_{ik})\right] \\
&amp;= -y_{ij} \cdot \frac{\partial }{\partial \hat{y}_{ij}} \log (\hat{y}_{ij})\\
&amp;= -\frac{y_{ij}}{\hat{y}_{ij}}
\end{align*}\]</span></p>
<section id="adding-backward-to-categoricalcrossentropyloss" class="level3">
<h3 class="anchored" data-anchor-id="adding-backward-to-categoricalcrossentropyloss">Adding <code>backward()</code> to <code>CategoricalCrossEntropyLoss</code></h3>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="co"># Cross-Entropy loss</span></span>
<span id="cb42-2"><a href="#cb42-2"></a><span class="kw">class</span> CategoricalCrossEntropyLoss(Loss):</span>
<span id="cb42-3"><a href="#cb42-3"></a></span>
<span id="cb42-4"><a href="#cb42-4"></a>    <span class="co"># Forward pass</span></span>
<span id="cb42-5"><a href="#cb42-5"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb42-6"><a href="#cb42-6"></a>        num_samples <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb42-7"><a href="#cb42-7"></a></span>
<span id="cb42-8"><a href="#cb42-8"></a>        <span class="co"># Clip data to prevent division by 0</span></span>
<span id="cb42-9"><a href="#cb42-9"></a>        <span class="co"># Clip both sides to not drag mean towards any value</span></span>
<span id="cb42-10"><a href="#cb42-10"></a>        epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb42-11"><a href="#cb42-11"></a>        y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span> <span class="op">-</span> epsilon)</span>
<span id="cb42-12"><a href="#cb42-12"></a></span>
<span id="cb42-13"><a href="#cb42-13"></a>        <span class="co"># If categorical labels</span></span>
<span id="cb42-14"><a href="#cb42-14"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb42-15"><a href="#cb42-15"></a>            correct_confidences <span class="op">=</span> y_pred_clipped[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb42-16"><a href="#cb42-16"></a>        <span class="co"># else if one-hot encoding</span></span>
<span id="cb42-17"><a href="#cb42-17"></a>        <span class="cf">elif</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb42-18"><a href="#cb42-18"></a>            correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred_clipped <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-19"><a href="#cb42-19"></a></span>
<span id="cb42-20"><a href="#cb42-20"></a>        neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb42-21"><a href="#cb42-21"></a>        <span class="cf">return</span> neg_log</span>
<span id="cb42-22"><a href="#cb42-22"></a></span>
<span id="cb42-23"><a href="#cb42-23"></a>    <span class="co"># Backward pass</span></span>
<span id="cb42-24"><a href="#cb42-24"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb42-25"><a href="#cb42-25"></a></span>
<span id="cb42-26"><a href="#cb42-26"></a>        <span class="co"># number of samples</span></span>
<span id="cb42-27"><a href="#cb42-27"></a>        batch_size <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb42-28"><a href="#cb42-28"></a></span>
<span id="cb42-29"><a href="#cb42-29"></a>        <span class="co"># number of labels</span></span>
<span id="cb42-30"><a href="#cb42-30"></a>        num_labels <span class="op">=</span> <span class="bu">len</span>(y_pred[<span class="dv">0</span>])</span>
<span id="cb42-31"><a href="#cb42-31"></a></span>
<span id="cb42-32"><a href="#cb42-32"></a>        <span class="co"># If labels are sparse, turn them into a one-hot vector</span></span>
<span id="cb42-33"><a href="#cb42-33"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb42-34"><a href="#cb42-34"></a>            y_true <span class="op">=</span> np.eye(num_labels)[y_true]</span>
<span id="cb42-35"><a href="#cb42-35"></a></span>
<span id="cb42-36"><a href="#cb42-36"></a>        <span class="co"># Calculate gradient</span></span>
<span id="cb42-37"><a href="#cb42-37"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> <span class="op">-</span>y_true <span class="op">/</span> y_pred</span>
<span id="cb42-38"><a href="#cb42-38"></a></span>
<span id="cb42-39"><a href="#cb42-39"></a>        <span class="co"># Normalize the gradient</span></span>
<span id="cb42-40"><a href="#cb42-40"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> <span class="va">self</span>.dloss_da <span class="op">/</span> batch_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="softmax-activation-function-derivative" class="level2">
<h2 class="anchored" data-anchor-id="softmax-activation-function-derivative">Softmax Activation function derivative</h2>
<p>We are interested to calculate the derivative of the softmax function. The softmax activation function is defined as:</p>
<p><span class="math display">\[\begin{align*}
S_{i,j} &amp;= \frac{e^{z_{i,j}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(S_{i,j}\)</span> denotes the output of the <span class="math inline">\(j\)</span>-th neuron for the <span class="math inline">\(i\)</span>-th sample. Thus, <span class="math inline">\(S_{i,j} = f(z_{i,1},\ldots,z_{i,d_l})\)</span>. Let’s calculate the partial derivative of <span class="math inline">\(S_{i,j}\)</span> with respect to <span class="math inline">\(z_{i,k}\)</span>.</p>
<p>By the <span class="math inline">\(u/v\)</span> rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= \frac{\sum_{l=1}^{d_l} e^{z_{i,l}} \cdot \frac{\partial e^{z_{i,j}}}{\partial z_{i,k}}-e^{z_{i,j}} \cdot \frac{\partial}{\partial z_{i,k}} \sum_{l=1}^{d_l} e^{z_{i,l}}}{\left(\sum_{l=1}^{d_l} e^{z_{i,l}}\right)^2}
\end{align*}\]</span></p>
<p>We have two cases. If <span class="math inline">\(j=k\)</span>, then <span class="math inline">\(\frac{\partial e^{z_{i,j}}}{\partial z_{i,k}} = e^{z_{i,k}}\)</span> and we get:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= \frac{e^{z_{i,k}} \cdot \sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}} \cdot e^{z_{i,k}}}{\left(\sum_{l=1}^{d_l} e^{z_{i,l}}\right)^2}\\
&amp;=\frac{e^{z_{i,k}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}} \cdot \frac{\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}}\\
&amp;=S_{i,k}(1-S_{i,k})
\end{align*}\]</span></p>
<p>In the case where <span class="math inline">\(j \neq k\)</span>, <span class="math inline">\(\frac{\partial e^{z_{i,j}}}{\partial z_{i,k}} = 0\)</span> and we have:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= -\frac{e^{z_{i,j}}}{\sum_{l=1}^{d_l}e^{z_{i,l}}}\cdot \frac{e^{z_{i,k}}}{\sum_{l=1}^{d_l}e^{z_{i,l}}}\\
&amp;=-S_{i,j} S_{i,k}
\end{align*}\]</span></p>
<p>So, the derivative of the softmax activation function can be expressed in terms of Kronecker’s delta as:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= S_{i,j}(\delta_{j,k} -  S_{i,k})\\
&amp;= S_{i,j} \delta_{j,k} - S_{i,j}S_{i,k}
\end{align*}\]</span></p>
<p>Now, like before, let’s say we have neural network with a single hidden layer with <span class="math inline">\(d_1 = 3\)</span> neurons. We apply the softmax activation function to the output of this layer. The jacobian matrix <span class="math inline">\(\frac{\partial S_i}{\partial z_i}\)</span> for the <span class="math inline">\(i\)</span>-th sample can be expressed as:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_i}{\partial z_i} &amp;=
\begin{bmatrix}
\frac{\partial S_{i1}}{\partial z_{i1}} &amp; \frac{\partial S_{i1}}{\partial z_{i2}} &amp; \frac{\partial S_{i1}}{\partial z_{i3}} \\
\frac{\partial S_{i2}}{\partial z_{i1}} &amp; \frac{\partial S_{i2}}{\partial z_{i2}} &amp; \frac{\partial S_{i2}}{\partial z_{i3}} \\
\frac{\partial S_{i3}}{\partial z_{i1}} &amp; \frac{\partial S_{i3}}{\partial z_{i2}} &amp; \frac{\partial S_{i3}}{\partial z_{i3}}
\end{bmatrix}\\
&amp;=\begin{bmatrix}
S_{i1}(\delta_{11} - S_{i1}) &amp; S_{i1}(\delta_{12} - S_{i2}) &amp; S_{i1}(\delta_{13} - S_{i3}) \\
S_{i2}(\delta_{21} - S_{i1}) &amp; S_{i2}(\delta_{22} - S_{i2}) &amp; S_{i2}(\delta_{23} - S_{i3}) \\
S_{i3}(\delta_{31} - S_{i1}) &amp; S_{i3}(\delta_{32} - S_{i2}) &amp; S_{i3}(\delta_{33} - S_{i3})
\end{bmatrix}\\
&amp;=\begin{bmatrix}
S_{i1}(1 - S_{i1}) &amp; S_{i1}(0 - S_{i2}) &amp; S_{i1}(0 - S_{i3}) \\
S_{i2}(0 - S_{i1}) &amp; S_{i2}(1 - S_{i2}) &amp; S_{i2}(0 - S_{i3}) \\
S_{i3}(0 - S_{i1}) &amp; S_{i3}(0 - S_{i2}) &amp; S_{i3}(1 - S_{i3})
\end{bmatrix}\\
&amp;=\begin{bmatrix}
S_{i1}\\
S_{i2}\\
S_{i3}
\end{bmatrix}\odot
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} -
\begin{bmatrix}
S_{i1}\\
S_{i2}\\
S_{i3}
\end{bmatrix}\begin{bmatrix}
S_{i1} &amp; S_{i2} &amp; S_{i3}
\end{bmatrix}
\end{align*}\]</span></p>
<p>Say the <code>softmax_output=[0.70, 0.10, 0.20]</code>. Then, in python, we can find the Jacobian matrix as:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-2"><a href="#cb43-2"></a></span>
<span id="cb43-3"><a href="#cb43-3"></a>softmax_output <span class="op">=</span> np.array([<span class="fl">0.70</span>, <span class="fl">0.10</span>, <span class="fl">0.20</span>])</span>
<span id="cb43-4"><a href="#cb43-4"></a></span>
<span id="cb43-5"><a href="#cb43-5"></a><span class="co"># Reshape as a column vector</span></span>
<span id="cb43-6"><a href="#cb43-6"></a>softmax_output <span class="op">=</span> softmax_output.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb43-7"><a href="#cb43-7"></a></span>
<span id="cb43-8"><a href="#cb43-8"></a>da_dz <span class="op">=</span> np.diagflat(softmax_output) <span class="op">-</span> np.dot(softmax_output, softmax_output.T)</span>
<span id="cb43-9"><a href="#cb43-9"></a></span>
<span id="cb43-10"><a href="#cb43-10"></a><span class="bu">print</span>(<span class="ss">f"softmax_output = </span><span class="sc">{</span>softmax_output<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-11"><a href="#cb43-11"></a><span class="bu">print</span>(<span class="ss">f"da_dz = </span><span class="sc">{</span>da_dz<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>softmax_output = [[0.7]
 [0.1]
 [0.2]]
da_dz = [[ 0.20999999 -0.07       -0.14      ]
 [-0.07        0.09       -0.02      ]
 [-0.14       -0.02        0.16      ]]</code></pre>
</div>
</div>
<p>What happens when we have a batch of inputs? By the chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial z_{11}} &amp;= \frac{\partial L}{\partial S_{11}} \cdot \frac{\partial S_{11}}{\partial z_{11}} + \frac{\partial L}{\partial S_{12}} \cdot \frac{\partial S_{12}}{\partial z_{11}} + \frac{\partial L}{\partial S_{13}}\cdot \frac{\partial S_{13}}{\partial z_{11}}
\end{align*}\]</span></p>
<p>In general,</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial z_{ij}} &amp;= \frac{\partial L}{\partial S_{i1}} \cdot \frac{\partial S_{i1}}{\partial z_{ij}} + \frac{\partial L}{\partial S_{i2}} \cdot \frac{\partial S_{i2}}{\partial z_{ij}} + \frac{\partial L}{\partial S_{i3}}\cdot \frac{\partial S_{i3}}{\partial z_{ij}}\\
&amp;=\sum_{k=1}^{3} \frac{\partial L}{\partial S_{ik}} \cdot \frac{\partial S_{ik}}{\partial z_{ij}}
\end{align*}\]</span></p>
<p>It follows that:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial z_i} &amp;= \begin{bmatrix}
\frac{\partial L}{\partial z_{i1}} &amp; \frac{\partial L}{\partial z_{i2}} &amp; \frac{\partial L}{\partial z_{i3}}
\end{bmatrix}\\
&amp;=\begin{bmatrix}
\frac{\partial L}{\partial S_{i1}} &amp; \frac{\partial L}{\partial S_{i2}} &amp; \frac{\partial L}{\partial S_{i3}}
\end{bmatrix} \begin{bmatrix}
\frac{\partial S_{i1}}{\partial z_{i1}} &amp; \frac{\partial S_{i1}}{\partial z_{i2}} &amp; \frac{\partial S_{i1}}{\partial z_{i3}} \\
\frac{\partial S_{i2}}{\partial z_{i1}} &amp; \frac{\partial S_{i2}}{\partial z_{i2}} &amp; \frac{\partial S_{i2}}{\partial z_{i3}} \\
\frac{\partial S_{i3}}{\partial z_{i1}} &amp; \frac{\partial S_{i3}}{\partial z_{i2}} &amp; \frac{\partial S_{i3}}{\partial z_{i3}}
\end{bmatrix}\\
&amp;=\frac{\partial L}{\partial S_i} \cdot \frac{\partial S_i}{\partial z_i}
\end{align*}\]</span></p>
<p>Now, <span class="math inline">\(\partial L/\partial S_i\)</span> has shape <code>[1,3]</code> and <span class="math inline">\(\partial S_i/\partial z_i\)</span> is a matrix of size <code>[3,3]</code>. So, <span class="math inline">\(\partial L/\partial z_i\)</span> will have dimensions <code>[1,3]</code>.</p>
</section>
<section id="softmax-backward-implementation" class="level2">
<h2 class="anchored" data-anchor-id="softmax-backward-implementation">Softmax <code>backward()</code> implementation</h2>
<p>We are now in a position to add <code>backward()</code> pass to the <code>SoftmaxActivation</code> layer.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="kw">class</span> SoftmaxActivation:</span>
<span id="cb45-2"><a href="#cb45-2"></a></span>
<span id="cb45-3"><a href="#cb45-3"></a>    <span class="co"># Forward pass</span></span>
<span id="cb45-4"><a href="#cb45-4"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb45-5"><a href="#cb45-5"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb45-6"><a href="#cb45-6"></a>        exp_values <span class="op">=</span> np.exp(inputs <span class="op">-</span> np.<span class="bu">max</span>(inputs, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb45-7"><a href="#cb45-7"></a>        probabilities <span class="op">=</span> exp_values <span class="op">/</span> np.<span class="bu">sum</span>(exp_values, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-8"><a href="#cb45-8"></a>        <span class="va">self</span>.output <span class="op">=</span> probabilities</span>
<span id="cb45-9"><a href="#cb45-9"></a></span>
<span id="cb45-10"><a href="#cb45-10"></a>    <span class="co"># Backward pass</span></span>
<span id="cb45-11"><a href="#cb45-11"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_da):</span>
<span id="cb45-12"><a href="#cb45-12"></a>        dloss_dz <span class="op">=</span> []</span>
<span id="cb45-13"><a href="#cb45-13"></a>        n <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.output)</span>
<span id="cb45-14"><a href="#cb45-14"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb45-15"><a href="#cb45-15"></a>            softmax_output <span class="op">=</span> <span class="va">self</span>.output[i]</span>
<span id="cb45-16"><a href="#cb45-16"></a></span>
<span id="cb45-17"><a href="#cb45-17"></a>            <span class="co"># Reshape as a column vector</span></span>
<span id="cb45-18"><a href="#cb45-18"></a>            softmax_output <span class="op">=</span> softmax_output.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb45-19"><a href="#cb45-19"></a></span>
<span id="cb45-20"><a href="#cb45-20"></a>            dsoftmax_dz <span class="op">=</span> np.diagflat(softmax_output) <span class="op">-</span> np.dot(</span>
<span id="cb45-21"><a href="#cb45-21"></a>                softmax_output, softmax_output.T</span>
<span id="cb45-22"><a href="#cb45-22"></a>            )</span>
<span id="cb45-23"><a href="#cb45-23"></a>            dloss_dz.append(np.dot(dloss_da[i], dsoftmax_dz))</span>
<span id="cb45-24"><a href="#cb45-24"></a></span>
<span id="cb45-25"><a href="#cb45-25"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> np.array(dloss_dz)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="categorical-cross-entropy-loss-and-softmax-activation-function-derivative" class="level2">
<h2 class="anchored" data-anchor-id="categorical-cross-entropy-loss-and-softmax-activation-function-derivative">Categorical cross-entropy loss and softmax activation function derivative</h2>
<p>The derivative of the categorical cross entropy loss and softmax activation function can be combined and results in a faster and simple implementation. The current implementation of the <code>backward</code> function in <code>SoftMaxActivation</code> is not vectorized and has a loop.</p>
<p>Let’s focus again on <span class="math inline">\(\frac{\partial L_{i}}{\partial z_{ij}}\)</span>. We have:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L_i}{\partial z_{ij}} &amp;= \sum_{k} \frac{\partial L_i}{\partial S_{ik}} \frac{\partial S_{ik}}{\partial z_{ij}} \\
&amp;= \frac{\partial L_i}{S_{ij}} \cdot \frac{\partial S_{ij}}{\partial z_{ij}} + \sum_{k\neq j}\frac{\partial L_i}{\partial S_{ik}} \frac{\partial S_{ik}}{\partial z_{ij}} \\
&amp;= -\frac{y_{ij}}{\hat{y}_{ij}}\hat{y}_{ij}(1-\hat{y}_{ij}) + \sum_{k \neq j}-\frac{y_{ik}}{\hat{y}_{ik}}\cdot \hat{y}_{ik}(0 - \hat{y}_{ij})\\
&amp;= -\frac{y_{ij}}{\cancel{\hat{y}_{ij}}}\cancel{\hat{y}_{ij}}(1-\hat{y}_{ij}) + \sum_{k \neq j}-\frac{y_{ik}}{\cancel{\hat{y}_{ik}}}\cdot \cancel{\hat{y}_{ik}}(0 - \hat{y}_{ij})\\
&amp;= -y_{ij} + y_{ij}\hat{y}_{ij} + \sum_{k\neq j}y_{ik} \hat{y}_{ij}\\
&amp;= -y_{ij} + \hat{y}_{ij}(\sum_{k}y_{ik})\\
&amp;= \hat{y}_{ij} - y_{ij}
\end{align*}\]</span></p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="kw">class</span> CategoricalCrossEntropySoftmax:</span>
<span id="cb46-2"><a href="#cb46-2"></a></span>
<span id="cb46-3"><a href="#cb46-3"></a>    <span class="co"># create activation and loss function objects</span></span>
<span id="cb46-4"><a href="#cb46-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb46-5"><a href="#cb46-5"></a>        <span class="va">self</span>.activation <span class="op">=</span> SoftmaxActivation()</span>
<span id="cb46-6"><a href="#cb46-6"></a>        <span class="va">self</span>.loss <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb46-7"><a href="#cb46-7"></a></span>
<span id="cb46-8"><a href="#cb46-8"></a>    <span class="co"># forward pass</span></span>
<span id="cb46-9"><a href="#cb46-9"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs, y_true):</span>
<span id="cb46-10"><a href="#cb46-10"></a></span>
<span id="cb46-11"><a href="#cb46-11"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb46-12"><a href="#cb46-12"></a>        <span class="va">self</span>.activation.forward(inputs)</span>
<span id="cb46-13"><a href="#cb46-13"></a></span>
<span id="cb46-14"><a href="#cb46-14"></a>        <span class="va">self</span>.output <span class="op">=</span> <span class="va">self</span>.activation.output</span>
<span id="cb46-15"><a href="#cb46-15"></a></span>
<span id="cb46-16"><a href="#cb46-16"></a>        <span class="cf">return</span> <span class="va">self</span>.loss.calculate(<span class="va">self</span>.output, y_true)</span>
<span id="cb46-17"><a href="#cb46-17"></a></span>
<span id="cb46-18"><a href="#cb46-18"></a>    <span class="co"># Backward pass</span></span>
<span id="cb46-19"><a href="#cb46-19"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb46-20"><a href="#cb46-20"></a>        <span class="co"># number of samples</span></span>
<span id="cb46-21"><a href="#cb46-21"></a>        batch_size <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb46-22"><a href="#cb46-22"></a></span>
<span id="cb46-23"><a href="#cb46-23"></a>        <span class="co"># number of labels</span></span>
<span id="cb46-24"><a href="#cb46-24"></a>        num_labels <span class="op">=</span> <span class="bu">len</span>(y_pred[<span class="dv">0</span>])</span>
<span id="cb46-25"><a href="#cb46-25"></a></span>
<span id="cb46-26"><a href="#cb46-26"></a>        <span class="co"># If labels are sparse, turn them into a one-hot vector</span></span>
<span id="cb46-27"><a href="#cb46-27"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb46-28"><a href="#cb46-28"></a>            y_true <span class="op">=</span> np.eye(num_labels)[y_true]</span>
<span id="cb46-29"><a href="#cb46-29"></a></span>
<span id="cb46-30"><a href="#cb46-30"></a>        <span class="co"># Calculate the gradient</span></span>
<span id="cb46-31"><a href="#cb46-31"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> y_pred <span class="op">-</span> y_true</span>
<span id="cb46-32"><a href="#cb46-32"></a></span>
<span id="cb46-33"><a href="#cb46-33"></a>        <span class="co"># Normalize the gradient</span></span>
<span id="cb46-34"><a href="#cb46-34"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> <span class="va">self</span>.dloss_dz <span class="op">/</span> batch_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now test if the combined backward step returns the same values compared to when we backpropogate gradients through both of the functions separately.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-2"><a href="#cb47-2"></a><span class="im">import</span> nnfs</span>
<span id="cb47-3"><a href="#cb47-3"></a></span>
<span id="cb47-4"><a href="#cb47-4"></a>nnfs.init()</span>
<span id="cb47-5"><a href="#cb47-5"></a></span>
<span id="cb47-6"><a href="#cb47-6"></a>softmax_outputs <span class="op">=</span> np.array([[<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>], [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>], [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]])</span>
<span id="cb47-7"><a href="#cb47-7"></a></span>
<span id="cb47-8"><a href="#cb47-8"></a>class_targets <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb47-9"><a href="#cb47-9"></a></span>
<span id="cb47-10"><a href="#cb47-10"></a></span>
<span id="cb47-11"><a href="#cb47-11"></a>activation <span class="op">=</span> SoftmaxActivation()</span>
<span id="cb47-12"><a href="#cb47-12"></a>activation.output <span class="op">=</span> softmax_outputs</span>
<span id="cb47-13"><a href="#cb47-13"></a></span>
<span id="cb47-14"><a href="#cb47-14"></a>loss <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb47-15"><a href="#cb47-15"></a>loss.backward(softmax_outputs, class_targets)</span>
<span id="cb47-16"><a href="#cb47-16"></a><span class="bu">print</span>(<span class="st">"Gradients : separate loss and activation"</span>)</span>
<span id="cb47-17"><a href="#cb47-17"></a><span class="bu">print</span>(<span class="ss">f"dloss_da = </span><span class="sc">{</span>loss<span class="sc">.</span>dloss_da<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-18"><a href="#cb47-18"></a></span>
<span id="cb47-19"><a href="#cb47-19"></a>activation.backward(loss.dloss_da)</span>
<span id="cb47-20"><a href="#cb47-20"></a><span class="bu">print</span>(<span class="ss">f"dloss_dz = </span><span class="sc">{</span>activation<span class="sc">.</span>dloss_dz<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-21"><a href="#cb47-21"></a></span>
<span id="cb47-22"><a href="#cb47-22"></a>softmax_cce <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb47-23"><a href="#cb47-23"></a>softmax_cce.backward(softmax_outputs, class_targets)</span>
<span id="cb47-24"><a href="#cb47-24"></a><span class="bu">print</span>(<span class="st">"Gradients : combined loss and activation"</span>)</span>
<span id="cb47-25"><a href="#cb47-25"></a><span class="bu">print</span>(<span class="ss">f"dloss_dz = </span><span class="sc">{</span>softmax_cce<span class="sc">.</span>dloss_dz<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Gradients : separate loss and activation
dloss_da = [[-0.47619048 -0.         -0.        ]
 [-0.         -0.66666667 -0.        ]
 [-0.         -0.37037037 -0.        ]]
dloss_dz = [[-0.09999999  0.03333334  0.06666667]
 [ 0.03333334 -0.16666667  0.13333334]
 [ 0.00666667 -0.03333333  0.02666667]]
Gradients : combined loss and activation
dloss_dz = [[-0.1         0.03333333  0.06666667]
 [ 0.03333333 -0.16666667  0.13333333]
 [ 0.00666667 -0.03333333  0.02666667]]</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-06-05">

<title>Backpropogation â€“ quantdev.blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-d129a44951930463e8a313df5966fbea.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-52232225ae3909a0ea66e9fd7c84c945.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Backpropogation</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 5, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#calculating-the-network-error-with-loss" id="toc-calculating-the-network-error-with-loss" class="nav-link active" data-scroll-target="#calculating-the-network-error-with-loss">Calculating the network error with Loss</a>
  <ul class="collapse">
  <li><a href="#logit-vector" id="toc-logit-vector" class="nav-link" data-scroll-target="#logit-vector">Logit vector</a></li>
  <li><a href="#entropy-cross-entropy-and-kl-divergence" id="toc-entropy-cross-entropy-and-kl-divergence" class="nav-link" data-scroll-target="#entropy-cross-entropy-and-kl-divergence">Entropy, Cross-Entropy and KL-Divergence</a></li>
  <li><a href="#categorical-cross-entropy-loss-function" id="toc-categorical-cross-entropy-loss-function" class="nav-link" data-scroll-target="#categorical-cross-entropy-loss-function">Categorical cross-entropy loss function</a></li>
  </ul></li>
  <li><a href="#categorical-cross-entropy-loss-class" id="toc-categorical-cross-entropy-loss-class" class="nav-link" data-scroll-target="#categorical-cross-entropy-loss-class">Categorical Cross-Entropy Loss Class</a></li>
  <li><a href="#backpropogation" id="toc-backpropogation" class="nav-link" data-scroll-target="#backpropogation">Backpropogation</a></li>
  <li><a href="#backprop-for-a-single-neuron---a-python-implementation" id="toc-backprop-for-a-single-neuron---a-python-implementation" class="nav-link" data-scroll-target="#backprop-for-a-single-neuron---a-python-implementation">Backprop for a single neuron - a python implementation</a></li>
  <li><a href="#backprop-for-a-layer-of-neurons" id="toc-backprop-for-a-layer-of-neurons" class="nav-link" data-scroll-target="#backprop-for-a-layer-of-neurons">Backprop for a layer of neurons</a>
  <ul class="collapse">
  <li><a href="#gradient-of-the-loss-with-respect-to-haty" id="toc-gradient-of-the-loss-with-respect-to-haty" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-haty">Gradient of the loss with respect to <span class="math inline">\(\hat{y}\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-a" id="toc-gradient-of-the-loss-with-respect-to-a" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-a">Gradient of the loss with respect to <span class="math inline">\(a\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-z" id="toc-gradient-of-the-loss-with-respect-to-z" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-z">Gradient of the loss with respect to <span class="math inline">\(z\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-weights-w" id="toc-gradient-of-the-loss-with-respect-to-weights-w" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-weights-w">Gradient of the loss with respect to weights <span class="math inline">\(W\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-the-biases-b" id="toc-gradient-of-the-loss-with-respect-to-the-biases-b" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-the-biases-b">Gradient of the loss with respect to the biases <span class="math inline">\(b\)</span></a></li>
  <li><a href="#naive-python-implementation" id="toc-naive-python-implementation" class="nav-link" data-scroll-target="#naive-python-implementation">Naive Python implementation</a></li>
  </ul></li>
  <li><a href="#backprop-with-a-batch-of-inputs" id="toc-backprop-with-a-batch-of-inputs" class="nav-link" data-scroll-target="#backprop-with-a-batch-of-inputs">Backprop with a batch of inputs</a>
  <ul class="collapse">
  <li><a href="#gradient-of-the-loss-with-respect-to-weights-w-1" id="toc-gradient-of-the-loss-with-respect-to-weights-w-1" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-weights-w-1">Gradient of the loss with respect to weights <span class="math inline">\(w\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-the-biases-b-1" id="toc-gradient-of-the-loss-with-respect-to-the-biases-b-1" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-the-biases-b-1">Gradient of the loss with respect to the biases <span class="math inline">\(b\)</span></a></li>
  <li><a href="#gradient-of-the-loss-with-respect-to-the-inputs" id="toc-gradient-of-the-loss-with-respect-to-the-inputs" class="nav-link" data-scroll-target="#gradient-of-the-loss-with-respect-to-the-inputs">Gradient of the loss with respect to the inputs</a></li>
  </ul></li>
  <li><a href="#adding-backward-to-denselayer" id="toc-adding-backward-to-denselayer" class="nav-link" data-scroll-target="#adding-backward-to-denselayer">Adding <code>backward()</code> to <code>DenseLayer</code></a></li>
  <li><a href="#categorical-cross-entropy-loss-derivative" id="toc-categorical-cross-entropy-loss-derivative" class="nav-link" data-scroll-target="#categorical-cross-entropy-loss-derivative">Categorical cross-entropy loss derivative</a>
  <ul class="collapse">
  <li><a href="#adding-backward-to-categoricalcrossentropyloss" id="toc-adding-backward-to-categoricalcrossentropyloss" class="nav-link" data-scroll-target="#adding-backward-to-categoricalcrossentropyloss">Adding <code>backward()</code> to <code>CategoricalCrossEntropyLoss</code></a></li>
  </ul></li>
  <li><a href="#softmax-activation-function-derivative" id="toc-softmax-activation-function-derivative" class="nav-link" data-scroll-target="#softmax-activation-function-derivative">Softmax Activation function derivative</a></li>
  <li><a href="#softmax-backward-implementation" id="toc-softmax-backward-implementation" class="nav-link" data-scroll-target="#softmax-backward-implementation">Softmax <code>backward()</code> implementation</a></li>
  <li><a href="#categorical-cross-entropy-loss-and-softmax-activation-function-derivative" id="toc-categorical-cross-entropy-loss-and-softmax-activation-function-derivative" class="nav-link" data-scroll-target="#categorical-cross-entropy-loss-and-softmax-activation-function-derivative">Categorical cross-entropy loss and softmax activation function derivative</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="calculating-the-network-error-with-loss" class="level2">
<h2 class="anchored" data-anchor-id="calculating-the-network-error-with-loss">Calculating the network error with Loss</h2>
<p>With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the modelâ€™s accuracy and confidence. To do this, we calculate the error in our model. The <em>loss function</em> also referred to as the <em>cost function</em> quantifies the error.</p>
<section id="logit-vector" class="level3">
<h3 class="anchored" data-anchor-id="logit-vector">Logit vector</h3>
<p>Let <span class="math inline">\(\vec{l} = \mathbf{w}\cdot \mathbf{x} + \mathbf{b}\)</span> be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the <strong>logit vector</strong> in machine learning literature.</p>
</section>
<section id="entropy-cross-entropy-and-kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="entropy-cross-entropy-and-kl-divergence">Entropy, Cross-Entropy and KL-Divergence</h3>
<p>Let <span class="math inline">\(X\)</span> be a random variable with possible outcomes <span class="math inline">\(\mathcal{X}\)</span>. Let <span class="math inline">\(P\)</span> be the true probability distribution of <span class="math inline">\(X\)</span> with probability mass function <span class="math inline">\(p(x)\)</span>. Let <span class="math inline">\(Q\)</span> be an approximating distribution with probability mass function <span class="math inline">\(q(x)\)</span>.</p>
<p><em>Definition</em>. The entropy of <span class="math inline">\(P\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
H(P) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log p(x)
\end{align*}\]</span></p>
<p>In information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm <span class="math inline">\(\log p(x)\)</span>, <em>we concentrate on the order of the surprise</em>. Entropy, then, is an expectation over the uncertainties or the <em>expected surprise</em>.</p>
<p><em>Definition</em>. The cross-entropy of <span class="math inline">\(Q\)</span> relative to <span class="math inline">\(P\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
H(P,Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log q(x)
\end{align*}\]</span></p>
<p><em>Definition</em>. For discrete distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> defined on the sample space <span class="math inline">\(\mathcal{X}\)</span>, the <em>Kullback-Leibler(KL) divergence</em> (or relative entropy) from <span class="math inline">\(Q\)</span> to <span class="math inline">\(P\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
D_{KL}(P||Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)}
\end{align*}\]</span></p>
<p>Intuitively, it is the expected excess surprise from using <span class="math inline">\(Q\)</span> as a model instead of <span class="math inline">\(P\)</span>, when the actual distribution is <span class="math inline">\(P\)</span>. Note that, <span class="math inline">\(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)</span>, so it is not symmetric and hence it is not a norm.</p>
</section>
<section id="categorical-cross-entropy-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="categorical-cross-entropy-loss-function">Categorical cross-entropy loss function</h3>
<p>We are going to work on a multi-class classification problem.</p>
<p>For any input <span class="math inline">\(\mathbf{x}_i\)</span>, the target vector <span class="math inline">\(\mathbf{y}_i\)</span> could be specified using <em>one-hot</em> encoding or an integer in the range <code>[0,numClasses)</code>.</p>
<p>Letâ€™s say, we have <code>numClasses = 3</code>.</p>
<p>In one-hot encoding, the target vector <code>y_true</code> is an array like <code>[1, 0, 0]</code>, <code>[0, 1, 0]</code>, or <code>[0, 0, 1]</code>. The category/class is determined by the index which is <strong>hot</strong>. For example, if <code>y_true</code> equals <code>[0, 1, 0]</code>, then the sample belongs to class <span class="math inline">\(1\)</span>, whilst if <code>y_true</code> equals <code>[0, 0, 1]</code>, the sample belongs to class <span class="math inline">\(2\)</span>.</p>
<p>In integer encoding, the target vector <code>y_true</code> is an integer. For example, if <code>y_true</code> equals <span class="math inline">\(1\)</span>, the sample belongs to class <span class="math inline">\(1\)</span>, whilst if <code>y_true</code> equals <span class="math inline">\(2\)</span>, the sample belongs to class <span class="math inline">\(2\)</span>.</p>
<p>The <code>categorical_crossentropy</code> is defined as:</p>
<p><span class="math display">\[\begin{align*}
L_i = -\sum_{j} y_{i,j} \log(\hat{y}_{i,j})
\end{align*}\]</span></p>
<p>Assume that we have a softmax output <span class="math inline">\(\hat{\mathbf{y}}_i\)</span>, <code>[0.7, 0.1, 0.2]</code> and target vector <span class="math inline">\(\mathbf{y}_i\)</span> <code>[1, 0, 0]</code>. Then, we can compute the categorical cross entropy loss as:</p>
<p><span class="math display">\[\begin{align*}
-\left(1\cdot \log (0.7) + 0 \cdot \log (0.1) + 0 \cdot \log(0.2)\right) = 0.35667494
\end{align*}\]</span></p>
<p>Letâ€™s that we have a batch of <span class="math inline">\(3\)</span> samples. Additionally, suppose the target <code>y_true</code> is integer encoded. After running through the softmax activation function, the networkâ€™s output layer yields:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>load_ext itikz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb2-4"><a href="#cb2-4"></a>    [</span>
<span id="cb2-5"><a href="#cb2-5"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb2-6"><a href="#cb2-6"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb2-7"><a href="#cb2-7"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb2-8"><a href="#cb2-8"></a>    ]</span>
<span id="cb2-9"><a href="#cb2-9"></a>)</span>
<span id="cb2-10"><a href="#cb2-10"></a></span>
<span id="cb2-11"><a href="#cb2-11"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="cf">for</span> targ_index, distribution <span class="kw">in</span> <span class="bu">zip</span>(y_true,y_pred):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="bu">print</span>(distribution[targ_index])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.7
0.5
0.08</code></pre>
</div>
</div>
<p>This can be simplified.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="bu">print</span>(y_pred[[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>],y_true])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.7  0.5  0.08]</code></pre>
</div>
</div>
<p><code>numpy</code> lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="bu">print</span>(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.7  0.5  0.08]</code></pre>
</div>
</div>
<p>The categorical cross-entropy loss for each of the samples is:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="bu">print</span>(<span class="op">-</span>np.log(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.35667494 0.69314718 2.52572864]</code></pre>
</div>
</div>
<p>Finally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>neg_log <span class="op">=</span> <span class="op">-</span>np.log(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true])</span>
<span id="cb11-2"><a href="#cb11-2"></a>average_loss <span class="op">=</span> np.mean(neg_log)</span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="bu">print</span>(average_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.191850256268978</code></pre>
</div>
</div>
<p>In the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If <code>y_true.shape</code> has <span class="math inline">\(2\)</span> dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if <code>y_true</code> is a list, that is <code>y_true.shape</code> has <span class="math inline">\(1\)</span> dimension, then it means, we have <em>sparse labels</em>/integer encoding.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2"></a></span>
<span id="cb13-3"><a href="#cb13-3"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb13-4"><a href="#cb13-4"></a>    [</span>
<span id="cb13-5"><a href="#cb13-5"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb13-6"><a href="#cb13-6"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb13-7"><a href="#cb13-7"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb13-8"><a href="#cb13-8"></a>    ]</span>
<span id="cb13-9"><a href="#cb13-9"></a>)</span>
<span id="cb13-10"><a href="#cb13-10"></a></span>
<span id="cb13-11"><a href="#cb13-11"></a>y_true <span class="op">=</span> np.array(</span>
<span id="cb13-12"><a href="#cb13-12"></a>    [</span>
<span id="cb13-13"><a href="#cb13-13"></a>        [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb13-14"><a href="#cb13-14"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb13-15"><a href="#cb13-15"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb13-16"><a href="#cb13-16"></a>    ]</span>
<span id="cb13-17"><a href="#cb13-17"></a>)</span>
<span id="cb13-18"><a href="#cb13-18"></a></span>
<span id="cb13-19"><a href="#cb13-19"></a>correct_confidences <span class="op">=</span> np.array([])</span>
<span id="cb13-20"><a href="#cb13-20"></a></span>
<span id="cb13-21"><a href="#cb13-21"></a><span class="co"># If categorical labels</span></span>
<span id="cb13-22"><a href="#cb13-22"></a><span class="cf">if</span>(<span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">1</span>):</span>
<span id="cb13-23"><a href="#cb13-23"></a>    correct_confidences <span class="op">=</span> y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb13-24"><a href="#cb13-24"></a><span class="cf">elif</span>(<span class="bu">len</span>(y_pred.shape)<span class="op">==</span><span class="dv">2</span>):</span>
<span id="cb13-25"><a href="#cb13-25"></a>    correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-26"><a href="#cb13-26"></a></span>
<span id="cb13-27"><a href="#cb13-27"></a>neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb13-28"><a href="#cb13-28"></a>average_loss <span class="op">=</span> np.mean(neg_log)</span>
<span id="cb13-29"><a href="#cb13-29"></a><span class="bu">print</span>(average_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.191850256268978</code></pre>
</div>
</div>
<p>If the neural network output <code>y_pred</code> for some reason is the vector <code>[1, 0, 0]</code>, this would result in <code>numpy.log</code> function returning a negative infinity. To avoid such situations, itâ€™s safer to apply a ceil and floor to <code>y_pred</code>.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span><span class="op">-</span>epsilon)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="categorical-cross-entropy-loss-class" class="level2">
<h2 class="anchored" data-anchor-id="categorical-cross-entropy-loss-class">Categorical Cross-Entropy Loss Class</h2>
<p>I first create an abstract base class <code>Loss</code>. Every <code>Loss</code> object exposes the <code>calculate</code> method which in turn calls <code>Loss</code> objectâ€™s forward method to compute the log-loss for each sample and then takes an average of the sample losses.</p>
<p><code>CategoricalCrossEntropyLoss</code> class is a child class of <code>Loss</code> and provides an implementation of the <code>forward</code> method.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="im">import</span> nnfs</span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="im">from</span> nnfs.datasets <span class="im">import</span> spiral_data</span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="im">from</span> abc <span class="im">import</span> abstractmethod</span>
<span id="cb16-5"><a href="#cb16-5"></a></span>
<span id="cb16-6"><a href="#cb16-6"></a></span>
<span id="cb16-7"><a href="#cb16-7"></a><span class="co"># Abstract base class for losses</span></span>
<span id="cb16-8"><a href="#cb16-8"></a><span class="kw">class</span> Loss:</span>
<span id="cb16-9"><a href="#cb16-9"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb16-10"><a href="#cb16-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb16-11"><a href="#cb16-11"></a>        <span class="cf">pass</span></span>
<span id="cb16-12"><a href="#cb16-12"></a></span>
<span id="cb16-13"><a href="#cb16-13"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb16-14"><a href="#cb16-14"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb16-15"><a href="#cb16-15"></a>        <span class="cf">pass</span></span>
<span id="cb16-16"><a href="#cb16-16"></a></span>
<span id="cb16-17"><a href="#cb16-17"></a>    <span class="co"># Calculates the data and regularization losses</span></span>
<span id="cb16-18"><a href="#cb16-18"></a>    <span class="co"># given model output and ground truth values</span></span>
<span id="cb16-19"><a href="#cb16-19"></a>    <span class="kw">def</span> calculate(<span class="va">self</span>, output, y):</span>
<span id="cb16-20"><a href="#cb16-20"></a></span>
<span id="cb16-21"><a href="#cb16-21"></a>        <span class="co"># Calculate the sample losses</span></span>
<span id="cb16-22"><a href="#cb16-22"></a>        sample_losses <span class="op">=</span> <span class="va">self</span>.forward(output, y)</span>
<span id="cb16-23"><a href="#cb16-23"></a></span>
<span id="cb16-24"><a href="#cb16-24"></a>        <span class="co"># Calculate the mean loss</span></span>
<span id="cb16-25"><a href="#cb16-25"></a>        data_loss <span class="op">=</span> np.mean(sample_losses)</span>
<span id="cb16-26"><a href="#cb16-26"></a></span>
<span id="cb16-27"><a href="#cb16-27"></a>        <span class="co"># Return loss</span></span>
<span id="cb16-28"><a href="#cb16-28"></a>        <span class="cf">return</span> data_loss</span>
<span id="cb16-29"><a href="#cb16-29"></a></span>
<span id="cb16-30"><a href="#cb16-30"></a></span>
<span id="cb16-31"><a href="#cb16-31"></a><span class="co"># Cross-Entropy loss</span></span>
<span id="cb16-32"><a href="#cb16-32"></a><span class="kw">class</span> CategoricalCrossEntropyLoss(Loss):</span>
<span id="cb16-33"><a href="#cb16-33"></a></span>
<span id="cb16-34"><a href="#cb16-34"></a>    <span class="co"># Forward pass</span></span>
<span id="cb16-35"><a href="#cb16-35"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb16-36"><a href="#cb16-36"></a>        num_samples <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb16-37"><a href="#cb16-37"></a></span>
<span id="cb16-38"><a href="#cb16-38"></a>        <span class="co"># Clip data to prevent division by 0</span></span>
<span id="cb16-39"><a href="#cb16-39"></a>        <span class="co"># Clip both sides to not drag mean towards any value</span></span>
<span id="cb16-40"><a href="#cb16-40"></a>        epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb16-41"><a href="#cb16-41"></a>        y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span> <span class="op">-</span> epsilon)</span>
<span id="cb16-42"><a href="#cb16-42"></a></span>
<span id="cb16-43"><a href="#cb16-43"></a>        <span class="co"># If categorical labels</span></span>
<span id="cb16-44"><a href="#cb16-44"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb16-45"><a href="#cb16-45"></a>            correct_confidences <span class="op">=</span> y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb16-46"><a href="#cb16-46"></a>        <span class="co"># else if one-hot encoding</span></span>
<span id="cb16-47"><a href="#cb16-47"></a>        <span class="cf">elif</span> <span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb16-48"><a href="#cb16-48"></a>            correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-49"><a href="#cb16-49"></a></span>
<span id="cb16-50"><a href="#cb16-50"></a>        neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb16-51"><a href="#cb16-51"></a>        <span class="cf">return</span> neg_log</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using the manual created outputs and targets, we have:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb17-2"><a href="#cb17-2"></a>    [</span>
<span id="cb17-3"><a href="#cb17-3"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb17-4"><a href="#cb17-4"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb17-5"><a href="#cb17-5"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb17-6"><a href="#cb17-6"></a>    ]</span>
<span id="cb17-7"><a href="#cb17-7"></a>)</span>
<span id="cb17-8"><a href="#cb17-8"></a></span>
<span id="cb17-9"><a href="#cb17-9"></a>y_true <span class="op">=</span> np.array(</span>
<span id="cb17-10"><a href="#cb17-10"></a>    [</span>
<span id="cb17-11"><a href="#cb17-11"></a>        [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb17-12"><a href="#cb17-12"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb17-13"><a href="#cb17-13"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb17-14"><a href="#cb17-14"></a>    ]</span>
<span id="cb17-15"><a href="#cb17-15"></a>)</span>
<span id="cb17-16"><a href="#cb17-16"></a></span>
<span id="cb17-17"><a href="#cb17-17"></a>loss_function <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb17-18"><a href="#cb17-18"></a>loss <span class="op">=</span> loss_function.calculate(y_pred, y_true)</span>
<span id="cb17-19"><a href="#cb17-19"></a><span class="bu">print</span>(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.191850256268978</code></pre>
</div>
</div>
</section>
<section id="backpropogation" class="level2">
<h2 class="anchored" data-anchor-id="backpropogation">Backpropogation</h2>
<p>Backpropogation consists going backwards along the edges and passing along gradients. We are going to chop up a neuron into itâ€™s elementary operations and draw a computational graph. Each node in the graph receives an upstream gradient. The goal is pass on the correct downstream gradient.</p>
<p>Each node has a <em>local gradient</em> - the gradient of itâ€™s output with respect to itâ€™s input. Consider a node receiving an input <span class="math inline">\(z\)</span> and producing an output <span class="math inline">\(h=f(z)\)</span>. Then, we have:</p>
<div class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb19-2"><a href="#cb19-2"></a>\begin{tikzpicture}</span>
<span id="cb19-3"><a href="#cb19-3"></a>    \node [circle,minimum size<span class="op">=</span><span class="dv">40</span><span class="er">mm</span>,draw] (f) at (<span class="dv">0</span>,<span class="dv">0</span>) {\huge $f$}<span class="op">;</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>    \node [blue] (localgrad) at (<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>) {\huge $\frac{\partial h}{\partial z}$}<span class="op">;</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>    \node [blue] (lgrad) at (<span class="fl">0.0</span>,<span class="dv">1</span>) {\large Local gradient}<span class="op">;</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="fl">1.80</span>,<span class="dv">1</span>) <span class="op">--</span> node [above,midway] {\huge $h$} (<span class="dv">5</span>,<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node [below,midway] {\huge $\frac{\partial s}{\partial h}$} (<span class="fl">1.80</span>,<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-8"><a href="#cb19-8"></a>    \node [] (upgrad) at (<span class="fl">4.0</span>,<span class="op">-</span><span class="dv">3</span>) {\huge Upstream gradient}<span class="op">;</span></span>
<span id="cb19-9"><a href="#cb19-9"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="op">-</span><span class="dv">5</span>,<span class="dv">1</span>) <span class="op">--</span> node [above,midway] {\huge $z$} (<span class="op">-</span><span class="fl">1.80</span>,<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="op">-</span><span class="fl">1.80</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node [below,midway] {\huge $\frac{\partial s}{\partial z} <span class="op">=</span> \frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial z}$} (<span class="op">-</span><span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-11"><a href="#cb19-11"></a>    \node [] (downgrad) at (<span class="op">-</span><span class="fl">4.0</span>,<span class="op">-</span><span class="dv">3</span>) {\huge Downstream gradient}<span class="op">;</span></span>
<span id="cb19-12"><a href="#cb19-12"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-13-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The downstream gradient <span class="math inline">\(\frac{\partial s}{\partial z}\)</span> equals the upstream graient <span class="math inline">\(\frac{\partial s}{\partial h}\)</span> times the local gradient <span class="math inline">\(\frac{\partial h}{\partial z}\)</span>.</p>
<p>What about nodes with multiple inputs? Say that, <span class="math inline">\(h=f(x,y)\)</span>. Multiple inputs imply multiple local gradients.</p>
<div class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb20-2"><a href="#cb20-2"></a>\begin{tikzpicture}[x<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,y<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,yscale<span class="op">=-</span><span class="dv">1</span>,scale<span class="op">=</span><span class="fl">1.75</span>]</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="op">%</span>uncomment <span class="cf">if</span> require: \path (<span class="dv">0</span>,<span class="dv">216</span>)<span class="op">;</span> <span class="op">%</span><span class="bu">set</span> diagram left start at <span class="dv">0</span>, <span class="kw">and</span> has height of <span class="dv">216</span></span>
<span id="cb20-4"><a href="#cb20-4"></a></span>
<span id="cb20-5"><a href="#cb20-5"></a><span class="op">%</span>Shape: Circle [<span class="bu">id</span>:dp08328772161506959] </span>
<span id="cb20-6"><a href="#cb20-6"></a>\draw   (<span class="fl">302.75</span>,<span class="fl">83.38</span>) .. controls (<span class="fl">302.75</span>,<span class="fl">53.62</span>) <span class="kw">and</span> (<span class="fl">326.87</span>,<span class="fl">29.5</span>) .. (<span class="fl">356.63</span>,<span class="fl">29.5</span>) .. controls (<span class="fl">386.38</span>,<span class="fl">29.5</span>) <span class="kw">and</span> (<span class="fl">410.5</span>,<span class="fl">53.62</span>) .. (<span class="fl">410.5</span>,<span class="fl">83.38</span>) .. controls (<span class="fl">410.5</span>,<span class="fl">113.13</span>) <span class="kw">and</span> (<span class="fl">386.38</span>,<span class="fl">137.25</span>) .. (<span class="fl">356.63</span>,<span class="fl">137.25</span>) .. controls (<span class="fl">326.87</span>,<span class="fl">137.25</span>) <span class="kw">and</span> (<span class="fl">302.75</span>,<span class="fl">113.13</span>) .. (<span class="fl">302.75</span>,<span class="fl">83.38</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb20-7"><a href="#cb20-7"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da2730189357413113] </span>
<span id="cb20-8"><a href="#cb20-8"></a>\draw    (<span class="dv">406</span>,<span class="fl">59.38</span>) <span class="op">--</span> (<span class="fl">513.5</span>,<span class="fl">59.74</span>) <span class="op">;</span></span>
<span id="cb20-9"><a href="#cb20-9"></a>\draw [shift<span class="op">=</span>{(<span class="fl">515.5</span>,<span class="fl">59.75</span>)}, rotate <span class="op">=</span> <span class="fl">180.2</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-10"><a href="#cb20-10"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da21080101466010737] </span>
<span id="cb20-11"><a href="#cb20-11"></a>\draw    (<span class="dv">515</span>,<span class="fl">110.75</span>) <span class="op">--</span> (<span class="dv">405</span>,<span class="fl">110.26</span>) <span class="op">;</span></span>
<span id="cb20-12"><a href="#cb20-12"></a>\draw [shift<span class="op">=</span>{(<span class="dv">403</span>,<span class="fl">110.25</span>)}, rotate <span class="op">=</span> <span class="fl">0.26</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-13"><a href="#cb20-13"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da05192158713361961] </span>
<span id="cb20-14"><a href="#cb20-14"></a>\draw    (<span class="dv">209</span>,<span class="fl">1.75</span>) <span class="op">--</span> (<span class="fl">309.71</span>,<span class="fl">51.37</span>) <span class="op">;</span></span>
<span id="cb20-15"><a href="#cb20-15"></a>\draw [shift<span class="op">=</span>{(<span class="fl">311.5</span>,<span class="fl">52.25</span>)}, rotate <span class="op">=</span> <span class="fl">206.23</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-16"><a href="#cb20-16"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da3568530309648137] </span>
<span id="cb20-17"><a href="#cb20-17"></a>\draw    (<span class="dv">305</span>,<span class="fl">68.25</span>) <span class="op">--</span> (<span class="fl">204.31</span>,<span class="fl">20.61</span>) <span class="op">;</span></span>
<span id="cb20-18"><a href="#cb20-18"></a>\draw [shift<span class="op">=</span>{(<span class="fl">202.5</span>,<span class="fl">19.75</span>)}, rotate <span class="op">=</span> <span class="fl">25.32</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-19"><a href="#cb20-19"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da4437541566257528] </span>
<span id="cb20-20"><a href="#cb20-20"></a>\draw    (<span class="dv">205</span>,<span class="fl">167.25</span>) <span class="op">--</span> (<span class="fl">311.2</span>,<span class="fl">116.12</span>) <span class="op">;</span></span>
<span id="cb20-21"><a href="#cb20-21"></a>\draw [shift<span class="op">=</span>{(<span class="dv">313</span>,<span class="fl">115.25</span>)}, rotate <span class="op">=</span> <span class="fl">154.29</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-22"><a href="#cb20-22"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da2672766038605987] </span>
<span id="cb20-23"><a href="#cb20-23"></a>\draw    (<span class="fl">304.5</span>,<span class="fl">101.75</span>) <span class="op">--</span> (<span class="fl">205.82</span>,<span class="fl">146.92</span>) <span class="op">;</span></span>
<span id="cb20-24"><a href="#cb20-24"></a>\draw [shift<span class="op">=</span>{(<span class="dv">204</span>,<span class="fl">147.75</span>)}, rotate <span class="op">=</span> <span class="fl">335.41</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb20-25"><a href="#cb20-25"></a></span>
<span id="cb20-26"><a href="#cb20-26"></a><span class="op">%</span> Text Node</span>
<span id="cb20-27"><a href="#cb20-27"></a>\draw (<span class="dv">352</span>,<span class="fl">76.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $f$}<span class="op">;</span></span>
<span id="cb20-28"><a href="#cb20-28"></a><span class="op">%</span> Text Node</span>
<span id="cb20-29"><a href="#cb20-29"></a>\draw (<span class="fl">318.5</span>,<span class="fl">44.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial h}{\partial x}$}<span class="op">;</span></span>
<span id="cb20-30"><a href="#cb20-30"></a><span class="op">%</span> Text Node</span>
<span id="cb20-31"><a href="#cb20-31"></a>\draw (<span class="fl">318.5</span>,<span class="fl">88.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">36</span><span class="op">;</span> blue, <span class="dv">255</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial h}{\partial y}$}<span class="op">;</span></span>
<span id="cb20-32"><a href="#cb20-32"></a><span class="op">%</span> Text Node</span>
<span id="cb20-33"><a href="#cb20-33"></a>\draw (<span class="fl">258.5</span>,<span class="fl">7.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $x$}<span class="op">;</span></span>
<span id="cb20-34"><a href="#cb20-34"></a><span class="op">%</span> Text Node</span>
<span id="cb20-35"><a href="#cb20-35"></a>\draw (<span class="dv">264</span>,<span class="fl">136.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $y$}<span class="op">;</span></span>
<span id="cb20-36"><a href="#cb20-36"></a><span class="op">%</span> Text Node</span>
<span id="cb20-37"><a href="#cb20-37"></a>\draw (<span class="fl">151.5</span>,<span class="fl">96.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial y} <span class="op">=</span>\frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial y}$}<span class="op">;</span></span>
<span id="cb20-38"><a href="#cb20-38"></a><span class="op">%</span> Text Node</span>
<span id="cb20-39"><a href="#cb20-39"></a>\draw (<span class="dv">150</span>,<span class="fl">33.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial x} <span class="op">=</span>\frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial x}$}<span class="op">;</span></span>
<span id="cb20-40"><a href="#cb20-40"></a><span class="op">%</span> Text Node</span>
<span id="cb20-41"><a href="#cb20-41"></a>\draw (<span class="fl">322.5</span>,<span class="fl">4.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $h<span class="op">=</span>f(x,y)$}<span class="op">;</span></span>
<span id="cb20-42"><a href="#cb20-42"></a><span class="op">%</span> Text Node</span>
<span id="cb20-43"><a href="#cb20-43"></a>\draw (<span class="fl">449.5</span>,<span class="fl">39.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $h$}<span class="op">;</span></span>
<span id="cb20-44"><a href="#cb20-44"></a><span class="op">%</span> Text Node</span>
<span id="cb20-45"><a href="#cb20-45"></a>\draw (<span class="fl">451.5</span>,<span class="fl">112.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $\frac{\partial s}{\partial h}$}<span class="op">;</span></span>
<span id="cb20-46"><a href="#cb20-46"></a><span class="op">%</span> Text Node</span>
<span id="cb20-47"><a href="#cb20-47"></a>\draw (<span class="fl">164.5</span>,<span class="fl">172.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $ \begin{array}{l}</span>
<span id="cb20-48"><a href="#cb20-48"></a>Downstream\ \<span class="op">\</span></span>
<span id="cb20-49"><a href="#cb20-49"></a>gradients</span>
<span id="cb20-50"><a href="#cb20-50"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb20-51"><a href="#cb20-51"></a><span class="op">%</span> Text Node</span>
<span id="cb20-52"><a href="#cb20-52"></a>\draw (<span class="fl">430.5</span>,<span class="fl">175.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $ \begin{array}{l}</span>
<span id="cb20-53"><a href="#cb20-53"></a>Upstream\ \<span class="op">\</span></span>
<span id="cb20-54"><a href="#cb20-54"></a>gradients</span>
<span id="cb20-55"><a href="#cb20-55"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb20-56"><a href="#cb20-56"></a><span class="op">%</span> Text Node</span>
<span id="cb20-57"><a href="#cb20-57"></a>\draw (<span class="fl">318.5</span>,<span class="fl">173.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">3</span><span class="op">;</span> green, <span class="dv">50</span><span class="op">;</span> blue, <span class="dv">255</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $ \begin{array}{l}</span>
<span id="cb20-58"><a href="#cb20-58"></a>Local\ \<span class="op">\</span></span>
<span id="cb20-59"><a href="#cb20-59"></a>gradients</span>
<span id="cb20-60"><a href="#cb20-60"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb20-61"><a href="#cb20-61"></a></span>
<span id="cb20-62"><a href="#cb20-62"></a></span>
<span id="cb20-63"><a href="#cb20-63"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-14-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Letâ€™s start with a simple forward pass with <span class="math inline">\(1\)</span> neuron. Letâ€™s say, we have the following input vector, weights and bias:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>x <span class="op">=</span> [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>]  <span class="co"># input values</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>w <span class="op">=</span> [<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>] <span class="co"># weights</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>b <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="co"># Forward pass</span></span>
<span id="cb21-6"><a href="#cb21-6"></a>z <span class="op">=</span> np.dot(x,w) <span class="op">+</span> b</span>
<span id="cb21-7"><a href="#cb21-7"></a></span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="co"># ReLU Activation function</span></span>
<span id="cb21-9"><a href="#cb21-9"></a>y <span class="op">=</span> <span class="bu">max</span>(z, <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb22-2"><a href="#cb22-2"></a>\begin{tikzpicture}</span>
<span id="cb22-3"><a href="#cb22-3"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb22-4"><a href="#cb22-4"></a>{</span>
<span id="cb22-5"><a href="#cb22-5"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb22-6"><a href="#cb22-6"></a>}</span>
<span id="cb22-7"><a href="#cb22-7"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb22-8"><a href="#cb22-8"></a>{</span>
<span id="cb22-9"><a href="#cb22-9"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb22-10"><a href="#cb22-10"></a>}</span>
<span id="cb22-11"><a href="#cb22-11"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb22-12"><a href="#cb22-12"></a>{</span>
<span id="cb22-13"><a href="#cb22-13"></a>    \node[circle, </span>
<span id="cb22-14"><a href="#cb22-14"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb22-15"><a href="#cb22-15"></a>        draw,</span>
<span id="cb22-16"><a href="#cb22-16"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb22-17"><a href="#cb22-17"></a>        </span>
<span id="cb22-18"><a href="#cb22-18"></a>}</span>
<span id="cb22-19"><a href="#cb22-19"></a></span>
<span id="cb22-20"><a href="#cb22-20"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb22-21"><a href="#cb22-21"></a></span>
<span id="cb22-22"><a href="#cb22-22"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb22-23"><a href="#cb22-23"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb22-24"><a href="#cb22-24"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb22-25"><a href="#cb22-25"></a></span>
<span id="cb22-26"><a href="#cb22-26"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb22-27"><a href="#cb22-27"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb22-28"><a href="#cb22-28"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb22-29"><a href="#cb22-29"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb22-30"><a href="#cb22-30"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb22-31"><a href="#cb22-31"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb22-32"><a href="#cb22-32"></a></span>
<span id="cb22-33"><a href="#cb22-33"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb22-34"><a href="#cb22-34"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb22-35"><a href="#cb22-35"></a></span>
<span id="cb22-36"><a href="#cb22-36"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb22-37"><a href="#cb22-37"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb22-38"><a href="#cb22-38"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb22-39"><a href="#cb22-39"></a></span>
<span id="cb22-40"><a href="#cb22-40"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb22-41"><a href="#cb22-41"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb22-42"><a href="#cb22-42"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb22-43"><a href="#cb22-43"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The ReLU function <span class="math inline">\(f(x)=\max(x,0)\)</span> is differentiable everywhere except at <span class="math inline">\(x = 0\)</span>. We define <span class="math inline">\(f'(x)\)</span> as:</p>
<p><span class="math display">\[\begin{align*}
f'(x) =
\begin{cases}
1 &amp; x &gt; 0 \\
0 &amp; \text{otherwise}
\end{cases}
\end{align*}\]</span></p>
<p>In Python, we write:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>relu_dz <span class="op">=</span> (<span class="fl">1.</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The input to the ReLU function is <span class="math inline">\(6.00\)</span>, so the derivative equals <span class="math inline">\(1.00\)</span>. We multiply this local gradient by the upstream gradient to calculate the downstream gradient.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2"></a></span>
<span id="cb24-3"><a href="#cb24-3"></a>x <span class="op">=</span> [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>]  <span class="co"># input values</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>w <span class="op">=</span> [<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>]  <span class="co"># weights</span></span>
<span id="cb24-5"><a href="#cb24-5"></a>b <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb24-6"><a href="#cb24-6"></a></span>
<span id="cb24-7"><a href="#cb24-7"></a><span class="co"># Forward pass</span></span>
<span id="cb24-8"><a href="#cb24-8"></a>z <span class="op">=</span> np.dot(x, w) <span class="op">+</span> b</span>
<span id="cb24-9"><a href="#cb24-9"></a></span>
<span id="cb24-10"><a href="#cb24-10"></a><span class="co"># ReLU Activation function</span></span>
<span id="cb24-11"><a href="#cb24-11"></a>y <span class="op">=</span> <span class="bu">max</span>(z, <span class="dv">0</span>)</span>
<span id="cb24-12"><a href="#cb24-12"></a></span>
<span id="cb24-13"><a href="#cb24-13"></a><span class="co"># Backward pass</span></span>
<span id="cb24-14"><a href="#cb24-14"></a><span class="co"># Upstream gradient</span></span>
<span id="cb24-15"><a href="#cb24-15"></a>ds_drelu <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb24-16"><a href="#cb24-16"></a></span>
<span id="cb24-17"><a href="#cb24-17"></a><span class="co"># Derivative of the ReLU and the chain rule</span></span>
<span id="cb24-18"><a href="#cb24-18"></a>drelu_dz <span class="op">=</span> <span class="fl">1.0</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb24-19"><a href="#cb24-19"></a>ds_dz <span class="op">=</span> ds_drelu <span class="op">*</span> drelu_dz</span>
<span id="cb24-20"><a href="#cb24-20"></a><span class="bu">print</span>(ds_dz)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.0</code></pre>
</div>
</div>
<p>The results with the derivative of the ReLU function and chain rule look as follows:</p>
<div class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb26-2"><a href="#cb26-2"></a>\begin{tikzpicture}</span>
<span id="cb26-3"><a href="#cb26-3"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb26-4"><a href="#cb26-4"></a>{</span>
<span id="cb26-5"><a href="#cb26-5"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb26-6"><a href="#cb26-6"></a>}</span>
<span id="cb26-7"><a href="#cb26-7"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb26-8"><a href="#cb26-8"></a>{</span>
<span id="cb26-9"><a href="#cb26-9"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb26-10"><a href="#cb26-10"></a>}</span>
<span id="cb26-11"><a href="#cb26-11"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb26-12"><a href="#cb26-12"></a>{</span>
<span id="cb26-13"><a href="#cb26-13"></a>    \node[circle, </span>
<span id="cb26-14"><a href="#cb26-14"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb26-15"><a href="#cb26-15"></a>        draw,</span>
<span id="cb26-16"><a href="#cb26-16"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb26-17"><a href="#cb26-17"></a>        </span>
<span id="cb26-18"><a href="#cb26-18"></a>}</span>
<span id="cb26-19"><a href="#cb26-19"></a></span>
<span id="cb26-20"><a href="#cb26-20"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb26-21"><a href="#cb26-21"></a></span>
<span id="cb26-22"><a href="#cb26-22"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb26-23"><a href="#cb26-23"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb26-24"><a href="#cb26-24"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb26-25"><a href="#cb26-25"></a></span>
<span id="cb26-26"><a href="#cb26-26"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb26-27"><a href="#cb26-27"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb26-28"><a href="#cb26-28"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb26-29"><a href="#cb26-29"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb26-30"><a href="#cb26-30"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb26-31"><a href="#cb26-31"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb26-32"><a href="#cb26-32"></a></span>
<span id="cb26-33"><a href="#cb26-33"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb26-34"><a href="#cb26-34"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb26-35"><a href="#cb26-35"></a></span>
<span id="cb26-36"><a href="#cb26-36"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb26-37"><a href="#cb26-37"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb26-38"><a href="#cb26-38"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb26-39"><a href="#cb26-39"></a></span>
<span id="cb26-40"><a href="#cb26-40"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb26-41"><a href="#cb26-41"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb26-42"><a href="#cb26-42"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb26-43"><a href="#cb26-43"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb26-44"><a href="#cb26-44"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="18">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-19-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Moving backward through our neural network, consider the add function <span class="math inline">\(f(x,y,z)=x + y + z\)</span>. The partial derivatives <span class="math inline">\(\frac{\partial f}{\partial x}\)</span>, <span class="math inline">\(\frac{\partial f}{\partial y}\)</span> and <span class="math inline">\(\frac{\partial f}{\partial z}\)</span> are all equal to <span class="math inline">\(1\)</span>. So, the <strong>add gate</strong> always takes on the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># Local gradients for the + function</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>dz_dw0x0 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-3"><a href="#cb27-3"></a>dz_dw1x1 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-4"><a href="#cb27-4"></a>dz_dw2x2 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-5"><a href="#cb27-5"></a>dz_db <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-6"><a href="#cb27-6"></a></span>
<span id="cb27-7"><a href="#cb27-7"></a><span class="co"># Calculate the downstream gradients</span></span>
<span id="cb27-8"><a href="#cb27-8"></a>ds_dw0x0 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw0x0</span>
<span id="cb27-9"><a href="#cb27-9"></a>ds_dw1x1 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw1x1</span>
<span id="cb27-10"><a href="#cb27-10"></a>ds_dw2x2 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw2x2</span>
<span id="cb27-11"><a href="#cb27-11"></a>ds_db <span class="op">=</span> ds_dz <span class="op">*</span> dz_db</span>
<span id="cb27-12"><a href="#cb27-12"></a><span class="bu">print</span>(ds_dw0x0, ds_dw1x1, ds_dw2x2, ds_db)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.0 1.0 1.0 1.0</code></pre>
</div>
</div>
<p>We can update the computation graph as:</p>
<div class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb29-2"><a href="#cb29-2"></a>\begin{tikzpicture}</span>
<span id="cb29-3"><a href="#cb29-3"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb29-4"><a href="#cb29-4"></a>{</span>
<span id="cb29-5"><a href="#cb29-5"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb29-6"><a href="#cb29-6"></a>}</span>
<span id="cb29-7"><a href="#cb29-7"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb29-8"><a href="#cb29-8"></a>{</span>
<span id="cb29-9"><a href="#cb29-9"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb29-10"><a href="#cb29-10"></a>}</span>
<span id="cb29-11"><a href="#cb29-11"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb29-12"><a href="#cb29-12"></a>{</span>
<span id="cb29-13"><a href="#cb29-13"></a>    \node[circle, </span>
<span id="cb29-14"><a href="#cb29-14"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb29-15"><a href="#cb29-15"></a>        draw,</span>
<span id="cb29-16"><a href="#cb29-16"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb29-17"><a href="#cb29-17"></a>        </span>
<span id="cb29-18"><a href="#cb29-18"></a>}</span>
<span id="cb29-19"><a href="#cb29-19"></a></span>
<span id="cb29-20"><a href="#cb29-20"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb29-21"><a href="#cb29-21"></a></span>
<span id="cb29-22"><a href="#cb29-22"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb29-23"><a href="#cb29-23"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb29-24"><a href="#cb29-24"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb29-25"><a href="#cb29-25"></a></span>
<span id="cb29-26"><a href="#cb29-26"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb29-27"><a href="#cb29-27"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb29-28"><a href="#cb29-28"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb29-29"><a href="#cb29-29"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb29-30"><a href="#cb29-30"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb29-31"><a href="#cb29-31"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb29-32"><a href="#cb29-32"></a></span>
<span id="cb29-33"><a href="#cb29-33"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb29-34"><a href="#cb29-34"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb29-35"><a href="#cb29-35"></a></span>
<span id="cb29-36"><a href="#cb29-36"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb29-37"><a href="#cb29-37"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb29-38"><a href="#cb29-38"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb29-39"><a href="#cb29-39"></a></span>
<span id="cb29-40"><a href="#cb29-40"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb29-41"><a href="#cb29-41"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb29-42"><a href="#cb29-42"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-43"><a href="#cb29-43"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-44"><a href="#cb29-44"></a>\node [red] (C) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">3.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-45"><a href="#cb29-45"></a>\node [red] (D) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-46"><a href="#cb29-46"></a>\node [red] (E) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">7.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-47"><a href="#cb29-47"></a>\node [red] (f) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">12.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb29-48"><a href="#cb29-48"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now, consider the production function <span class="math inline">\(f(x,y) = x * y\)</span>. The gradients of <span class="math inline">\(f\)</span> are <span class="math inline">\(\frac{\partial f}{\partial x} = y\)</span>, <span class="math inline">\(\frac{\partial f}{\partial y} = x\)</span>. The <strong>multiply gate</strong> is therefore a little less easy to interpret. Its local gradients are the input values, except switched and this is multiplied by the upstream gradient.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Local gradients for the * function</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>dw0x0_dx0 <span class="op">=</span> w[<span class="dv">0</span>]</span>
<span id="cb30-3"><a href="#cb30-3"></a>dw0x0_dw0 <span class="op">=</span> x[<span class="dv">0</span>]</span>
<span id="cb30-4"><a href="#cb30-4"></a>dw1x1_dx1 <span class="op">=</span> w[<span class="dv">1</span>]</span>
<span id="cb30-5"><a href="#cb30-5"></a>dw1x1_dw1 <span class="op">=</span> x[<span class="dv">1</span>]</span>
<span id="cb30-6"><a href="#cb30-6"></a>dw2x2_dx2 <span class="op">=</span> w[<span class="dv">2</span>]</span>
<span id="cb30-7"><a href="#cb30-7"></a>dw2x2_dw2 <span class="op">=</span> x[<span class="dv">2</span>]</span>
<span id="cb30-8"><a href="#cb30-8"></a></span>
<span id="cb30-9"><a href="#cb30-9"></a><span class="co"># Calculate the downstream gradients</span></span>
<span id="cb30-10"><a href="#cb30-10"></a>ds_dx0 <span class="op">=</span> ds_dw0x0 <span class="op">*</span> dw0x0_dx0</span>
<span id="cb30-11"><a href="#cb30-11"></a>ds_dw0 <span class="op">=</span> ds_dw0x0 <span class="op">*</span> dw0x0_dw0</span>
<span id="cb30-12"><a href="#cb30-12"></a>ds_dx1 <span class="op">=</span> ds_dw1x1 <span class="op">*</span> dw1x1_dx1</span>
<span id="cb30-13"><a href="#cb30-13"></a>ds_dw1 <span class="op">=</span> ds_dw1x1 <span class="op">*</span> dw1x1_dw1</span>
<span id="cb30-14"><a href="#cb30-14"></a>ds_dx2 <span class="op">=</span> ds_dw2x2 <span class="op">*</span> dw2x2_dx2</span>
<span id="cb30-15"><a href="#cb30-15"></a>ds_dw2 <span class="op">=</span> ds_dw2x2 <span class="op">*</span> dw2x2_dw2</span>
<span id="cb30-16"><a href="#cb30-16"></a></span>
<span id="cb30-17"><a href="#cb30-17"></a><span class="bu">print</span>(ds_dx0, ds_dw0, ds_dx1, ds_dw1, ds_dx2, ds_dw2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-3.0 1.0 -1.0 -2.0 2.0 3.0</code></pre>
</div>
</div>
<p>We can update the computation graph as follows:</p>
<div class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb32-2"><a href="#cb32-2"></a>\begin{tikzpicture}</span>
<span id="cb32-3"><a href="#cb32-3"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb32-4"><a href="#cb32-4"></a>{</span>
<span id="cb32-5"><a href="#cb32-5"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb32-6"><a href="#cb32-6"></a>}</span>
<span id="cb32-7"><a href="#cb32-7"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb32-8"><a href="#cb32-8"></a>{</span>
<span id="cb32-9"><a href="#cb32-9"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb32-10"><a href="#cb32-10"></a>}</span>
<span id="cb32-11"><a href="#cb32-11"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb32-12"><a href="#cb32-12"></a>{</span>
<span id="cb32-13"><a href="#cb32-13"></a>    \node[circle, </span>
<span id="cb32-14"><a href="#cb32-14"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb32-15"><a href="#cb32-15"></a>        draw,</span>
<span id="cb32-16"><a href="#cb32-16"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb32-17"><a href="#cb32-17"></a>        </span>
<span id="cb32-18"><a href="#cb32-18"></a>}</span>
<span id="cb32-19"><a href="#cb32-19"></a></span>
<span id="cb32-20"><a href="#cb32-20"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb32-21"><a href="#cb32-21"></a></span>
<span id="cb32-22"><a href="#cb32-22"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb32-23"><a href="#cb32-23"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb32-24"><a href="#cb32-24"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb32-25"><a href="#cb32-25"></a></span>
<span id="cb32-26"><a href="#cb32-26"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb32-27"><a href="#cb32-27"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb32-28"><a href="#cb32-28"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb32-29"><a href="#cb32-29"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb32-30"><a href="#cb32-30"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb32-31"><a href="#cb32-31"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb32-32"><a href="#cb32-32"></a></span>
<span id="cb32-33"><a href="#cb32-33"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb32-34"><a href="#cb32-34"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb32-35"><a href="#cb32-35"></a></span>
<span id="cb32-36"><a href="#cb32-36"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb32-37"><a href="#cb32-37"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb32-38"><a href="#cb32-38"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb32-39"><a href="#cb32-39"></a></span>
<span id="cb32-40"><a href="#cb32-40"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb32-41"><a href="#cb32-41"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb32-42"><a href="#cb32-42"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-43"><a href="#cb32-43"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-44"><a href="#cb32-44"></a>\node [red] (C) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">3.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-45"><a href="#cb32-45"></a>\node [red] (D) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-46"><a href="#cb32-46"></a>\node [red] (E) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">7.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-47"><a href="#cb32-47"></a>\node [red] (F) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">12.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb32-48"><a href="#cb32-48"></a>\node [red] (G) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">0.75</span>) {\large $<span class="op">-</span><span class="fl">3.0</span>$}<span class="op">;</span></span>
<span id="cb32-49"><a href="#cb32-49"></a>\node [red] (H) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">2</span>) {\large $<span class="fl">1.0</span>$}<span class="op">;</span></span>
<span id="cb32-50"><a href="#cb32-50"></a>\node [red] (I) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">4.75</span>) {\large $<span class="op">-</span><span class="fl">1.0</span>$}<span class="op">;</span></span>
<span id="cb32-51"><a href="#cb32-51"></a>\node [red] (J) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">6</span>) {\large $<span class="op">-</span><span class="fl">2.0</span>$}<span class="op">;</span></span>
<span id="cb32-52"><a href="#cb32-52"></a>\node [red] (K) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">8.75</span>) {\large $<span class="fl">2.0</span>$}<span class="op">;</span></span>
<span id="cb32-53"><a href="#cb32-53"></a>\node [red] (L) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">10</span>) {\large $<span class="fl">3.0</span>$}<span class="op">;</span></span>
<span id="cb32-54"><a href="#cb32-54"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-23-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Gradients sum at outward branches. Consider the following computation graph:</p>
<div class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb33-2"><a href="#cb33-2"></a>\begin{tikzpicture}[x<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,y<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,yscale<span class="op">=-</span><span class="dv">1</span>,xscale<span class="op">=</span><span class="dv">1</span>]</span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="op">%</span>uncomment <span class="cf">if</span> require: \path (<span class="dv">0</span>,<span class="dv">211</span>)<span class="op">;</span> <span class="op">%</span><span class="bu">set</span> diagram left start at <span class="dv">0</span>, <span class="kw">and</span> has height of <span class="dv">211</span></span>
<span id="cb33-4"><a href="#cb33-4"></a></span>
<span id="cb33-5"><a href="#cb33-5"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp4612472925724298] </span>
<span id="cb33-6"><a href="#cb33-6"></a>\draw   (<span class="fl">444.62</span>,<span class="dv">95</span>) .. controls (<span class="fl">444.62</span>,<span class="fl">81.19</span>) <span class="kw">and</span> (<span class="fl">455.38</span>,<span class="dv">70</span>) .. (<span class="fl">468.64</span>,<span class="dv">70</span>) .. controls (<span class="fl">481.91</span>,<span class="dv">70</span>) <span class="kw">and</span> (<span class="fl">492.66</span>,<span class="fl">81.19</span>) .. (<span class="fl">492.66</span>,<span class="dv">95</span>) .. controls (<span class="fl">492.66</span>,<span class="fl">108.81</span>) <span class="kw">and</span> (<span class="fl">481.91</span>,<span class="dv">120</span>) .. (<span class="fl">468.64</span>,<span class="dv">120</span>) .. controls (<span class="fl">455.38</span>,<span class="dv">120</span>) <span class="kw">and</span> (<span class="fl">444.62</span>,<span class="fl">108.81</span>) .. (<span class="fl">444.62</span>,<span class="dv">95</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-7"><a href="#cb33-7"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp4844626229099638] </span>
<span id="cb33-8"><a href="#cb33-8"></a>\draw   (<span class="fl">299.33</span>,<span class="fl">31.5</span>) .. controls (<span class="fl">299.33</span>,<span class="fl">17.69</span>) <span class="kw">and</span> (<span class="fl">310.08</span>,<span class="fl">6.5</span>) .. (<span class="fl">323.35</span>,<span class="fl">6.5</span>) .. controls (<span class="fl">336.61</span>,<span class="fl">6.5</span>) <span class="kw">and</span> (<span class="fl">347.37</span>,<span class="fl">17.69</span>) .. (<span class="fl">347.37</span>,<span class="fl">31.5</span>) .. controls (<span class="fl">347.37</span>,<span class="fl">45.31</span>) <span class="kw">and</span> (<span class="fl">336.61</span>,<span class="fl">56.5</span>) .. (<span class="fl">323.35</span>,<span class="fl">56.5</span>) .. controls (<span class="fl">310.08</span>,<span class="fl">56.5</span>) <span class="kw">and</span> (<span class="fl">299.33</span>,<span class="fl">45.31</span>) .. (<span class="fl">299.33</span>,<span class="fl">31.5</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-9"><a href="#cb33-9"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp2271780920027553] </span>
<span id="cb33-10"><a href="#cb33-10"></a>\draw   (<span class="fl">303.25</span>,<span class="fl">94.7</span>) .. controls (<span class="fl">303.25</span>,<span class="fl">80.89</span>) <span class="kw">and</span> (<span class="dv">314</span>,<span class="fl">69.7</span>) .. (<span class="fl">327.27</span>,<span class="fl">69.7</span>) .. controls (<span class="fl">340.53</span>,<span class="fl">69.7</span>) <span class="kw">and</span> (<span class="fl">351.29</span>,<span class="fl">80.89</span>) .. (<span class="fl">351.29</span>,<span class="fl">94.7</span>) .. controls (<span class="fl">351.29</span>,<span class="fl">108.51</span>) <span class="kw">and</span> (<span class="fl">340.53</span>,<span class="fl">119.7</span>) .. (<span class="fl">327.27</span>,<span class="fl">119.7</span>) .. controls (<span class="dv">314</span>,<span class="fl">119.7</span>) <span class="kw">and</span> (<span class="fl">303.25</span>,<span class="fl">108.51</span>) .. (<span class="fl">303.25</span>,<span class="fl">94.7</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-11"><a href="#cb33-11"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp150108609534231] </span>
<span id="cb33-12"><a href="#cb33-12"></a>\draw   (<span class="fl">299.25</span>,<span class="fl">167.7</span>) .. controls (<span class="fl">299.25</span>,<span class="fl">153.89</span>) <span class="kw">and</span> (<span class="dv">310</span>,<span class="fl">142.7</span>) .. (<span class="fl">323.27</span>,<span class="fl">142.7</span>) .. controls (<span class="fl">336.53</span>,<span class="fl">142.7</span>) <span class="kw">and</span> (<span class="fl">347.29</span>,<span class="fl">153.89</span>) .. (<span class="fl">347.29</span>,<span class="fl">167.7</span>) .. controls (<span class="fl">347.29</span>,<span class="fl">181.51</span>) <span class="kw">and</span> (<span class="fl">336.53</span>,<span class="fl">192.7</span>) .. (<span class="fl">323.27</span>,<span class="fl">192.7</span>) .. controls (<span class="dv">310</span>,<span class="fl">192.7</span>) <span class="kw">and</span> (<span class="fl">299.25</span>,<span class="fl">181.51</span>) .. (<span class="fl">299.25</span>,<span class="fl">167.7</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-13"><a href="#cb33-13"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da7844123205705824] </span>
<span id="cb33-14"><a href="#cb33-14"></a>\draw    (<span class="fl">347.37</span>,<span class="fl">31.5</span>) <span class="op">--</span> (<span class="fl">450.04</span>,<span class="fl">76.06</span>) <span class="op">;</span></span>
<span id="cb33-15"><a href="#cb33-15"></a>\draw [shift<span class="op">=</span>{(<span class="fl">452.79</span>,<span class="fl">77.25</span>)}, rotate <span class="op">=</span> <span class="fl">203.46</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-16"><a href="#cb33-16"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da814168086414518] </span>
<span id="cb33-17"><a href="#cb33-17"></a>\draw    (<span class="fl">351.29</span>,<span class="fl">94.7</span>) <span class="op">--</span> (<span class="fl">441.62</span>,<span class="fl">94.99</span>) <span class="op">;</span></span>
<span id="cb33-18"><a href="#cb33-18"></a>\draw [shift<span class="op">=</span>{(<span class="fl">444.62</span>,<span class="dv">95</span>)}, rotate <span class="op">=</span> <span class="fl">180.18</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-19"><a href="#cb33-19"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da7411937688169676] </span>
<span id="cb33-20"><a href="#cb33-20"></a>\draw    (<span class="fl">347.29</span>,<span class="fl">167.7</span>) <span class="op">--</span> (<span class="fl">446.35</span>,<span class="fl">110.75</span>) <span class="op">;</span></span>
<span id="cb33-21"><a href="#cb33-21"></a>\draw [shift<span class="op">=</span>{(<span class="fl">448.95</span>,<span class="fl">109.25</span>)}, rotate <span class="op">=</span> <span class="fl">150.1</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-22"><a href="#cb33-22"></a><span class="op">%</span>Shape: Circle [<span class="bu">id</span>:dp515320046458885] </span>
<span id="cb33-23"><a href="#cb33-23"></a>\draw   (<span class="dv">163</span>,<span class="dv">96</span>) .. controls (<span class="dv">163</span>,<span class="fl">82.19</span>) <span class="kw">and</span> (<span class="fl">174.19</span>,<span class="dv">71</span>) .. (<span class="dv">188</span>,<span class="dv">71</span>) .. controls (<span class="fl">201.81</span>,<span class="dv">71</span>) <span class="kw">and</span> (<span class="dv">213</span>,<span class="fl">82.19</span>) .. (<span class="dv">213</span>,<span class="dv">96</span>) .. controls (<span class="dv">213</span>,<span class="fl">109.81</span>) <span class="kw">and</span> (<span class="fl">201.81</span>,<span class="dv">121</span>) .. (<span class="dv">188</span>,<span class="dv">121</span>) .. controls (<span class="fl">174.19</span>,<span class="dv">121</span>) <span class="kw">and</span> (<span class="dv">163</span>,<span class="fl">109.81</span>) .. (<span class="dv">163</span>,<span class="dv">96</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb33-24"><a href="#cb33-24"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da6219161786925074] </span>
<span id="cb33-25"><a href="#cb33-25"></a>\draw    (<span class="fl">492.66</span>,<span class="dv">95</span>) <span class="op">--</span> (<span class="dv">567</span>,<span class="fl">94.52</span>) <span class="op">;</span></span>
<span id="cb33-26"><a href="#cb33-26"></a>\draw [shift<span class="op">=</span>{(<span class="dv">570</span>,<span class="fl">94.5</span>)}, rotate <span class="op">=</span> <span class="fl">179.63</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-27"><a href="#cb33-27"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da5694521418691749] </span>
<span id="cb33-28"><a href="#cb33-28"></a>\draw    (<span class="fl">84.5</span>,<span class="fl">95.75</span>) <span class="op">--</span> (<span class="dv">160</span>,<span class="fl">95.99</span>) <span class="op">;</span></span>
<span id="cb33-29"><a href="#cb33-29"></a>\draw [shift<span class="op">=</span>{(<span class="dv">163</span>,<span class="dv">96</span>)}, rotate <span class="op">=</span> <span class="fl">180.18</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">8.04</span>,<span class="op">-</span><span class="fl">3.86</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">8.04</span>,<span class="fl">3.86</span>) <span class="op">--</span> (<span class="fl">5.34</span>,<span class="dv">0</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-30"><a href="#cb33-30"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da08990804845355682] </span>
<span id="cb33-31"><a href="#cb33-31"></a>\draw    (<span class="fl">210.69</span>,<span class="fl">85.5</span>) <span class="op">--</span> (<span class="fl">296.86</span>,<span class="fl">31.4</span>) <span class="op">;</span></span>
<span id="cb33-32"><a href="#cb33-32"></a>\draw [shift<span class="op">=</span>{(<span class="fl">299.4</span>,<span class="fl">29.8</span>)}, rotate <span class="op">=</span> <span class="fl">147.88</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-33"><a href="#cb33-33"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da1505672958459916] </span>
<span id="cb33-34"><a href="#cb33-34"></a>\draw    (<span class="fl">212.61</span>,<span class="dv">96</span>) <span class="op">--</span> (<span class="fl">300.4</span>,<span class="fl">95.03</span>) <span class="op">;</span></span>
<span id="cb33-35"><a href="#cb33-35"></a>\draw [shift<span class="op">=</span>{(<span class="fl">303.4</span>,<span class="dv">95</span>)}, rotate <span class="op">=</span> <span class="fl">179.37</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-36"><a href="#cb33-36"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da23258128449735227] </span>
<span id="cb33-37"><a href="#cb33-37"></a>\draw    (<span class="dv">203</span>,<span class="fl">116.5</span>) <span class="op">--</span> (<span class="fl">296.36</span>,<span class="fl">167.17</span>) <span class="op">;</span></span>
<span id="cb33-38"><a href="#cb33-38"></a>\draw [shift<span class="op">=</span>{(<span class="dv">299</span>,<span class="fl">168.6</span>)}, rotate <span class="op">=</span> <span class="fl">208.49</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb33-39"><a href="#cb33-39"></a></span>
<span id="cb33-40"><a href="#cb33-40"></a><span class="op">%</span> Text Node</span>
<span id="cb33-41"><a href="#cb33-41"></a>\draw (<span class="fl">464.08</span>,<span class="fl">84.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $s$}<span class="op">;</span></span>
<span id="cb33-42"><a href="#cb33-42"></a><span class="op">%</span> Text Node</span>
<span id="cb33-43"><a href="#cb33-43"></a>\draw (<span class="fl">317.25</span>,<span class="fl">18.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">1</span>}$}<span class="op">;</span></span>
<span id="cb33-44"><a href="#cb33-44"></a><span class="op">%</span> Text Node</span>
<span id="cb33-45"><a href="#cb33-45"></a>\draw (<span class="fl">321.65</span>,<span class="fl">82.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">2</span>}$}<span class="op">;</span></span>
<span id="cb33-46"><a href="#cb33-46"></a><span class="op">%</span> Text Node</span>
<span id="cb33-47"><a href="#cb33-47"></a>\draw (<span class="fl">317.65</span>,<span class="fl">155.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">3</span>}$}<span class="op">;</span></span>
<span id="cb33-48"><a href="#cb33-48"></a><span class="op">%</span> Text Node</span>
<span id="cb33-49"><a href="#cb33-49"></a>\draw (<span class="fl">365.04</span>,<span class="fl">44.2</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">1</span>}}$}<span class="op">;</span></span>
<span id="cb33-50"><a href="#cb33-50"></a><span class="op">%</span> Text Node</span>
<span id="cb33-51"><a href="#cb33-51"></a>\draw (<span class="fl">365.52</span>,<span class="fl">94.3</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">2</span>}}$}<span class="op">;</span></span>
<span id="cb33-52"><a href="#cb33-52"></a><span class="op">%</span> Text Node</span>
<span id="cb33-53"><a href="#cb33-53"></a>\draw (<span class="fl">366.72</span>,<span class="dv">154</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">3</span>}}$}<span class="op">;</span></span>
<span id="cb33-54"><a href="#cb33-54"></a><span class="op">%</span> Text Node</span>
<span id="cb33-55"><a href="#cb33-55"></a>\draw (<span class="fl">183.5</span>,<span class="fl">85.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $a$}<span class="op">;</span></span>
<span id="cb33-56"><a href="#cb33-56"></a><span class="op">%</span> Text Node</span>
<span id="cb33-57"><a href="#cb33-57"></a>\draw (<span class="fl">304.78</span>,<span class="fl">21.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">1</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-58"><a href="#cb33-58"></a><span class="op">%</span> Text Node</span>
<span id="cb33-59"><a href="#cb33-59"></a>\draw (<span class="fl">305.82</span>,<span class="fl">84.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">2</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-60"><a href="#cb33-60"></a><span class="op">%</span> Text Node</span>
<span id="cb33-61"><a href="#cb33-61"></a>\draw (<span class="fl">303.26</span>,<span class="fl">156.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">3</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-62"><a href="#cb33-62"></a><span class="op">%</span> Text Node</span>
<span id="cb33-63"><a href="#cb33-63"></a>\draw (<span class="fl">251.38</span>,<span class="fl">53.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">1</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">1</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-64"><a href="#cb33-64"></a><span class="op">%</span> Text Node</span>
<span id="cb33-65"><a href="#cb33-65"></a>\draw (<span class="fl">249.38</span>,<span class="fl">99.8</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">2</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">2</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-66"><a href="#cb33-66"></a><span class="op">%</span> Text Node</span>
<span id="cb33-67"><a href="#cb33-67"></a>\draw (<span class="fl">245.78</span>,<span class="fl">165.8</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">3</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">3</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb33-68"><a href="#cb33-68"></a></span>
<span id="cb33-69"><a href="#cb33-69"></a></span>
<span id="cb33-70"><a href="#cb33-70"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-24-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The upstream gradient for the node <span class="math inline">\(a\)</span> is <span class="math inline">\(\frac{ds}{da}\)</span>. By the law of total derivatives:</p>
<p><span class="math display">\[\begin{align*}
\frac{ds}{da} = \frac{\partial s}{\partial z^1} \cdot \frac{\partial z^1}{\partial a} + \frac{\partial s}{\partial z^2} \cdot \frac{\partial z^2}{\partial a} + \frac{\partial s}{\partial z^3} \cdot \frac{\partial z^3}{\partial a}
\end{align*}\]</span></p>
</section>
<section id="backprop-for-a-single-neuron---a-python-implementation" class="level2">
<h2 class="anchored" data-anchor-id="backprop-for-a-single-neuron---a-python-implementation">Backprop for a single neuron - a python implementation</h2>
<p>We can write a naive implementation for the backprop algorithm for a single neuron.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-2"><a href="#cb34-2"></a></span>
<span id="cb34-3"><a href="#cb34-3"></a>weights <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>])</span>
<span id="cb34-4"><a href="#cb34-4"></a>bias <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb34-5"><a href="#cb34-5"></a>inputs <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>])</span>
<span id="cb34-6"><a href="#cb34-6"></a>target_output <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb34-7"><a href="#cb34-7"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb34-8"><a href="#cb34-8"></a></span>
<span id="cb34-9"><a href="#cb34-9"></a></span>
<span id="cb34-10"><a href="#cb34-10"></a><span class="kw">def</span> relu(x):</span>
<span id="cb34-11"><a href="#cb34-11"></a>    <span class="cf">return</span> np.maximum(x, <span class="dv">0</span>)</span>
<span id="cb34-12"><a href="#cb34-12"></a></span>
<span id="cb34-13"><a href="#cb34-13"></a></span>
<span id="cb34-14"><a href="#cb34-14"></a><span class="kw">def</span> relu_derivative(x):</span>
<span id="cb34-15"><a href="#cb34-15"></a>    <span class="cf">return</span> np.where(x <span class="op">&gt;</span> <span class="dv">0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb34-16"><a href="#cb34-16"></a></span>
<span id="cb34-17"><a href="#cb34-17"></a></span>
<span id="cb34-18"><a href="#cb34-18"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb34-19"><a href="#cb34-19"></a>    <span class="co"># Forward pass</span></span>
<span id="cb34-20"><a href="#cb34-20"></a>    z <span class="op">=</span> np.dot(weights, inputs) <span class="op">+</span> bias</span>
<span id="cb34-21"><a href="#cb34-21"></a>    a <span class="op">=</span> relu(z)</span>
<span id="cb34-22"><a href="#cb34-22"></a>    loss <span class="op">=</span> (a <span class="op">-</span> target_output) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb34-23"><a href="#cb34-23"></a></span>
<span id="cb34-24"><a href="#cb34-24"></a>    <span class="co"># Backward pass</span></span>
<span id="cb34-25"><a href="#cb34-25"></a>    dloss_da <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (a <span class="op">-</span> target_output)</span>
<span id="cb34-26"><a href="#cb34-26"></a>    dloss_dz <span class="op">=</span> dloss_da <span class="op">*</span> relu_derivative(z)</span>
<span id="cb34-27"><a href="#cb34-27"></a>    dz_dx <span class="op">=</span> weights</span>
<span id="cb34-28"><a href="#cb34-28"></a>    dz_dw <span class="op">=</span> inputs</span>
<span id="cb34-29"><a href="#cb34-29"></a>    dz_db <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb34-30"><a href="#cb34-30"></a>    dloss_dx <span class="op">=</span> dloss_dz <span class="op">*</span> dz_dx</span>
<span id="cb34-31"><a href="#cb34-31"></a>    dloss_dw <span class="op">=</span> dloss_dz <span class="op">*</span> dz_dw</span>
<span id="cb34-32"><a href="#cb34-32"></a>    dloss_db <span class="op">=</span> dloss_dz <span class="op">*</span> dz_db</span>
<span id="cb34-33"><a href="#cb34-33"></a></span>
<span id="cb34-34"><a href="#cb34-34"></a>    <span class="co"># Update the weights and bias</span></span>
<span id="cb34-35"><a href="#cb34-35"></a>    weights <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_dw</span>
<span id="cb34-36"><a href="#cb34-36"></a>    bias <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_db</span>
<span id="cb34-37"><a href="#cb34-37"></a></span>
<span id="cb34-38"><a href="#cb34-38"></a>    <span class="co"># print the loss for this iteration</span></span>
<span id="cb34-39"><a href="#cb34-39"></a>    <span class="cf">if</span> (<span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb34-40"><a href="#cb34-40"></a>        <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span><span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-41"><a href="#cb34-41"></a></span>
<span id="cb34-42"><a href="#cb34-42"></a><span class="bu">print</span>(<span class="st">"Final weights : "</span>, weights)</span>
<span id="cb34-43"><a href="#cb34-43"></a><span class="bu">print</span>(<span class="st">"Final bias : "</span>, bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 10, loss: 20.80624545154949
Iteration 20, loss: 11.314318574097976
Iteration 30, loss: 6.152662434665503
Iteration 40, loss: 3.345783025909011
Iteration 50, loss: 1.8194178821496518
Iteration 60, loss: 0.9893891517327431
Iteration 70, loss: 0.5380242236653578
Iteration 80, loss: 0.29257452918677535
Iteration 90, loss: 0.1591003738562249
Iteration 100, loss: 0.08651788326054576
Iteration 110, loss: 0.04704793547908108
Iteration 120, loss: 0.025584401159906914
Iteration 130, loss: 0.013912652617925996
Iteration 140, loss: 0.007565621788733219
Iteration 150, loss: 0.004114142329436494
Iteration 160, loss: 0.00223724732474303
Iteration 170, loss: 0.0012166024389232565
Iteration 180, loss: 0.0006615815238773228
Iteration 190, loss: 0.0003597642900693548
Iteration 200, loss: 0.00019563778572677352
Final weights :  [-3.3990955  -0.20180899  0.80271349]
Final bias :  0.6009044964039992</code></pre>
</div>
</div>
</section>
<section id="backprop-for-a-layer-of-neurons" class="level2">
<h2 class="anchored" data-anchor-id="backprop-for-a-layer-of-neurons">Backprop for a layer of neurons</h2>
<p>We are now in a position to write a naive implementation of the backprop algorithm for a layer of neurons.</p>
<p>A neural network with a single hidden layer is shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="backprop.png" class="img-fluid figure-img"></p>
<figcaption>backprop</figcaption>
</figure>
</div>
<p>Let <span class="math inline">\(\mathcal{L}\)</span> be a loss function of a neural network to minimize. Let <span class="math inline">\(x \in \mathbf{R}^{d_0}\)</span> be a single sample(input). Let <span class="math inline">\(d_{l}\)</span> be number of neurons(inputs) in layer <span class="math inline">\(l\)</span>. In our example, <span class="math inline">\(x \in \mathbf{R}^4\)</span>.</p>
<p>Letâ€™s derive expressions for all the derivatives we want to compute.</p>
<section id="gradient-of-the-loss-with-respect-to-haty" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-haty">Gradient of the loss with respect to <span class="math inline">\(\hat{y}\)</span></h3>
<p>The gradient of the loss function <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(\hat{y}\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial \hat{y}} &amp;= 2*(\hat{y} - y)
\end{align*}\]</span></p>
</section>
<section id="gradient-of-the-loss-with-respect-to-a" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-a">Gradient of the loss with respect to <span class="math inline">\(a\)</span></h3>
<p>The gradient of <span class="math inline">\(\hat{y}\)</span> with respect to <span class="math inline">\(a_1, a_2, a_3\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \hat{y}}{\partial a} &amp;= \left[\frac{\partial \hat{y}}{\partial a_1}, \frac{\partial \hat{y}}{\partial a_2}, \frac{\partial \hat{y}}{\partial a_3}\right] = [1, 1, 1]
\end{align*}\]</span></p>
<p>So, by chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial a} &amp;= \left[\frac{\partial \mathcal{L}}{\partial a_1}, \frac{\partial \mathcal{L}}{\partial a_2}, \frac{\partial \mathcal{L}}{\partial a_3}\right] \\
&amp;=\left[\frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_1}, \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_2}, \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_3}\right] \\
&amp;= \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a}
\end{align*}\]</span></p>
<p>This vector has the shape <code>[1,layer_width]</code>. In this example, itâ€™s dimensions are <code>(1,3)</code>.</p>
</section>
<section id="gradient-of-the-loss-with-respect-to-z" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-z">Gradient of the loss with respect to <span class="math inline">\(z\)</span></h3>
<p>In our example, <span class="math inline">\(a_1 = max(z_1,0)\)</span>, <span class="math inline">\(a_2 = max(z_2,0)\)</span> and <span class="math inline">\(a_3 = max(z_3,0)\)</span>. Consequently, the derivative:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial a}{\partial z} &amp;= \left[\frac{\partial a_1}{\partial z_1}, \frac{\partial a_2}{\partial z_2}, \frac{\partial a_3}{\partial z_3}\right]\\
&amp;= \left[1_{(z_1 &gt; 0)}, 1_{(z_2 &gt; 0)}, 1_{(z_3 &gt; 0)}\right]
\end{align*}\]</span></p>
<p>Again this vector has shape <code>[1,layer_width]</code>, which in our example equals <code>(1,3)</code>.</p>
<p>By the chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial z} &amp;= \left[\frac{\partial \mathcal{L}}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1}, \frac{\partial \mathcal{L}}{\partial a_2} \cdot \frac{\partial a_2}{\partial z_2}, \frac{\partial \mathcal{L}}{\partial a_3} \cdot \frac{\partial a_3}{\partial z_3}\right]\\
&amp;= \frac{\partial \mathcal{L}}{\partial a} \odot \frac{\partial \mathcal{a}}{\partial z}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> denotes the element wise product of the two vectors. The gradient of the loss with respect to <span class="math inline">\(z\)</span>, is also a vector of shape <code>[1,layer_width]</code>.</p>
</section>
<section id="gradient-of-the-loss-with-respect-to-weights-w" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-weights-w">Gradient of the loss with respect to weights <span class="math inline">\(W\)</span></h3>
<p>Since</p>
<p><span class="math display">\[\begin{align*}
z_1 &amp;= w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + b_1 \\
z_2 &amp;= w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + b_2 \\
z_3 &amp;= w_{31}x_1 + w_{32}x_2 + w_{23}x_3 + w_{24}x_4 + b_3
\end{align*}\]</span></p>
<p>it follows that: <span class="math display">\[\begin{align*}
\frac{\partial z_i}{\partial w_{ij}} = x_j
\end{align*}\]</span></p>
<p>Now,</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{ij}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i} \cdot \frac{\partial z_i}{\partial w_{ij}} \\
&amp;= \frac{\partial \mathcal{L}}{\partial z_i} \cdot x_j
\end{align*}\]</span></p>
<p>In other words:</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial w_{11}} \\
\frac{\partial \mathcal{L}}{\partial w_{12}} \\
\frac{\partial \mathcal{L}}{\partial w_{13}} \\
\frac{\partial \mathcal{L}}{\partial w_{14}}
\end{bmatrix}
&amp;= \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{11}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{12}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{13}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{14}}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_1\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_2\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_3\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_4
\end{bmatrix}
\end{align*}\]</span></p>
<p>Putting this together, we define the jacobian matrix <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial W}\)</span> as:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial W}&amp;=\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial w_{11}} &amp; \frac{\partial \mathcal{L}}{\partial w_{21}} &amp; \frac{\partial \mathcal{L}}{\partial w_{31}} &amp; \frac{\partial \mathcal{L}}{\partial w_{41}} \\
\frac{\partial \mathcal{L}}{\partial w_{12}} &amp; \frac{\partial \mathcal{L}}{\partial w_{22}} &amp; \frac{\partial \mathcal{L}}{\partial w_{32}} &amp; \frac{\partial \mathcal{L}}{\partial w_{42}} \\
\frac{\partial \mathcal{L}}{\partial w_{13}} &amp; \frac{\partial \mathcal{L}}{\partial w_{23}} &amp; \frac{\partial \mathcal{L}}{\partial w_{33}} &amp; \frac{\partial \mathcal{L}}{\partial w_{43}} \\
\frac{\partial \mathcal{L}}{\partial w_{14}} &amp; \frac{\partial \mathcal{L}}{\partial w_{24}} &amp; \frac{\partial \mathcal{L}}{\partial w_{34}} &amp; \frac{\partial \mathcal{L}}{\partial w_{44}} \\
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{11}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{21}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{31}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{12}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{22}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{32}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{13}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{23}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{33}}\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{14}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{24}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{34}}
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_1 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_1 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_1 \\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_2 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_2 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_2\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_3 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_3 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_3\\
\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_4 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_4 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_4
\end{bmatrix}\\
&amp;= \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix} \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial z_1} &amp; \frac{\partial \mathcal{L}}{\partial z_2} &amp; \frac{\partial \mathcal{L}}{\partial z_3}
\end{bmatrix} \\
&amp;= X^T \cdot \frac{\partial \mathcal{L}}{\partial z}
\end{align*}\]</span></p>
<p>The dimensions of <span class="math inline">\(X^T\)</span> and <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial z}\)</span> are <code>[input_size,1]</code> and <code>[1,layer_width]</code> respectively. Therefore, <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial W}\)</span> will be of dimensions <code>[input_size,layer_width]</code>. In our example this equals <code>(4,3)</code>.</p>
<p>The first column of <span class="math inline">\(X^T \cdot \frac{\partial \mathcal{L}}{\partial z}\)</span> gives the derivative with respect to the first neuronâ€™s weights, the second column gives the derivative with respect to the second neuronâ€™s weights and so forth.</p>
</section>
<section id="gradient-of-the-loss-with-respect-to-the-biases-b" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-the-biases-b">Gradient of the loss with respect to the biases <span class="math inline">\(b\)</span></h3>
<p>Since</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial z}{\partial b} &amp;= \left[\frac{\partial z_1}{\partial b_1}, \frac{\partial z_2}{\partial b_2}, \frac{\partial z_3}{\partial b_3}\right]\\
&amp;= [1,1,1]
\end{align*}\]</span></p>
<p>It follows that:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial b} &amp;= \left[\frac{\partial \mathcal{L}}{\partial b_1}, \frac{\partial \mathcal{L}}{\partial b_2}, \frac{\partial \mathcal{L}}{\partial b_3}\right]\\
&amp;= \left[\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial b_1}, \frac{\partial \mathcal{L}}{\partial b_2} \cdot \frac{\partial z_2}{\partial b_21}, \frac{\partial \mathcal{L}}{\partial b_3}\cdot \cdot \frac{\partial z_3}{\partial b_3}\right]\\
&amp;=\left[\frac{\partial \mathcal{L}}{\partial z_1} \cdot 1, \frac{\partial \mathcal{L}}{\partial b_2} \cdot 1, \frac{\partial \mathcal{L}}{\partial b_3}\cdot \cdot 1\right]\\
&amp;= \frac{\partial \mathcal{L}}{\partial z}
\end{align*}\]</span></p>
</section>
<section id="naive-python-implementation" class="level3">
<h3 class="anchored" data-anchor-id="naive-python-implementation">Naive Python implementation</h3>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-2"><a href="#cb36-2"></a></span>
<span id="cb36-3"><a href="#cb36-3"></a>inputs <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb36-4"><a href="#cb36-4"></a>weights <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>], [<span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>], [<span class="fl">0.9</span>, <span class="fl">1.0</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span>]])</span>
<span id="cb36-5"><a href="#cb36-5"></a></span>
<span id="cb36-6"><a href="#cb36-6"></a>biases <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>])</span>
<span id="cb36-7"><a href="#cb36-7"></a></span>
<span id="cb36-8"><a href="#cb36-8"></a><span class="co"># Learning rate</span></span>
<span id="cb36-9"><a href="#cb36-9"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb36-10"><a href="#cb36-10"></a></span>
<span id="cb36-11"><a href="#cb36-11"></a></span>
<span id="cb36-12"><a href="#cb36-12"></a><span class="co"># ReLU Activation function and its derivative</span></span>
<span id="cb36-13"><a href="#cb36-13"></a><span class="kw">def</span> relu(x):</span>
<span id="cb36-14"><a href="#cb36-14"></a>    <span class="cf">return</span> np.maximum(x, <span class="dv">0</span>)</span>
<span id="cb36-15"><a href="#cb36-15"></a></span>
<span id="cb36-16"><a href="#cb36-16"></a></span>
<span id="cb36-17"><a href="#cb36-17"></a><span class="kw">def</span> relu_derivative(z):</span>
<span id="cb36-18"><a href="#cb36-18"></a>    <span class="cf">return</span> np.where(z <span class="op">&gt;</span> <span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb36-19"><a href="#cb36-19"></a></span>
<span id="cb36-20"><a href="#cb36-20"></a></span>
<span id="cb36-21"><a href="#cb36-21"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb36-22"><a href="#cb36-22"></a>    <span class="co"># Forward pass</span></span>
<span id="cb36-23"><a href="#cb36-23"></a>    z <span class="op">=</span> np.dot(weights, inputs) <span class="op">+</span> biases</span>
<span id="cb36-24"><a href="#cb36-24"></a>    a <span class="op">=</span> relu(z)</span>
<span id="cb36-25"><a href="#cb36-25"></a>    y_pred <span class="op">=</span> np.<span class="bu">sum</span>(a)</span>
<span id="cb36-26"><a href="#cb36-26"></a>    y_true <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb36-27"><a href="#cb36-27"></a>    loss <span class="op">=</span> (y_pred <span class="op">-</span> y_true) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb36-28"><a href="#cb36-28"></a></span>
<span id="cb36-29"><a href="#cb36-29"></a>    <span class="co"># Backward pass</span></span>
<span id="cb36-30"><a href="#cb36-30"></a>    <span class="co"># Gradient of loss with respect to y_pred</span></span>
<span id="cb36-31"><a href="#cb36-31"></a>    dloss_dy <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (y_pred <span class="op">-</span> y_true)</span>
<span id="cb36-32"><a href="#cb36-32"></a></span>
<span id="cb36-33"><a href="#cb36-33"></a>    <span class="co"># Gradient of y_pred with respect to a</span></span>
<span id="cb36-34"><a href="#cb36-34"></a>    dy_da <span class="op">=</span> np.ones_like(a)</span>
<span id="cb36-35"><a href="#cb36-35"></a></span>
<span id="cb36-36"><a href="#cb36-36"></a>    <span class="co"># Gradient of the activation function with respect to z</span></span>
<span id="cb36-37"><a href="#cb36-37"></a>    da_dz <span class="op">=</span> relu_derivative(z)</span>
<span id="cb36-38"><a href="#cb36-38"></a></span>
<span id="cb36-39"><a href="#cb36-39"></a>    <span class="co"># Gradient of z with respect to the weights</span></span>
<span id="cb36-40"><a href="#cb36-40"></a>    dz_dw <span class="op">=</span> inputs</span>
<span id="cb36-41"><a href="#cb36-41"></a></span>
<span id="cb36-42"><a href="#cb36-42"></a>    <span class="co"># Gradient of z with respect to inputs</span></span>
<span id="cb36-43"><a href="#cb36-43"></a>    dz_dx <span class="op">=</span> weights</span>
<span id="cb36-44"><a href="#cb36-44"></a></span>
<span id="cb36-45"><a href="#cb36-45"></a>    <span class="co"># Gradient of loss with respect to a</span></span>
<span id="cb36-46"><a href="#cb36-46"></a>    dloss_da <span class="op">=</span> dloss_dy <span class="op">*</span> dy_da</span>
<span id="cb36-47"><a href="#cb36-47"></a></span>
<span id="cb36-48"><a href="#cb36-48"></a>    <span class="co"># Gradient of loss with respect to z</span></span>
<span id="cb36-49"><a href="#cb36-49"></a>    dloss_dz <span class="op">=</span> dloss_da <span class="op">*</span> da_dz</span>
<span id="cb36-50"><a href="#cb36-50"></a></span>
<span id="cb36-51"><a href="#cb36-51"></a>    <span class="co"># Gradient of loss with respect to the weights</span></span>
<span id="cb36-52"><a href="#cb36-52"></a>    dloss_dw <span class="op">=</span> np.outer(dloss_dz, dz_dw)</span>
<span id="cb36-53"><a href="#cb36-53"></a></span>
<span id="cb36-54"><a href="#cb36-54"></a>    <span class="co"># Gradient of loss with respect to biases</span></span>
<span id="cb36-55"><a href="#cb36-55"></a>    dloss_db <span class="op">=</span> dloss_dz</span>
<span id="cb36-56"><a href="#cb36-56"></a></span>
<span id="cb36-57"><a href="#cb36-57"></a>    weights <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_dw</span>
<span id="cb36-58"><a href="#cb36-58"></a>    biases <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_db</span>
<span id="cb36-59"><a href="#cb36-59"></a></span>
<span id="cb36-60"><a href="#cb36-60"></a>    <span class="cf">if</span> (<span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">20</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb36-61"><a href="#cb36-61"></a>        <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span><span class="bu">iter</span><span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-62"><a href="#cb36-62"></a></span>
<span id="cb36-63"><a href="#cb36-63"></a><span class="bu">print</span>(<span class="st">"Final weights : "</span>, weights)</span>
<span id="cb36-64"><a href="#cb36-64"></a><span class="bu">print</span>(<span class="st">"Final bias : "</span>, biases)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration 20, loss = 6.057433318678514
Iteration 40, loss = 0.4681684867419663
Iteration 60, loss = 0.03618392815029436
Iteration 80, loss = 0.0027965928794077364
Iteration 100, loss = 0.00021614380010564146
Iteration 120, loss = 1.670537841532316e-05
Iteration 140, loss = 1.2911296454618448e-06
Iteration 160, loss = 9.978916489916474e-08
Iteration 180, loss = 7.712531012091791e-09
Iteration 200, loss = 5.96088109107831e-10
Final weights :  [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]
 [ 0.25975286  0.11950572 -0.02074143 -0.16098857]
 [ 0.53548461  0.27096922  0.00645383 -0.25806156]]
Final bias :  [-0.00698895 -0.04024714 -0.06451539]</code></pre>
</div>
</div>
</section>
</section>
<section id="backprop-with-a-batch-of-inputs" class="level2">
<h2 class="anchored" data-anchor-id="backprop-with-a-batch-of-inputs">Backprop with a batch of inputs</h2>
<p>Let <span class="math inline">\(x\)</span> be a batch of inputs of dimensions <code>[batch_size,input_size]</code>. Consider</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>x <span class="op">=</span> np.array(</span>
<span id="cb38-2"><a href="#cb38-2"></a>    [</span>
<span id="cb38-3"><a href="#cb38-3"></a>        [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">2.5</span>],</span>
<span id="cb38-4"><a href="#cb38-4"></a>        [<span class="dv">2</span>, <span class="dv">5</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb38-5"><a href="#cb38-5"></a>        [<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">2.7</span>, <span class="fl">3.3</span>, <span class="op">-</span><span class="fl">0.8</span>]</span>
<span id="cb38-6"><a href="#cb38-6"></a>    ]</span>
<span id="cb38-7"><a href="#cb38-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>of shape <code>(3,4)</code>. Each sample will give one loss. Hence, the total loss <span class="math inline">\(\mathcal{L} = L_1 + L_2 + L_3\)</span>.</p>
<section id="gradient-of-the-loss-with-respect-to-weights-w-1" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-weights-w-1">Gradient of the loss with respect to weights <span class="math inline">\(w\)</span></h3>
<p>I am going to denote use the following convention for the <span class="math inline">\(z\)</span>â€™s:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}[c|ccc]
\text{} &amp; \text{Neuron}-1 &amp; \text{Neuron}-2 &amp; \text{Neuron}-3\\
\hline
\text{Sample}-1 &amp; z_{11} &amp; z_{12} &amp; z_{13} \\
\text{Sample}-2 &amp; z_{21} &amp; z_{22} &amp; z_{23} \\
\text{Sample}-3 &amp; z_{31} &amp; z_{32} &amp; z_{33} \\
\text{Sample}-4 &amp; z_{41} &amp; z_{42} &amp; z_{43}
\end{array}
\end{align*}\]</span></p>
<p>In this case <span class="math inline">\(\frac{d\mathcal{L}}{dz}\)</span> will be a matrix of partial derivatives of shape <code>[batch_size,layer_width]</code>.</p>
<p>I can write:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial w_{11}} &amp;= \frac{\partial L_1}{\partial w_{11}} + \frac{\partial L_2}{\partial w_{11}} + \frac{\partial L_3}{\partial w_{11}} \\
&amp;= \frac{\partial L_1}{\partial z_{11}}\cdot \frac{\partial z_{11}}{\partial w_{11}} + \frac{\partial L_2}{\partial z_{21}}\cdot\frac{\partial z_{21}}{\partial w_{11}} + \frac{\partial L_3}{\partial z_{31}} \cdot \frac{\partial z_{31}}{\partial w_{11}}\\
&amp;=\frac{\partial L_1}{\partial z_{11}}\cdot x_{11} + \frac{\partial L_2}{\partial z_{21}}\cdot x_{21} + \frac{\partial L_3}{\partial z_{31}} \cdot x_{31}
\end{align*}\]</span></p>
<p>If you work out the derivatives of the loss function with respect to each of the weights, you would find:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial W} &amp;= X^T \cdot \frac{\partial \mathcal{L}}{\partial z}
\end{align*}\]</span></p>
<p><code>X.T</code> has shape <code>[input_size,batch_size]</code> and <code>dloss_dz</code> has shape <code>[batch_size,layer_width]</code>, so the matrix product will have dimensions <code>[input_size,layer_width]</code>.</p>
</section>
<section id="gradient-of-the-loss-with-respect-to-the-biases-b-1" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-the-biases-b-1">Gradient of the loss with respect to the biases <span class="math inline">\(b\)</span></h3>
<p>Consider again:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial b_1} &amp;= \frac{\partial L}{\partial z_{11}} \cdot \frac{\partial z_{11}}{\partial b_1} + \frac{\partial L}{\partial z_{21}} \cdot \frac{\partial z_{21}}{\partial b_1} + \frac{\partial L}{\partial z_{31}} \cdot \frac{\partial z_{31}}{\partial b_1} \\
&amp;= \frac{\partial L}{\partial z_{11}} \cdot 1 + \frac{\partial L}{\partial z_{21}} \cdot 1 + \frac{\partial L}{\partial z_{31}} \cdot 1
\end{align*}\]</span></p>
<p>So, to find the partial derivative of the loss with respect to <span class="math inline">\(b_1\)</span>, we will just look at the partial derivatives of the loss with respect to the first neuron and then add them up.</p>
<p>In python, we would write this as</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>dloss_dbiases <span class="op">=</span> np.<span class="bu">sum</span>(dloss_dz, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="gradient-of-the-loss-with-respect-to-the-inputs" class="level3">
<h3 class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-the-inputs">Gradient of the loss with respect to the inputs</h3>
<p>The gradients of the loss with respect to the weights in the layer <span class="math inline">\(l\)</span>, require the gradients of the loss with respect to the inputs in layer <span class="math inline">\(l+1\)</span>. Itâ€™s easy to see that:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial w_{11}^{(l)}} &amp;= \frac{\partial L}{\partial x_1^{(l+1)}}\cdot \frac{\partial x_1^{(l+1)}}{\partial z_{1}^{l}} \cdot \frac{\partial z_1^{(l)}}{\partial w_{11}^{(l)}}
\end{align*}\]</span></p>
<p>What is <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_1}\)</span>, <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_2}\)</span>, <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_3}\)</span> and <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial x_4}\)</span>?</p>
<p>By the chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial x_1} &amp;= \frac{\partial L}{\partial z_1}\cdot \frac{\partial z_1}{\partial x_1} +  \frac{\partial L}{\partial z_2}\cdot \frac{\partial z_2}{\partial x_1} +  \frac{\partial L}{\partial z_3}\cdot \frac{\partial z_3}{\partial x_1} \\
&amp;= \frac{\partial L}{\partial z_1}\cdot w_{11} +  \frac{\partial L}{\partial z_2}\cdot w_{21} +  \frac{\partial L}{\partial z_3}\cdot w_{31}
\end{align*}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial x_1} &amp; \frac{\partial \mathcal{L}}{\partial x_2} &amp; \frac{\partial \mathcal{L}}{\partial x_3} &amp; \frac{\partial \mathcal{L}}{\partial x_4}
\end{bmatrix} &amp;=
\begin{bmatrix}
\frac{\partial L}{\partial z_1} &amp; \frac{\partial L}{\partial z_2} &amp; \frac{\partial L}{\partial z_3}
\end{bmatrix}
\begin{bmatrix}
w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14}\\
w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24}\\
w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34}
\end{bmatrix}\\
\frac{\partial \mathcal{L}}{\partial x} &amp;= \frac{\partial L}{\partial z} \cdot W
\end{align*}\]</span></p>
<p>What if we have a batch of input data of 3 examples? In such case, <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial z}\)</span> will have shape <code>(3,3)</code> and <span class="math inline">\(W\)</span> will have shape <code>(3,4)</code>. So, we can multiply them and the result would be <code>(3,4)</code>.</p>
</section>
</section>
<section id="adding-backward-to-denselayer" class="level2">
<h2 class="anchored" data-anchor-id="adding-backward-to-denselayer">Adding <code>backward()</code> to <code>DenseLayer</code></h2>
<p>We will now add backward pass code to the <code>DenseLayer</code> and <code>ReLUActivation</code> classes.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="im">from</span> nnfs.datasets <span class="im">import</span> spiral_data</span>
<span id="cb40-2"><a href="#cb40-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb40-3"><a href="#cb40-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-4"><a href="#cb40-4"></a><span class="im">import</span> nnfs</span>
<span id="cb40-5"><a href="#cb40-5"></a></span>
<span id="cb40-6"><a href="#cb40-6"></a>nnfs.init()</span>
<span id="cb40-7"><a href="#cb40-7"></a></span>
<span id="cb40-8"><a href="#cb40-8"></a></span>
<span id="cb40-9"><a href="#cb40-9"></a><span class="kw">class</span> DenseLayer:</span>
<span id="cb40-10"><a href="#cb40-10"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_inputs, n_neurons):</span>
<span id="cb40-11"><a href="#cb40-11"></a>        <span class="va">self</span>.width <span class="op">=</span> n_neurons</span>
<span id="cb40-12"><a href="#cb40-12"></a>        <span class="co"># Weight vectors per neuron</span></span>
<span id="cb40-13"><a href="#cb40-13"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.array(</span>
<span id="cb40-14"><a href="#cb40-14"></a>            [[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>], [<span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>], [<span class="fl">0.9</span>, <span class="fl">1.0</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span>]]</span>
<span id="cb40-15"><a href="#cb40-15"></a>        )</span>
<span id="cb40-16"><a href="#cb40-16"></a>        <span class="va">self</span>.biases <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>])</span>
<span id="cb40-17"><a href="#cb40-17"></a></span>
<span id="cb40-18"><a href="#cb40-18"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb40-19"><a href="#cb40-19"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb40-20"><a href="#cb40-20"></a>        <span class="va">self</span>.output <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights.T) <span class="op">+</span> <span class="va">self</span>.biases</span>
<span id="cb40-21"><a href="#cb40-21"></a></span>
<span id="cb40-22"><a href="#cb40-22"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_dz):</span>
<span id="cb40-23"><a href="#cb40-23"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> dloss_dz</span>
<span id="cb40-24"><a href="#cb40-24"></a>        <span class="va">self</span>.dz_dweights <span class="op">=</span> <span class="va">self</span>.inputs</span>
<span id="cb40-25"><a href="#cb40-25"></a>        <span class="va">self</span>.dz_dbiases <span class="op">=</span> np.ones_like(<span class="va">self</span>.inputs)</span>
<span id="cb40-26"><a href="#cb40-26"></a>        <span class="va">self</span>.dz_dinputs <span class="op">=</span> <span class="va">self</span>.weights</span>
<span id="cb40-27"><a href="#cb40-27"></a>        <span class="va">self</span>.dloss_dweights <span class="op">=</span> np.dot(<span class="va">self</span>.inputs.T, <span class="va">self</span>.dloss_dz).T</span>
<span id="cb40-28"><a href="#cb40-28"></a>        <span class="va">self</span>.dloss_dbiases <span class="op">=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.dloss_dz, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-29"><a href="#cb40-29"></a>        <span class="va">self</span>.dloss_dinputs <span class="op">=</span> np.dot(<span class="va">self</span>.dloss_dz, <span class="va">self</span>.dz_dinputs)</span>
<span id="cb40-30"><a href="#cb40-30"></a></span>
<span id="cb40-31"><a href="#cb40-31"></a></span>
<span id="cb40-32"><a href="#cb40-32"></a><span class="kw">class</span> ReLUActivation:</span>
<span id="cb40-33"><a href="#cb40-33"></a></span>
<span id="cb40-34"><a href="#cb40-34"></a>    <span class="co"># Forward pass</span></span>
<span id="cb40-35"><a href="#cb40-35"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb40-36"><a href="#cb40-36"></a>        <span class="co"># Calculate output values from the inputs</span></span>
<span id="cb40-37"><a href="#cb40-37"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb40-38"><a href="#cb40-38"></a>        <span class="va">self</span>.output <span class="op">=</span> np.maximum(<span class="dv">0</span>, inputs)</span>
<span id="cb40-39"><a href="#cb40-39"></a></span>
<span id="cb40-40"><a href="#cb40-40"></a>    <span class="co"># Backward pass</span></span>
<span id="cb40-41"><a href="#cb40-41"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_da):</span>
<span id="cb40-42"><a href="#cb40-42"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> dloss_da</span>
<span id="cb40-43"><a href="#cb40-43"></a>        <span class="va">self</span>.da_dz <span class="op">=</span> np.where(<span class="va">self</span>.inputs <span class="op">&gt;</span> <span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb40-44"><a href="#cb40-44"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> <span class="va">self</span>.dloss_da <span class="op">*</span> <span class="va">self</span>.da_dz</span>
<span id="cb40-45"><a href="#cb40-45"></a></span>
<span id="cb40-46"><a href="#cb40-46"></a></span>
<span id="cb40-47"><a href="#cb40-47"></a><span class="co"># Create dataset</span></span>
<span id="cb40-48"><a href="#cb40-48"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">2.5</span>], [<span class="dv">2</span>, <span class="dv">5</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">2.7</span>, <span class="fl">3.3</span>, <span class="op">-</span><span class="fl">0.8</span>]])</span>
<span id="cb40-49"><a href="#cb40-49"></a></span>
<span id="cb40-50"><a href="#cb40-50"></a><span class="co"># Create a dense layer with 4 input features and 3 output values</span></span>
<span id="cb40-51"><a href="#cb40-51"></a>dense1 <span class="op">=</span> DenseLayer(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb40-52"><a href="#cb40-52"></a>relu <span class="op">=</span> ReLUActivation()</span>
<span id="cb40-53"><a href="#cb40-53"></a></span>
<span id="cb40-54"><a href="#cb40-54"></a><span class="co"># Perform a forward pass of our training data through this layer</span></span>
<span id="cb40-55"><a href="#cb40-55"></a>dense1.forward(X)</span>
<span id="cb40-56"><a href="#cb40-56"></a>relu.forward(dense1.output)</span>
<span id="cb40-57"><a href="#cb40-57"></a></span>
<span id="cb40-58"><a href="#cb40-58"></a><span class="co"># Calculate loss</span></span>
<span id="cb40-59"><a href="#cb40-59"></a>y_pred <span class="op">=</span> np.<span class="bu">sum</span>(relu.output)</span>
<span id="cb40-60"><a href="#cb40-60"></a>y_true <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb40-61"><a href="#cb40-61"></a>loss <span class="op">=</span> (y_pred <span class="op">-</span> y_true) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb40-62"><a href="#cb40-62"></a></span>
<span id="cb40-63"><a href="#cb40-63"></a><span class="co"># Gradient of the loss with respect to y</span></span>
<span id="cb40-64"><a href="#cb40-64"></a>dloss_dy <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (y_pred <span class="op">-</span> y_true)</span>
<span id="cb40-65"><a href="#cb40-65"></a>dy_da <span class="op">=</span> np.ones_like(relu.output)</span>
<span id="cb40-66"><a href="#cb40-66"></a>dloss_da <span class="op">=</span> dloss_dy <span class="op">*</span> dy_da</span>
<span id="cb40-67"><a href="#cb40-67"></a></span>
<span id="cb40-68"><a href="#cb40-68"></a>relu.backward(dloss_da)</span>
<span id="cb40-69"><a href="#cb40-69"></a>dense1.backward(relu.dloss_dz)</span>
<span id="cb40-70"><a href="#cb40-70"></a><span class="bu">print</span>(<span class="ss">f"dloss_dweights = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dweights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-71"><a href="#cb40-71"></a><span class="bu">print</span>(<span class="ss">f"dloss_dbiases = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dbiases<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-72"><a href="#cb40-72"></a><span class="bu">print</span>(<span class="ss">f"dloss_dinputs = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dinputs<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dloss_dweights = [[124.560005 805.48804  440.112    307.24802 ]
 [124.560005 805.48804  440.112    307.24802 ]
 [124.560005 805.48804  440.112    307.24802 ]]
dloss_dbiases = [[249.12000303 249.12000303 249.12000303]]
dloss_dinputs = [[124.560005 149.472    174.384    199.296   ]
 [124.560005 149.472    174.384    199.296   ]
 [124.560005 149.472    174.384    199.296   ]]</code></pre>
</div>
</div>
</section>
<section id="categorical-cross-entropy-loss-derivative" class="level2">
<h2 class="anchored" data-anchor-id="categorical-cross-entropy-loss-derivative">Categorical cross-entropy loss derivative</h2>
<p>The cross-entropy loss of the <span class="math inline">\(i\)</span>-th sample is given by:</p>
<p><span class="math display">\[\begin{align*}
L_i = -\sum_k y_{ik}log(\hat{y}_ik)
\end{align*}\]</span></p>
<p>Differentiating with respect to <span class="math inline">\(\hat{y}_{ij}\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L_i}{\partial \hat{y}_{ij}} &amp;= -\frac{\partial}{\partial \hat{y}_{ik}} \left[\sum_k y_{ik}\log (\hat{y}_{ik})\right] \\
&amp;= -y_{ij} \cdot \frac{\partial }{\partial \hat{y}_{ij}} \log (\hat{y}_{ij})\\
&amp;= -\frac{y_{ij}}{\hat{y}_{ij}}
\end{align*}\]</span></p>
<section id="adding-backward-to-categoricalcrossentropyloss" class="level3">
<h3 class="anchored" data-anchor-id="adding-backward-to-categoricalcrossentropyloss">Adding <code>backward()</code> to <code>CategoricalCrossEntropyLoss</code></h3>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="co"># Cross-Entropy loss</span></span>
<span id="cb42-2"><a href="#cb42-2"></a><span class="kw">class</span> CategoricalCrossEntropyLoss(Loss):</span>
<span id="cb42-3"><a href="#cb42-3"></a></span>
<span id="cb42-4"><a href="#cb42-4"></a>    <span class="co"># Forward pass</span></span>
<span id="cb42-5"><a href="#cb42-5"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb42-6"><a href="#cb42-6"></a>        num_samples <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb42-7"><a href="#cb42-7"></a></span>
<span id="cb42-8"><a href="#cb42-8"></a>        <span class="co"># Clip data to prevent division by 0</span></span>
<span id="cb42-9"><a href="#cb42-9"></a>        <span class="co"># Clip both sides to not drag mean towards any value</span></span>
<span id="cb42-10"><a href="#cb42-10"></a>        epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb42-11"><a href="#cb42-11"></a>        y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span> <span class="op">-</span> epsilon)</span>
<span id="cb42-12"><a href="#cb42-12"></a></span>
<span id="cb42-13"><a href="#cb42-13"></a>        <span class="co"># If categorical labels</span></span>
<span id="cb42-14"><a href="#cb42-14"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb42-15"><a href="#cb42-15"></a>            correct_confidences <span class="op">=</span> y_pred_clipped[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb42-16"><a href="#cb42-16"></a>        <span class="co"># else if one-hot encoding</span></span>
<span id="cb42-17"><a href="#cb42-17"></a>        <span class="cf">elif</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb42-18"><a href="#cb42-18"></a>            correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred_clipped <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-19"><a href="#cb42-19"></a></span>
<span id="cb42-20"><a href="#cb42-20"></a>        neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb42-21"><a href="#cb42-21"></a>        <span class="cf">return</span> neg_log</span>
<span id="cb42-22"><a href="#cb42-22"></a></span>
<span id="cb42-23"><a href="#cb42-23"></a>    <span class="co"># Backward pass</span></span>
<span id="cb42-24"><a href="#cb42-24"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb42-25"><a href="#cb42-25"></a></span>
<span id="cb42-26"><a href="#cb42-26"></a>        <span class="co"># number of samples</span></span>
<span id="cb42-27"><a href="#cb42-27"></a>        batch_size <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb42-28"><a href="#cb42-28"></a></span>
<span id="cb42-29"><a href="#cb42-29"></a>        <span class="co"># number of labels</span></span>
<span id="cb42-30"><a href="#cb42-30"></a>        num_labels <span class="op">=</span> <span class="bu">len</span>(y_pred[<span class="dv">0</span>])</span>
<span id="cb42-31"><a href="#cb42-31"></a></span>
<span id="cb42-32"><a href="#cb42-32"></a>        <span class="co"># If labels are sparse, turn them into a one-hot vector</span></span>
<span id="cb42-33"><a href="#cb42-33"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb42-34"><a href="#cb42-34"></a>            y_true <span class="op">=</span> np.eye(num_labels)[y_true]</span>
<span id="cb42-35"><a href="#cb42-35"></a></span>
<span id="cb42-36"><a href="#cb42-36"></a>        <span class="co"># Calculate gradient</span></span>
<span id="cb42-37"><a href="#cb42-37"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> <span class="op">-</span>y_true <span class="op">/</span> y_pred</span>
<span id="cb42-38"><a href="#cb42-38"></a></span>
<span id="cb42-39"><a href="#cb42-39"></a>        <span class="co"># Normalize the gradient</span></span>
<span id="cb42-40"><a href="#cb42-40"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> <span class="va">self</span>.dloss_da <span class="op">/</span> batch_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="softmax-activation-function-derivative" class="level2">
<h2 class="anchored" data-anchor-id="softmax-activation-function-derivative">Softmax Activation function derivative</h2>
<p>We are interested to calculate the derivative of the softmax function. The softmax activation function is defined as:</p>
<p><span class="math display">\[\begin{align*}
S_{i,j} &amp;= \frac{e^{z_{i,j}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(S_{i,j}\)</span> denotes the output of the <span class="math inline">\(j\)</span>-th neuron for the <span class="math inline">\(i\)</span>-th sample. Thus, <span class="math inline">\(S_{i,j} = f(z_{i,1},\ldots,z_{i,d_l})\)</span>. Letâ€™s calculate the partial derivative of <span class="math inline">\(S_{i,j}\)</span> with respect to <span class="math inline">\(z_{i,k}\)</span>.</p>
<p>By the <span class="math inline">\(u/v\)</span> rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= \frac{\sum_{l=1}^{d_l} e^{z_{i,l}} \cdot \frac{\partial e^{z_{i,j}}}{\partial z_{i,k}}-e^{z_{i,j}} \cdot \frac{\partial}{\partial z_{i,k}} \sum_{l=1}^{d_l} e^{z_{i,l}}}{\left(\sum_{l=1}^{d_l} e^{z_{i,l}}\right)^2}
\end{align*}\]</span></p>
<p>We have two cases. If <span class="math inline">\(j=k\)</span>, then <span class="math inline">\(\frac{\partial e^{z_{i,j}}}{\partial z_{i,k}} = e^{z_{i,k}}\)</span> and we get:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= \frac{e^{z_{i,k}} \cdot \sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}} \cdot e^{z_{i,k}}}{\left(\sum_{l=1}^{d_l} e^{z_{i,l}}\right)^2}\\
&amp;=\frac{e^{z_{i,k}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}} \cdot \frac{\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}}\\
&amp;=S_{i,k}(1-S_{i,k})
\end{align*}\]</span></p>
<p>In the case where <span class="math inline">\(j \neq k\)</span>, <span class="math inline">\(\frac{\partial e^{z_{i,j}}}{\partial z_{i,k}} = 0\)</span> and we have:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= -\frac{e^{z_{i,j}}}{\sum_{l=1}^{d_l}e^{z_{i,l}}}\cdot \frac{e^{z_{i,k}}}{\sum_{l=1}^{d_l}e^{z_{i,l}}}\\
&amp;=-S_{i,j} S_{i,k}
\end{align*}\]</span></p>
<p>So, the derivative of the softmax activation function can be expressed in terms of Kroneckerâ€™s delta as:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= S_{i,j}(\delta_{j,k} -  S_{i,k})\\
&amp;= S_{i,j} \delta_{j,k} - S_{i,j}S_{i,k}
\end{align*}\]</span></p>
<p>Now, like before, letâ€™s say we have neural network with a single hidden layer with <span class="math inline">\(d_1 = 3\)</span> neurons. We apply the softmax activation function to the output of this layer. The jacobian matrix <span class="math inline">\(\frac{\partial S_i}{\partial z_i}\)</span> for the <span class="math inline">\(i\)</span>-th sample can be expressed as:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial S_i}{\partial z_i} &amp;=
\begin{bmatrix}
\frac{\partial S_{i1}}{\partial z_{i1}} &amp; \frac{\partial S_{i1}}{\partial z_{i2}} &amp; \frac{\partial S_{i1}}{\partial z_{i3}} \\
\frac{\partial S_{i2}}{\partial z_{i1}} &amp; \frac{\partial S_{i2}}{\partial z_{i2}} &amp; \frac{\partial S_{i2}}{\partial z_{i3}} \\
\frac{\partial S_{i3}}{\partial z_{i1}} &amp; \frac{\partial S_{i3}}{\partial z_{i2}} &amp; \frac{\partial S_{i3}}{\partial z_{i3}}
\end{bmatrix}\\
&amp;=\begin{bmatrix}
S_{i1}(\delta_{11} - S_{i1}) &amp; S_{i1}(\delta_{12} - S_{i2}) &amp; S_{i1}(\delta_{13} - S_{i3}) \\
S_{i2}(\delta_{21} - S_{i1}) &amp; S_{i2}(\delta_{22} - S_{i2}) &amp; S_{i2}(\delta_{23} - S_{i3}) \\
S_{i3}(\delta_{31} - S_{i1}) &amp; S_{i3}(\delta_{32} - S_{i2}) &amp; S_{i3}(\delta_{33} - S_{i3})
\end{bmatrix}\\
&amp;=\begin{bmatrix}
S_{i1}(1 - S_{i1}) &amp; S_{i1}(0 - S_{i2}) &amp; S_{i1}(0 - S_{i3}) \\
S_{i2}(0 - S_{i1}) &amp; S_{i2}(1 - S_{i2}) &amp; S_{i2}(0 - S_{i3}) \\
S_{i3}(0 - S_{i1}) &amp; S_{i3}(0 - S_{i2}) &amp; S_{i3}(1 - S_{i3})
\end{bmatrix}\\
&amp;=\begin{bmatrix}
S_{i1}\\
S_{i2}\\
S_{i3}
\end{bmatrix}\odot
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} -
\begin{bmatrix}
S_{i1}\\
S_{i2}\\
S_{i3}
\end{bmatrix}\begin{bmatrix}
S_{i1} &amp; S_{i2} &amp; S_{i3}
\end{bmatrix}
\end{align*}\]</span></p>
<p>Say the <code>softmax_output=[0.70, 0.10, 0.20]</code>. Then, in python, we can find the Jacobian matrix as:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-2"><a href="#cb43-2"></a></span>
<span id="cb43-3"><a href="#cb43-3"></a>softmax_output <span class="op">=</span> np.array([<span class="fl">0.70</span>, <span class="fl">0.10</span>, <span class="fl">0.20</span>])</span>
<span id="cb43-4"><a href="#cb43-4"></a></span>
<span id="cb43-5"><a href="#cb43-5"></a><span class="co"># Reshape as a column vector</span></span>
<span id="cb43-6"><a href="#cb43-6"></a>softmax_output <span class="op">=</span> softmax_output.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb43-7"><a href="#cb43-7"></a></span>
<span id="cb43-8"><a href="#cb43-8"></a>da_dz <span class="op">=</span> np.diagflat(softmax_output) <span class="op">-</span> np.dot(softmax_output, softmax_output.T)</span>
<span id="cb43-9"><a href="#cb43-9"></a></span>
<span id="cb43-10"><a href="#cb43-10"></a><span class="bu">print</span>(<span class="ss">f"softmax_output = </span><span class="sc">{</span>softmax_output<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb43-11"><a href="#cb43-11"></a><span class="bu">print</span>(<span class="ss">f"da_dz = </span><span class="sc">{</span>da_dz<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>softmax_output = [[0.7]
 [0.1]
 [0.2]]
da_dz = [[ 0.20999999 -0.07       -0.14      ]
 [-0.07        0.09       -0.02      ]
 [-0.14       -0.02        0.16      ]]</code></pre>
</div>
</div>
<p>What happens when we have a batch of inputs? By the chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial z_{11}} &amp;= \frac{\partial L}{\partial S_{11}} \cdot \frac{\partial S_{11}}{\partial z_{11}} + \frac{\partial L}{\partial S_{12}} \cdot \frac{\partial S_{12}}{\partial z_{11}} + \frac{\partial L}{\partial S_{13}}\cdot \frac{\partial S_{13}}{\partial z_{11}}
\end{align*}\]</span></p>
<p>In general,</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial z_{ij}} &amp;= \frac{\partial L}{\partial S_{i1}} \cdot \frac{\partial S_{i1}}{\partial z_{ij}} + \frac{\partial L}{\partial S_{i2}} \cdot \frac{\partial S_{i2}}{\partial z_{ij}} + \frac{\partial L}{\partial S_{i3}}\cdot \frac{\partial S_{i3}}{\partial z_{ij}}\\
&amp;=\sum_{k=1}^{3} \frac{\partial L}{\partial S_{ik}} \cdot \frac{\partial S_{ik}}{\partial z_{ij}}
\end{align*}\]</span></p>
<p>It follows that:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L}{\partial z_i} &amp;= \begin{bmatrix}
\frac{\partial L}{\partial z_{i1}} &amp; \frac{\partial L}{\partial z_{i2}} &amp; \frac{\partial L}{\partial z_{i3}}
\end{bmatrix}\\
&amp;=\begin{bmatrix}
\frac{\partial L}{\partial S_{i1}} &amp; \frac{\partial L}{\partial S_{i2}} &amp; \frac{\partial L}{\partial S_{i3}}
\end{bmatrix} \begin{bmatrix}
\frac{\partial S_{i1}}{\partial z_{i1}} &amp; \frac{\partial S_{i1}}{\partial z_{i2}} &amp; \frac{\partial S_{i1}}{\partial z_{i3}} \\
\frac{\partial S_{i2}}{\partial z_{i1}} &amp; \frac{\partial S_{i2}}{\partial z_{i2}} &amp; \frac{\partial S_{i2}}{\partial z_{i3}} \\
\frac{\partial S_{i3}}{\partial z_{i1}} &amp; \frac{\partial S_{i3}}{\partial z_{i2}} &amp; \frac{\partial S_{i3}}{\partial z_{i3}}
\end{bmatrix}\\
&amp;=\frac{\partial L}{\partial S_i} \cdot \frac{\partial S_i}{\partial z_i}
\end{align*}\]</span></p>
<p>Now, <span class="math inline">\(\partial L/\partial S_i\)</span> has shape <code>[1,3]</code> and <span class="math inline">\(\partial S_i/\partial z_i\)</span> is a matrix of size <code>[3,3]</code>. So, <span class="math inline">\(\partial L/\partial z_i\)</span> will have dimensions <code>[1,3]</code>.</p>
</section>
<section id="softmax-backward-implementation" class="level2">
<h2 class="anchored" data-anchor-id="softmax-backward-implementation">Softmax <code>backward()</code> implementation</h2>
<p>We are now in a position to add <code>backward()</code> pass to the <code>SoftmaxActivation</code> layer.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="kw">class</span> SoftmaxActivation:</span>
<span id="cb45-2"><a href="#cb45-2"></a></span>
<span id="cb45-3"><a href="#cb45-3"></a>    <span class="co"># Forward pass</span></span>
<span id="cb45-4"><a href="#cb45-4"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb45-5"><a href="#cb45-5"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb45-6"><a href="#cb45-6"></a>        exp_values <span class="op">=</span> np.exp(inputs <span class="op">-</span> np.<span class="bu">max</span>(inputs, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb45-7"><a href="#cb45-7"></a>        probabilities <span class="op">=</span> exp_values <span class="op">/</span> np.<span class="bu">sum</span>(exp_values, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-8"><a href="#cb45-8"></a>        <span class="va">self</span>.output <span class="op">=</span> probabilities</span>
<span id="cb45-9"><a href="#cb45-9"></a></span>
<span id="cb45-10"><a href="#cb45-10"></a>    <span class="co"># Backward pass</span></span>
<span id="cb45-11"><a href="#cb45-11"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_da):</span>
<span id="cb45-12"><a href="#cb45-12"></a>        dloss_dz <span class="op">=</span> []</span>
<span id="cb45-13"><a href="#cb45-13"></a>        n <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.output)</span>
<span id="cb45-14"><a href="#cb45-14"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb45-15"><a href="#cb45-15"></a>            softmax_output <span class="op">=</span> <span class="va">self</span>.output[i]</span>
<span id="cb45-16"><a href="#cb45-16"></a></span>
<span id="cb45-17"><a href="#cb45-17"></a>            <span class="co"># Reshape as a column vector</span></span>
<span id="cb45-18"><a href="#cb45-18"></a>            softmax_output <span class="op">=</span> softmax_output.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb45-19"><a href="#cb45-19"></a></span>
<span id="cb45-20"><a href="#cb45-20"></a>            dsoftmax_dz <span class="op">=</span> np.diagflat(softmax_output) <span class="op">-</span> np.dot(</span>
<span id="cb45-21"><a href="#cb45-21"></a>                softmax_output, softmax_output.T</span>
<span id="cb45-22"><a href="#cb45-22"></a>            )</span>
<span id="cb45-23"><a href="#cb45-23"></a>            dloss_dz.append(np.dot(dloss_da[i], dsoftmax_dz))</span>
<span id="cb45-24"><a href="#cb45-24"></a></span>
<span id="cb45-25"><a href="#cb45-25"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> np.array(dloss_dz)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="categorical-cross-entropy-loss-and-softmax-activation-function-derivative" class="level2">
<h2 class="anchored" data-anchor-id="categorical-cross-entropy-loss-and-softmax-activation-function-derivative">Categorical cross-entropy loss and softmax activation function derivative</h2>
<p>The derivative of the categorical cross entropy loss and softmax activation function can be combined and results in a faster and simple implementation. The current implementation of the <code>backward</code> function in <code>SoftMaxActivation</code> is not vectorized and has a loop.</p>
<p>Letâ€™s focus again on <span class="math inline">\(\frac{\partial L_{i}}{\partial z_{ij}}\)</span>. We have:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L_i}{\partial z_{ij}} &amp;= \sum_{k} \frac{\partial L_i}{\partial S_{ik}} \frac{\partial S_{ik}}{\partial z_{ij}} \\
&amp;= \frac{\partial L_i}{S_{ij}} \cdot \frac{\partial S_{ij}}{\partial z_{ij}} + \sum_{k\neq j}\frac{\partial L_i}{\partial S_{ik}} \frac{\partial S_{ik}}{\partial z_{ij}} \\
&amp;= -\frac{y_{ij}}{\hat{y}_{ij}}\hat{y}_{ij}(1-\hat{y}_{ij}) + \sum_{k \neq j}-\frac{y_{ik}}{\hat{y}_{ik}}\cdot \hat{y}_{ik}(0 - \hat{y}_{ij})\\
&amp;= -\frac{y_{ij}}{\cancel{\hat{y}_{ij}}}\cancel{\hat{y}_{ij}}(1-\hat{y}_{ij}) + \sum_{k \neq j}-\frac{y_{ik}}{\cancel{\hat{y}_{ik}}}\cdot \cancel{\hat{y}_{ik}}(0 - \hat{y}_{ij})\\
&amp;= -y_{ij} + y_{ij}\hat{y}_{ij} + \sum_{k\neq j}y_{ik} \hat{y}_{ij}\\
&amp;= -y_{ij} + \hat{y}_{ij}(\sum_{k}y_{ik})\\
&amp;= \hat{y}_{ij} - y_{ij}
\end{align*}\]</span></p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="kw">class</span> CategoricalCrossEntropySoftmax:</span>
<span id="cb46-2"><a href="#cb46-2"></a></span>
<span id="cb46-3"><a href="#cb46-3"></a>    <span class="co"># create activation and loss function objects</span></span>
<span id="cb46-4"><a href="#cb46-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb46-5"><a href="#cb46-5"></a>        <span class="va">self</span>.activation <span class="op">=</span> SoftmaxActivation()</span>
<span id="cb46-6"><a href="#cb46-6"></a>        <span class="va">self</span>.loss <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb46-7"><a href="#cb46-7"></a></span>
<span id="cb46-8"><a href="#cb46-8"></a>    <span class="co"># forward pass</span></span>
<span id="cb46-9"><a href="#cb46-9"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs, y_true):</span>
<span id="cb46-10"><a href="#cb46-10"></a></span>
<span id="cb46-11"><a href="#cb46-11"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb46-12"><a href="#cb46-12"></a>        <span class="va">self</span>.activation.forward(inputs)</span>
<span id="cb46-13"><a href="#cb46-13"></a></span>
<span id="cb46-14"><a href="#cb46-14"></a>        <span class="va">self</span>.output <span class="op">=</span> <span class="va">self</span>.activation.output</span>
<span id="cb46-15"><a href="#cb46-15"></a></span>
<span id="cb46-16"><a href="#cb46-16"></a>        <span class="cf">return</span> <span class="va">self</span>.loss.calculate(<span class="va">self</span>.output, y_true)</span>
<span id="cb46-17"><a href="#cb46-17"></a></span>
<span id="cb46-18"><a href="#cb46-18"></a>    <span class="co"># Backward pass</span></span>
<span id="cb46-19"><a href="#cb46-19"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb46-20"><a href="#cb46-20"></a>        <span class="co"># number of samples</span></span>
<span id="cb46-21"><a href="#cb46-21"></a>        batch_size <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb46-22"><a href="#cb46-22"></a></span>
<span id="cb46-23"><a href="#cb46-23"></a>        <span class="co"># number of labels</span></span>
<span id="cb46-24"><a href="#cb46-24"></a>        num_labels <span class="op">=</span> <span class="bu">len</span>(y_pred[<span class="dv">0</span>])</span>
<span id="cb46-25"><a href="#cb46-25"></a></span>
<span id="cb46-26"><a href="#cb46-26"></a>        <span class="co"># If labels are sparse, turn them into a one-hot vector</span></span>
<span id="cb46-27"><a href="#cb46-27"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb46-28"><a href="#cb46-28"></a>            y_true <span class="op">=</span> np.eye(num_labels)[y_true]</span>
<span id="cb46-29"><a href="#cb46-29"></a></span>
<span id="cb46-30"><a href="#cb46-30"></a>        <span class="co"># Calculate the gradient</span></span>
<span id="cb46-31"><a href="#cb46-31"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> y_pred <span class="op">-</span> y_true</span>
<span id="cb46-32"><a href="#cb46-32"></a></span>
<span id="cb46-33"><a href="#cb46-33"></a>        <span class="co"># Normalize the gradient</span></span>
<span id="cb46-34"><a href="#cb46-34"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> <span class="va">self</span>.dloss_dz <span class="op">/</span> batch_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now test if the combined backward step returns the same values compared to when we backpropogate gradients through both of the functions separately.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-2"><a href="#cb47-2"></a><span class="im">import</span> nnfs</span>
<span id="cb47-3"><a href="#cb47-3"></a></span>
<span id="cb47-4"><a href="#cb47-4"></a>nnfs.init()</span>
<span id="cb47-5"><a href="#cb47-5"></a></span>
<span id="cb47-6"><a href="#cb47-6"></a>softmax_outputs <span class="op">=</span> np.array([[<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>], [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>], [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]])</span>
<span id="cb47-7"><a href="#cb47-7"></a></span>
<span id="cb47-8"><a href="#cb47-8"></a>class_targets <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb47-9"><a href="#cb47-9"></a></span>
<span id="cb47-10"><a href="#cb47-10"></a></span>
<span id="cb47-11"><a href="#cb47-11"></a>activation <span class="op">=</span> SoftmaxActivation()</span>
<span id="cb47-12"><a href="#cb47-12"></a>activation.output <span class="op">=</span> softmax_outputs</span>
<span id="cb47-13"><a href="#cb47-13"></a></span>
<span id="cb47-14"><a href="#cb47-14"></a>loss <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb47-15"><a href="#cb47-15"></a>loss.backward(softmax_outputs, class_targets)</span>
<span id="cb47-16"><a href="#cb47-16"></a><span class="bu">print</span>(<span class="st">"Gradients : separate loss and activation"</span>)</span>
<span id="cb47-17"><a href="#cb47-17"></a><span class="bu">print</span>(<span class="ss">f"dloss_da = </span><span class="sc">{</span>loss<span class="sc">.</span>dloss_da<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-18"><a href="#cb47-18"></a></span>
<span id="cb47-19"><a href="#cb47-19"></a>activation.backward(loss.dloss_da)</span>
<span id="cb47-20"><a href="#cb47-20"></a><span class="bu">print</span>(<span class="ss">f"dloss_dz = </span><span class="sc">{</span>activation<span class="sc">.</span>dloss_dz<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-21"><a href="#cb47-21"></a></span>
<span id="cb47-22"><a href="#cb47-22"></a>softmax_cce <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb47-23"><a href="#cb47-23"></a>softmax_cce.backward(softmax_outputs, class_targets)</span>
<span id="cb47-24"><a href="#cb47-24"></a><span class="bu">print</span>(<span class="st">"Gradients : combined loss and activation"</span>)</span>
<span id="cb47-25"><a href="#cb47-25"></a><span class="bu">print</span>(<span class="ss">f"dloss_dz = </span><span class="sc">{</span>softmax_cce<span class="sc">.</span>dloss_dz<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Gradients : separate loss and activation
dloss_da = [[-0.47619048 -0.         -0.        ]
 [-0.         -0.66666667 -0.        ]
 [-0.         -0.37037037 -0.        ]]
dloss_dz = [[-0.09999999  0.03333334  0.06666667]
 [ 0.03333334 -0.16666667  0.13333334]
 [ 0.00666667 -0.03333333  0.02666667]]
Gradients : combined loss and activation
dloss_dz = [[-0.1         0.03333333  0.06666667]
 [ 0.03333333 -0.16666667  0.13333333]
 [ 0.00666667 -0.03333333  0.02666667]]</code></pre>
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb49" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb49-1"><a href="#cb49-1"></a><span class="co">---</span></span>
<span id="cb49-2"><a href="#cb49-2"></a><span class="an">title:</span><span class="co"> "Backpropogation"</span></span>
<span id="cb49-3"><a href="#cb49-3"></a><span class="an">author:</span><span class="co"> "Quasar"</span></span>
<span id="cb49-4"><a href="#cb49-4"></a><span class="an">date:</span><span class="co"> "2024-06-05"</span></span>
<span id="cb49-5"><a href="#cb49-5"></a><span class="an">categories:</span><span class="co"> [Machine Learning]      </span></span>
<span id="cb49-6"><a href="#cb49-6"></a><span class="an">image:</span><span class="co"> "image.jpg"</span></span>
<span id="cb49-7"><a href="#cb49-7"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb49-8"><a href="#cb49-8"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb49-9"><a href="#cb49-9"></a><span class="co">---</span></span>
<span id="cb49-10"><a href="#cb49-10"></a></span>
<span id="cb49-11"><a href="#cb49-11"></a><span class="fu">## Calculating the network error with Loss</span></span>
<span id="cb49-12"><a href="#cb49-12"></a></span>
<span id="cb49-13"><a href="#cb49-13"></a>With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model's accuracy and confidence. To do this, we calculate the error in our model. The *loss function* also referred to as the *cost function* quantifies the error. </span>
<span id="cb49-14"><a href="#cb49-14"></a></span>
<span id="cb49-15"><a href="#cb49-15"></a><span class="fu">### Logit vector</span></span>
<span id="cb49-16"><a href="#cb49-16"></a></span>
<span id="cb49-17"><a href="#cb49-17"></a>Let $\vec{l} = \mathbf{w}\cdot \mathbf{x} + \mathbf{b}$ be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the **logit vector** in machine learning literature.</span>
<span id="cb49-18"><a href="#cb49-18"></a></span>
<span id="cb49-19"><a href="#cb49-19"></a><span class="fu">### Entropy, Cross-Entropy and KL-Divergence</span></span>
<span id="cb49-20"><a href="#cb49-20"></a></span>
<span id="cb49-21"><a href="#cb49-21"></a>Let $X$ be a random variable with possible outcomes $\mathcal{X}$. Let $P$ be the true probability distribution of $X$ with probability mass function $p(x)$. Let $Q$ be an approximating distribution with probability mass function $q(x)$.</span>
<span id="cb49-22"><a href="#cb49-22"></a></span>
<span id="cb49-23"><a href="#cb49-23"></a>*Definition*.  The entropy of $P$ is defined as:</span>
<span id="cb49-24"><a href="#cb49-24"></a></span>
<span id="cb49-25"><a href="#cb49-25"></a>\begin{align*}</span>
<span id="cb49-26"><a href="#cb49-26"></a>H(P) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log p(x)</span>
<span id="cb49-27"><a href="#cb49-27"></a>\end{align*}</span>
<span id="cb49-28"><a href="#cb49-28"></a></span>
<span id="cb49-29"><a href="#cb49-29"></a>In information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm $\log p(x)$, *we concentrate on the order of the surprise*. Entropy, then, is an expectation over the uncertainties or the *expected surprise*. </span>
<span id="cb49-30"><a href="#cb49-30"></a></span>
<span id="cb49-31"><a href="#cb49-31"></a>*Definition*.  The cross-entropy of $Q$ relative to $P$ is defined as:</span>
<span id="cb49-32"><a href="#cb49-32"></a></span>
<span id="cb49-33"><a href="#cb49-33"></a>\begin{align*}</span>
<span id="cb49-34"><a href="#cb49-34"></a>H(P,Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log q(x)</span>
<span id="cb49-35"><a href="#cb49-35"></a>\end{align*}</span>
<span id="cb49-36"><a href="#cb49-36"></a></span>
<span id="cb49-37"><a href="#cb49-37"></a>*Definition*. For discrete distributions $P$ and $Q$ defined on the sample space $\mathcal{X}$, the *Kullback-Leibler(KL) divergence* (or relative entropy) from $Q$ to $P$ is defined as:</span>
<span id="cb49-38"><a href="#cb49-38"></a></span>
<span id="cb49-39"><a href="#cb49-39"></a>\begin{align*}</span>
<span id="cb49-40"><a href="#cb49-40"></a>D_{KL}(P||Q) = -\sum_{x\in\mathcal{X}} p(x) \cdot \log \frac{p(x)}{q(x)}</span>
<span id="cb49-41"><a href="#cb49-41"></a>\end{align*}</span>
<span id="cb49-42"><a href="#cb49-42"></a></span>
<span id="cb49-43"><a href="#cb49-43"></a>Intuitively, it is the expected excess surprise from using $Q$ as a model instead of $P$, when the actual distribution is $P$. Note that, $D_{KL}(P||Q) \neq D_{KL}(Q||P)$, so it is not symmetric and hence it is not a norm.</span>
<span id="cb49-44"><a href="#cb49-44"></a></span>
<span id="cb49-45"><a href="#cb49-45"></a><span class="fu">### Categorical cross-entropy loss function</span></span>
<span id="cb49-46"><a href="#cb49-46"></a></span>
<span id="cb49-47"><a href="#cb49-47"></a>We are going to work on a multi-class classification problem. </span>
<span id="cb49-48"><a href="#cb49-48"></a></span>
<span id="cb49-49"><a href="#cb49-49"></a>For any input $\mathbf{x}_i$, the target vector $\mathbf{y}_i$ could be specified using *one-hot* encoding or an integer in the range <span class="in">`[0,numClasses)`</span>. </span>
<span id="cb49-50"><a href="#cb49-50"></a></span>
<span id="cb49-51"><a href="#cb49-51"></a>Let's say, we have <span class="in">`numClasses = 3`</span>. </span>
<span id="cb49-52"><a href="#cb49-52"></a></span>
<span id="cb49-53"><a href="#cb49-53"></a>In one-hot encoding, the target vector <span class="in">`y_true`</span> is an array like <span class="in">`[1, 0, 0]`</span>, <span class="in">`[0, 1, 0]`</span>, or <span class="in">`[0, 0, 1]`</span>. The category/class is determined by the index which is **hot**. For example, if <span class="in">`y_true`</span> equals <span class="in">`[0, 1, 0]`</span>, then the sample belongs to class $1$, whilst if <span class="in">`y_true`</span> equals <span class="in">`[0, 0, 1]`</span>, the sample belongs to class $2$. </span>
<span id="cb49-54"><a href="#cb49-54"></a></span>
<span id="cb49-55"><a href="#cb49-55"></a>In integer encoding, the target vector <span class="in">`y_true`</span> is an integer. For example, if <span class="in">`y_true`</span> equals $1$, the sample belongs to class $1$, whilst if <span class="in">`y_true`</span> equals $2$, the sample belongs to class $2$. </span>
<span id="cb49-56"><a href="#cb49-56"></a></span>
<span id="cb49-57"><a href="#cb49-57"></a>The <span class="in">`categorical_crossentropy`</span> is defined as:</span>
<span id="cb49-58"><a href="#cb49-58"></a></span>
<span id="cb49-59"><a href="#cb49-59"></a>\begin{align*}</span>
<span id="cb49-60"><a href="#cb49-60"></a>L_i = -\sum_{j} y_{i,j} \log(\hat{y}_{i,j})</span>
<span id="cb49-61"><a href="#cb49-61"></a>\end{align*}</span>
<span id="cb49-62"><a href="#cb49-62"></a></span>
<span id="cb49-63"><a href="#cb49-63"></a>Assume that we have a softmax output $\hat{\mathbf{y}}_i$, <span class="in">`[0.7, 0.1, 0.2]`</span> and target vector $\mathbf{y}_i$ <span class="in">`[1, 0, 0]`</span>. Then, we can compute the categorical cross entropy loss as:</span>
<span id="cb49-64"><a href="#cb49-64"></a></span>
<span id="cb49-65"><a href="#cb49-65"></a>\begin{align*}</span>
<span id="cb49-66"><a href="#cb49-66"></a>-\left(1\cdot \log (0.7) + 0 \cdot \log (0.1) + 0 \cdot \log(0.2)\right) = 0.35667494</span>
<span id="cb49-67"><a href="#cb49-67"></a>\end{align*}</span>
<span id="cb49-68"><a href="#cb49-68"></a></span>
<span id="cb49-69"><a href="#cb49-69"></a>Let's that we have a batch of $3$ samples. Additionally, suppose the target <span class="in">`y_true`</span> is integer encoded. After running through the softmax activation function, the network's output layer yields:</span>
<span id="cb49-70"><a href="#cb49-70"></a></span>
<span id="cb49-73"><a href="#cb49-73"></a><span class="in">```{python}</span></span>
<span id="cb49-74"><a href="#cb49-74"></a><span class="op">%</span>load_ext itikz</span>
<span id="cb49-75"><a href="#cb49-75"></a><span class="in">```</span></span>
<span id="cb49-76"><a href="#cb49-76"></a></span>
<span id="cb49-79"><a href="#cb49-79"></a><span class="in">```{python}</span></span>
<span id="cb49-80"><a href="#cb49-80"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-81"><a href="#cb49-81"></a></span>
<span id="cb49-82"><a href="#cb49-82"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb49-83"><a href="#cb49-83"></a>    [</span>
<span id="cb49-84"><a href="#cb49-84"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb49-85"><a href="#cb49-85"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb49-86"><a href="#cb49-86"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb49-87"><a href="#cb49-87"></a>    ]</span>
<span id="cb49-88"><a href="#cb49-88"></a>)</span>
<span id="cb49-89"><a href="#cb49-89"></a></span>
<span id="cb49-90"><a href="#cb49-90"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb49-91"><a href="#cb49-91"></a><span class="in">```</span></span>
<span id="cb49-92"><a href="#cb49-92"></a></span>
<span id="cb49-93"><a href="#cb49-93"></a>With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:</span>
<span id="cb49-94"><a href="#cb49-94"></a></span>
<span id="cb49-97"><a href="#cb49-97"></a><span class="in">```{python}</span></span>
<span id="cb49-98"><a href="#cb49-98"></a><span class="cf">for</span> targ_index, distribution <span class="kw">in</span> <span class="bu">zip</span>(y_true,y_pred):</span>
<span id="cb49-99"><a href="#cb49-99"></a>    <span class="bu">print</span>(distribution[targ_index])</span>
<span id="cb49-100"><a href="#cb49-100"></a><span class="in">```</span></span>
<span id="cb49-101"><a href="#cb49-101"></a></span>
<span id="cb49-102"><a href="#cb49-102"></a>This can be simplified. </span>
<span id="cb49-103"><a href="#cb49-103"></a></span>
<span id="cb49-106"><a href="#cb49-106"></a><span class="in">```{python}</span></span>
<span id="cb49-107"><a href="#cb49-107"></a><span class="bu">print</span>(y_pred[[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>],y_true])</span>
<span id="cb49-108"><a href="#cb49-108"></a><span class="in">```</span></span>
<span id="cb49-109"><a href="#cb49-109"></a></span>
<span id="cb49-110"><a href="#cb49-110"></a><span class="in">`numpy`</span> lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:</span>
<span id="cb49-111"><a href="#cb49-111"></a></span>
<span id="cb49-114"><a href="#cb49-114"></a><span class="in">```{python}</span></span>
<span id="cb49-115"><a href="#cb49-115"></a><span class="bu">print</span>(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true])</span>
<span id="cb49-116"><a href="#cb49-116"></a><span class="in">```</span></span>
<span id="cb49-117"><a href="#cb49-117"></a></span>
<span id="cb49-118"><a href="#cb49-118"></a>The categorical cross-entropy loss for each of the samples is:</span>
<span id="cb49-119"><a href="#cb49-119"></a></span>
<span id="cb49-122"><a href="#cb49-122"></a><span class="in">```{python}</span></span>
<span id="cb49-123"><a href="#cb49-123"></a><span class="bu">print</span>(<span class="op">-</span>np.log(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true]))</span>
<span id="cb49-124"><a href="#cb49-124"></a><span class="in">```</span></span>
<span id="cb49-125"><a href="#cb49-125"></a></span>
<span id="cb49-126"><a href="#cb49-126"></a>Finally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:</span>
<span id="cb49-127"><a href="#cb49-127"></a></span>
<span id="cb49-130"><a href="#cb49-130"></a><span class="in">```{python}</span></span>
<span id="cb49-131"><a href="#cb49-131"></a>neg_log <span class="op">=</span> <span class="op">-</span>np.log(y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)),y_true])</span>
<span id="cb49-132"><a href="#cb49-132"></a>average_loss <span class="op">=</span> np.mean(neg_log)</span>
<span id="cb49-133"><a href="#cb49-133"></a><span class="bu">print</span>(average_loss)</span>
<span id="cb49-134"><a href="#cb49-134"></a><span class="in">```</span></span>
<span id="cb49-135"><a href="#cb49-135"></a></span>
<span id="cb49-136"><a href="#cb49-136"></a>In the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If <span class="in">`y_true.shape`</span> has $2$ dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if <span class="in">`y_true`</span> is a list, that is <span class="in">`y_true.shape`</span> has $1$ dimension, then it means, we have *sparse labels*/integer encoding. </span>
<span id="cb49-137"><a href="#cb49-137"></a></span>
<span id="cb49-140"><a href="#cb49-140"></a><span class="in">```{python}</span></span>
<span id="cb49-141"><a href="#cb49-141"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-142"><a href="#cb49-142"></a></span>
<span id="cb49-143"><a href="#cb49-143"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb49-144"><a href="#cb49-144"></a>    [</span>
<span id="cb49-145"><a href="#cb49-145"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb49-146"><a href="#cb49-146"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb49-147"><a href="#cb49-147"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb49-148"><a href="#cb49-148"></a>    ]</span>
<span id="cb49-149"><a href="#cb49-149"></a>)</span>
<span id="cb49-150"><a href="#cb49-150"></a></span>
<span id="cb49-151"><a href="#cb49-151"></a>y_true <span class="op">=</span> np.array(</span>
<span id="cb49-152"><a href="#cb49-152"></a>    [</span>
<span id="cb49-153"><a href="#cb49-153"></a>        [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb49-154"><a href="#cb49-154"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb49-155"><a href="#cb49-155"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb49-156"><a href="#cb49-156"></a>    ]</span>
<span id="cb49-157"><a href="#cb49-157"></a>)</span>
<span id="cb49-158"><a href="#cb49-158"></a></span>
<span id="cb49-159"><a href="#cb49-159"></a>correct_confidences <span class="op">=</span> np.array([])</span>
<span id="cb49-160"><a href="#cb49-160"></a></span>
<span id="cb49-161"><a href="#cb49-161"></a><span class="co"># If categorical labels</span></span>
<span id="cb49-162"><a href="#cb49-162"></a><span class="cf">if</span>(<span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">1</span>):</span>
<span id="cb49-163"><a href="#cb49-163"></a>    correct_confidences <span class="op">=</span> y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb49-164"><a href="#cb49-164"></a><span class="cf">elif</span>(<span class="bu">len</span>(y_pred.shape)<span class="op">==</span><span class="dv">2</span>):</span>
<span id="cb49-165"><a href="#cb49-165"></a>    correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-166"><a href="#cb49-166"></a></span>
<span id="cb49-167"><a href="#cb49-167"></a>neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb49-168"><a href="#cb49-168"></a>average_loss <span class="op">=</span> np.mean(neg_log)</span>
<span id="cb49-169"><a href="#cb49-169"></a><span class="bu">print</span>(average_loss)</span>
<span id="cb49-170"><a href="#cb49-170"></a><span class="in">```</span></span>
<span id="cb49-171"><a href="#cb49-171"></a></span>
<span id="cb49-172"><a href="#cb49-172"></a>If the neural network output <span class="in">`y_pred`</span> for some reason is the vector <span class="in">`[1, 0, 0]`</span>, this would result in <span class="in">`numpy.log`</span> function returning a negative infinity. To avoid such situations, it's safer to apply a ceil and floor to <span class="in">`y_pred`</span>. </span>
<span id="cb49-173"><a href="#cb49-173"></a></span>
<span id="cb49-176"><a href="#cb49-176"></a><span class="in">```{python}</span></span>
<span id="cb49-177"><a href="#cb49-177"></a>epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb49-178"><a href="#cb49-178"></a>y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span><span class="op">-</span>epsilon)</span>
<span id="cb49-179"><a href="#cb49-179"></a><span class="in">```</span></span>
<span id="cb49-180"><a href="#cb49-180"></a></span>
<span id="cb49-181"><a href="#cb49-181"></a><span class="fu">## Categorical Cross-Entropy Loss Class</span></span>
<span id="cb49-182"><a href="#cb49-182"></a></span>
<span id="cb49-183"><a href="#cb49-183"></a>I first create an abstract base class <span class="in">`Loss`</span>. Every <span class="in">`Loss`</span> object exposes the <span class="in">`calculate`</span> method which in turn calls <span class="in">`Loss`</span> object's forward method to compute the log-loss for each sample and then takes an average of the sample losses.</span>
<span id="cb49-184"><a href="#cb49-184"></a></span>
<span id="cb49-185"><a href="#cb49-185"></a><span class="in">`CategoricalCrossEntropyLoss`</span> class is a child class of <span class="in">`Loss`</span> and provides an implementation of the <span class="in">`forward`</span> method.</span>
<span id="cb49-186"><a href="#cb49-186"></a></span>
<span id="cb49-189"><a href="#cb49-189"></a><span class="in">```{python}</span></span>
<span id="cb49-190"><a href="#cb49-190"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-191"><a href="#cb49-191"></a><span class="im">import</span> nnfs</span>
<span id="cb49-192"><a href="#cb49-192"></a><span class="im">from</span> nnfs.datasets <span class="im">import</span> spiral_data</span>
<span id="cb49-193"><a href="#cb49-193"></a><span class="im">from</span> abc <span class="im">import</span> abstractmethod</span>
<span id="cb49-194"><a href="#cb49-194"></a></span>
<span id="cb49-195"><a href="#cb49-195"></a></span>
<span id="cb49-196"><a href="#cb49-196"></a><span class="co"># Abstract base class for losses</span></span>
<span id="cb49-197"><a href="#cb49-197"></a><span class="kw">class</span> Loss:</span>
<span id="cb49-198"><a href="#cb49-198"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb49-199"><a href="#cb49-199"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb49-200"><a href="#cb49-200"></a>        <span class="cf">pass</span></span>
<span id="cb49-201"><a href="#cb49-201"></a></span>
<span id="cb49-202"><a href="#cb49-202"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb49-203"><a href="#cb49-203"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb49-204"><a href="#cb49-204"></a>        <span class="cf">pass</span></span>
<span id="cb49-205"><a href="#cb49-205"></a></span>
<span id="cb49-206"><a href="#cb49-206"></a>    <span class="co"># Calculates the data and regularization losses</span></span>
<span id="cb49-207"><a href="#cb49-207"></a>    <span class="co"># given model output and ground truth values</span></span>
<span id="cb49-208"><a href="#cb49-208"></a>    <span class="kw">def</span> calculate(<span class="va">self</span>, output, y):</span>
<span id="cb49-209"><a href="#cb49-209"></a></span>
<span id="cb49-210"><a href="#cb49-210"></a>        <span class="co"># Calculate the sample losses</span></span>
<span id="cb49-211"><a href="#cb49-211"></a>        sample_losses <span class="op">=</span> <span class="va">self</span>.forward(output, y)</span>
<span id="cb49-212"><a href="#cb49-212"></a></span>
<span id="cb49-213"><a href="#cb49-213"></a>        <span class="co"># Calculate the mean loss</span></span>
<span id="cb49-214"><a href="#cb49-214"></a>        data_loss <span class="op">=</span> np.mean(sample_losses)</span>
<span id="cb49-215"><a href="#cb49-215"></a></span>
<span id="cb49-216"><a href="#cb49-216"></a>        <span class="co"># Return loss</span></span>
<span id="cb49-217"><a href="#cb49-217"></a>        <span class="cf">return</span> data_loss</span>
<span id="cb49-218"><a href="#cb49-218"></a></span>
<span id="cb49-219"><a href="#cb49-219"></a></span>
<span id="cb49-220"><a href="#cb49-220"></a><span class="co"># Cross-Entropy loss</span></span>
<span id="cb49-221"><a href="#cb49-221"></a><span class="kw">class</span> CategoricalCrossEntropyLoss(Loss):</span>
<span id="cb49-222"><a href="#cb49-222"></a></span>
<span id="cb49-223"><a href="#cb49-223"></a>    <span class="co"># Forward pass</span></span>
<span id="cb49-224"><a href="#cb49-224"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb49-225"><a href="#cb49-225"></a>        num_samples <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb49-226"><a href="#cb49-226"></a></span>
<span id="cb49-227"><a href="#cb49-227"></a>        <span class="co"># Clip data to prevent division by 0</span></span>
<span id="cb49-228"><a href="#cb49-228"></a>        <span class="co"># Clip both sides to not drag mean towards any value</span></span>
<span id="cb49-229"><a href="#cb49-229"></a>        epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb49-230"><a href="#cb49-230"></a>        y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span> <span class="op">-</span> epsilon)</span>
<span id="cb49-231"><a href="#cb49-231"></a></span>
<span id="cb49-232"><a href="#cb49-232"></a>        <span class="co"># If categorical labels</span></span>
<span id="cb49-233"><a href="#cb49-233"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb49-234"><a href="#cb49-234"></a>            correct_confidences <span class="op">=</span> y_pred[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb49-235"><a href="#cb49-235"></a>        <span class="co"># else if one-hot encoding</span></span>
<span id="cb49-236"><a href="#cb49-236"></a>        <span class="cf">elif</span> <span class="bu">len</span>(y_pred.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb49-237"><a href="#cb49-237"></a>            correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-238"><a href="#cb49-238"></a></span>
<span id="cb49-239"><a href="#cb49-239"></a>        neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb49-240"><a href="#cb49-240"></a>        <span class="cf">return</span> neg_log</span>
<span id="cb49-241"><a href="#cb49-241"></a><span class="in">```</span></span>
<span id="cb49-242"><a href="#cb49-242"></a></span>
<span id="cb49-243"><a href="#cb49-243"></a>Using the manual created outputs and targets, we have:</span>
<span id="cb49-244"><a href="#cb49-244"></a></span>
<span id="cb49-247"><a href="#cb49-247"></a><span class="in">```{python}</span></span>
<span id="cb49-248"><a href="#cb49-248"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb49-249"><a href="#cb49-249"></a>    [</span>
<span id="cb49-250"><a href="#cb49-250"></a>        [<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb49-251"><a href="#cb49-251"></a>        [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>],</span>
<span id="cb49-252"><a href="#cb49-252"></a>        [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]</span>
<span id="cb49-253"><a href="#cb49-253"></a>    ]</span>
<span id="cb49-254"><a href="#cb49-254"></a>)</span>
<span id="cb49-255"><a href="#cb49-255"></a></span>
<span id="cb49-256"><a href="#cb49-256"></a>y_true <span class="op">=</span> np.array(</span>
<span id="cb49-257"><a href="#cb49-257"></a>    [</span>
<span id="cb49-258"><a href="#cb49-258"></a>        [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb49-259"><a href="#cb49-259"></a>        [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb49-260"><a href="#cb49-260"></a>        [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb49-261"><a href="#cb49-261"></a>    ]</span>
<span id="cb49-262"><a href="#cb49-262"></a>)</span>
<span id="cb49-263"><a href="#cb49-263"></a></span>
<span id="cb49-264"><a href="#cb49-264"></a>loss_function <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb49-265"><a href="#cb49-265"></a>loss <span class="op">=</span> loss_function.calculate(y_pred, y_true)</span>
<span id="cb49-266"><a href="#cb49-266"></a><span class="bu">print</span>(loss)</span>
<span id="cb49-267"><a href="#cb49-267"></a><span class="in">```</span></span>
<span id="cb49-268"><a href="#cb49-268"></a></span>
<span id="cb49-269"><a href="#cb49-269"></a><span class="fu">## Backpropogation</span></span>
<span id="cb49-270"><a href="#cb49-270"></a></span>
<span id="cb49-271"><a href="#cb49-271"></a>Backpropogation consists going backwards along the edges and passing along gradients. We are going to chop up a neuron into it's elementary operations and draw a computational graph. Each node in the graph receives an upstream gradient. The goal is pass on the correct downstream gradient.</span>
<span id="cb49-272"><a href="#cb49-272"></a></span>
<span id="cb49-273"><a href="#cb49-273"></a>Each node has a *local gradient* - the gradient of it's output with respect to it's input. Consider a node receiving an input $z$ and producing an output $h=f(z)$. Then, we have:</span>
<span id="cb49-274"><a href="#cb49-274"></a></span>
<span id="cb49-277"><a href="#cb49-277"></a><span class="in">```{python}</span></span>
<span id="cb49-278"><a href="#cb49-278"></a><span class="co"># | code-fold: true</span></span>
<span id="cb49-279"><a href="#cb49-279"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb49-280"><a href="#cb49-280"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb49-281"><a href="#cb49-281"></a>\begin{tikzpicture}</span>
<span id="cb49-282"><a href="#cb49-282"></a>    \node [circle,minimum size<span class="op">=</span><span class="dv">40</span><span class="er">mm</span>,draw] (f) at (<span class="dv">0</span>,<span class="dv">0</span>) {\huge $f$}<span class="op">;</span></span>
<span id="cb49-283"><a href="#cb49-283"></a>    \node [blue] (localgrad) at (<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>) {\huge $\frac{\partial h}{\partial z}$}<span class="op">;</span></span>
<span id="cb49-284"><a href="#cb49-284"></a>    \node [blue] (lgrad) at (<span class="fl">0.0</span>,<span class="dv">1</span>) {\large Local gradient}<span class="op">;</span></span>
<span id="cb49-285"><a href="#cb49-285"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="fl">1.80</span>,<span class="dv">1</span>) <span class="op">--</span> node [above,midway] {\huge $h$} (<span class="dv">5</span>,<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb49-286"><a href="#cb49-286"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node [below,midway] {\huge $\frac{\partial s}{\partial h}$} (<span class="fl">1.80</span>,<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb49-287"><a href="#cb49-287"></a>    \node [] (upgrad) at (<span class="fl">4.0</span>,<span class="op">-</span><span class="dv">3</span>) {\huge Upstream gradient}<span class="op">;</span></span>
<span id="cb49-288"><a href="#cb49-288"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="op">-</span><span class="dv">5</span>,<span class="dv">1</span>) <span class="op">--</span> node [above,midway] {\huge $z$} (<span class="op">-</span><span class="fl">1.80</span>,<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb49-289"><a href="#cb49-289"></a>    \draw [<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="op">-</span><span class="fl">1.80</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node [below,midway] {\huge $\frac{\partial s}{\partial z} <span class="op">=</span> \frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial z}$} (<span class="op">-</span><span class="dv">5</span>,<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb49-290"><a href="#cb49-290"></a>    \node [] (downgrad) at (<span class="op">-</span><span class="fl">4.0</span>,<span class="op">-</span><span class="dv">3</span>) {\huge Downstream gradient}<span class="op">;</span></span>
<span id="cb49-291"><a href="#cb49-291"></a>\end{tikzpicture}</span>
<span id="cb49-292"><a href="#cb49-292"></a><span class="in">```</span></span>
<span id="cb49-293"><a href="#cb49-293"></a></span>
<span id="cb49-294"><a href="#cb49-294"></a> The downstream gradient $\frac{\partial s}{\partial z}$ equals the upstream graient $\frac{\partial s}{\partial h}$ times the local gradient $\frac{\partial h}{\partial z}$. </span>
<span id="cb49-295"><a href="#cb49-295"></a></span>
<span id="cb49-296"><a href="#cb49-296"></a> What about nodes with multiple inputs? Say that, $h=f(x,y)$. Multiple inputs imply multiple local gradients.</span>
<span id="cb49-297"><a href="#cb49-297"></a></span>
<span id="cb49-300"><a href="#cb49-300"></a><span class="in">```{python}</span></span>
<span id="cb49-301"><a href="#cb49-301"></a><span class="co"># | code-fold: true</span></span>
<span id="cb49-302"><a href="#cb49-302"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb49-303"><a href="#cb49-303"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb49-304"><a href="#cb49-304"></a>\begin{tikzpicture}[x<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,y<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,yscale<span class="op">=-</span><span class="dv">1</span>,scale<span class="op">=</span><span class="fl">1.75</span>]</span>
<span id="cb49-305"><a href="#cb49-305"></a><span class="op">%</span>uncomment <span class="cf">if</span> require: \path (<span class="dv">0</span>,<span class="dv">216</span>)<span class="op">;</span> <span class="op">%</span><span class="bu">set</span> diagram left start at <span class="dv">0</span>, <span class="kw">and</span> has height of <span class="dv">216</span></span>
<span id="cb49-306"><a href="#cb49-306"></a></span>
<span id="cb49-307"><a href="#cb49-307"></a><span class="op">%</span>Shape: Circle [<span class="bu">id</span>:dp08328772161506959] </span>
<span id="cb49-308"><a href="#cb49-308"></a>\draw   (<span class="fl">302.75</span>,<span class="fl">83.38</span>) .. controls (<span class="fl">302.75</span>,<span class="fl">53.62</span>) <span class="kw">and</span> (<span class="fl">326.87</span>,<span class="fl">29.5</span>) .. (<span class="fl">356.63</span>,<span class="fl">29.5</span>) .. controls (<span class="fl">386.38</span>,<span class="fl">29.5</span>) <span class="kw">and</span> (<span class="fl">410.5</span>,<span class="fl">53.62</span>) .. (<span class="fl">410.5</span>,<span class="fl">83.38</span>) .. controls (<span class="fl">410.5</span>,<span class="fl">113.13</span>) <span class="kw">and</span> (<span class="fl">386.38</span>,<span class="fl">137.25</span>) .. (<span class="fl">356.63</span>,<span class="fl">137.25</span>) .. controls (<span class="fl">326.87</span>,<span class="fl">137.25</span>) <span class="kw">and</span> (<span class="fl">302.75</span>,<span class="fl">113.13</span>) .. (<span class="fl">302.75</span>,<span class="fl">83.38</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb49-309"><a href="#cb49-309"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da2730189357413113] </span>
<span id="cb49-310"><a href="#cb49-310"></a>\draw    (<span class="dv">406</span>,<span class="fl">59.38</span>) <span class="op">--</span> (<span class="fl">513.5</span>,<span class="fl">59.74</span>) <span class="op">;</span></span>
<span id="cb49-311"><a href="#cb49-311"></a>\draw [shift<span class="op">=</span>{(<span class="fl">515.5</span>,<span class="fl">59.75</span>)}, rotate <span class="op">=</span> <span class="fl">180.2</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb49-312"><a href="#cb49-312"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da21080101466010737] </span>
<span id="cb49-313"><a href="#cb49-313"></a>\draw    (<span class="dv">515</span>,<span class="fl">110.75</span>) <span class="op">--</span> (<span class="dv">405</span>,<span class="fl">110.26</span>) <span class="op">;</span></span>
<span id="cb49-314"><a href="#cb49-314"></a>\draw [shift<span class="op">=</span>{(<span class="dv">403</span>,<span class="fl">110.25</span>)}, rotate <span class="op">=</span> <span class="fl">0.26</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb49-315"><a href="#cb49-315"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da05192158713361961] </span>
<span id="cb49-316"><a href="#cb49-316"></a>\draw    (<span class="dv">209</span>,<span class="fl">1.75</span>) <span class="op">--</span> (<span class="fl">309.71</span>,<span class="fl">51.37</span>) <span class="op">;</span></span>
<span id="cb49-317"><a href="#cb49-317"></a>\draw [shift<span class="op">=</span>{(<span class="fl">311.5</span>,<span class="fl">52.25</span>)}, rotate <span class="op">=</span> <span class="fl">206.23</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb49-318"><a href="#cb49-318"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da3568530309648137] </span>
<span id="cb49-319"><a href="#cb49-319"></a>\draw    (<span class="dv">305</span>,<span class="fl">68.25</span>) <span class="op">--</span> (<span class="fl">204.31</span>,<span class="fl">20.61</span>) <span class="op">;</span></span>
<span id="cb49-320"><a href="#cb49-320"></a>\draw [shift<span class="op">=</span>{(<span class="fl">202.5</span>,<span class="fl">19.75</span>)}, rotate <span class="op">=</span> <span class="fl">25.32</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb49-321"><a href="#cb49-321"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da4437541566257528] </span>
<span id="cb49-322"><a href="#cb49-322"></a>\draw    (<span class="dv">205</span>,<span class="fl">167.25</span>) <span class="op">--</span> (<span class="fl">311.2</span>,<span class="fl">116.12</span>) <span class="op">;</span></span>
<span id="cb49-323"><a href="#cb49-323"></a>\draw [shift<span class="op">=</span>{(<span class="dv">313</span>,<span class="fl">115.25</span>)}, rotate <span class="op">=</span> <span class="fl">154.29</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb49-324"><a href="#cb49-324"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da2672766038605987] </span>
<span id="cb49-325"><a href="#cb49-325"></a>\draw    (<span class="fl">304.5</span>,<span class="fl">101.75</span>) <span class="op">--</span> (<span class="fl">205.82</span>,<span class="fl">146.92</span>) <span class="op">;</span></span>
<span id="cb49-326"><a href="#cb49-326"></a>\draw [shift<span class="op">=</span>{(<span class="dv">204</span>,<span class="fl">147.75</span>)}, rotate <span class="op">=</span> <span class="fl">335.41</span>] [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.75</span>]    (<span class="fl">10.93</span>,<span class="op">-</span><span class="fl">3.29</span>) .. controls (<span class="fl">6.95</span>,<span class="op">-</span><span class="fl">1.4</span>) <span class="kw">and</span> (<span class="fl">3.31</span>,<span class="op">-</span><span class="fl">0.3</span>) .. (<span class="dv">0</span>,<span class="dv">0</span>) .. controls (<span class="fl">3.31</span>,<span class="fl">0.3</span>) <span class="kw">and</span> (<span class="fl">6.95</span>,<span class="fl">1.4</span>) .. (<span class="fl">10.93</span>,<span class="fl">3.29</span>)   <span class="op">;</span></span>
<span id="cb49-327"><a href="#cb49-327"></a></span>
<span id="cb49-328"><a href="#cb49-328"></a><span class="op">%</span> Text Node</span>
<span id="cb49-329"><a href="#cb49-329"></a>\draw (<span class="dv">352</span>,<span class="fl">76.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $f$}<span class="op">;</span></span>
<span id="cb49-330"><a href="#cb49-330"></a><span class="op">%</span> Text Node</span>
<span id="cb49-331"><a href="#cb49-331"></a>\draw (<span class="fl">318.5</span>,<span class="fl">44.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial h}{\partial x}$}<span class="op">;</span></span>
<span id="cb49-332"><a href="#cb49-332"></a><span class="op">%</span> Text Node</span>
<span id="cb49-333"><a href="#cb49-333"></a>\draw (<span class="fl">318.5</span>,<span class="fl">88.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">36</span><span class="op">;</span> blue, <span class="dv">255</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial h}{\partial y}$}<span class="op">;</span></span>
<span id="cb49-334"><a href="#cb49-334"></a><span class="op">%</span> Text Node</span>
<span id="cb49-335"><a href="#cb49-335"></a>\draw (<span class="fl">258.5</span>,<span class="fl">7.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $x$}<span class="op">;</span></span>
<span id="cb49-336"><a href="#cb49-336"></a><span class="op">%</span> Text Node</span>
<span id="cb49-337"><a href="#cb49-337"></a>\draw (<span class="dv">264</span>,<span class="fl">136.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $y$}<span class="op">;</span></span>
<span id="cb49-338"><a href="#cb49-338"></a><span class="op">%</span> Text Node</span>
<span id="cb49-339"><a href="#cb49-339"></a>\draw (<span class="fl">151.5</span>,<span class="fl">96.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial y} <span class="op">=</span>\frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial y}$}<span class="op">;</span></span>
<span id="cb49-340"><a href="#cb49-340"></a><span class="op">%</span> Text Node</span>
<span id="cb49-341"><a href="#cb49-341"></a>\draw (<span class="dv">150</span>,<span class="fl">33.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\small,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">28</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial x} <span class="op">=</span>\frac{\partial s}{\partial h} \cdot \frac{\partial h}{\partial x}$}<span class="op">;</span></span>
<span id="cb49-342"><a href="#cb49-342"></a><span class="op">%</span> Text Node</span>
<span id="cb49-343"><a href="#cb49-343"></a>\draw (<span class="fl">322.5</span>,<span class="fl">4.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $h<span class="op">=</span>f(x,y)$}<span class="op">;</span></span>
<span id="cb49-344"><a href="#cb49-344"></a><span class="op">%</span> Text Node</span>
<span id="cb49-345"><a href="#cb49-345"></a>\draw (<span class="fl">449.5</span>,<span class="fl">39.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $h$}<span class="op">;</span></span>
<span id="cb49-346"><a href="#cb49-346"></a><span class="op">%</span> Text Node</span>
<span id="cb49-347"><a href="#cb49-347"></a>\draw (<span class="fl">451.5</span>,<span class="fl">112.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $\frac{\partial s}{\partial h}$}<span class="op">;</span></span>
<span id="cb49-348"><a href="#cb49-348"></a><span class="op">%</span> Text Node</span>
<span id="cb49-349"><a href="#cb49-349"></a>\draw (<span class="fl">164.5</span>,<span class="fl">172.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $ \begin{array}{l}</span>
<span id="cb49-350"><a href="#cb49-350"></a>Downstream\ \<span class="op">\</span></span>
<span id="cb49-351"><a href="#cb49-351"></a>gradients</span>
<span id="cb49-352"><a href="#cb49-352"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb49-353"><a href="#cb49-353"></a><span class="op">%</span> Text Node</span>
<span id="cb49-354"><a href="#cb49-354"></a>\draw (<span class="fl">430.5</span>,<span class="fl">175.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $ \begin{array}{l}</span>
<span id="cb49-355"><a href="#cb49-355"></a>Upstream\ \<span class="op">\</span></span>
<span id="cb49-356"><a href="#cb49-356"></a>gradients</span>
<span id="cb49-357"><a href="#cb49-357"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb49-358"><a href="#cb49-358"></a><span class="op">%</span> Text Node</span>
<span id="cb49-359"><a href="#cb49-359"></a>\draw (<span class="fl">318.5</span>,<span class="fl">173.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">3</span><span class="op">;</span> green, <span class="dv">50</span><span class="op">;</span> blue, <span class="dv">255</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $ \begin{array}{l}</span>
<span id="cb49-360"><a href="#cb49-360"></a>Local\ \<span class="op">\</span></span>
<span id="cb49-361"><a href="#cb49-361"></a>gradients</span>
<span id="cb49-362"><a href="#cb49-362"></a>\end{array}$}<span class="op">;</span></span>
<span id="cb49-363"><a href="#cb49-363"></a></span>
<span id="cb49-364"><a href="#cb49-364"></a></span>
<span id="cb49-365"><a href="#cb49-365"></a>\end{tikzpicture}</span>
<span id="cb49-366"><a href="#cb49-366"></a><span class="in">```</span></span>
<span id="cb49-367"><a href="#cb49-367"></a></span>
<span id="cb49-368"><a href="#cb49-368"></a>Let's start with a simple forward pass with $1$ neuron. Let's say, we have the following input vector, weights and bias:</span>
<span id="cb49-369"><a href="#cb49-369"></a></span>
<span id="cb49-372"><a href="#cb49-372"></a><span class="in">```{python}</span></span>
<span id="cb49-373"><a href="#cb49-373"></a>x <span class="op">=</span> [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>]  <span class="co"># input values</span></span>
<span id="cb49-374"><a href="#cb49-374"></a>w <span class="op">=</span> [<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>] <span class="co"># weights</span></span>
<span id="cb49-375"><a href="#cb49-375"></a>b <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb49-376"><a href="#cb49-376"></a></span>
<span id="cb49-377"><a href="#cb49-377"></a><span class="co"># Forward pass</span></span>
<span id="cb49-378"><a href="#cb49-378"></a>z <span class="op">=</span> np.dot(x,w) <span class="op">+</span> b</span>
<span id="cb49-379"><a href="#cb49-379"></a></span>
<span id="cb49-380"><a href="#cb49-380"></a><span class="co"># ReLU Activation function</span></span>
<span id="cb49-381"><a href="#cb49-381"></a>y <span class="op">=</span> <span class="bu">max</span>(z, <span class="dv">0</span>)</span>
<span id="cb49-382"><a href="#cb49-382"></a><span class="in">```</span></span>
<span id="cb49-383"><a href="#cb49-383"></a></span>
<span id="cb49-386"><a href="#cb49-386"></a><span class="in">```{python}</span></span>
<span id="cb49-387"><a href="#cb49-387"></a><span class="co"># | code-fold: true</span></span>
<span id="cb49-388"><a href="#cb49-388"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb49-389"><a href="#cb49-389"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb49-390"><a href="#cb49-390"></a>\begin{tikzpicture}</span>
<span id="cb49-391"><a href="#cb49-391"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-392"><a href="#cb49-392"></a>{</span>
<span id="cb49-393"><a href="#cb49-393"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb49-394"><a href="#cb49-394"></a>}</span>
<span id="cb49-395"><a href="#cb49-395"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-396"><a href="#cb49-396"></a>{</span>
<span id="cb49-397"><a href="#cb49-397"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb49-398"><a href="#cb49-398"></a>}</span>
<span id="cb49-399"><a href="#cb49-399"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-400"><a href="#cb49-400"></a>{</span>
<span id="cb49-401"><a href="#cb49-401"></a>    \node[circle, </span>
<span id="cb49-402"><a href="#cb49-402"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb49-403"><a href="#cb49-403"></a>        draw,</span>
<span id="cb49-404"><a href="#cb49-404"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb49-405"><a href="#cb49-405"></a>        </span>
<span id="cb49-406"><a href="#cb49-406"></a>}</span>
<span id="cb49-407"><a href="#cb49-407"></a></span>
<span id="cb49-408"><a href="#cb49-408"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb49-409"><a href="#cb49-409"></a></span>
<span id="cb49-410"><a href="#cb49-410"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb49-411"><a href="#cb49-411"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb49-412"><a href="#cb49-412"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb49-413"><a href="#cb49-413"></a></span>
<span id="cb49-414"><a href="#cb49-414"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb49-415"><a href="#cb49-415"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb49-416"><a href="#cb49-416"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb49-417"><a href="#cb49-417"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb49-418"><a href="#cb49-418"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb49-419"><a href="#cb49-419"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb49-420"><a href="#cb49-420"></a></span>
<span id="cb49-421"><a href="#cb49-421"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb49-422"><a href="#cb49-422"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb49-423"><a href="#cb49-423"></a></span>
<span id="cb49-424"><a href="#cb49-424"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-425"><a href="#cb49-425"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-426"><a href="#cb49-426"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-427"><a href="#cb49-427"></a></span>
<span id="cb49-428"><a href="#cb49-428"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb49-429"><a href="#cb49-429"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb49-430"><a href="#cb49-430"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-431"><a href="#cb49-431"></a>\end{tikzpicture}</span>
<span id="cb49-432"><a href="#cb49-432"></a><span class="in">```</span></span>
<span id="cb49-433"><a href="#cb49-433"></a></span>
<span id="cb49-434"><a href="#cb49-434"></a>The ReLU function $f(x)=\max(x,0)$ is differentiable everywhere except at $x = 0$. We define $f'(x)$ as:</span>
<span id="cb49-435"><a href="#cb49-435"></a></span>
<span id="cb49-436"><a href="#cb49-436"></a>\begin{align*}</span>
<span id="cb49-437"><a href="#cb49-437"></a>f'(x) = </span>
<span id="cb49-438"><a href="#cb49-438"></a>\begin{cases}</span>
<span id="cb49-439"><a href="#cb49-439"></a>1 &amp; x &gt; 0 <span class="sc">\\</span></span>
<span id="cb49-440"><a href="#cb49-440"></a>0 &amp; \text{otherwise}</span>
<span id="cb49-441"><a href="#cb49-441"></a>\end{cases}</span>
<span id="cb49-442"><a href="#cb49-442"></a>\end{align*}</span>
<span id="cb49-443"><a href="#cb49-443"></a></span>
<span id="cb49-444"><a href="#cb49-444"></a>In Python, we write:</span>
<span id="cb49-445"><a href="#cb49-445"></a></span>
<span id="cb49-448"><a href="#cb49-448"></a><span class="in">```{python}</span></span>
<span id="cb49-449"><a href="#cb49-449"></a>relu_dz <span class="op">=</span> (<span class="fl">1.</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.</span>)</span>
<span id="cb49-450"><a href="#cb49-450"></a><span class="in">```</span></span>
<span id="cb49-451"><a href="#cb49-451"></a></span>
<span id="cb49-452"><a href="#cb49-452"></a>The input to the ReLU function is $6.00$, so the derivative equals $1.00$. We multiply this local gradient by the upstream gradient to calculate the downstream gradient. </span>
<span id="cb49-453"><a href="#cb49-453"></a></span>
<span id="cb49-456"><a href="#cb49-456"></a><span class="in">```{python}</span></span>
<span id="cb49-457"><a href="#cb49-457"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-458"><a href="#cb49-458"></a></span>
<span id="cb49-459"><a href="#cb49-459"></a>x <span class="op">=</span> [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>]  <span class="co"># input values</span></span>
<span id="cb49-460"><a href="#cb49-460"></a>w <span class="op">=</span> [<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>]  <span class="co"># weights</span></span>
<span id="cb49-461"><a href="#cb49-461"></a>b <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb49-462"><a href="#cb49-462"></a></span>
<span id="cb49-463"><a href="#cb49-463"></a><span class="co"># Forward pass</span></span>
<span id="cb49-464"><a href="#cb49-464"></a>z <span class="op">=</span> np.dot(x, w) <span class="op">+</span> b</span>
<span id="cb49-465"><a href="#cb49-465"></a></span>
<span id="cb49-466"><a href="#cb49-466"></a><span class="co"># ReLU Activation function</span></span>
<span id="cb49-467"><a href="#cb49-467"></a>y <span class="op">=</span> <span class="bu">max</span>(z, <span class="dv">0</span>)</span>
<span id="cb49-468"><a href="#cb49-468"></a></span>
<span id="cb49-469"><a href="#cb49-469"></a><span class="co"># Backward pass</span></span>
<span id="cb49-470"><a href="#cb49-470"></a><span class="co"># Upstream gradient</span></span>
<span id="cb49-471"><a href="#cb49-471"></a>ds_drelu <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb49-472"><a href="#cb49-472"></a></span>
<span id="cb49-473"><a href="#cb49-473"></a><span class="co"># Derivative of the ReLU and the chain rule</span></span>
<span id="cb49-474"><a href="#cb49-474"></a>drelu_dz <span class="op">=</span> <span class="fl">1.0</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb49-475"><a href="#cb49-475"></a>ds_dz <span class="op">=</span> ds_drelu <span class="op">*</span> drelu_dz</span>
<span id="cb49-476"><a href="#cb49-476"></a><span class="bu">print</span>(ds_dz)</span>
<span id="cb49-477"><a href="#cb49-477"></a><span class="in">```</span></span>
<span id="cb49-478"><a href="#cb49-478"></a></span>
<span id="cb49-479"><a href="#cb49-479"></a>The results with the derivative of the ReLU function and chain rule look as follows:</span>
<span id="cb49-480"><a href="#cb49-480"></a></span>
<span id="cb49-481"><a href="#cb49-481"></a></span>
<span id="cb49-484"><a href="#cb49-484"></a><span class="in">```{python}</span></span>
<span id="cb49-485"><a href="#cb49-485"></a><span class="co"># | code-fold: true</span></span>
<span id="cb49-486"><a href="#cb49-486"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb49-487"><a href="#cb49-487"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb49-488"><a href="#cb49-488"></a>\begin{tikzpicture}</span>
<span id="cb49-489"><a href="#cb49-489"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-490"><a href="#cb49-490"></a>{</span>
<span id="cb49-491"><a href="#cb49-491"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb49-492"><a href="#cb49-492"></a>}</span>
<span id="cb49-493"><a href="#cb49-493"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-494"><a href="#cb49-494"></a>{</span>
<span id="cb49-495"><a href="#cb49-495"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb49-496"><a href="#cb49-496"></a>}</span>
<span id="cb49-497"><a href="#cb49-497"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-498"><a href="#cb49-498"></a>{</span>
<span id="cb49-499"><a href="#cb49-499"></a>    \node[circle, </span>
<span id="cb49-500"><a href="#cb49-500"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb49-501"><a href="#cb49-501"></a>        draw,</span>
<span id="cb49-502"><a href="#cb49-502"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb49-503"><a href="#cb49-503"></a>        </span>
<span id="cb49-504"><a href="#cb49-504"></a>}</span>
<span id="cb49-505"><a href="#cb49-505"></a></span>
<span id="cb49-506"><a href="#cb49-506"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb49-507"><a href="#cb49-507"></a></span>
<span id="cb49-508"><a href="#cb49-508"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb49-509"><a href="#cb49-509"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb49-510"><a href="#cb49-510"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb49-511"><a href="#cb49-511"></a></span>
<span id="cb49-512"><a href="#cb49-512"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb49-513"><a href="#cb49-513"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb49-514"><a href="#cb49-514"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb49-515"><a href="#cb49-515"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb49-516"><a href="#cb49-516"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb49-517"><a href="#cb49-517"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb49-518"><a href="#cb49-518"></a></span>
<span id="cb49-519"><a href="#cb49-519"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb49-520"><a href="#cb49-520"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb49-521"><a href="#cb49-521"></a></span>
<span id="cb49-522"><a href="#cb49-522"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-523"><a href="#cb49-523"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-524"><a href="#cb49-524"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-525"><a href="#cb49-525"></a></span>
<span id="cb49-526"><a href="#cb49-526"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb49-527"><a href="#cb49-527"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb49-528"><a href="#cb49-528"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-529"><a href="#cb49-529"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-530"><a href="#cb49-530"></a>\end{tikzpicture}</span>
<span id="cb49-531"><a href="#cb49-531"></a><span class="in">```</span></span>
<span id="cb49-532"><a href="#cb49-532"></a></span>
<span id="cb49-533"><a href="#cb49-533"></a>Moving backward through our neural network, consider the add function $f(x,y,z)=x + y + z$. The partial derivatives $\frac{\partial f}{\partial x}$, $\frac{\partial f}{\partial y}$ and $\frac{\partial f}{\partial z}$ are all equal to $1$. So, the **add gate** always takes on the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.</span>
<span id="cb49-534"><a href="#cb49-534"></a></span>
<span id="cb49-537"><a href="#cb49-537"></a><span class="in">```{python}</span></span>
<span id="cb49-538"><a href="#cb49-538"></a><span class="co"># Local gradients for the + function</span></span>
<span id="cb49-539"><a href="#cb49-539"></a>dz_dw0x0 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb49-540"><a href="#cb49-540"></a>dz_dw1x1 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb49-541"><a href="#cb49-541"></a>dz_dw2x2 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb49-542"><a href="#cb49-542"></a>dz_db <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb49-543"><a href="#cb49-543"></a></span>
<span id="cb49-544"><a href="#cb49-544"></a><span class="co"># Calculate the downstream gradients</span></span>
<span id="cb49-545"><a href="#cb49-545"></a>ds_dw0x0 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw0x0</span>
<span id="cb49-546"><a href="#cb49-546"></a>ds_dw1x1 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw1x1</span>
<span id="cb49-547"><a href="#cb49-547"></a>ds_dw2x2 <span class="op">=</span> ds_dz <span class="op">*</span> dz_dw2x2</span>
<span id="cb49-548"><a href="#cb49-548"></a>ds_db <span class="op">=</span> ds_dz <span class="op">*</span> dz_db</span>
<span id="cb49-549"><a href="#cb49-549"></a><span class="bu">print</span>(ds_dw0x0, ds_dw1x1, ds_dw2x2, ds_db)</span>
<span id="cb49-550"><a href="#cb49-550"></a><span class="in">```</span></span>
<span id="cb49-551"><a href="#cb49-551"></a></span>
<span id="cb49-552"><a href="#cb49-552"></a>We can update the computation graph as:</span>
<span id="cb49-553"><a href="#cb49-553"></a></span>
<span id="cb49-556"><a href="#cb49-556"></a><span class="in">```{python}</span></span>
<span id="cb49-557"><a href="#cb49-557"></a><span class="co"># | code-fold: true</span></span>
<span id="cb49-558"><a href="#cb49-558"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb49-559"><a href="#cb49-559"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb49-560"><a href="#cb49-560"></a>\begin{tikzpicture}</span>
<span id="cb49-561"><a href="#cb49-561"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-562"><a href="#cb49-562"></a>{</span>
<span id="cb49-563"><a href="#cb49-563"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb49-564"><a href="#cb49-564"></a>}</span>
<span id="cb49-565"><a href="#cb49-565"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-566"><a href="#cb49-566"></a>{</span>
<span id="cb49-567"><a href="#cb49-567"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb49-568"><a href="#cb49-568"></a>}</span>
<span id="cb49-569"><a href="#cb49-569"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-570"><a href="#cb49-570"></a>{</span>
<span id="cb49-571"><a href="#cb49-571"></a>    \node[circle, </span>
<span id="cb49-572"><a href="#cb49-572"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb49-573"><a href="#cb49-573"></a>        draw,</span>
<span id="cb49-574"><a href="#cb49-574"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb49-575"><a href="#cb49-575"></a>        </span>
<span id="cb49-576"><a href="#cb49-576"></a>}</span>
<span id="cb49-577"><a href="#cb49-577"></a></span>
<span id="cb49-578"><a href="#cb49-578"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb49-579"><a href="#cb49-579"></a></span>
<span id="cb49-580"><a href="#cb49-580"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb49-581"><a href="#cb49-581"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb49-582"><a href="#cb49-582"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb49-583"><a href="#cb49-583"></a></span>
<span id="cb49-584"><a href="#cb49-584"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb49-585"><a href="#cb49-585"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb49-586"><a href="#cb49-586"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb49-587"><a href="#cb49-587"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb49-588"><a href="#cb49-588"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb49-589"><a href="#cb49-589"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb49-590"><a href="#cb49-590"></a></span>
<span id="cb49-591"><a href="#cb49-591"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb49-592"><a href="#cb49-592"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb49-593"><a href="#cb49-593"></a></span>
<span id="cb49-594"><a href="#cb49-594"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-595"><a href="#cb49-595"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-596"><a href="#cb49-596"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-597"><a href="#cb49-597"></a></span>
<span id="cb49-598"><a href="#cb49-598"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb49-599"><a href="#cb49-599"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb49-600"><a href="#cb49-600"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-601"><a href="#cb49-601"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-602"><a href="#cb49-602"></a>\node [red] (C) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">3.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-603"><a href="#cb49-603"></a>\node [red] (D) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-604"><a href="#cb49-604"></a>\node [red] (E) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">7.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-605"><a href="#cb49-605"></a>\node [red] (f) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">12.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-606"><a href="#cb49-606"></a>\end{tikzpicture}</span>
<span id="cb49-607"><a href="#cb49-607"></a><span class="in">```</span></span>
<span id="cb49-608"><a href="#cb49-608"></a></span>
<span id="cb49-609"><a href="#cb49-609"></a>Now, consider the production function $f(x,y) = x * y$. The gradients of $f$ are $\frac{\partial f}{\partial x} = y$, $\frac{\partial f}{\partial y} = x$. The **multiply gate** is therefore a little less easy to interpret. Its local gradients are the input values, except switched and this is multiplied by the upstream gradient. </span>
<span id="cb49-610"><a href="#cb49-610"></a></span>
<span id="cb49-613"><a href="#cb49-613"></a><span class="in">```{python}</span></span>
<span id="cb49-614"><a href="#cb49-614"></a><span class="co"># Local gradients for the * function</span></span>
<span id="cb49-615"><a href="#cb49-615"></a>dw0x0_dx0 <span class="op">=</span> w[<span class="dv">0</span>]</span>
<span id="cb49-616"><a href="#cb49-616"></a>dw0x0_dw0 <span class="op">=</span> x[<span class="dv">0</span>]</span>
<span id="cb49-617"><a href="#cb49-617"></a>dw1x1_dx1 <span class="op">=</span> w[<span class="dv">1</span>]</span>
<span id="cb49-618"><a href="#cb49-618"></a>dw1x1_dw1 <span class="op">=</span> x[<span class="dv">1</span>]</span>
<span id="cb49-619"><a href="#cb49-619"></a>dw2x2_dx2 <span class="op">=</span> w[<span class="dv">2</span>]</span>
<span id="cb49-620"><a href="#cb49-620"></a>dw2x2_dw2 <span class="op">=</span> x[<span class="dv">2</span>]</span>
<span id="cb49-621"><a href="#cb49-621"></a></span>
<span id="cb49-622"><a href="#cb49-622"></a><span class="co"># Calculate the downstream gradients</span></span>
<span id="cb49-623"><a href="#cb49-623"></a>ds_dx0 <span class="op">=</span> ds_dw0x0 <span class="op">*</span> dw0x0_dx0</span>
<span id="cb49-624"><a href="#cb49-624"></a>ds_dw0 <span class="op">=</span> ds_dw0x0 <span class="op">*</span> dw0x0_dw0</span>
<span id="cb49-625"><a href="#cb49-625"></a>ds_dx1 <span class="op">=</span> ds_dw1x1 <span class="op">*</span> dw1x1_dx1</span>
<span id="cb49-626"><a href="#cb49-626"></a>ds_dw1 <span class="op">=</span> ds_dw1x1 <span class="op">*</span> dw1x1_dw1</span>
<span id="cb49-627"><a href="#cb49-627"></a>ds_dx2 <span class="op">=</span> ds_dw2x2 <span class="op">*</span> dw2x2_dx2</span>
<span id="cb49-628"><a href="#cb49-628"></a>ds_dw2 <span class="op">=</span> ds_dw2x2 <span class="op">*</span> dw2x2_dw2</span>
<span id="cb49-629"><a href="#cb49-629"></a></span>
<span id="cb49-630"><a href="#cb49-630"></a><span class="bu">print</span>(ds_dx0, ds_dw0, ds_dx1, ds_dw1, ds_dx2, ds_dw2)</span>
<span id="cb49-631"><a href="#cb49-631"></a><span class="in">```</span></span>
<span id="cb49-632"><a href="#cb49-632"></a></span>
<span id="cb49-633"><a href="#cb49-633"></a>We can update the computation graph as follows:</span>
<span id="cb49-634"><a href="#cb49-634"></a></span>
<span id="cb49-637"><a href="#cb49-637"></a><span class="in">```{python}</span></span>
<span id="cb49-638"><a href="#cb49-638"></a><span class="co"># | code-fold: true</span></span>
<span id="cb49-639"><a href="#cb49-639"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb49-640"><a href="#cb49-640"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb49-641"><a href="#cb49-641"></a>\begin{tikzpicture}</span>
<span id="cb49-642"><a href="#cb49-642"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-643"><a href="#cb49-643"></a>{</span>
<span id="cb49-644"><a href="#cb49-644"></a>    \node[] (Input<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span>) {\large $x[\i]$}<span class="op">;</span></span>
<span id="cb49-645"><a href="#cb49-645"></a>}</span>
<span id="cb49-646"><a href="#cb49-646"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-647"><a href="#cb49-647"></a>{</span>
<span id="cb49-648"><a href="#cb49-648"></a>    \node[] (Weight<span class="op">-</span>\i) at (<span class="dv">0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span><span class="op">-</span><span class="dv">2</span>) {\large $w[\i]$}<span class="op">;</span></span>
<span id="cb49-649"><a href="#cb49-649"></a>}</span>
<span id="cb49-650"><a href="#cb49-650"></a>\foreach \i <span class="kw">in</span> {<span class="dv">0</span>,...,<span class="dv">2</span>}</span>
<span id="cb49-651"><a href="#cb49-651"></a>{</span>
<span id="cb49-652"><a href="#cb49-652"></a>    \node[circle, </span>
<span id="cb49-653"><a href="#cb49-653"></a>        minimum size <span class="op">=</span> <span class="dv">15</span><span class="er">mm</span>,</span>
<span id="cb49-654"><a href="#cb49-654"></a>        draw,</span>
<span id="cb49-655"><a href="#cb49-655"></a>        ] (Mult<span class="op">-</span>\i) at (<span class="fl">3.0</span>,<span class="op">-</span>\i <span class="op">*</span> <span class="dv">4</span> <span class="op">-</span> <span class="dv">1</span>) {\large $\times$}<span class="op">;</span></span>
<span id="cb49-656"><a href="#cb49-656"></a>        </span>
<span id="cb49-657"><a href="#cb49-657"></a>}</span>
<span id="cb49-658"><a href="#cb49-658"></a></span>
<span id="cb49-659"><a href="#cb49-659"></a>\node [] (bias) at (<span class="dv">0</span>,<span class="op">-</span><span class="dv">12</span>) {\large $b$}<span class="op">;</span></span>
<span id="cb49-660"><a href="#cb49-660"></a></span>
<span id="cb49-661"><a href="#cb49-661"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (Add) at (<span class="dv">6</span>,<span class="op">-</span><span class="dv">5</span>) {\large <span class="op">+</span>}<span class="op">;</span></span>
<span id="cb49-662"><a href="#cb49-662"></a>\node [circle,minimum size<span class="op">=</span><span class="dv">15</span><span class="er">mm</span>,draw] (ReLU) at (<span class="dv">9</span>,<span class="op">-</span><span class="dv">5</span>) {\large $\<span class="bu">max</span>(x,<span class="dv">0</span>)$}<span class="op">;</span></span>
<span id="cb49-663"><a href="#cb49-663"></a>\node [] (NextLayer) at (<span class="dv">12</span>,<span class="op">-</span><span class="dv">5</span>) {}<span class="op">;</span></span>
<span id="cb49-664"><a href="#cb49-664"></a></span>
<span id="cb49-665"><a href="#cb49-665"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb49-666"><a href="#cb49-666"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$} (Mult<span class="op">-</span><span class="dv">0</span>)<span class="op">;</span>   </span>
<span id="cb49-667"><a href="#cb49-667"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb49-668"><a href="#cb49-668"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">1.0</span>$}(Mult<span class="op">-</span><span class="dv">1</span>)<span class="op">;</span>   </span>
<span id="cb49-669"><a href="#cb49-669"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Input<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">3.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb49-670"><a href="#cb49-670"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Weight<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Mult<span class="op">-</span><span class="dv">2</span>)<span class="op">;</span>   </span>
<span id="cb49-671"><a href="#cb49-671"></a></span>
<span id="cb49-672"><a href="#cb49-672"></a>\draw (bias) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">1.0</span>$}(<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>)<span class="op">;</span></span>
<span id="cb49-673"><a href="#cb49-673"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (<span class="dv">6</span>,<span class="op">-</span><span class="dv">12</span>) <span class="op">--</span> (Add)<span class="op">;</span></span>
<span id="cb49-674"><a href="#cb49-674"></a></span>
<span id="cb49-675"><a href="#cb49-675"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">0</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="op">-</span><span class="fl">3.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-676"><a href="#cb49-676"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">1</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">2.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-677"><a href="#cb49-677"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Mult<span class="op">-</span><span class="dv">2</span>) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(Add)<span class="op">;</span>   </span>
<span id="cb49-678"><a href="#cb49-678"></a></span>
<span id="cb49-679"><a href="#cb49-679"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (Add) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(ReLU)<span class="op">;</span>   </span>
<span id="cb49-680"><a href="#cb49-680"></a>\draw[<span class="op">-&gt;</span>, shorten <span class="op">&gt;=</span><span class="dv">1</span><span class="er">pt</span>] (ReLU) <span class="op">--</span> node[midway,above,blue] {\large $<span class="fl">6.0</span>$}(NextLayer)<span class="op">;</span></span>
<span id="cb49-681"><a href="#cb49-681"></a>\node [red] (A) at (<span class="dv">11</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-682"><a href="#cb49-682"></a>\node [red] (B) at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-683"><a href="#cb49-683"></a>\node [red] (C) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">3.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-684"><a href="#cb49-684"></a>\node [red] (D) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">5.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-685"><a href="#cb49-685"></a>\node [red] (E) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">7.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-686"><a href="#cb49-686"></a>\node [red] (F) at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">12.5</span>) {\large $<span class="fl">1.00</span>$}<span class="op">;</span></span>
<span id="cb49-687"><a href="#cb49-687"></a>\node [red] (G) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">0.75</span>) {\large $<span class="op">-</span><span class="fl">3.0</span>$}<span class="op">;</span></span>
<span id="cb49-688"><a href="#cb49-688"></a>\node [red] (H) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">2</span>) {\large $<span class="fl">1.0</span>$}<span class="op">;</span></span>
<span id="cb49-689"><a href="#cb49-689"></a>\node [red] (I) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">4.75</span>) {\large $<span class="op">-</span><span class="fl">1.0</span>$}<span class="op">;</span></span>
<span id="cb49-690"><a href="#cb49-690"></a>\node [red] (J) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">6</span>) {\large $<span class="op">-</span><span class="fl">2.0</span>$}<span class="op">;</span></span>
<span id="cb49-691"><a href="#cb49-691"></a>\node [red] (K) at (<span class="dv">1</span>,<span class="op">-</span><span class="fl">8.75</span>) {\large $<span class="fl">2.0</span>$}<span class="op">;</span></span>
<span id="cb49-692"><a href="#cb49-692"></a>\node [red] (L) at (<span class="dv">1</span>,<span class="op">-</span><span class="dv">10</span>) {\large $<span class="fl">3.0</span>$}<span class="op">;</span></span>
<span id="cb49-693"><a href="#cb49-693"></a>\end{tikzpicture}</span>
<span id="cb49-694"><a href="#cb49-694"></a><span class="in">```</span></span>
<span id="cb49-695"><a href="#cb49-695"></a></span>
<span id="cb49-696"><a href="#cb49-696"></a>Gradients sum at outward branches. Consider the following computation graph:</span>
<span id="cb49-697"><a href="#cb49-697"></a></span>
<span id="cb49-700"><a href="#cb49-700"></a><span class="in">```{python}</span></span>
<span id="cb49-701"><a href="#cb49-701"></a><span class="co"># | code-fold: true</span></span>
<span id="cb49-702"><a href="#cb49-702"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb49-703"><a href="#cb49-703"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb49-704"><a href="#cb49-704"></a>\begin{tikzpicture}[x<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,y<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>,yscale<span class="op">=-</span><span class="dv">1</span>,xscale<span class="op">=</span><span class="dv">1</span>]</span>
<span id="cb49-705"><a href="#cb49-705"></a><span class="op">%</span>uncomment <span class="cf">if</span> require: \path (<span class="dv">0</span>,<span class="dv">211</span>)<span class="op">;</span> <span class="op">%</span><span class="bu">set</span> diagram left start at <span class="dv">0</span>, <span class="kw">and</span> has height of <span class="dv">211</span></span>
<span id="cb49-706"><a href="#cb49-706"></a></span>
<span id="cb49-707"><a href="#cb49-707"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp4612472925724298] </span>
<span id="cb49-708"><a href="#cb49-708"></a>\draw   (<span class="fl">444.62</span>,<span class="dv">95</span>) .. controls (<span class="fl">444.62</span>,<span class="fl">81.19</span>) <span class="kw">and</span> (<span class="fl">455.38</span>,<span class="dv">70</span>) .. (<span class="fl">468.64</span>,<span class="dv">70</span>) .. controls (<span class="fl">481.91</span>,<span class="dv">70</span>) <span class="kw">and</span> (<span class="fl">492.66</span>,<span class="fl">81.19</span>) .. (<span class="fl">492.66</span>,<span class="dv">95</span>) .. controls (<span class="fl">492.66</span>,<span class="fl">108.81</span>) <span class="kw">and</span> (<span class="fl">481.91</span>,<span class="dv">120</span>) .. (<span class="fl">468.64</span>,<span class="dv">120</span>) .. controls (<span class="fl">455.38</span>,<span class="dv">120</span>) <span class="kw">and</span> (<span class="fl">444.62</span>,<span class="fl">108.81</span>) .. (<span class="fl">444.62</span>,<span class="dv">95</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb49-709"><a href="#cb49-709"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp4844626229099638] </span>
<span id="cb49-710"><a href="#cb49-710"></a>\draw   (<span class="fl">299.33</span>,<span class="fl">31.5</span>) .. controls (<span class="fl">299.33</span>,<span class="fl">17.69</span>) <span class="kw">and</span> (<span class="fl">310.08</span>,<span class="fl">6.5</span>) .. (<span class="fl">323.35</span>,<span class="fl">6.5</span>) .. controls (<span class="fl">336.61</span>,<span class="fl">6.5</span>) <span class="kw">and</span> (<span class="fl">347.37</span>,<span class="fl">17.69</span>) .. (<span class="fl">347.37</span>,<span class="fl">31.5</span>) .. controls (<span class="fl">347.37</span>,<span class="fl">45.31</span>) <span class="kw">and</span> (<span class="fl">336.61</span>,<span class="fl">56.5</span>) .. (<span class="fl">323.35</span>,<span class="fl">56.5</span>) .. controls (<span class="fl">310.08</span>,<span class="fl">56.5</span>) <span class="kw">and</span> (<span class="fl">299.33</span>,<span class="fl">45.31</span>) .. (<span class="fl">299.33</span>,<span class="fl">31.5</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb49-711"><a href="#cb49-711"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp2271780920027553] </span>
<span id="cb49-712"><a href="#cb49-712"></a>\draw   (<span class="fl">303.25</span>,<span class="fl">94.7</span>) .. controls (<span class="fl">303.25</span>,<span class="fl">80.89</span>) <span class="kw">and</span> (<span class="dv">314</span>,<span class="fl">69.7</span>) .. (<span class="fl">327.27</span>,<span class="fl">69.7</span>) .. controls (<span class="fl">340.53</span>,<span class="fl">69.7</span>) <span class="kw">and</span> (<span class="fl">351.29</span>,<span class="fl">80.89</span>) .. (<span class="fl">351.29</span>,<span class="fl">94.7</span>) .. controls (<span class="fl">351.29</span>,<span class="fl">108.51</span>) <span class="kw">and</span> (<span class="fl">340.53</span>,<span class="fl">119.7</span>) .. (<span class="fl">327.27</span>,<span class="fl">119.7</span>) .. controls (<span class="dv">314</span>,<span class="fl">119.7</span>) <span class="kw">and</span> (<span class="fl">303.25</span>,<span class="fl">108.51</span>) .. (<span class="fl">303.25</span>,<span class="fl">94.7</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb49-713"><a href="#cb49-713"></a><span class="op">%</span>Shape: Ellipse [<span class="bu">id</span>:dp150108609534231] </span>
<span id="cb49-714"><a href="#cb49-714"></a>\draw   (<span class="fl">299.25</span>,<span class="fl">167.7</span>) .. controls (<span class="fl">299.25</span>,<span class="fl">153.89</span>) <span class="kw">and</span> (<span class="dv">310</span>,<span class="fl">142.7</span>) .. (<span class="fl">323.27</span>,<span class="fl">142.7</span>) .. controls (<span class="fl">336.53</span>,<span class="fl">142.7</span>) <span class="kw">and</span> (<span class="fl">347.29</span>,<span class="fl">153.89</span>) .. (<span class="fl">347.29</span>,<span class="fl">167.7</span>) .. controls (<span class="fl">347.29</span>,<span class="fl">181.51</span>) <span class="kw">and</span> (<span class="fl">336.53</span>,<span class="fl">192.7</span>) .. (<span class="fl">323.27</span>,<span class="fl">192.7</span>) .. controls (<span class="dv">310</span>,<span class="fl">192.7</span>) <span class="kw">and</span> (<span class="fl">299.25</span>,<span class="fl">181.51</span>) .. (<span class="fl">299.25</span>,<span class="fl">167.7</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb49-715"><a href="#cb49-715"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da7844123205705824] </span>
<span id="cb49-716"><a href="#cb49-716"></a>\draw    (<span class="fl">347.37</span>,<span class="fl">31.5</span>) <span class="op">--</span> (<span class="fl">450.04</span>,<span class="fl">76.06</span>) <span class="op">;</span></span>
<span id="cb49-717"><a href="#cb49-717"></a>\draw [shift<span class="op">=</span>{(<span class="fl">452.79</span>,<span class="fl">77.25</span>)}, rotate <span class="op">=</span> <span class="fl">203.46</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb49-718"><a href="#cb49-718"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da814168086414518] </span>
<span id="cb49-719"><a href="#cb49-719"></a>\draw    (<span class="fl">351.29</span>,<span class="fl">94.7</span>) <span class="op">--</span> (<span class="fl">441.62</span>,<span class="fl">94.99</span>) <span class="op">;</span></span>
<span id="cb49-720"><a href="#cb49-720"></a>\draw [shift<span class="op">=</span>{(<span class="fl">444.62</span>,<span class="dv">95</span>)}, rotate <span class="op">=</span> <span class="fl">180.18</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb49-721"><a href="#cb49-721"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da7411937688169676] </span>
<span id="cb49-722"><a href="#cb49-722"></a>\draw    (<span class="fl">347.29</span>,<span class="fl">167.7</span>) <span class="op">--</span> (<span class="fl">446.35</span>,<span class="fl">110.75</span>) <span class="op">;</span></span>
<span id="cb49-723"><a href="#cb49-723"></a>\draw [shift<span class="op">=</span>{(<span class="fl">448.95</span>,<span class="fl">109.25</span>)}, rotate <span class="op">=</span> <span class="fl">150.1</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb49-724"><a href="#cb49-724"></a><span class="op">%</span>Shape: Circle [<span class="bu">id</span>:dp515320046458885] </span>
<span id="cb49-725"><a href="#cb49-725"></a>\draw   (<span class="dv">163</span>,<span class="dv">96</span>) .. controls (<span class="dv">163</span>,<span class="fl">82.19</span>) <span class="kw">and</span> (<span class="fl">174.19</span>,<span class="dv">71</span>) .. (<span class="dv">188</span>,<span class="dv">71</span>) .. controls (<span class="fl">201.81</span>,<span class="dv">71</span>) <span class="kw">and</span> (<span class="dv">213</span>,<span class="fl">82.19</span>) .. (<span class="dv">213</span>,<span class="dv">96</span>) .. controls (<span class="dv">213</span>,<span class="fl">109.81</span>) <span class="kw">and</span> (<span class="fl">201.81</span>,<span class="dv">121</span>) .. (<span class="dv">188</span>,<span class="dv">121</span>) .. controls (<span class="fl">174.19</span>,<span class="dv">121</span>) <span class="kw">and</span> (<span class="dv">163</span>,<span class="fl">109.81</span>) .. (<span class="dv">163</span>,<span class="dv">96</span>) <span class="op">--</span> cycle <span class="op">;</span></span>
<span id="cb49-726"><a href="#cb49-726"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da6219161786925074] </span>
<span id="cb49-727"><a href="#cb49-727"></a>\draw    (<span class="fl">492.66</span>,<span class="dv">95</span>) <span class="op">--</span> (<span class="dv">567</span>,<span class="fl">94.52</span>) <span class="op">;</span></span>
<span id="cb49-728"><a href="#cb49-728"></a>\draw [shift<span class="op">=</span>{(<span class="dv">570</span>,<span class="fl">94.5</span>)}, rotate <span class="op">=</span> <span class="fl">179.63</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb49-729"><a href="#cb49-729"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da5694521418691749] </span>
<span id="cb49-730"><a href="#cb49-730"></a>\draw    (<span class="fl">84.5</span>,<span class="fl">95.75</span>) <span class="op">--</span> (<span class="dv">160</span>,<span class="fl">95.99</span>) <span class="op">;</span></span>
<span id="cb49-731"><a href="#cb49-731"></a>\draw [shift<span class="op">=</span>{(<span class="dv">163</span>,<span class="dv">96</span>)}, rotate <span class="op">=</span> <span class="fl">180.18</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">8.04</span>,<span class="op">-</span><span class="fl">3.86</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">8.04</span>,<span class="fl">3.86</span>) <span class="op">--</span> (<span class="fl">5.34</span>,<span class="dv">0</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb49-732"><a href="#cb49-732"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da08990804845355682] </span>
<span id="cb49-733"><a href="#cb49-733"></a>\draw    (<span class="fl">210.69</span>,<span class="fl">85.5</span>) <span class="op">--</span> (<span class="fl">296.86</span>,<span class="fl">31.4</span>) <span class="op">;</span></span>
<span id="cb49-734"><a href="#cb49-734"></a>\draw [shift<span class="op">=</span>{(<span class="fl">299.4</span>,<span class="fl">29.8</span>)}, rotate <span class="op">=</span> <span class="fl">147.88</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb49-735"><a href="#cb49-735"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da1505672958459916] </span>
<span id="cb49-736"><a href="#cb49-736"></a>\draw    (<span class="fl">212.61</span>,<span class="dv">96</span>) <span class="op">--</span> (<span class="fl">300.4</span>,<span class="fl">95.03</span>) <span class="op">;</span></span>
<span id="cb49-737"><a href="#cb49-737"></a>\draw [shift<span class="op">=</span>{(<span class="fl">303.4</span>,<span class="dv">95</span>)}, rotate <span class="op">=</span> <span class="fl">179.37</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb49-738"><a href="#cb49-738"></a><span class="op">%</span>Straight Lines [<span class="bu">id</span>:da23258128449735227] </span>
<span id="cb49-739"><a href="#cb49-739"></a>\draw    (<span class="dv">203</span>,<span class="fl">116.5</span>) <span class="op">--</span> (<span class="fl">296.36</span>,<span class="fl">167.17</span>) <span class="op">;</span></span>
<span id="cb49-740"><a href="#cb49-740"></a>\draw [shift<span class="op">=</span>{(<span class="dv">299</span>,<span class="fl">168.6</span>)}, rotate <span class="op">=</span> <span class="fl">208.49</span>] [fill<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">0</span><span class="op">;</span> blue, <span class="dv">0</span> }  ][line width<span class="op">=</span><span class="fl">0.08</span>]  [draw opacity<span class="op">=</span><span class="dv">0</span>] (<span class="fl">6.25</span>,<span class="op">-</span><span class="dv">3</span>) <span class="op">--</span> (<span class="dv">0</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="fl">6.25</span>,<span class="dv">3</span>) <span class="op">--</span> cycle    <span class="op">;</span></span>
<span id="cb49-741"><a href="#cb49-741"></a></span>
<span id="cb49-742"><a href="#cb49-742"></a><span class="op">%</span> Text Node</span>
<span id="cb49-743"><a href="#cb49-743"></a>\draw (<span class="fl">464.08</span>,<span class="fl">84.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $s$}<span class="op">;</span></span>
<span id="cb49-744"><a href="#cb49-744"></a><span class="op">%</span> Text Node</span>
<span id="cb49-745"><a href="#cb49-745"></a>\draw (<span class="fl">317.25</span>,<span class="fl">18.9</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">1</span>}$}<span class="op">;</span></span>
<span id="cb49-746"><a href="#cb49-746"></a><span class="op">%</span> Text Node</span>
<span id="cb49-747"><a href="#cb49-747"></a>\draw (<span class="fl">321.65</span>,<span class="fl">82.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">2</span>}$}<span class="op">;</span></span>
<span id="cb49-748"><a href="#cb49-748"></a><span class="op">%</span> Text Node</span>
<span id="cb49-749"><a href="#cb49-749"></a>\draw (<span class="fl">317.65</span>,<span class="fl">155.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $z<span class="op">^</span>{<span class="dv">3</span>}$}<span class="op">;</span></span>
<span id="cb49-750"><a href="#cb49-750"></a><span class="op">%</span> Text Node</span>
<span id="cb49-751"><a href="#cb49-751"></a>\draw (<span class="fl">365.04</span>,<span class="fl">44.2</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">1</span>}}$}<span class="op">;</span></span>
<span id="cb49-752"><a href="#cb49-752"></a><span class="op">%</span> Text Node</span>
<span id="cb49-753"><a href="#cb49-753"></a>\draw (<span class="fl">365.52</span>,<span class="fl">94.3</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">2</span>}}$}<span class="op">;</span></span>
<span id="cb49-754"><a href="#cb49-754"></a><span class="op">%</span> Text Node</span>
<span id="cb49-755"><a href="#cb49-755"></a>\draw (<span class="fl">366.72</span>,<span class="dv">154</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\huge $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">3</span>}}$}<span class="op">;</span></span>
<span id="cb49-756"><a href="#cb49-756"></a><span class="op">%</span> Text Node</span>
<span id="cb49-757"><a href="#cb49-757"></a>\draw (<span class="fl">183.5</span>,<span class="fl">85.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]    {\huge $a$}<span class="op">;</span></span>
<span id="cb49-758"><a href="#cb49-758"></a><span class="op">%</span> Text Node</span>
<span id="cb49-759"><a href="#cb49-759"></a>\draw (<span class="fl">304.78</span>,<span class="fl">21.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">1</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb49-760"><a href="#cb49-760"></a><span class="op">%</span> Text Node</span>
<span id="cb49-761"><a href="#cb49-761"></a>\draw (<span class="fl">305.82</span>,<span class="fl">84.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">2</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb49-762"><a href="#cb49-762"></a><span class="op">%</span> Text Node</span>
<span id="cb49-763"><a href="#cb49-763"></a>\draw (<span class="fl">303.26</span>,<span class="fl">156.6</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial z<span class="op">^</span>{<span class="dv">3</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb49-764"><a href="#cb49-764"></a><span class="op">%</span> Text Node</span>
<span id="cb49-765"><a href="#cb49-765"></a>\draw (<span class="fl">251.38</span>,<span class="fl">53.4</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">1</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">1</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb49-766"><a href="#cb49-766"></a><span class="op">%</span> Text Node</span>
<span id="cb49-767"><a href="#cb49-767"></a>\draw (<span class="fl">249.38</span>,<span class="fl">99.8</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">2</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">2</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb49-768"><a href="#cb49-768"></a><span class="op">%</span> Text Node</span>
<span id="cb49-769"><a href="#cb49-769"></a>\draw (<span class="fl">245.78</span>,<span class="fl">165.8</span>) node [anchor<span class="op">=</span>north west][inner sep<span class="op">=</span><span class="fl">0.75</span><span class="er">pt</span>]  [font<span class="op">=</span>\tiny,color<span class="op">=</span>{rgb, <span class="dv">255</span>:red, <span class="dv">0</span><span class="op">;</span> green, <span class="dv">13</span><span class="op">;</span> blue, <span class="dv">247</span> }  ,opacity<span class="op">=</span><span class="dv">1</span> ]  {\normalsize $\frac{\partial s}{\partial z<span class="op">^</span>{<span class="dv">3</span>}} \cdot \frac{\partial z<span class="op">^</span>{<span class="dv">3</span>}}{\partial a}$}<span class="op">;</span></span>
<span id="cb49-770"><a href="#cb49-770"></a></span>
<span id="cb49-771"><a href="#cb49-771"></a></span>
<span id="cb49-772"><a href="#cb49-772"></a>\end{tikzpicture}</span>
<span id="cb49-773"><a href="#cb49-773"></a></span>
<span id="cb49-774"><a href="#cb49-774"></a><span class="in">```</span></span>
<span id="cb49-775"><a href="#cb49-775"></a></span>
<span id="cb49-776"><a href="#cb49-776"></a>The upstream gradient for the node $a$ is $\frac{ds}{da}$. By the law of total derivatives:</span>
<span id="cb49-777"><a href="#cb49-777"></a></span>
<span id="cb49-778"><a href="#cb49-778"></a>\begin{align*}</span>
<span id="cb49-779"><a href="#cb49-779"></a>\frac{ds}{da} = \frac{\partial s}{\partial z^1} \cdot \frac{\partial z^1}{\partial a} + \frac{\partial s}{\partial z^2} \cdot \frac{\partial z^2}{\partial a} + \frac{\partial s}{\partial z^3} \cdot \frac{\partial z^3}{\partial a}</span>
<span id="cb49-780"><a href="#cb49-780"></a>\end{align*}</span>
<span id="cb49-781"><a href="#cb49-781"></a></span>
<span id="cb49-782"><a href="#cb49-782"></a><span class="fu">## Backprop for a single neuron - a python implementation </span></span>
<span id="cb49-783"><a href="#cb49-783"></a></span>
<span id="cb49-784"><a href="#cb49-784"></a>We can write a naive implementation for the backprop algorithm for a single neuron.</span>
<span id="cb49-785"><a href="#cb49-785"></a></span>
<span id="cb49-788"><a href="#cb49-788"></a><span class="in">```{python}</span></span>
<span id="cb49-789"><a href="#cb49-789"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-790"><a href="#cb49-790"></a></span>
<span id="cb49-791"><a href="#cb49-791"></a>weights <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">3.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">2.0</span>])</span>
<span id="cb49-792"><a href="#cb49-792"></a>bias <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb49-793"><a href="#cb49-793"></a>inputs <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">3.0</span>])</span>
<span id="cb49-794"><a href="#cb49-794"></a>target_output <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb49-795"><a href="#cb49-795"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb49-796"><a href="#cb49-796"></a></span>
<span id="cb49-797"><a href="#cb49-797"></a></span>
<span id="cb49-798"><a href="#cb49-798"></a><span class="kw">def</span> relu(x):</span>
<span id="cb49-799"><a href="#cb49-799"></a>    <span class="cf">return</span> np.maximum(x, <span class="dv">0</span>)</span>
<span id="cb49-800"><a href="#cb49-800"></a></span>
<span id="cb49-801"><a href="#cb49-801"></a></span>
<span id="cb49-802"><a href="#cb49-802"></a><span class="kw">def</span> relu_derivative(x):</span>
<span id="cb49-803"><a href="#cb49-803"></a>    <span class="cf">return</span> np.where(x <span class="op">&gt;</span> <span class="dv">0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb49-804"><a href="#cb49-804"></a></span>
<span id="cb49-805"><a href="#cb49-805"></a></span>
<span id="cb49-806"><a href="#cb49-806"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb49-807"><a href="#cb49-807"></a>    <span class="co"># Forward pass</span></span>
<span id="cb49-808"><a href="#cb49-808"></a>    z <span class="op">=</span> np.dot(weights, inputs) <span class="op">+</span> bias</span>
<span id="cb49-809"><a href="#cb49-809"></a>    a <span class="op">=</span> relu(z)</span>
<span id="cb49-810"><a href="#cb49-810"></a>    loss <span class="op">=</span> (a <span class="op">-</span> target_output) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb49-811"><a href="#cb49-811"></a></span>
<span id="cb49-812"><a href="#cb49-812"></a>    <span class="co"># Backward pass</span></span>
<span id="cb49-813"><a href="#cb49-813"></a>    dloss_da <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (a <span class="op">-</span> target_output)</span>
<span id="cb49-814"><a href="#cb49-814"></a>    dloss_dz <span class="op">=</span> dloss_da <span class="op">*</span> relu_derivative(z)</span>
<span id="cb49-815"><a href="#cb49-815"></a>    dz_dx <span class="op">=</span> weights</span>
<span id="cb49-816"><a href="#cb49-816"></a>    dz_dw <span class="op">=</span> inputs</span>
<span id="cb49-817"><a href="#cb49-817"></a>    dz_db <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb49-818"><a href="#cb49-818"></a>    dloss_dx <span class="op">=</span> dloss_dz <span class="op">*</span> dz_dx</span>
<span id="cb49-819"><a href="#cb49-819"></a>    dloss_dw <span class="op">=</span> dloss_dz <span class="op">*</span> dz_dw</span>
<span id="cb49-820"><a href="#cb49-820"></a>    dloss_db <span class="op">=</span> dloss_dz <span class="op">*</span> dz_db</span>
<span id="cb49-821"><a href="#cb49-821"></a></span>
<span id="cb49-822"><a href="#cb49-822"></a>    <span class="co"># Update the weights and bias</span></span>
<span id="cb49-823"><a href="#cb49-823"></a>    weights <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_dw</span>
<span id="cb49-824"><a href="#cb49-824"></a>    bias <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_db</span>
<span id="cb49-825"><a href="#cb49-825"></a></span>
<span id="cb49-826"><a href="#cb49-826"></a>    <span class="co"># print the loss for this iteration</span></span>
<span id="cb49-827"><a href="#cb49-827"></a>    <span class="cf">if</span> (<span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb49-828"><a href="#cb49-828"></a>        <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span><span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-829"><a href="#cb49-829"></a></span>
<span id="cb49-830"><a href="#cb49-830"></a><span class="bu">print</span>(<span class="st">"Final weights : "</span>, weights)</span>
<span id="cb49-831"><a href="#cb49-831"></a><span class="bu">print</span>(<span class="st">"Final bias : "</span>, bias)</span>
<span id="cb49-832"><a href="#cb49-832"></a><span class="in">```</span></span>
<span id="cb49-833"><a href="#cb49-833"></a></span>
<span id="cb49-834"><a href="#cb49-834"></a><span class="fu">## Backprop for a layer of neurons</span></span>
<span id="cb49-835"><a href="#cb49-835"></a></span>
<span id="cb49-836"><a href="#cb49-836"></a>We are now in a position to write a naive implementation of the backprop algorithm for a layer of neurons. </span>
<span id="cb49-837"><a href="#cb49-837"></a></span>
<span id="cb49-838"><a href="#cb49-838"></a>A neural network with a single hidden layer is shown below. </span>
<span id="cb49-839"><a href="#cb49-839"></a></span>
<span id="cb49-840"><a href="#cb49-840"></a><span class="al">![backprop](backprop.png)</span>{fig-align="center"}</span>
<span id="cb49-841"><a href="#cb49-841"></a></span>
<span id="cb49-842"><a href="#cb49-842"></a>Let $\mathcal{L}$ be a loss function of a neural network to minimize. Let $x \in \mathbf{R}^{d_0}$ be a single sample(input). Let $d_{l}$ be number of neurons(inputs) in layer $l$. In our example, $x \in \mathbf{R}^4$. </span>
<span id="cb49-843"><a href="#cb49-843"></a></span>
<span id="cb49-844"><a href="#cb49-844"></a>Let's derive expressions for all the derivatives we want to compute.</span>
<span id="cb49-845"><a href="#cb49-845"></a></span>
<span id="cb49-846"><a href="#cb49-846"></a><span class="fu">### Gradient of the loss with respect to $\hat{y}$</span></span>
<span id="cb49-847"><a href="#cb49-847"></a></span>
<span id="cb49-848"><a href="#cb49-848"></a>The gradient of the loss function $\mathcal{L}$ with respect to $\hat{y}$ is:</span>
<span id="cb49-849"><a href="#cb49-849"></a></span>
<span id="cb49-850"><a href="#cb49-850"></a>\begin{align*}</span>
<span id="cb49-851"><a href="#cb49-851"></a>\frac{\partial \mathcal{L}}{\partial \hat{y}} &amp;= 2*(\hat{y} - y)</span>
<span id="cb49-852"><a href="#cb49-852"></a>\end{align*}</span>
<span id="cb49-853"><a href="#cb49-853"></a></span>
<span id="cb49-854"><a href="#cb49-854"></a><span class="fu">### Gradient of the loss with respect to $a$</span></span>
<span id="cb49-855"><a href="#cb49-855"></a></span>
<span id="cb49-856"><a href="#cb49-856"></a>The gradient of $\hat{y}$ with respect to $a_1, a_2, a_3$ is:</span>
<span id="cb49-857"><a href="#cb49-857"></a></span>
<span id="cb49-858"><a href="#cb49-858"></a>\begin{align*}</span>
<span id="cb49-859"><a href="#cb49-859"></a>\frac{\partial \hat{y}}{\partial a} &amp;= \left<span class="co">[</span><span class="ot">\frac{\partial \hat{y}}{\partial a_1}, \frac{\partial \hat{y}}{\partial a_2}, \frac{\partial \hat{y}}{\partial a_3}\right</span><span class="co">]</span> = <span class="co">[</span><span class="ot">1, 1, 1</span><span class="co">]</span></span>
<span id="cb49-860"><a href="#cb49-860"></a>\end{align*}</span>
<span id="cb49-861"><a href="#cb49-861"></a></span>
<span id="cb49-862"><a href="#cb49-862"></a>So, by chain rule:</span>
<span id="cb49-863"><a href="#cb49-863"></a></span>
<span id="cb49-864"><a href="#cb49-864"></a>\begin{align*}</span>
<span id="cb49-865"><a href="#cb49-865"></a>\frac{\partial \mathcal{L}}{\partial a} &amp;= \left<span class="co">[</span><span class="ot">\frac{\partial \mathcal{L}}{\partial a_1}, \frac{\partial \mathcal{L}}{\partial a_2}, \frac{\partial \mathcal{L}}{\partial a_3}\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb49-866"><a href="#cb49-866"></a>&amp;=\left<span class="co">[</span><span class="ot">\frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_1}, \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_2}, \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_3}\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb49-867"><a href="#cb49-867"></a>&amp;= \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a}</span>
<span id="cb49-868"><a href="#cb49-868"></a>\end{align*}</span>
<span id="cb49-869"><a href="#cb49-869"></a></span>
<span id="cb49-870"><a href="#cb49-870"></a>This vector has the shape <span class="in">`[1,layer_width]`</span>. In this example, it's dimensions are <span class="in">`(1,3)`</span>. </span>
<span id="cb49-871"><a href="#cb49-871"></a></span>
<span id="cb49-872"><a href="#cb49-872"></a><span class="fu">### Gradient of the loss with respect to $z$</span></span>
<span id="cb49-873"><a href="#cb49-873"></a></span>
<span id="cb49-874"><a href="#cb49-874"></a>In our example, $a_1 = max(z_1,0)$, $a_2 = max(z_2,0)$ and $a_3 = max(z_3,0)$. Consequently, the derivative:</span>
<span id="cb49-875"><a href="#cb49-875"></a></span>
<span id="cb49-876"><a href="#cb49-876"></a>\begin{align*}</span>
<span id="cb49-877"><a href="#cb49-877"></a>\frac{\partial a}{\partial z} &amp;= \left<span class="co">[</span><span class="ot">\frac{\partial a_1}{\partial z_1}, \frac{\partial a_2}{\partial z_2}, \frac{\partial a_3}{\partial z_3}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb49-878"><a href="#cb49-878"></a>&amp;= \left<span class="co">[</span><span class="ot">1_{(z_1 &gt; 0)}, 1_{(z_2 &gt; 0)}, 1_{(z_3 &gt; 0)}\right</span><span class="co">]</span></span>
<span id="cb49-879"><a href="#cb49-879"></a>\end{align*}</span>
<span id="cb49-880"><a href="#cb49-880"></a></span>
<span id="cb49-881"><a href="#cb49-881"></a>Again this vector has shape <span class="in">`[1,layer_width]`</span>, which in our example equals <span class="in">`(1,3)`</span>.</span>
<span id="cb49-882"><a href="#cb49-882"></a></span>
<span id="cb49-883"><a href="#cb49-883"></a>By the chain rule:</span>
<span id="cb49-884"><a href="#cb49-884"></a></span>
<span id="cb49-885"><a href="#cb49-885"></a>\begin{align*}</span>
<span id="cb49-886"><a href="#cb49-886"></a>\frac{\partial \mathcal{L}}{\partial z} &amp;= \left<span class="co">[</span><span class="ot">\frac{\partial \mathcal{L}}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1}, \frac{\partial \mathcal{L}}{\partial a_2} \cdot \frac{\partial a_2}{\partial z_2}, \frac{\partial \mathcal{L}}{\partial a_3} \cdot \frac{\partial a_3}{\partial z_3}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb49-887"><a href="#cb49-887"></a>&amp;= \frac{\partial \mathcal{L}}{\partial a} \odot \frac{\partial \mathcal{a}}{\partial z}</span>
<span id="cb49-888"><a href="#cb49-888"></a>\end{align*}</span>
<span id="cb49-889"><a href="#cb49-889"></a></span>
<span id="cb49-890"><a href="#cb49-890"></a>where $\odot$ denotes the element wise product of the two vectors. The gradient of the loss with respect to $z$, is also a vector of shape <span class="in">`[1,layer_width]`</span>. </span>
<span id="cb49-891"><a href="#cb49-891"></a></span>
<span id="cb49-892"><a href="#cb49-892"></a><span class="fu">### Gradient of the loss with respect to weights $W$</span></span>
<span id="cb49-893"><a href="#cb49-893"></a></span>
<span id="cb49-894"><a href="#cb49-894"></a>Since </span>
<span id="cb49-895"><a href="#cb49-895"></a></span>
<span id="cb49-896"><a href="#cb49-896"></a>\begin{align*}</span>
<span id="cb49-897"><a href="#cb49-897"></a>z_1 &amp;= w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + b_1 <span class="sc">\\</span></span>
<span id="cb49-898"><a href="#cb49-898"></a>z_2 &amp;= w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + b_2 <span class="sc">\\</span></span>
<span id="cb49-899"><a href="#cb49-899"></a>z_3 &amp;= w_{31}x_1 + w_{32}x_2 + w_{23}x_3 + w_{24}x_4 + b_3 </span>
<span id="cb49-900"><a href="#cb49-900"></a>\end{align*}</span>
<span id="cb49-901"><a href="#cb49-901"></a></span>
<span id="cb49-902"><a href="#cb49-902"></a>it follows that:</span>
<span id="cb49-903"><a href="#cb49-903"></a>\begin{align*}</span>
<span id="cb49-904"><a href="#cb49-904"></a>\frac{\partial z_i}{\partial w_{ij}} = x_j</span>
<span id="cb49-905"><a href="#cb49-905"></a>\end{align*}</span>
<span id="cb49-906"><a href="#cb49-906"></a></span>
<span id="cb49-907"><a href="#cb49-907"></a>Now, </span>
<span id="cb49-908"><a href="#cb49-908"></a></span>
<span id="cb49-909"><a href="#cb49-909"></a>\begin{align*}</span>
<span id="cb49-910"><a href="#cb49-910"></a>\frac{\partial \mathcal{L}}{\partial w_{ij}} &amp;= \frac{\partial \mathcal{L}}{\partial z_i} \cdot \frac{\partial z_i}{\partial w_{ij}} <span class="sc">\\</span></span>
<span id="cb49-911"><a href="#cb49-911"></a>&amp;= \frac{\partial \mathcal{L}}{\partial z_i} \cdot x_j </span>
<span id="cb49-912"><a href="#cb49-912"></a>\end{align*}</span>
<span id="cb49-913"><a href="#cb49-913"></a></span>
<span id="cb49-914"><a href="#cb49-914"></a>In other words:</span>
<span id="cb49-915"><a href="#cb49-915"></a></span>
<span id="cb49-916"><a href="#cb49-916"></a>\begin{align*}</span>
<span id="cb49-917"><a href="#cb49-917"></a>\begin{bmatrix}</span>
<span id="cb49-918"><a href="#cb49-918"></a>\frac{\partial \mathcal{L}}{\partial w_{11}} <span class="sc">\\</span></span>
<span id="cb49-919"><a href="#cb49-919"></a>\frac{\partial \mathcal{L}}{\partial w_{12}} <span class="sc">\\</span></span>
<span id="cb49-920"><a href="#cb49-920"></a>\frac{\partial \mathcal{L}}{\partial w_{13}} <span class="sc">\\</span></span>
<span id="cb49-921"><a href="#cb49-921"></a>\frac{\partial \mathcal{L}}{\partial w_{14}} </span>
<span id="cb49-922"><a href="#cb49-922"></a>\end{bmatrix}</span>
<span id="cb49-923"><a href="#cb49-923"></a>&amp;= \begin{bmatrix}</span>
<span id="cb49-924"><a href="#cb49-924"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{11}}<span class="sc">\\</span></span>
<span id="cb49-925"><a href="#cb49-925"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{12}}<span class="sc">\\</span></span>
<span id="cb49-926"><a href="#cb49-926"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{13}}<span class="sc">\\</span></span>
<span id="cb49-927"><a href="#cb49-927"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{14}}</span>
<span id="cb49-928"><a href="#cb49-928"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-929"><a href="#cb49-929"></a>&amp;= \begin{bmatrix}</span>
<span id="cb49-930"><a href="#cb49-930"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_1<span class="sc">\\</span></span>
<span id="cb49-931"><a href="#cb49-931"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_2<span class="sc">\\</span></span>
<span id="cb49-932"><a href="#cb49-932"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_3<span class="sc">\\</span></span>
<span id="cb49-933"><a href="#cb49-933"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_4</span>
<span id="cb49-934"><a href="#cb49-934"></a>\end{bmatrix}</span>
<span id="cb49-935"><a href="#cb49-935"></a>\end{align*}</span>
<span id="cb49-936"><a href="#cb49-936"></a></span>
<span id="cb49-937"><a href="#cb49-937"></a>Putting this together, we define the jacobian matrix $\frac{\partial \mathcal{L}}{\partial W}$ as:</span>
<span id="cb49-938"><a href="#cb49-938"></a></span>
<span id="cb49-939"><a href="#cb49-939"></a>\begin{align*}</span>
<span id="cb49-940"><a href="#cb49-940"></a>\frac{\partial \mathcal{L}}{\partial W}&amp;=\begin{bmatrix}</span>
<span id="cb49-941"><a href="#cb49-941"></a>\frac{\partial \mathcal{L}}{\partial w_{11}} &amp; \frac{\partial \mathcal{L}}{\partial w_{21}} &amp; \frac{\partial \mathcal{L}}{\partial w_{31}} &amp; \frac{\partial \mathcal{L}}{\partial w_{41}} <span class="sc">\\</span></span>
<span id="cb49-942"><a href="#cb49-942"></a>\frac{\partial \mathcal{L}}{\partial w_{12}} &amp; \frac{\partial \mathcal{L}}{\partial w_{22}} &amp; \frac{\partial \mathcal{L}}{\partial w_{32}} &amp; \frac{\partial \mathcal{L}}{\partial w_{42}} <span class="sc">\\</span></span>
<span id="cb49-943"><a href="#cb49-943"></a>\frac{\partial \mathcal{L}}{\partial w_{13}} &amp; \frac{\partial \mathcal{L}}{\partial w_{23}} &amp; \frac{\partial \mathcal{L}}{\partial w_{33}} &amp; \frac{\partial \mathcal{L}}{\partial w_{43}} <span class="sc">\\</span></span>
<span id="cb49-944"><a href="#cb49-944"></a>\frac{\partial \mathcal{L}}{\partial w_{14}} &amp; \frac{\partial \mathcal{L}}{\partial w_{24}} &amp; \frac{\partial \mathcal{L}}{\partial w_{34}} &amp; \frac{\partial \mathcal{L}}{\partial w_{44}} <span class="sc">\\</span></span>
<span id="cb49-945"><a href="#cb49-945"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-946"><a href="#cb49-946"></a>&amp;= \begin{bmatrix}</span>
<span id="cb49-947"><a href="#cb49-947"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{11}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{21}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{31}}<span class="sc">\\</span></span>
<span id="cb49-948"><a href="#cb49-948"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{12}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{22}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{32}}<span class="sc">\\</span></span>
<span id="cb49-949"><a href="#cb49-949"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{13}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{23}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{33}}<span class="sc">\\</span></span>
<span id="cb49-950"><a href="#cb49-950"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_{14}} &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_{24}} &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{34}}</span>
<span id="cb49-951"><a href="#cb49-951"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-952"><a href="#cb49-952"></a>&amp;= \begin{bmatrix}</span>
<span id="cb49-953"><a href="#cb49-953"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_1 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_1 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_1 <span class="sc">\\</span></span>
<span id="cb49-954"><a href="#cb49-954"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_2 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_2 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_2<span class="sc">\\</span></span>
<span id="cb49-955"><a href="#cb49-955"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_3 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_3 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_3<span class="sc">\\</span></span>
<span id="cb49-956"><a href="#cb49-956"></a>\frac{\partial \mathcal{L}}{\partial z_1} \cdot x_4 &amp; \frac{\partial \mathcal{L}}{\partial z_2} \cdot x_4 &amp; \frac{\partial \mathcal{L}}{\partial z_3} \cdot x_4 </span>
<span id="cb49-957"><a href="#cb49-957"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-958"><a href="#cb49-958"></a>&amp;= \begin{bmatrix}</span>
<span id="cb49-959"><a href="#cb49-959"></a>x_1 <span class="sc">\\</span></span>
<span id="cb49-960"><a href="#cb49-960"></a>x_2 <span class="sc">\\</span></span>
<span id="cb49-961"><a href="#cb49-961"></a>x_3 <span class="sc">\\</span></span>
<span id="cb49-962"><a href="#cb49-962"></a>x_4</span>
<span id="cb49-963"><a href="#cb49-963"></a>\end{bmatrix} \begin{bmatrix}</span>
<span id="cb49-964"><a href="#cb49-964"></a>\frac{\partial \mathcal{L}}{\partial z_1} &amp; \frac{\partial \mathcal{L}}{\partial z_2} &amp; \frac{\partial \mathcal{L}}{\partial z_3}</span>
<span id="cb49-965"><a href="#cb49-965"></a>\end{bmatrix} <span class="sc">\\</span></span>
<span id="cb49-966"><a href="#cb49-966"></a>&amp;= X^T \cdot \frac{\partial \mathcal{L}}{\partial z}</span>
<span id="cb49-967"><a href="#cb49-967"></a>\end{align*}</span>
<span id="cb49-968"><a href="#cb49-968"></a></span>
<span id="cb49-969"><a href="#cb49-969"></a>The dimensions of $X^T$ and $\frac{\partial \mathcal{L}}{\partial z}$ are <span class="in">`[input_size,1]`</span> and <span class="in">`[1,layer_width]`</span> respectively. Therefore, $\frac{\partial \mathcal{L}}{\partial W}$ will be of dimensions <span class="in">`[input_size,layer_width]`</span>. In our example this equals <span class="in">`(4,3)`</span>.</span>
<span id="cb49-970"><a href="#cb49-970"></a></span>
<span id="cb49-971"><a href="#cb49-971"></a>The first column of $X^T \cdot \frac{\partial \mathcal{L}}{\partial z}$ gives the derivative with respect to the first neuron's weights, the second column gives the derivative with respect to the second neuron's weights and so forth.</span>
<span id="cb49-972"><a href="#cb49-972"></a></span>
<span id="cb49-973"><a href="#cb49-973"></a><span class="fu">### Gradient of the loss with respect to the biases $b$</span></span>
<span id="cb49-974"><a href="#cb49-974"></a></span>
<span id="cb49-975"><a href="#cb49-975"></a>Since</span>
<span id="cb49-976"><a href="#cb49-976"></a></span>
<span id="cb49-977"><a href="#cb49-977"></a>\begin{align*}</span>
<span id="cb49-978"><a href="#cb49-978"></a>\frac{\partial z}{\partial b} &amp;= \left<span class="co">[</span><span class="ot">\frac{\partial z_1}{\partial b_1}, \frac{\partial z_2}{\partial b_2}, \frac{\partial z_3}{\partial b_3}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb49-979"><a href="#cb49-979"></a>&amp;= <span class="co">[</span><span class="ot">1,1,1</span><span class="co">]</span></span>
<span id="cb49-980"><a href="#cb49-980"></a>\end{align*}</span>
<span id="cb49-981"><a href="#cb49-981"></a></span>
<span id="cb49-982"><a href="#cb49-982"></a>It follows that:</span>
<span id="cb49-983"><a href="#cb49-983"></a></span>
<span id="cb49-984"><a href="#cb49-984"></a>\begin{align*}</span>
<span id="cb49-985"><a href="#cb49-985"></a>\frac{\partial \mathcal{L}}{\partial b} &amp;= \left<span class="co">[</span><span class="ot">\frac{\partial \mathcal{L}}{\partial b_1}, \frac{\partial \mathcal{L}}{\partial b_2}, \frac{\partial \mathcal{L}}{\partial b_3}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb49-986"><a href="#cb49-986"></a>&amp;= \left<span class="co">[</span><span class="ot">\frac{\partial \mathcal{L}}{\partial z_1} \cdot \frac{\partial z_1}{\partial b_1}, \frac{\partial \mathcal{L}}{\partial b_2} \cdot \frac{\partial z_2}{\partial b_21}, \frac{\partial \mathcal{L}}{\partial b_3}\cdot \cdot \frac{\partial z_3}{\partial b_3}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb49-987"><a href="#cb49-987"></a>&amp;=\left<span class="co">[</span><span class="ot">\frac{\partial \mathcal{L}}{\partial z_1} \cdot 1, \frac{\partial \mathcal{L}}{\partial b_2} \cdot 1, \frac{\partial \mathcal{L}}{\partial b_3}\cdot \cdot 1\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb49-988"><a href="#cb49-988"></a>&amp;= \frac{\partial \mathcal{L}}{\partial z}</span>
<span id="cb49-989"><a href="#cb49-989"></a>\end{align*}</span>
<span id="cb49-990"><a href="#cb49-990"></a></span>
<span id="cb49-991"><a href="#cb49-991"></a><span class="fu">### Naive Python implementation</span></span>
<span id="cb49-992"><a href="#cb49-992"></a></span>
<span id="cb49-995"><a href="#cb49-995"></a><span class="in">```{python}</span></span>
<span id="cb49-996"><a href="#cb49-996"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-997"><a href="#cb49-997"></a></span>
<span id="cb49-998"><a href="#cb49-998"></a>inputs <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb49-999"><a href="#cb49-999"></a>weights <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>], [<span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>], [<span class="fl">0.9</span>, <span class="fl">1.0</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span>]])</span>
<span id="cb49-1000"><a href="#cb49-1000"></a></span>
<span id="cb49-1001"><a href="#cb49-1001"></a>biases <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>])</span>
<span id="cb49-1002"><a href="#cb49-1002"></a></span>
<span id="cb49-1003"><a href="#cb49-1003"></a><span class="co"># Learning rate</span></span>
<span id="cb49-1004"><a href="#cb49-1004"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb49-1005"><a href="#cb49-1005"></a></span>
<span id="cb49-1006"><a href="#cb49-1006"></a></span>
<span id="cb49-1007"><a href="#cb49-1007"></a><span class="co"># ReLU Activation function and its derivative</span></span>
<span id="cb49-1008"><a href="#cb49-1008"></a><span class="kw">def</span> relu(x):</span>
<span id="cb49-1009"><a href="#cb49-1009"></a>    <span class="cf">return</span> np.maximum(x, <span class="dv">0</span>)</span>
<span id="cb49-1010"><a href="#cb49-1010"></a></span>
<span id="cb49-1011"><a href="#cb49-1011"></a></span>
<span id="cb49-1012"><a href="#cb49-1012"></a><span class="kw">def</span> relu_derivative(z):</span>
<span id="cb49-1013"><a href="#cb49-1013"></a>    <span class="cf">return</span> np.where(z <span class="op">&gt;</span> <span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb49-1014"><a href="#cb49-1014"></a></span>
<span id="cb49-1015"><a href="#cb49-1015"></a></span>
<span id="cb49-1016"><a href="#cb49-1016"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb49-1017"><a href="#cb49-1017"></a>    <span class="co"># Forward pass</span></span>
<span id="cb49-1018"><a href="#cb49-1018"></a>    z <span class="op">=</span> np.dot(weights, inputs) <span class="op">+</span> biases</span>
<span id="cb49-1019"><a href="#cb49-1019"></a>    a <span class="op">=</span> relu(z)</span>
<span id="cb49-1020"><a href="#cb49-1020"></a>    y_pred <span class="op">=</span> np.<span class="bu">sum</span>(a)</span>
<span id="cb49-1021"><a href="#cb49-1021"></a>    y_true <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb49-1022"><a href="#cb49-1022"></a>    loss <span class="op">=</span> (y_pred <span class="op">-</span> y_true) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb49-1023"><a href="#cb49-1023"></a></span>
<span id="cb49-1024"><a href="#cb49-1024"></a>    <span class="co"># Backward pass</span></span>
<span id="cb49-1025"><a href="#cb49-1025"></a>    <span class="co"># Gradient of loss with respect to y_pred</span></span>
<span id="cb49-1026"><a href="#cb49-1026"></a>    dloss_dy <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (y_pred <span class="op">-</span> y_true)</span>
<span id="cb49-1027"><a href="#cb49-1027"></a></span>
<span id="cb49-1028"><a href="#cb49-1028"></a>    <span class="co"># Gradient of y_pred with respect to a</span></span>
<span id="cb49-1029"><a href="#cb49-1029"></a>    dy_da <span class="op">=</span> np.ones_like(a)</span>
<span id="cb49-1030"><a href="#cb49-1030"></a></span>
<span id="cb49-1031"><a href="#cb49-1031"></a>    <span class="co"># Gradient of the activation function with respect to z</span></span>
<span id="cb49-1032"><a href="#cb49-1032"></a>    da_dz <span class="op">=</span> relu_derivative(z)</span>
<span id="cb49-1033"><a href="#cb49-1033"></a></span>
<span id="cb49-1034"><a href="#cb49-1034"></a>    <span class="co"># Gradient of z with respect to the weights</span></span>
<span id="cb49-1035"><a href="#cb49-1035"></a>    dz_dw <span class="op">=</span> inputs</span>
<span id="cb49-1036"><a href="#cb49-1036"></a></span>
<span id="cb49-1037"><a href="#cb49-1037"></a>    <span class="co"># Gradient of z with respect to inputs</span></span>
<span id="cb49-1038"><a href="#cb49-1038"></a>    dz_dx <span class="op">=</span> weights</span>
<span id="cb49-1039"><a href="#cb49-1039"></a></span>
<span id="cb49-1040"><a href="#cb49-1040"></a>    <span class="co"># Gradient of loss with respect to a</span></span>
<span id="cb49-1041"><a href="#cb49-1041"></a>    dloss_da <span class="op">=</span> dloss_dy <span class="op">*</span> dy_da</span>
<span id="cb49-1042"><a href="#cb49-1042"></a></span>
<span id="cb49-1043"><a href="#cb49-1043"></a>    <span class="co"># Gradient of loss with respect to z</span></span>
<span id="cb49-1044"><a href="#cb49-1044"></a>    dloss_dz <span class="op">=</span> dloss_da <span class="op">*</span> da_dz</span>
<span id="cb49-1045"><a href="#cb49-1045"></a></span>
<span id="cb49-1046"><a href="#cb49-1046"></a>    <span class="co"># Gradient of loss with respect to the weights</span></span>
<span id="cb49-1047"><a href="#cb49-1047"></a>    dloss_dw <span class="op">=</span> np.outer(dloss_dz, dz_dw)</span>
<span id="cb49-1048"><a href="#cb49-1048"></a></span>
<span id="cb49-1049"><a href="#cb49-1049"></a>    <span class="co"># Gradient of loss with respect to biases</span></span>
<span id="cb49-1050"><a href="#cb49-1050"></a>    dloss_db <span class="op">=</span> dloss_dz</span>
<span id="cb49-1051"><a href="#cb49-1051"></a></span>
<span id="cb49-1052"><a href="#cb49-1052"></a>    weights <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_dw</span>
<span id="cb49-1053"><a href="#cb49-1053"></a>    biases <span class="op">-=</span> learning_rate <span class="op">*</span> dloss_db</span>
<span id="cb49-1054"><a href="#cb49-1054"></a></span>
<span id="cb49-1055"><a href="#cb49-1055"></a>    <span class="cf">if</span> (<span class="bu">iter</span> <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">20</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb49-1056"><a href="#cb49-1056"></a>        <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span><span class="bu">iter</span><span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1057"><a href="#cb49-1057"></a></span>
<span id="cb49-1058"><a href="#cb49-1058"></a><span class="bu">print</span>(<span class="st">"Final weights : "</span>, weights)</span>
<span id="cb49-1059"><a href="#cb49-1059"></a><span class="bu">print</span>(<span class="st">"Final bias : "</span>, biases)</span>
<span id="cb49-1060"><a href="#cb49-1060"></a><span class="in">```</span></span>
<span id="cb49-1061"><a href="#cb49-1061"></a></span>
<span id="cb49-1062"><a href="#cb49-1062"></a></span>
<span id="cb49-1063"><a href="#cb49-1063"></a><span class="fu">## Backprop with a batch of inputs</span></span>
<span id="cb49-1064"><a href="#cb49-1064"></a></span>
<span id="cb49-1065"><a href="#cb49-1065"></a>Let $x$ be a batch of inputs of dimensions <span class="in">`[batch_size,input_size]`</span>. Consider</span>
<span id="cb49-1066"><a href="#cb49-1066"></a></span>
<span id="cb49-1069"><a href="#cb49-1069"></a><span class="in">```{python}</span></span>
<span id="cb49-1070"><a href="#cb49-1070"></a>x <span class="op">=</span> np.array(</span>
<span id="cb49-1071"><a href="#cb49-1071"></a>    [</span>
<span id="cb49-1072"><a href="#cb49-1072"></a>        [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">2.5</span>],</span>
<span id="cb49-1073"><a href="#cb49-1073"></a>        [<span class="dv">2</span>, <span class="dv">5</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb49-1074"><a href="#cb49-1074"></a>        [<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">2.7</span>, <span class="fl">3.3</span>, <span class="op">-</span><span class="fl">0.8</span>]</span>
<span id="cb49-1075"><a href="#cb49-1075"></a>    ]</span>
<span id="cb49-1076"><a href="#cb49-1076"></a>)</span>
<span id="cb49-1077"><a href="#cb49-1077"></a><span class="in">```</span></span>
<span id="cb49-1078"><a href="#cb49-1078"></a></span>
<span id="cb49-1079"><a href="#cb49-1079"></a>of shape <span class="in">`(3,4)`</span>. Each sample will give one loss. Hence, the total loss $\mathcal{L} = L_1 + L_2 + L_3$. </span>
<span id="cb49-1080"><a href="#cb49-1080"></a></span>
<span id="cb49-1081"><a href="#cb49-1081"></a><span class="fu">### Gradient of the loss with respect to weights $w$</span></span>
<span id="cb49-1082"><a href="#cb49-1082"></a></span>
<span id="cb49-1083"><a href="#cb49-1083"></a>I am going to denote use the following convention for the $z$'s:</span>
<span id="cb49-1084"><a href="#cb49-1084"></a></span>
<span id="cb49-1085"><a href="#cb49-1085"></a>\begin{align*}</span>
<span id="cb49-1086"><a href="#cb49-1086"></a>\begin{array}<span class="co">[</span><span class="ot">c|ccc</span><span class="co">]</span></span>
<span id="cb49-1087"><a href="#cb49-1087"></a>\text{} &amp; \text{Neuron}-1 &amp; \text{Neuron}-2 &amp; \text{Neuron}-3<span class="sc">\\</span></span>
<span id="cb49-1088"><a href="#cb49-1088"></a>\hline</span>
<span id="cb49-1089"><a href="#cb49-1089"></a>\text{Sample}-1 &amp; z_{11} &amp; z_{12} &amp; z_{13} <span class="sc">\\</span></span>
<span id="cb49-1090"><a href="#cb49-1090"></a>\text{Sample}-2 &amp; z_{21} &amp; z_{22} &amp; z_{23} <span class="sc">\\</span></span>
<span id="cb49-1091"><a href="#cb49-1091"></a>\text{Sample}-3 &amp; z_{31} &amp; z_{32} &amp; z_{33} <span class="sc">\\</span></span>
<span id="cb49-1092"><a href="#cb49-1092"></a>\text{Sample}-4 &amp; z_{41} &amp; z_{42} &amp; z_{43}</span>
<span id="cb49-1093"><a href="#cb49-1093"></a>\end{array}</span>
<span id="cb49-1094"><a href="#cb49-1094"></a>\end{align*}</span>
<span id="cb49-1095"><a href="#cb49-1095"></a></span>
<span id="cb49-1096"><a href="#cb49-1096"></a>In this case $\frac{d\mathcal{L}}{dz}$ will be a matrix of partial derivatives of shape <span class="in">`[batch_size,layer_width]`</span>. </span>
<span id="cb49-1097"><a href="#cb49-1097"></a></span>
<span id="cb49-1098"><a href="#cb49-1098"></a>I can write:</span>
<span id="cb49-1099"><a href="#cb49-1099"></a></span>
<span id="cb49-1100"><a href="#cb49-1100"></a>\begin{align*}</span>
<span id="cb49-1101"><a href="#cb49-1101"></a>\frac{\partial \mathcal{L}}{\partial w_{11}} &amp;= \frac{\partial L_1}{\partial w_{11}} + \frac{\partial L_2}{\partial w_{11}} + \frac{\partial L_3}{\partial w_{11}} <span class="sc">\\</span></span>
<span id="cb49-1102"><a href="#cb49-1102"></a>&amp;= \frac{\partial L_1}{\partial z_{11}}\cdot \frac{\partial z_{11}}{\partial w_{11}} + \frac{\partial L_2}{\partial z_{21}}\cdot\frac{\partial z_{21}}{\partial w_{11}} + \frac{\partial L_3}{\partial z_{31}} \cdot \frac{\partial z_{31}}{\partial w_{11}}<span class="sc">\\</span></span>
<span id="cb49-1103"><a href="#cb49-1103"></a>&amp;=\frac{\partial L_1}{\partial z_{11}}\cdot x_{11} + \frac{\partial L_2}{\partial z_{21}}\cdot x_{21} + \frac{\partial L_3}{\partial z_{31}} \cdot x_{31}</span>
<span id="cb49-1104"><a href="#cb49-1104"></a>\end{align*}</span>
<span id="cb49-1105"><a href="#cb49-1105"></a></span>
<span id="cb49-1106"><a href="#cb49-1106"></a>If you work out the derivatives of the loss function with respect to each of the weights, you would find:</span>
<span id="cb49-1107"><a href="#cb49-1107"></a></span>
<span id="cb49-1108"><a href="#cb49-1108"></a>\begin{align*}</span>
<span id="cb49-1109"><a href="#cb49-1109"></a>\frac{\partial \mathcal{L}}{\partial W} &amp;= X^T \cdot \frac{\partial \mathcal{L}}{\partial z}</span>
<span id="cb49-1110"><a href="#cb49-1110"></a>\end{align*}</span>
<span id="cb49-1111"><a href="#cb49-1111"></a></span>
<span id="cb49-1112"><a href="#cb49-1112"></a><span class="in">`X.T`</span> has shape <span class="in">`[input_size,batch_size]`</span> and <span class="in">`dloss_dz`</span> has shape <span class="in">`[batch_size,layer_width]`</span>, so the matrix product will have dimensions <span class="in">`[input_size,layer_width]`</span>. </span>
<span id="cb49-1113"><a href="#cb49-1113"></a></span>
<span id="cb49-1114"><a href="#cb49-1114"></a><span class="fu">### Gradient of the loss with respect to the biases $b$</span></span>
<span id="cb49-1115"><a href="#cb49-1115"></a></span>
<span id="cb49-1116"><a href="#cb49-1116"></a>Consider again:</span>
<span id="cb49-1117"><a href="#cb49-1117"></a></span>
<span id="cb49-1118"><a href="#cb49-1118"></a>\begin{align*}</span>
<span id="cb49-1119"><a href="#cb49-1119"></a>\frac{\partial \mathcal{L}}{\partial b_1} &amp;= \frac{\partial L}{\partial z_{11}} \cdot \frac{\partial z_{11}}{\partial b_1} + \frac{\partial L}{\partial z_{21}} \cdot \frac{\partial z_{21}}{\partial b_1} + \frac{\partial L}{\partial z_{31}} \cdot \frac{\partial z_{31}}{\partial b_1} <span class="sc">\\</span></span>
<span id="cb49-1120"><a href="#cb49-1120"></a>&amp;= \frac{\partial L}{\partial z_{11}} \cdot 1 + \frac{\partial L}{\partial z_{21}} \cdot 1 + \frac{\partial L}{\partial z_{31}} \cdot 1</span>
<span id="cb49-1121"><a href="#cb49-1121"></a>\end{align*}</span>
<span id="cb49-1122"><a href="#cb49-1122"></a></span>
<span id="cb49-1123"><a href="#cb49-1123"></a>So, to find the partial derivative of the loss with respect to $b_1$, we will just look at the partial derivatives of the loss with respect to the first neuron and then add them up.</span>
<span id="cb49-1124"><a href="#cb49-1124"></a></span>
<span id="cb49-1125"><a href="#cb49-1125"></a>In python, we would write this as</span>
<span id="cb49-1126"><a href="#cb49-1126"></a></span>
<span id="cb49-1127"><a href="#cb49-1127"></a><span class="in">```python</span></span>
<span id="cb49-1128"><a href="#cb49-1128"></a>dloss_dbiases <span class="op">=</span> np.<span class="bu">sum</span>(dloss_dz, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-1129"><a href="#cb49-1129"></a><span class="in">```</span></span>
<span id="cb49-1130"><a href="#cb49-1130"></a></span>
<span id="cb49-1131"><a href="#cb49-1131"></a><span class="fu">### Gradient of the loss with respect to the inputs</span></span>
<span id="cb49-1132"><a href="#cb49-1132"></a></span>
<span id="cb49-1133"><a href="#cb49-1133"></a>The gradients of the loss with respect to the weights in the layer $l$, require the gradients of the loss with respect to the inputs in layer $l+1$. It's easy to see that:</span>
<span id="cb49-1134"><a href="#cb49-1134"></a></span>
<span id="cb49-1135"><a href="#cb49-1135"></a>\begin{align*}</span>
<span id="cb49-1136"><a href="#cb49-1136"></a>\frac{\partial L}{\partial w_{11}^{(l)}} &amp;= \frac{\partial L}{\partial x_1^{(l+1)}}\cdot \frac{\partial x_1^{(l+1)}}{\partial z_{1}^{l}} \cdot \frac{\partial z_1^{(l)}}{\partial w_{11}^{(l)}}</span>
<span id="cb49-1137"><a href="#cb49-1137"></a>\end{align*}</span>
<span id="cb49-1138"><a href="#cb49-1138"></a></span>
<span id="cb49-1139"><a href="#cb49-1139"></a>What is $\frac{\partial \mathcal{L}}{\partial x_1}$, $\frac{\partial \mathcal{L}}{\partial x_2}$, $\frac{\partial \mathcal{L}}{\partial x_3}$ and $\frac{\partial \mathcal{L}}{\partial x_4}$?</span>
<span id="cb49-1140"><a href="#cb49-1140"></a></span>
<span id="cb49-1141"><a href="#cb49-1141"></a>By the chain rule:</span>
<span id="cb49-1142"><a href="#cb49-1142"></a></span>
<span id="cb49-1143"><a href="#cb49-1143"></a>\begin{align*}</span>
<span id="cb49-1144"><a href="#cb49-1144"></a>\frac{\partial \mathcal{L}}{\partial x_1} &amp;= \frac{\partial L}{\partial z_1}\cdot \frac{\partial z_1}{\partial x_1} +  \frac{\partial L}{\partial z_2}\cdot \frac{\partial z_2}{\partial x_1} +  \frac{\partial L}{\partial z_3}\cdot \frac{\partial z_3}{\partial x_1} <span class="sc">\\</span></span>
<span id="cb49-1145"><a href="#cb49-1145"></a>&amp;= \frac{\partial L}{\partial z_1}\cdot w_{11} +  \frac{\partial L}{\partial z_2}\cdot w_{21} +  \frac{\partial L}{\partial z_3}\cdot w_{31}</span>
<span id="cb49-1146"><a href="#cb49-1146"></a>\end{align*}</span>
<span id="cb49-1147"><a href="#cb49-1147"></a></span>
<span id="cb49-1148"><a href="#cb49-1148"></a>Consequently,</span>
<span id="cb49-1149"><a href="#cb49-1149"></a></span>
<span id="cb49-1150"><a href="#cb49-1150"></a>\begin{align*}</span>
<span id="cb49-1151"><a href="#cb49-1151"></a>\begin{bmatrix}</span>
<span id="cb49-1152"><a href="#cb49-1152"></a>\frac{\partial \mathcal{L}}{\partial x_1} &amp; \frac{\partial \mathcal{L}}{\partial x_2} &amp; \frac{\partial \mathcal{L}}{\partial x_3} &amp; \frac{\partial \mathcal{L}}{\partial x_4}</span>
<span id="cb49-1153"><a href="#cb49-1153"></a>\end{bmatrix} &amp;= </span>
<span id="cb49-1154"><a href="#cb49-1154"></a>\begin{bmatrix}</span>
<span id="cb49-1155"><a href="#cb49-1155"></a>\frac{\partial L}{\partial z_1} &amp; \frac{\partial L}{\partial z_2} &amp; \frac{\partial L}{\partial z_3}</span>
<span id="cb49-1156"><a href="#cb49-1156"></a>\end{bmatrix}</span>
<span id="cb49-1157"><a href="#cb49-1157"></a>\begin{bmatrix}</span>
<span id="cb49-1158"><a href="#cb49-1158"></a>w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14}<span class="sc">\\</span></span>
<span id="cb49-1159"><a href="#cb49-1159"></a>w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24}<span class="sc">\\</span></span>
<span id="cb49-1160"><a href="#cb49-1160"></a>w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34}</span>
<span id="cb49-1161"><a href="#cb49-1161"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-1162"><a href="#cb49-1162"></a>\frac{\partial \mathcal{L}}{\partial x} &amp;= \frac{\partial L}{\partial z} \cdot W</span>
<span id="cb49-1163"><a href="#cb49-1163"></a>\end{align*}</span>
<span id="cb49-1164"><a href="#cb49-1164"></a></span>
<span id="cb49-1165"><a href="#cb49-1165"></a>What if we have a batch of input data of 3 examples? In such case, $\frac{\partial \mathcal{L}}{\partial z}$ will have shape <span class="in">`(3,3)`</span> and $W$ will have shape <span class="in">`(3,4)`</span>. So, we can multiply them and the result would be <span class="in">`(3,4)`</span>. </span>
<span id="cb49-1166"><a href="#cb49-1166"></a></span>
<span id="cb49-1167"><a href="#cb49-1167"></a><span class="fu">## Adding `backward()` to `DenseLayer`</span></span>
<span id="cb49-1168"><a href="#cb49-1168"></a></span>
<span id="cb49-1169"><a href="#cb49-1169"></a>We will now add backward pass code to the <span class="in">`DenseLayer`</span> and <span class="in">`ReLUActivation`</span> classes.</span>
<span id="cb49-1170"><a href="#cb49-1170"></a></span>
<span id="cb49-1173"><a href="#cb49-1173"></a><span class="in">```{python}</span></span>
<span id="cb49-1174"><a href="#cb49-1174"></a><span class="im">from</span> nnfs.datasets <span class="im">import</span> spiral_data</span>
<span id="cb49-1175"><a href="#cb49-1175"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb49-1176"><a href="#cb49-1176"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-1177"><a href="#cb49-1177"></a><span class="im">import</span> nnfs</span>
<span id="cb49-1178"><a href="#cb49-1178"></a></span>
<span id="cb49-1179"><a href="#cb49-1179"></a>nnfs.init()</span>
<span id="cb49-1180"><a href="#cb49-1180"></a></span>
<span id="cb49-1181"><a href="#cb49-1181"></a></span>
<span id="cb49-1182"><a href="#cb49-1182"></a><span class="kw">class</span> DenseLayer:</span>
<span id="cb49-1183"><a href="#cb49-1183"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_inputs, n_neurons):</span>
<span id="cb49-1184"><a href="#cb49-1184"></a>        <span class="va">self</span>.width <span class="op">=</span> n_neurons</span>
<span id="cb49-1185"><a href="#cb49-1185"></a>        <span class="co"># Weight vectors per neuron</span></span>
<span id="cb49-1186"><a href="#cb49-1186"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.array(</span>
<span id="cb49-1187"><a href="#cb49-1187"></a>            [[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>], [<span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>], [<span class="fl">0.9</span>, <span class="fl">1.0</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span>]]</span>
<span id="cb49-1188"><a href="#cb49-1188"></a>        )</span>
<span id="cb49-1189"><a href="#cb49-1189"></a>        <span class="va">self</span>.biases <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>])</span>
<span id="cb49-1190"><a href="#cb49-1190"></a></span>
<span id="cb49-1191"><a href="#cb49-1191"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb49-1192"><a href="#cb49-1192"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb49-1193"><a href="#cb49-1193"></a>        <span class="va">self</span>.output <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights.T) <span class="op">+</span> <span class="va">self</span>.biases</span>
<span id="cb49-1194"><a href="#cb49-1194"></a></span>
<span id="cb49-1195"><a href="#cb49-1195"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_dz):</span>
<span id="cb49-1196"><a href="#cb49-1196"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> dloss_dz</span>
<span id="cb49-1197"><a href="#cb49-1197"></a>        <span class="va">self</span>.dz_dweights <span class="op">=</span> <span class="va">self</span>.inputs</span>
<span id="cb49-1198"><a href="#cb49-1198"></a>        <span class="va">self</span>.dz_dbiases <span class="op">=</span> np.ones_like(<span class="va">self</span>.inputs)</span>
<span id="cb49-1199"><a href="#cb49-1199"></a>        <span class="va">self</span>.dz_dinputs <span class="op">=</span> <span class="va">self</span>.weights</span>
<span id="cb49-1200"><a href="#cb49-1200"></a>        <span class="va">self</span>.dloss_dweights <span class="op">=</span> np.dot(<span class="va">self</span>.inputs.T, <span class="va">self</span>.dloss_dz).T</span>
<span id="cb49-1201"><a href="#cb49-1201"></a>        <span class="va">self</span>.dloss_dbiases <span class="op">=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.dloss_dz, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-1202"><a href="#cb49-1202"></a>        <span class="va">self</span>.dloss_dinputs <span class="op">=</span> np.dot(<span class="va">self</span>.dloss_dz, <span class="va">self</span>.dz_dinputs)</span>
<span id="cb49-1203"><a href="#cb49-1203"></a></span>
<span id="cb49-1204"><a href="#cb49-1204"></a></span>
<span id="cb49-1205"><a href="#cb49-1205"></a><span class="kw">class</span> ReLUActivation:</span>
<span id="cb49-1206"><a href="#cb49-1206"></a></span>
<span id="cb49-1207"><a href="#cb49-1207"></a>    <span class="co"># Forward pass</span></span>
<span id="cb49-1208"><a href="#cb49-1208"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb49-1209"><a href="#cb49-1209"></a>        <span class="co"># Calculate output values from the inputs</span></span>
<span id="cb49-1210"><a href="#cb49-1210"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb49-1211"><a href="#cb49-1211"></a>        <span class="va">self</span>.output <span class="op">=</span> np.maximum(<span class="dv">0</span>, inputs)</span>
<span id="cb49-1212"><a href="#cb49-1212"></a></span>
<span id="cb49-1213"><a href="#cb49-1213"></a>    <span class="co"># Backward pass</span></span>
<span id="cb49-1214"><a href="#cb49-1214"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_da):</span>
<span id="cb49-1215"><a href="#cb49-1215"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> dloss_da</span>
<span id="cb49-1216"><a href="#cb49-1216"></a>        <span class="va">self</span>.da_dz <span class="op">=</span> np.where(<span class="va">self</span>.inputs <span class="op">&gt;</span> <span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>)</span>
<span id="cb49-1217"><a href="#cb49-1217"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> <span class="va">self</span>.dloss_da <span class="op">*</span> <span class="va">self</span>.da_dz</span>
<span id="cb49-1218"><a href="#cb49-1218"></a></span>
<span id="cb49-1219"><a href="#cb49-1219"></a></span>
<span id="cb49-1220"><a href="#cb49-1220"></a><span class="co"># Create dataset</span></span>
<span id="cb49-1221"><a href="#cb49-1221"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">2.5</span>], [<span class="dv">2</span>, <span class="dv">5</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">2.7</span>, <span class="fl">3.3</span>, <span class="op">-</span><span class="fl">0.8</span>]])</span>
<span id="cb49-1222"><a href="#cb49-1222"></a></span>
<span id="cb49-1223"><a href="#cb49-1223"></a><span class="co"># Create a dense layer with 4 input features and 3 output values</span></span>
<span id="cb49-1224"><a href="#cb49-1224"></a>dense1 <span class="op">=</span> DenseLayer(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb49-1225"><a href="#cb49-1225"></a>relu <span class="op">=</span> ReLUActivation()</span>
<span id="cb49-1226"><a href="#cb49-1226"></a></span>
<span id="cb49-1227"><a href="#cb49-1227"></a><span class="co"># Perform a forward pass of our training data through this layer</span></span>
<span id="cb49-1228"><a href="#cb49-1228"></a>dense1.forward(X)</span>
<span id="cb49-1229"><a href="#cb49-1229"></a>relu.forward(dense1.output)</span>
<span id="cb49-1230"><a href="#cb49-1230"></a></span>
<span id="cb49-1231"><a href="#cb49-1231"></a><span class="co"># Calculate loss</span></span>
<span id="cb49-1232"><a href="#cb49-1232"></a>y_pred <span class="op">=</span> np.<span class="bu">sum</span>(relu.output)</span>
<span id="cb49-1233"><a href="#cb49-1233"></a>y_true <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb49-1234"><a href="#cb49-1234"></a>loss <span class="op">=</span> (y_pred <span class="op">-</span> y_true) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb49-1235"><a href="#cb49-1235"></a></span>
<span id="cb49-1236"><a href="#cb49-1236"></a><span class="co"># Gradient of the loss with respect to y</span></span>
<span id="cb49-1237"><a href="#cb49-1237"></a>dloss_dy <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (y_pred <span class="op">-</span> y_true)</span>
<span id="cb49-1238"><a href="#cb49-1238"></a>dy_da <span class="op">=</span> np.ones_like(relu.output)</span>
<span id="cb49-1239"><a href="#cb49-1239"></a>dloss_da <span class="op">=</span> dloss_dy <span class="op">*</span> dy_da</span>
<span id="cb49-1240"><a href="#cb49-1240"></a></span>
<span id="cb49-1241"><a href="#cb49-1241"></a>relu.backward(dloss_da)</span>
<span id="cb49-1242"><a href="#cb49-1242"></a>dense1.backward(relu.dloss_dz)</span>
<span id="cb49-1243"><a href="#cb49-1243"></a><span class="bu">print</span>(<span class="ss">f"dloss_dweights = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dweights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1244"><a href="#cb49-1244"></a><span class="bu">print</span>(<span class="ss">f"dloss_dbiases = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dbiases<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1245"><a href="#cb49-1245"></a><span class="bu">print</span>(<span class="ss">f"dloss_dinputs = </span><span class="sc">{</span>dense1<span class="sc">.</span>dloss_dinputs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1246"><a href="#cb49-1246"></a><span class="in">```</span></span>
<span id="cb49-1247"><a href="#cb49-1247"></a></span>
<span id="cb49-1248"><a href="#cb49-1248"></a><span class="fu">## Categorical cross-entropy loss derivative</span></span>
<span id="cb49-1249"><a href="#cb49-1249"></a></span>
<span id="cb49-1250"><a href="#cb49-1250"></a>The cross-entropy loss of the $i$-th sample is given by:</span>
<span id="cb49-1251"><a href="#cb49-1251"></a></span>
<span id="cb49-1252"><a href="#cb49-1252"></a>\begin{align*}</span>
<span id="cb49-1253"><a href="#cb49-1253"></a>L_i = -\sum_k y_{ik}log(\hat{y}_ik)</span>
<span id="cb49-1254"><a href="#cb49-1254"></a>\end{align*}</span>
<span id="cb49-1255"><a href="#cb49-1255"></a></span>
<span id="cb49-1256"><a href="#cb49-1256"></a>Differentiating with respect to $\hat{y}_{ij}$, we have:</span>
<span id="cb49-1257"><a href="#cb49-1257"></a></span>
<span id="cb49-1258"><a href="#cb49-1258"></a>\begin{align*}</span>
<span id="cb49-1259"><a href="#cb49-1259"></a>\frac{\partial L_i}{\partial \hat{y}_{ij}} &amp;= -\frac{\partial}{\partial \hat{y}_{ik}} \left[\sum_k y_{ik}\log (\hat{y}_{ik})\right] <span class="sc">\\</span></span>
<span id="cb49-1260"><a href="#cb49-1260"></a>&amp;= -y_{ij} \cdot \frac{\partial }{\partial \hat{y}_{ij}} \log (\hat{y}_{ij})<span class="sc">\\</span></span>
<span id="cb49-1261"><a href="#cb49-1261"></a>&amp;= -\frac{y_{ij}}{\hat{y}_{ij}}</span>
<span id="cb49-1262"><a href="#cb49-1262"></a>\end{align*}</span>
<span id="cb49-1263"><a href="#cb49-1263"></a></span>
<span id="cb49-1264"><a href="#cb49-1264"></a><span class="fu">### Adding `backward()` to `CategoricalCrossEntropyLoss`</span></span>
<span id="cb49-1265"><a href="#cb49-1265"></a></span>
<span id="cb49-1268"><a href="#cb49-1268"></a><span class="in">```{python}</span></span>
<span id="cb49-1269"><a href="#cb49-1269"></a><span class="co"># Cross-Entropy loss</span></span>
<span id="cb49-1270"><a href="#cb49-1270"></a><span class="kw">class</span> CategoricalCrossEntropyLoss(Loss):</span>
<span id="cb49-1271"><a href="#cb49-1271"></a></span>
<span id="cb49-1272"><a href="#cb49-1272"></a>    <span class="co"># Forward pass</span></span>
<span id="cb49-1273"><a href="#cb49-1273"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb49-1274"><a href="#cb49-1274"></a>        num_samples <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb49-1275"><a href="#cb49-1275"></a></span>
<span id="cb49-1276"><a href="#cb49-1276"></a>        <span class="co"># Clip data to prevent division by 0</span></span>
<span id="cb49-1277"><a href="#cb49-1277"></a>        <span class="co"># Clip both sides to not drag mean towards any value</span></span>
<span id="cb49-1278"><a href="#cb49-1278"></a>        epsilon <span class="op">=</span> <span class="fl">1e-7</span></span>
<span id="cb49-1279"><a href="#cb49-1279"></a>        y_pred_clipped <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span> <span class="op">-</span> epsilon)</span>
<span id="cb49-1280"><a href="#cb49-1280"></a></span>
<span id="cb49-1281"><a href="#cb49-1281"></a>        <span class="co"># If categorical labels</span></span>
<span id="cb49-1282"><a href="#cb49-1282"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb49-1283"><a href="#cb49-1283"></a>            correct_confidences <span class="op">=</span> y_pred_clipped[<span class="bu">range</span>(<span class="bu">len</span>(y_pred)), y_true]</span>
<span id="cb49-1284"><a href="#cb49-1284"></a>        <span class="co"># else if one-hot encoding</span></span>
<span id="cb49-1285"><a href="#cb49-1285"></a>        <span class="cf">elif</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb49-1286"><a href="#cb49-1286"></a>            correct_confidences <span class="op">=</span> np.<span class="bu">sum</span>(y_pred_clipped <span class="op">*</span> y_true, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-1287"><a href="#cb49-1287"></a></span>
<span id="cb49-1288"><a href="#cb49-1288"></a>        neg_log <span class="op">=</span> <span class="op">-</span>np.log(correct_confidences)</span>
<span id="cb49-1289"><a href="#cb49-1289"></a>        <span class="cf">return</span> neg_log</span>
<span id="cb49-1290"><a href="#cb49-1290"></a></span>
<span id="cb49-1291"><a href="#cb49-1291"></a>    <span class="co"># Backward pass</span></span>
<span id="cb49-1292"><a href="#cb49-1292"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb49-1293"><a href="#cb49-1293"></a></span>
<span id="cb49-1294"><a href="#cb49-1294"></a>        <span class="co"># number of samples</span></span>
<span id="cb49-1295"><a href="#cb49-1295"></a>        batch_size <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb49-1296"><a href="#cb49-1296"></a></span>
<span id="cb49-1297"><a href="#cb49-1297"></a>        <span class="co"># number of labels</span></span>
<span id="cb49-1298"><a href="#cb49-1298"></a>        num_labels <span class="op">=</span> <span class="bu">len</span>(y_pred[<span class="dv">0</span>])</span>
<span id="cb49-1299"><a href="#cb49-1299"></a></span>
<span id="cb49-1300"><a href="#cb49-1300"></a>        <span class="co"># If labels are sparse, turn them into a one-hot vector</span></span>
<span id="cb49-1301"><a href="#cb49-1301"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb49-1302"><a href="#cb49-1302"></a>            y_true <span class="op">=</span> np.eye(num_labels)[y_true]</span>
<span id="cb49-1303"><a href="#cb49-1303"></a></span>
<span id="cb49-1304"><a href="#cb49-1304"></a>        <span class="co"># Calculate gradient</span></span>
<span id="cb49-1305"><a href="#cb49-1305"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> <span class="op">-</span>y_true <span class="op">/</span> y_pred</span>
<span id="cb49-1306"><a href="#cb49-1306"></a></span>
<span id="cb49-1307"><a href="#cb49-1307"></a>        <span class="co"># Normalize the gradient</span></span>
<span id="cb49-1308"><a href="#cb49-1308"></a>        <span class="va">self</span>.dloss_da <span class="op">=</span> <span class="va">self</span>.dloss_da <span class="op">/</span> batch_size</span>
<span id="cb49-1309"><a href="#cb49-1309"></a><span class="in">```</span></span>
<span id="cb49-1310"><a href="#cb49-1310"></a></span>
<span id="cb49-1311"><a href="#cb49-1311"></a><span class="fu">## Softmax Activation function derivative</span></span>
<span id="cb49-1312"><a href="#cb49-1312"></a></span>
<span id="cb49-1313"><a href="#cb49-1313"></a>We are interested to calculate the derivative of the softmax function. The softmax activation function is defined as:</span>
<span id="cb49-1314"><a href="#cb49-1314"></a></span>
<span id="cb49-1315"><a href="#cb49-1315"></a>\begin{align*}</span>
<span id="cb49-1316"><a href="#cb49-1316"></a>S_{i,j} &amp;= \frac{e^{z_{i,j}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}}</span>
<span id="cb49-1317"><a href="#cb49-1317"></a>\end{align*}</span>
<span id="cb49-1318"><a href="#cb49-1318"></a></span>
<span id="cb49-1319"><a href="#cb49-1319"></a>where $S_{i,j}$ denotes the output of the $j$-th neuron for the $i$-th sample. Thus, $S_{i,j} = f(z_{i,1},\ldots,z_{i,d_l})$. Let's calculate the partial derivative of $S_{i,j}$ with respect to $z_{i,k}$.</span>
<span id="cb49-1320"><a href="#cb49-1320"></a></span>
<span id="cb49-1321"><a href="#cb49-1321"></a>By the $u/v$ rule:</span>
<span id="cb49-1322"><a href="#cb49-1322"></a></span>
<span id="cb49-1323"><a href="#cb49-1323"></a>\begin{align*}</span>
<span id="cb49-1324"><a href="#cb49-1324"></a>\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= \frac{\sum_{l=1}^{d_l} e^{z_{i,l}} \cdot \frac{\partial e^{z_{i,j}}}{\partial z_{i,k}}-e^{z_{i,j}} \cdot \frac{\partial}{\partial z_{i,k}} \sum_{l=1}^{d_l} e^{z_{i,l}}}{\left(\sum_{l=1}^{d_l} e^{z_{i,l}}\right)^2}</span>
<span id="cb49-1325"><a href="#cb49-1325"></a>\end{align*}</span>
<span id="cb49-1326"><a href="#cb49-1326"></a></span>
<span id="cb49-1327"><a href="#cb49-1327"></a>We have two cases. If $j=k$, then $\frac{\partial e^{z_{i,j}}}{\partial z_{i,k}} = e^{z_{i,k}}$ and we get:</span>
<span id="cb49-1328"><a href="#cb49-1328"></a></span>
<span id="cb49-1329"><a href="#cb49-1329"></a>\begin{align*}</span>
<span id="cb49-1330"><a href="#cb49-1330"></a>\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= \frac{e^{z_{i,k}} \cdot \sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}} \cdot e^{z_{i,k}}}{\left(\sum_{l=1}^{d_l} e^{z_{i,l}}\right)^2}<span class="sc">\\</span></span>
<span id="cb49-1331"><a href="#cb49-1331"></a>&amp;=\frac{e^{z_{i,k}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}} \cdot \frac{\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}}}{\sum_{l=1}^{d_l} e^{z_{i,l}}}<span class="sc">\\</span></span>
<span id="cb49-1332"><a href="#cb49-1332"></a>&amp;=S_{i,k}(1-S_{i,k})</span>
<span id="cb49-1333"><a href="#cb49-1333"></a>\end{align*}</span>
<span id="cb49-1334"><a href="#cb49-1334"></a></span>
<span id="cb49-1335"><a href="#cb49-1335"></a>In the case where $j \neq k$, $\frac{\partial e^{z_{i,j}}}{\partial z_{i,k}} = 0$ and we have:</span>
<span id="cb49-1336"><a href="#cb49-1336"></a></span>
<span id="cb49-1337"><a href="#cb49-1337"></a>\begin{align*}</span>
<span id="cb49-1338"><a href="#cb49-1338"></a>\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= -\frac{e^{z_{i,j}}}{\sum_{l=1}^{d_l}e^{z_{i,l}}}\cdot \frac{e^{z_{i,k}}}{\sum_{l=1}^{d_l}e^{z_{i,l}}}<span class="sc">\\</span></span>
<span id="cb49-1339"><a href="#cb49-1339"></a>&amp;=-S_{i,j} S_{i,k}</span>
<span id="cb49-1340"><a href="#cb49-1340"></a>\end{align*}</span>
<span id="cb49-1341"><a href="#cb49-1341"></a></span>
<span id="cb49-1342"><a href="#cb49-1342"></a>So, the derivative of the softmax activation function can be expressed in terms of Kronecker's delta as:</span>
<span id="cb49-1343"><a href="#cb49-1343"></a></span>
<span id="cb49-1344"><a href="#cb49-1344"></a>\begin{align*}</span>
<span id="cb49-1345"><a href="#cb49-1345"></a>\frac{\partial S_{i,j}}{\partial z_{i,k}} &amp;= S_{i,j}(\delta_{j,k} -  S_{i,k})<span class="sc">\\</span></span>
<span id="cb49-1346"><a href="#cb49-1346"></a>&amp;= S_{i,j} \delta_{j,k} - S_{i,j}S_{i,k}</span>
<span id="cb49-1347"><a href="#cb49-1347"></a>\end{align*}</span>
<span id="cb49-1348"><a href="#cb49-1348"></a></span>
<span id="cb49-1349"><a href="#cb49-1349"></a>Now, like before, let's say we have neural network with a single hidden layer with $d_1 = 3$ neurons. We apply the softmax activation function to the output of this layer. The jacobian matrix $\frac{\partial S_i}{\partial z_i}$ for the $i$-th sample can be expressed as:</span>
<span id="cb49-1350"><a href="#cb49-1350"></a></span>
<span id="cb49-1351"><a href="#cb49-1351"></a>\begin{align*}</span>
<span id="cb49-1352"><a href="#cb49-1352"></a>\frac{\partial S_i}{\partial z_i} &amp;= </span>
<span id="cb49-1353"><a href="#cb49-1353"></a>\begin{bmatrix}</span>
<span id="cb49-1354"><a href="#cb49-1354"></a>\frac{\partial S_{i1}}{\partial z_{i1}} &amp; \frac{\partial S_{i1}}{\partial z_{i2}} &amp; \frac{\partial S_{i1}}{\partial z_{i3}} <span class="sc">\\</span></span>
<span id="cb49-1355"><a href="#cb49-1355"></a>\frac{\partial S_{i2}}{\partial z_{i1}} &amp; \frac{\partial S_{i2}}{\partial z_{i2}} &amp; \frac{\partial S_{i2}}{\partial z_{i3}} <span class="sc">\\</span></span>
<span id="cb49-1356"><a href="#cb49-1356"></a>\frac{\partial S_{i3}}{\partial z_{i1}} &amp; \frac{\partial S_{i3}}{\partial z_{i2}} &amp; \frac{\partial S_{i3}}{\partial z_{i3}} </span>
<span id="cb49-1357"><a href="#cb49-1357"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-1358"><a href="#cb49-1358"></a>&amp;=\begin{bmatrix}</span>
<span id="cb49-1359"><a href="#cb49-1359"></a>S_{i1}(\delta_{11} - S_{i1}) &amp; S_{i1}(\delta_{12} - S_{i2}) &amp; S_{i1}(\delta_{13} - S_{i3}) <span class="sc">\\</span></span>
<span id="cb49-1360"><a href="#cb49-1360"></a>S_{i2}(\delta_{21} - S_{i1}) &amp; S_{i2}(\delta_{22} - S_{i2}) &amp; S_{i2}(\delta_{23} - S_{i3}) <span class="sc">\\</span></span>
<span id="cb49-1361"><a href="#cb49-1361"></a>S_{i3}(\delta_{31} - S_{i1}) &amp; S_{i3}(\delta_{32} - S_{i2}) &amp; S_{i3}(\delta_{33} - S_{i3}) </span>
<span id="cb49-1362"><a href="#cb49-1362"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-1363"><a href="#cb49-1363"></a>&amp;=\begin{bmatrix}</span>
<span id="cb49-1364"><a href="#cb49-1364"></a>S_{i1}(1 - S_{i1}) &amp; S_{i1}(0 - S_{i2}) &amp; S_{i1}(0 - S_{i3}) <span class="sc">\\</span></span>
<span id="cb49-1365"><a href="#cb49-1365"></a>S_{i2}(0 - S_{i1}) &amp; S_{i2}(1 - S_{i2}) &amp; S_{i2}(0 - S_{i3}) <span class="sc">\\</span></span>
<span id="cb49-1366"><a href="#cb49-1366"></a>S_{i3}(0 - S_{i1}) &amp; S_{i3}(0 - S_{i2}) &amp; S_{i3}(1 - S_{i3}) </span>
<span id="cb49-1367"><a href="#cb49-1367"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-1368"><a href="#cb49-1368"></a>&amp;=\begin{bmatrix}</span>
<span id="cb49-1369"><a href="#cb49-1369"></a>S_{i1}<span class="sc">\\</span></span>
<span id="cb49-1370"><a href="#cb49-1370"></a>S_{i2}<span class="sc">\\</span></span>
<span id="cb49-1371"><a href="#cb49-1371"></a>S_{i3}</span>
<span id="cb49-1372"><a href="#cb49-1372"></a>\end{bmatrix}\odot </span>
<span id="cb49-1373"><a href="#cb49-1373"></a>\begin{bmatrix}</span>
<span id="cb49-1374"><a href="#cb49-1374"></a>1 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb49-1375"><a href="#cb49-1375"></a>0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb49-1376"><a href="#cb49-1376"></a>0 &amp; 0 &amp; 1</span>
<span id="cb49-1377"><a href="#cb49-1377"></a>\end{bmatrix} - </span>
<span id="cb49-1378"><a href="#cb49-1378"></a>\begin{bmatrix}</span>
<span id="cb49-1379"><a href="#cb49-1379"></a>S_{i1}<span class="sc">\\</span></span>
<span id="cb49-1380"><a href="#cb49-1380"></a>S_{i2}<span class="sc">\\</span></span>
<span id="cb49-1381"><a href="#cb49-1381"></a>S_{i3}</span>
<span id="cb49-1382"><a href="#cb49-1382"></a>\end{bmatrix}\begin{bmatrix}</span>
<span id="cb49-1383"><a href="#cb49-1383"></a>S_{i1} &amp; S_{i2} &amp; S_{i3}</span>
<span id="cb49-1384"><a href="#cb49-1384"></a>\end{bmatrix}</span>
<span id="cb49-1385"><a href="#cb49-1385"></a>\end{align*}</span>
<span id="cb49-1386"><a href="#cb49-1386"></a></span>
<span id="cb49-1387"><a href="#cb49-1387"></a>Say the <span class="in">`softmax_output=[0.70, 0.10, 0.20]`</span>. Then, in python, we can find the Jacobian matrix as:</span>
<span id="cb49-1388"><a href="#cb49-1388"></a></span>
<span id="cb49-1391"><a href="#cb49-1391"></a><span class="in">```{python}</span></span>
<span id="cb49-1392"><a href="#cb49-1392"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-1393"><a href="#cb49-1393"></a></span>
<span id="cb49-1394"><a href="#cb49-1394"></a>softmax_output <span class="op">=</span> np.array([<span class="fl">0.70</span>, <span class="fl">0.10</span>, <span class="fl">0.20</span>])</span>
<span id="cb49-1395"><a href="#cb49-1395"></a></span>
<span id="cb49-1396"><a href="#cb49-1396"></a><span class="co"># Reshape as a column vector</span></span>
<span id="cb49-1397"><a href="#cb49-1397"></a>softmax_output <span class="op">=</span> softmax_output.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb49-1398"><a href="#cb49-1398"></a></span>
<span id="cb49-1399"><a href="#cb49-1399"></a>da_dz <span class="op">=</span> np.diagflat(softmax_output) <span class="op">-</span> np.dot(softmax_output, softmax_output.T)</span>
<span id="cb49-1400"><a href="#cb49-1400"></a></span>
<span id="cb49-1401"><a href="#cb49-1401"></a><span class="bu">print</span>(<span class="ss">f"softmax_output = </span><span class="sc">{</span>softmax_output<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1402"><a href="#cb49-1402"></a><span class="bu">print</span>(<span class="ss">f"da_dz = </span><span class="sc">{</span>da_dz<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1403"><a href="#cb49-1403"></a><span class="in">```</span></span>
<span id="cb49-1404"><a href="#cb49-1404"></a></span>
<span id="cb49-1405"><a href="#cb49-1405"></a>What happens when we have a batch of inputs? By the chain rule:</span>
<span id="cb49-1406"><a href="#cb49-1406"></a></span>
<span id="cb49-1407"><a href="#cb49-1407"></a>\begin{align*}</span>
<span id="cb49-1408"><a href="#cb49-1408"></a>\frac{\partial L}{\partial z_{11}} &amp;= \frac{\partial L}{\partial S_{11}} \cdot \frac{\partial S_{11}}{\partial z_{11}} + \frac{\partial L}{\partial S_{12}} \cdot \frac{\partial S_{12}}{\partial z_{11}} + \frac{\partial L}{\partial S_{13}}\cdot \frac{\partial S_{13}}{\partial z_{11}}</span>
<span id="cb49-1409"><a href="#cb49-1409"></a>\end{align*}</span>
<span id="cb49-1410"><a href="#cb49-1410"></a></span>
<span id="cb49-1411"><a href="#cb49-1411"></a>In general,</span>
<span id="cb49-1412"><a href="#cb49-1412"></a></span>
<span id="cb49-1413"><a href="#cb49-1413"></a>\begin{align*}</span>
<span id="cb49-1414"><a href="#cb49-1414"></a>\frac{\partial L}{\partial z_{ij}} &amp;= \frac{\partial L}{\partial S_{i1}} \cdot \frac{\partial S_{i1}}{\partial z_{ij}} + \frac{\partial L}{\partial S_{i2}} \cdot \frac{\partial S_{i2}}{\partial z_{ij}} + \frac{\partial L}{\partial S_{i3}}\cdot \frac{\partial S_{i3}}{\partial z_{ij}}<span class="sc">\\</span></span>
<span id="cb49-1415"><a href="#cb49-1415"></a>&amp;=\sum_{k=1}^{3} \frac{\partial L}{\partial S_{ik}} \cdot \frac{\partial S_{ik}}{\partial z_{ij}}</span>
<span id="cb49-1416"><a href="#cb49-1416"></a>\end{align*}</span>
<span id="cb49-1417"><a href="#cb49-1417"></a></span>
<span id="cb49-1418"><a href="#cb49-1418"></a>It follows that:</span>
<span id="cb49-1419"><a href="#cb49-1419"></a></span>
<span id="cb49-1420"><a href="#cb49-1420"></a>\begin{align*}</span>
<span id="cb49-1421"><a href="#cb49-1421"></a>\frac{\partial L}{\partial z_i} &amp;= \begin{bmatrix}</span>
<span id="cb49-1422"><a href="#cb49-1422"></a>\frac{\partial L}{\partial z_{i1}} &amp; \frac{\partial L}{\partial z_{i2}} &amp; \frac{\partial L}{\partial z_{i3}}</span>
<span id="cb49-1423"><a href="#cb49-1423"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-1424"><a href="#cb49-1424"></a>&amp;=\begin{bmatrix}</span>
<span id="cb49-1425"><a href="#cb49-1425"></a>\frac{\partial L}{\partial S_{i1}} &amp; \frac{\partial L}{\partial S_{i2}} &amp; \frac{\partial L}{\partial S_{i3}}</span>
<span id="cb49-1426"><a href="#cb49-1426"></a>\end{bmatrix} \begin{bmatrix}</span>
<span id="cb49-1427"><a href="#cb49-1427"></a>\frac{\partial S_{i1}}{\partial z_{i1}} &amp; \frac{\partial S_{i1}}{\partial z_{i2}} &amp; \frac{\partial S_{i1}}{\partial z_{i3}} <span class="sc">\\</span></span>
<span id="cb49-1428"><a href="#cb49-1428"></a>\frac{\partial S_{i2}}{\partial z_{i1}} &amp; \frac{\partial S_{i2}}{\partial z_{i2}} &amp; \frac{\partial S_{i2}}{\partial z_{i3}} <span class="sc">\\</span></span>
<span id="cb49-1429"><a href="#cb49-1429"></a>\frac{\partial S_{i3}}{\partial z_{i1}} &amp; \frac{\partial S_{i3}}{\partial z_{i2}} &amp; \frac{\partial S_{i3}}{\partial z_{i3}} </span>
<span id="cb49-1430"><a href="#cb49-1430"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb49-1431"><a href="#cb49-1431"></a>&amp;=\frac{\partial L}{\partial S_i} \cdot \frac{\partial S_i}{\partial z_i}</span>
<span id="cb49-1432"><a href="#cb49-1432"></a>\end{align*}</span>
<span id="cb49-1433"><a href="#cb49-1433"></a></span>
<span id="cb49-1434"><a href="#cb49-1434"></a>Now, $\partial L/\partial S_i$ has shape <span class="in">`[1,3]`</span> and $\partial S_i/\partial z_i$ is a matrix of size <span class="in">`[3,3]`</span>. So, $\partial L/\partial z_i$ will have dimensions <span class="in">`[1,3]`</span>.</span>
<span id="cb49-1435"><a href="#cb49-1435"></a></span>
<span id="cb49-1436"><a href="#cb49-1436"></a><span class="fu">## Softmax `backward()` implementation</span></span>
<span id="cb49-1437"><a href="#cb49-1437"></a></span>
<span id="cb49-1438"><a href="#cb49-1438"></a>We are now in a position to add <span class="in">`backward()`</span> pass to the <span class="in">`SoftmaxActivation`</span> layer.</span>
<span id="cb49-1439"><a href="#cb49-1439"></a></span>
<span id="cb49-1442"><a href="#cb49-1442"></a><span class="in">```{python}</span></span>
<span id="cb49-1443"><a href="#cb49-1443"></a><span class="kw">class</span> SoftmaxActivation:</span>
<span id="cb49-1444"><a href="#cb49-1444"></a></span>
<span id="cb49-1445"><a href="#cb49-1445"></a>    <span class="co"># Forward pass</span></span>
<span id="cb49-1446"><a href="#cb49-1446"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb49-1447"><a href="#cb49-1447"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb49-1448"><a href="#cb49-1448"></a>        exp_values <span class="op">=</span> np.exp(inputs <span class="op">-</span> np.<span class="bu">max</span>(inputs, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb49-1449"><a href="#cb49-1449"></a>        probabilities <span class="op">=</span> exp_values <span class="op">/</span> np.<span class="bu">sum</span>(exp_values, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-1450"><a href="#cb49-1450"></a>        <span class="va">self</span>.output <span class="op">=</span> probabilities</span>
<span id="cb49-1451"><a href="#cb49-1451"></a></span>
<span id="cb49-1452"><a href="#cb49-1452"></a>    <span class="co"># Backward pass</span></span>
<span id="cb49-1453"><a href="#cb49-1453"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, dloss_da):</span>
<span id="cb49-1454"><a href="#cb49-1454"></a>        dloss_dz <span class="op">=</span> []</span>
<span id="cb49-1455"><a href="#cb49-1455"></a>        n <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.output)</span>
<span id="cb49-1456"><a href="#cb49-1456"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb49-1457"><a href="#cb49-1457"></a>            softmax_output <span class="op">=</span> <span class="va">self</span>.output[i]</span>
<span id="cb49-1458"><a href="#cb49-1458"></a></span>
<span id="cb49-1459"><a href="#cb49-1459"></a>            <span class="co"># Reshape as a column vector</span></span>
<span id="cb49-1460"><a href="#cb49-1460"></a>            softmax_output <span class="op">=</span> softmax_output.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb49-1461"><a href="#cb49-1461"></a></span>
<span id="cb49-1462"><a href="#cb49-1462"></a>            dsoftmax_dz <span class="op">=</span> np.diagflat(softmax_output) <span class="op">-</span> np.dot(</span>
<span id="cb49-1463"><a href="#cb49-1463"></a>                softmax_output, softmax_output.T</span>
<span id="cb49-1464"><a href="#cb49-1464"></a>            )</span>
<span id="cb49-1465"><a href="#cb49-1465"></a>            dloss_dz.append(np.dot(dloss_da[i], dsoftmax_dz))</span>
<span id="cb49-1466"><a href="#cb49-1466"></a></span>
<span id="cb49-1467"><a href="#cb49-1467"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> np.array(dloss_dz)</span>
<span id="cb49-1468"><a href="#cb49-1468"></a>        </span>
<span id="cb49-1469"><a href="#cb49-1469"></a><span class="in">```</span></span>
<span id="cb49-1470"><a href="#cb49-1470"></a></span>
<span id="cb49-1471"><a href="#cb49-1471"></a><span class="fu">## Categorical cross-entropy loss and softmax activation function derivative</span></span>
<span id="cb49-1472"><a href="#cb49-1472"></a></span>
<span id="cb49-1473"><a href="#cb49-1473"></a>The derivative of the categorical cross entropy loss and softmax activation function can be combined and results in a faster and simple implementation. The current implementation of the <span class="in">`backward`</span> function in <span class="in">`SoftMaxActivation`</span> is not vectorized and has a loop.</span>
<span id="cb49-1474"><a href="#cb49-1474"></a></span>
<span id="cb49-1475"><a href="#cb49-1475"></a>Let's focus again on $\frac{\partial L_{i}}{\partial z_{ij}}$. We have:</span>
<span id="cb49-1476"><a href="#cb49-1476"></a></span>
<span id="cb49-1477"><a href="#cb49-1477"></a>\begin{align*}</span>
<span id="cb49-1478"><a href="#cb49-1478"></a>\frac{\partial L_i}{\partial z_{ij}} &amp;= \sum_{k} \frac{\partial L_i}{\partial S_{ik}} \frac{\partial S_{ik}}{\partial z_{ij}} <span class="sc">\\</span></span>
<span id="cb49-1479"><a href="#cb49-1479"></a>&amp;= \frac{\partial L_i}{S_{ij}} \cdot \frac{\partial S_{ij}}{\partial z_{ij}} + \sum_{k\neq j}\frac{\partial L_i}{\partial S_{ik}} \frac{\partial S_{ik}}{\partial z_{ij}} <span class="sc">\\</span></span>
<span id="cb49-1480"><a href="#cb49-1480"></a>&amp;= -\frac{y_{ij}}{\hat{y}_{ij}}\hat{y}_{ij}(1-\hat{y}_{ij}) + \sum_{k \neq j}-\frac{y_{ik}}{\hat{y}_{ik}}\cdot \hat{y}_{ik}(0 - \hat{y}_{ij})<span class="sc">\\</span></span>
<span id="cb49-1481"><a href="#cb49-1481"></a>&amp;= -\frac{y_{ij}}{\cancel{\hat{y}_{ij}}}\cancel{\hat{y}_{ij}}(1-\hat{y}_{ij}) + \sum_{k \neq j}-\frac{y_{ik}}{\cancel{\hat{y}_{ik}}}\cdot \cancel{\hat{y}_{ik}}(0 - \hat{y}_{ij})<span class="sc">\\</span></span>
<span id="cb49-1482"><a href="#cb49-1482"></a>&amp;= -y_{ij} + y_{ij}\hat{y}_{ij} + \sum_{k\neq j}y_{ik} \hat{y}_{ij}<span class="sc">\\</span></span>
<span id="cb49-1483"><a href="#cb49-1483"></a>&amp;= -y_{ij} + \hat{y}_{ij}(\sum_{k}y_{ik})<span class="sc">\\</span></span>
<span id="cb49-1484"><a href="#cb49-1484"></a>&amp;= \hat{y}_{ij} - y_{ij}</span>
<span id="cb49-1485"><a href="#cb49-1485"></a>\end{align*}</span>
<span id="cb49-1486"><a href="#cb49-1486"></a></span>
<span id="cb49-1489"><a href="#cb49-1489"></a><span class="in">```{python}</span></span>
<span id="cb49-1490"><a href="#cb49-1490"></a><span class="kw">class</span> CategoricalCrossEntropySoftmax:</span>
<span id="cb49-1491"><a href="#cb49-1491"></a></span>
<span id="cb49-1492"><a href="#cb49-1492"></a>    <span class="co"># create activation and loss function objects</span></span>
<span id="cb49-1493"><a href="#cb49-1493"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb49-1494"><a href="#cb49-1494"></a>        <span class="va">self</span>.activation <span class="op">=</span> SoftmaxActivation()</span>
<span id="cb49-1495"><a href="#cb49-1495"></a>        <span class="va">self</span>.loss <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb49-1496"><a href="#cb49-1496"></a></span>
<span id="cb49-1497"><a href="#cb49-1497"></a>    <span class="co"># forward pass</span></span>
<span id="cb49-1498"><a href="#cb49-1498"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs, y_true):</span>
<span id="cb49-1499"><a href="#cb49-1499"></a></span>
<span id="cb49-1500"><a href="#cb49-1500"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb49-1501"><a href="#cb49-1501"></a>        <span class="va">self</span>.activation.forward(inputs)</span>
<span id="cb49-1502"><a href="#cb49-1502"></a></span>
<span id="cb49-1503"><a href="#cb49-1503"></a>        <span class="va">self</span>.output <span class="op">=</span> <span class="va">self</span>.activation.output</span>
<span id="cb49-1504"><a href="#cb49-1504"></a></span>
<span id="cb49-1505"><a href="#cb49-1505"></a>        <span class="cf">return</span> <span class="va">self</span>.loss.calculate(<span class="va">self</span>.output, y_true)</span>
<span id="cb49-1506"><a href="#cb49-1506"></a></span>
<span id="cb49-1507"><a href="#cb49-1507"></a>    <span class="co"># Backward pass</span></span>
<span id="cb49-1508"><a href="#cb49-1508"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, y_pred, y_true):</span>
<span id="cb49-1509"><a href="#cb49-1509"></a>        <span class="co"># number of samples</span></span>
<span id="cb49-1510"><a href="#cb49-1510"></a>        batch_size <span class="op">=</span> <span class="bu">len</span>(y_pred)</span>
<span id="cb49-1511"><a href="#cb49-1511"></a></span>
<span id="cb49-1512"><a href="#cb49-1512"></a>        <span class="co"># number of labels</span></span>
<span id="cb49-1513"><a href="#cb49-1513"></a>        num_labels <span class="op">=</span> <span class="bu">len</span>(y_pred[<span class="dv">0</span>])</span>
<span id="cb49-1514"><a href="#cb49-1514"></a></span>
<span id="cb49-1515"><a href="#cb49-1515"></a>        <span class="co"># If labels are sparse, turn them into a one-hot vector</span></span>
<span id="cb49-1516"><a href="#cb49-1516"></a>        <span class="cf">if</span> <span class="bu">len</span>(y_true.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb49-1517"><a href="#cb49-1517"></a>            y_true <span class="op">=</span> np.eye(num_labels)[y_true]</span>
<span id="cb49-1518"><a href="#cb49-1518"></a></span>
<span id="cb49-1519"><a href="#cb49-1519"></a>        <span class="co"># Calculate the gradient</span></span>
<span id="cb49-1520"><a href="#cb49-1520"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> y_pred <span class="op">-</span> y_true</span>
<span id="cb49-1521"><a href="#cb49-1521"></a></span>
<span id="cb49-1522"><a href="#cb49-1522"></a>        <span class="co"># Normalize the gradient</span></span>
<span id="cb49-1523"><a href="#cb49-1523"></a>        <span class="va">self</span>.dloss_dz <span class="op">=</span> <span class="va">self</span>.dloss_dz <span class="op">/</span> batch_size</span>
<span id="cb49-1524"><a href="#cb49-1524"></a><span class="in">```</span></span>
<span id="cb49-1525"><a href="#cb49-1525"></a></span>
<span id="cb49-1526"><a href="#cb49-1526"></a>We can now test if the combined backward step returns the same values compared to when we backpropogate gradients through both of the functions separately.</span>
<span id="cb49-1527"><a href="#cb49-1527"></a></span>
<span id="cb49-1530"><a href="#cb49-1530"></a><span class="in">```{python}</span></span>
<span id="cb49-1531"><a href="#cb49-1531"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-1532"><a href="#cb49-1532"></a><span class="im">import</span> nnfs</span>
<span id="cb49-1533"><a href="#cb49-1533"></a></span>
<span id="cb49-1534"><a href="#cb49-1534"></a>nnfs.init()</span>
<span id="cb49-1535"><a href="#cb49-1535"></a></span>
<span id="cb49-1536"><a href="#cb49-1536"></a>softmax_outputs <span class="op">=</span> np.array([[<span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>], [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>], [<span class="fl">0.02</span>, <span class="fl">0.9</span>, <span class="fl">0.08</span>]])</span>
<span id="cb49-1537"><a href="#cb49-1537"></a></span>
<span id="cb49-1538"><a href="#cb49-1538"></a>class_targets <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb49-1539"><a href="#cb49-1539"></a></span>
<span id="cb49-1540"><a href="#cb49-1540"></a></span>
<span id="cb49-1541"><a href="#cb49-1541"></a>activation <span class="op">=</span> SoftmaxActivation()</span>
<span id="cb49-1542"><a href="#cb49-1542"></a>activation.output <span class="op">=</span> softmax_outputs</span>
<span id="cb49-1543"><a href="#cb49-1543"></a></span>
<span id="cb49-1544"><a href="#cb49-1544"></a>loss <span class="op">=</span> CategoricalCrossEntropyLoss()</span>
<span id="cb49-1545"><a href="#cb49-1545"></a>loss.backward(softmax_outputs, class_targets)</span>
<span id="cb49-1546"><a href="#cb49-1546"></a><span class="bu">print</span>(<span class="st">"Gradients : separate loss and activation"</span>)</span>
<span id="cb49-1547"><a href="#cb49-1547"></a><span class="bu">print</span>(<span class="ss">f"dloss_da = </span><span class="sc">{</span>loss<span class="sc">.</span>dloss_da<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1548"><a href="#cb49-1548"></a></span>
<span id="cb49-1549"><a href="#cb49-1549"></a>activation.backward(loss.dloss_da)</span>
<span id="cb49-1550"><a href="#cb49-1550"></a><span class="bu">print</span>(<span class="ss">f"dloss_dz = </span><span class="sc">{</span>activation<span class="sc">.</span>dloss_dz<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1551"><a href="#cb49-1551"></a></span>
<span id="cb49-1552"><a href="#cb49-1552"></a>softmax_cce <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb49-1553"><a href="#cb49-1553"></a>softmax_cce.backward(softmax_outputs, class_targets)</span>
<span id="cb49-1554"><a href="#cb49-1554"></a><span class="bu">print</span>(<span class="st">"Gradients : combined loss and activation"</span>)</span>
<span id="cb49-1555"><a href="#cb49-1555"></a><span class="bu">print</span>(<span class="ss">f"dloss_dz = </span><span class="sc">{</span>softmax_cce<span class="sc">.</span>dloss_dz<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-1556"><a href="#cb49-1556"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>
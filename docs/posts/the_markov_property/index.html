<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-07-12">

<title>The Markov Property – quantdev.blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-f2a1071e85750ec973bbb8a8f120da0f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-45936937dcb5bd7d3775fb9501a9a6d1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">The Markov Property</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Stochastic Calculus</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 12, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-markov-property-for-diffusions" id="toc-the-markov-property-for-diffusions" class="nav-link active" data-scroll-target="#the-markov-property-for-diffusions">The Markov Property for Diffusions</a></li>
  <li><a href="#the-strong-markov-property" id="toc-the-strong-markov-property" class="nav-link" data-scroll-target="#the-strong-markov-property">The Strong Markov Property</a></li>
  <li><a href="#the-heat-equation" id="toc-the-heat-equation" class="nav-link" data-scroll-target="#the-heat-equation">The Heat Equation</a></li>
  <li><a href="#solution-to-the-heat-pde-as-an-expectation-over-brownian-motion-paths" id="toc-solution-to-the-heat-pde-as-an-expectation-over-brownian-motion-paths" class="nav-link" data-scroll-target="#solution-to-the-heat-pde-as-an-expectation-over-brownian-motion-paths">Solution to the heat PDE as an expectation over Brownian-motion paths</a></li>
  <li><a href="#robert-browns-erratic-motion-of-pollen" id="toc-robert-browns-erratic-motion-of-pollen" class="nav-link" data-scroll-target="#robert-browns-erratic-motion-of-pollen">Robert Brown’s erratic motion of pollen</a>
  <ul class="collapse">
  <li><a href="#albert-einsteins-proof-of-the-existence-of-brownian-motion" id="toc-albert-einsteins-proof-of-the-existence-of-brownian-motion" class="nav-link" data-scroll-target="#albert-einsteins-proof-of-the-existence-of-brownian-motion">Albert Einstein’s proof of the existence of Brownian motion</a></li>
  </ul></li>
  <li><a href="#kolmogorovs-backward-equation" id="toc-kolmogorovs-backward-equation" class="nav-link" data-scroll-target="#kolmogorovs-backward-equation">Kolmogorov’s Backward Equation</a></li>
  <li><a href="#the-heat-equation-as-a-special-case-of-the-backward-equation" id="toc-the-heat-equation-as-a-special-case-of-the-backward-equation" class="nav-link" data-scroll-target="#the-heat-equation-as-a-special-case-of-the-backward-equation">The heat equation as a special case of the Backward equation</a></li>
  <li><a href="#kolmogorovs-forward-equation" id="toc-kolmogorovs-forward-equation" class="nav-link" data-scroll-target="#kolmogorovs-forward-equation">Kolmogorov’s forward equation</a></li>
  <li><a href="#the-feynman-kac-formula" id="toc-the-feynman-kac-formula" class="nav-link" data-scroll-target="#the-feynman-kac-formula">The Feynman-Kac Formula</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="the-markov-property-for-diffusions" class="level2">
<h2 class="anchored" data-anchor-id="the-markov-property-for-diffusions">The Markov Property for Diffusions</h2>
<p>Let’s start by exhibiting the Markov property of Brownian motion. To see this, consider <span class="math inline">\((\mathcal{F}_t,t\geq 0)\)</span>, the natural filtration of the Brownian motion <span class="math inline">\((B_t,t\geq 0)\)</span>. Consider <span class="math inline">\(g(B_t)\)</span> for some time <span class="math inline">\(t\)</span> and bounded function <span class="math inline">\(g\)</span>. (For example, <span class="math inline">\(g\)</span> could be an indicator function.) Consider also a random variable <span class="math inline">\(W\)</span> that is <span class="math inline">\(\mathcal{F}_s\)</span> measurable for <span class="math inline">\(s &lt; t\)</span>. (For example, <span class="math inline">\(W\)</span> could be <span class="math inline">\(B_s\)</span> or <span class="math inline">\(1_{B_s &gt; 0}\)</span>.) Let’s compute <span class="math inline">\(\mathbb{E}[g(B_t)W]\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[g(B_t)W] &amp;= \mathbb{E}[\mathbb{E}[Wg(B_t - B_s + B_s)|\mathcal{F}_s]]
\end{align*}\]</span></p>
<p>The random variable <span class="math inline">\((B_t - B_s)\)</span> follows a <span class="math inline">\(\mathcal{N}(0,t-s)\)</span> distribution. By LOTUS,</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}[g(B_t)W]
&amp;= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)|\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\
&amp;= \{\text{ Using the fact that }B_t - B_s \perp B_s\}\\
&amp;= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\
&amp;= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy
\end{align*}
\]</span></p>
<p>By Fubini’s theorem, the integral and the expectation operator can be interchanged, and since <span class="math inline">\(W\)</span> is <span class="math inline">\(\mathcal{F}_s\)</span> measurable, it follows from the definition of conditional expectations that:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}[Wg(B_t)] = \mathbb{E}[W\mathbb{E}[g(B_t)|\mathcal{F}_s]] = \mathbb{E}\left[W\int_{\mathbb{R}}g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy\right]
\end{align*}
\]</span></p>
<p>It follows that:</p>
<p><span id="eq-conditional-expectation-of-function-brownian-motion"><span class="math display">\[
\begin{align*}
\mathbb{E}[g(B_t)|\mathcal{F}_s] = \int_{\mathbb{R}}g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy
\end{align*}
\tag{1}\]</span></span></p>
<p>We make two important observations. First, the right hand side is a function of <span class="math inline">\(s,t\)</span> and <span class="math inline">\(B_s\)</span> only (and not of the Brownian motion before time s). In particular, we have:</p>
<p><span class="math display">\[
\mathbb{E}[g(B_t)|\mathcal{F}_s] = \mathbb{E}[g(B_t)|B_s]
\]</span></p>
<p>This holds for any bounded function <span class="math inline">\(g\)</span>. In particular, it holds for all indicator functions. This implies that the conditional distribution of <span class="math inline">\(B_t\)</span> given <span class="math inline">\(\mathcal{F}_s\)</span> depends solely on <span class="math inline">\(B_s\)</span>, and not on other values before time <span class="math inline">\(s\)</span>. Second, the right-hand side is <em>time-homogenous</em> in the sense that it depends on the time difference <span class="math inline">\(t-s\)</span>.</p>
<p>We have just shown that Brownian motion is a <em>time-homogenous Markov process</em>.</p>
<div id="def-markov-process" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Markov process.)</strong></span> Consider a stochastic process <span class="math inline">\((X_t,t\geq 0)\)</span> and its natural filtration <span class="math inline">\((\mathcal{F}_t,t\geq 0)\)</span>. It is said to be a <em>Markov process</em> if and only if for any (bounded) function <span class="math inline">\(g: \mathbb{R} \to \mathbb{R}\)</span>, we have:</p>
<p><span id="eq-markov-process"><span class="math display">\[
\mathbb{E}[g(X_t) | \mathcal{F}_s] = \mathbb{E}[g(X_t) | X_s], \quad \forall t \geq 0, \forall s \leq t
\tag{2}\]</span></span></p>
</div>
<p>This implies that <span class="math inline">\(\mathbb{E}[g(X_t)|\mathcal{F}_s]\)</span> is an explicit function of <span class="math inline">\(s\)</span>, <span class="math inline">\(t\)</span> and <span class="math inline">\(X_s\)</span>. It is said to be <em>time-homogenous</em>, if it is a function of <span class="math inline">\(t-s\)</span> and <span class="math inline">\(X_s\)</span>. Since the above holds for all bounded <span class="math inline">\(g\)</span>, the conditional distribution of <span class="math inline">\(X_t\)</span> given <span class="math inline">\(\mathcal{F}_s\)</span> is the same as the conditional distribution of <span class="math inline">\(X_t\)</span> given <span class="math inline">\(X_s\)</span>.</p>
<p>One way to compute the conditional distribution of <span class="math inline">\(X_t\)</span> given <span class="math inline">\(\mathcal{F}_s\)</span> is to compute the conditional MGF given <span class="math inline">\(\mathcal{F}_s\)</span>, that is:</p>
<p><span id="eq-conditional-mgf-of-xt"><span class="math display">\[
\mathbb{E}[e^{a X_t}|\mathcal{F}_s], \quad a \geq 0
\tag{3}\]</span></span></p>
<p>The process would be Markov, if the conditional MGF is an explicit function of <span class="math inline">\(s\)</span>, <span class="math inline">\(t\)</span> and <span class="math inline">\(X_s\)</span>.</p>
<div id="exm-brownian-motion-is-markov" class="theorem example">
<p><span class="theorem-title"><strong>Example 1</strong></span> (Brownian Motion is Markov) Let <span class="math inline">\((B_t,t\geq 0)\)</span> be a standard brownian motion. Our claim is that the brownian motion is a markov process.</p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[e^{a B_t}|\mathcal{F}_s] &amp;= \mathbb{E}[e^{a (B_t - B_s + B_s)}|\mathcal{F}_s]\\
&amp; \{ \text{ since }B_s \text{ is }\mathcal{F}_s-\text{ measurable }\}\\
&amp;= e^{a B_s} \mathbb{E}[e^{a (B_t - B_s)}|\mathcal{F}_s]\\
&amp; \{ \text{ since }B_t - B_s \perp \mathcal{F}_s \}\\
&amp;= e^{a B_s} \mathbb{E}[e^{a (B_t - B_s)}]\\
&amp;= e^{a B_s} e^{\frac{1}{2}a^2(t-s)}
\end{align*}\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>An equivalent (but more symmetric) way to express the Markov property is to say that <em>the future of the process is independent of the past, when conditioned on the present</em>. Concretely, this means that for any <span class="math inline">\(r &lt; s&lt; t\)</span>, we have that <span class="math inline">\(X_t\)</span> is independent of <span class="math inline">\(X_r\)</span>, when we condition on <span class="math inline">\(X_s\)</span>.</p>
<p>The conditional distribution of <span class="math inline">\(X_t\)</span> given <span class="math inline">\(X_s\)</span> is well described using <em>transition probabilities</em>. We will more interested in a case well these probabilities admit a density <span class="math inline">\(f_{X_t|X_s=x}(y)\)</span>. More precisely, for such a Markov process, we have:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}[g(X_t)|X_s = x] &amp;= \int_{\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\
&amp;=\int_{\mathbb{R}} g(y) p(y,t|x,s) dy
\end{align*}
\]</span></p>
<p>Here, we explicitly write the left-hand side as a function of space, that is, the position <span class="math inline">\(X_s\)</span>, by fixing <span class="math inline">\(X_s = x\)</span>. In words, the <em>transition probability density</em> <span class="math inline">\(p(y,t|x,s)\)</span> represents the probability density that starting from <span class="math inline">\(X_s = x\)</span> at time <span class="math inline">\(s\)</span>, the process ends up at <span class="math inline">\(X_t = y\)</span> at time <span class="math inline">\(t &gt; s\)</span>. If the process is time-homogenous, this only depends on the time difference <span class="math inline">\((t-s)\)</span> and we write <span class="math inline">\(p(y,t|x,s)\)</span>. From <a href="#eq-conditional-expectation-of-function-brownian-motion" class="quarto-xref">Equation&nbsp;1</a>, we can write: <span class="math display">\[
\mathbb{E}[g(B_t)|B_s = x] = \int_{\mathbb{R}} g(u + x) \frac{e^{-\frac{u^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} du
\]</span></p>
<p>In the above expression, the random variable <span class="math inline">\(B_t - B_s\)</span> takes some value <span class="math inline">\(u \in \mathbb{R}\)</span> and <span class="math inline">\(B_s = x\)</span> is fixed. Then, <span class="math inline">\(B_t\)</span> takes the value <span class="math inline">\(u + x\)</span>. Let <span class="math inline">\(y = u + x\)</span>. Then, <span class="math inline">\(u = y - x\)</span>. Consequently, we may write:</p>
<p><span class="math display">\[
\mathbb{E}[g(B_t)|B_s = x] = \int_{\mathbb{R}} g(y) \frac{e^{-\frac{(y-x)^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} dy
\]</span></p>
<p>So, the transition density function for standard Brownian motion is:</p>
<p><span id="eq-brownian-motion-transition-density-function"><span class="math display">\[
p(y,t|x,0)= \frac{e^{-\frac{(y-x)^2}{2s}}}{\sqrt{2\pi s}}, \quad s&gt;0, x,y\in\mathbb{R}
\tag{4}\]</span></span></p>
<p>This function is sometimes called the <em>heat kernel</em>, as it relates to the <em>heat equation</em>.</p>
<p>The Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process <span class="math inline">\((X_t,t\geq 0)\)</span> at different times. Consider the functions <span class="math inline">\(f = \mathbf{1}_A\)</span> and <span class="math inline">\(g = \mathbf{1}_B\)</span> from <span class="math inline">\(\mathbb{R} \to \mathbb{R}\)</span>, where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are two intervals in <span class="math inline">\(\mathbb{R}\)</span>. Let’s compute <span class="math inline">\(\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) = \mathbb{E}[\mathbf{1}_{A} \mathbf{1}_{B}] = \mathbb{E}[f(X_{t_1}) g(X_{t_2})]\)</span> for <span class="math inline">\(t_1 &lt; t_2\)</span>. By the properties of conditional expectation and the Markov property, we have:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &amp;= \mathbb{E}[f(X_{t_1})g(X_{t_2})]\\
&amp;= \mathbb{E}[f(X_{t_1})\mathbb{E}[g(X_{t_2})|\mathcal{F}_{t_1}]]\\
&amp;= \mathbb{E}[f(X_{t_1})\mathbb{E}[g(X_{t_2})|X_{t_1}]]
\end{align*}\]</span></p>
<p>Assuming that the process is time-homogenous and admits a transition density <span class="math inline">\(p(y,t|x,0)\)</span> as for Brownian motion, this becomes:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &amp;= \int_{\mathbb{R}} f(x_1) \left(\int_{\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \right) p(x_1,t_1|x_0,0) dx_1\\
&amp;= \int_{A} \left(\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \right) p(x_1,t_1|x_0,0) dx_1
\end{align*}\]</span></p>
<p>This easily generalizes to any finite-dimensional distribution of <span class="math inline">\((X_t, t\geq 0)\)</span>.</p>
<div id="exm-markov-versus-martingale" class="theorem example">
<p><span class="theorem-title"><strong>Example 2</strong></span> (Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift <span class="math inline">\((X_t, t \geq 0)\)</span>, where <span class="math inline">\(X_t = \sigma B_t + \mu t\)</span>. Conversely, take <span class="math inline">\(Y_t = \int_0^t X_s dB_s\)</span>, where <span class="math inline">\(X_s = \int_0^s B_u dB_u\)</span>. The integrand <span class="math inline">\(X_s\)</span> depends on whole Brownian motion path upto time <span class="math inline">\(s\)</span> and not just on <span class="math inline">\(B_s\)</span>.</p>
</div>
<div id="nte-functions-of-markov" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip&nbsp;1: Functions of Markov Processes
</div>
</div>
<div class="callout-body-container callout-body">
<p>It might be tempting to think that if <span class="math inline">\((X_t,t\geq 0)\)</span> is a Markov process, then the process defined by <span class="math inline">\(Y_t = f(X_t)\)</span> for some reasonable function <span class="math inline">\(f\)</span> is also Markov. Indeed, one could hope to write for an arbitrary bounded function <span class="math inline">\(g\)</span>:</p>
<p><span id="eq-functions-of-markov-process"><span class="math display">\[
\begin{align*}
\mathbb{E}[g(Y_t)|\mathcal{F}_s] = \mathbb{E}[g(f(X_t))|\mathcal{F}_s] = \mathbb{E}[g(f(X_t))|\mathcal{X}_s]
\end{align*}
\tag{5}\]</span></span></p>
<p>by using the Markov property of <span class="math inline">\((X_t,t\geq 0)\)</span>. The flaw in this reasoning is that the Markov property should hold for the natural fitration <span class="math inline">\((\mathcal{F}_t^Y,t\geq 0)\)</span> of the process <span class="math inline">\((Y_t,t\geq 0)\)</span> and not the one of <span class="math inline">\((X_t,t\geq 0)\)</span>, <span class="math inline">\((\mathcal{F}_t^X,t\geq 0)\)</span>. It might be that the filtration <span class="math inline">\((\mathcal{F}_t^Y,t\geq 0)\)</span> has less information that <span class="math inline">\((\mathcal{F}_t^X,t\geq 0)\)</span>, especially, if the function <span class="math inline">\(f\)</span> is not one-to-one. For example, if <span class="math inline">\(f(x)=x^2\)</span>, then <span class="math inline">\(\mathcal{F}_t^Y\)</span> has less information than <span class="math inline">\(\mathcal{F}_t^X\)</span> as we cannot recover the sign of <span class="math inline">\(X_t\)</span> knowing <span class="math inline">\(Y_t\)</span>. In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when <span class="math inline">\(f\)</span> is not one-to-one.</p>
</div>
</div>
<p>It turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.</p>
<div id="thm-diffusions-are-markov-processes" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Diffusions are Markov processes.)</strong></span> Let <span class="math inline">\((B_t,t\geq 0)\)</span> be a standard Brownian motion. Let <span class="math inline">\(\mu : \mathbb{R} \to \mathbb{R}\)</span> and <span class="math inline">\(\sigma: \mathbb{R} \to \mathbb{R}\)</span> be differentiable functions with bounded derivatives on <span class="math inline">\([0,T]\)</span>. Then, the diffusion with the SDE</p>
<p><span class="math display">\[
dX_t = \mu(X_t) dt + \sigma(X_t)dB_t, \quad X_0 = x_0
\]</span></p>
<p>defines a time-homogenous markov process on <span class="math inline">\([0,T]\)</span>.</p>
</div>
<p>An analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments.</p>
<p><em>Proof.</em></p>
<p>By the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes">existence and uniqueness theorem</a>, this stochastic initial value problem(SIVP) defines a unique continous adapted process <span class="math inline">\((X_t,t\leq T)\)</span>. Let <span class="math inline">\((\mathcal{F}_t^X,t\geq 0)\)</span> be the natural filtration of <span class="math inline">\((X_t,t\leq T)\)</span>. For a fixed <span class="math inline">\(t &gt; 0\)</span>, consider the process <span class="math inline">\(W_s = B_{t+s} - B_t, s \geq 0\)</span>. Let <span class="math inline">\((\mathcal{F}_t,t \geq 0)\)</span> be the natural filtration of <span class="math inline">\((B_t,t \geq 0)\)</span>. It turns out that the process <span class="math inline">\((W_s,s \geq 0)\)</span> is a standard brownian motion independent of <span class="math inline">\(\mathcal{F}_t\)</span> (<a href="#exr-shifted-brownian-motion" class="quarto-xref">Exercise&nbsp;1</a>). For <span class="math inline">\(s \geq 0\)</span>, we consider the SDE:</p>
<p><span class="math display">\[
dY_s = \mu (Y_s) ds + \sigma(Y_s) dW_s, \quad Y_0 = X_t
\]</span></p>
<p>Again by the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes">existence and uniqueness theorem</a>, there exists a unique solution to the SIVP that is adapted to the natural filtration of <span class="math inline">\(W\)</span>. Note that, the shifted process <span class="math inline">\((X_{t+s},s\geq 0)\)</span> is <em>the</em> solution to this SIVP since:</p>
<p><span class="math display">\[\begin{align*}
X_{t+s} &amp;= X_{t} + \int_{t}^{t+s}\mu(X_u) du + \int_{t}^{t+s}\sigma(X_u) dB_u
\end{align*}\]</span></p>
<p>Perform a change of variable <span class="math inline">\(v = u - t\)</span>. Then, <span class="math inline">\(dv = du\)</span>, <span class="math inline">\(dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v\)</span>. So,</p>
<p><span class="math display">\[\begin{align*}
X_{t+s} &amp;= X_{t} + \int_{0}^{s}\mu(X_{t+v}) dv + \int_{0}^{s}\sigma(X_{t+v}) dW_v
\end{align*}\]</span></p>
<p>Let <span class="math inline">\(Y_v= X_{t+v}\)</span>, <span class="math inline">\(Y_0 = X_t\)</span>. Then,</p>
<p><span class="math display">\[\begin{align*}
Y_s &amp;= Y_0 + \int_{0}^{s}\mu(Y_v) dv + \int_{0}^{s}\sigma(Y_v) dW_v
\end{align*}\]</span></p>
<p>Thus, we conclude that for any interval <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
\mathbb{P}(X_{t+s} \in A|\mathcal{F}_t^X) = \mathbb{P}(Y_s \in A | \mathcal{F}_t^X)
\]</span></p>
<p>But, since <span class="math inline">\((Y_s,s \geq 0)\)</span> depends on <span class="math inline">\(\mathcal{F}_t^X\)</span> only through <span class="math inline">\(X_t\)</span> (because <span class="math inline">\((W_s,s \geq 0)\)</span> is independent of <span class="math inline">\(\mathcal{F}_t\)</span>), we conclude that <span class="math inline">\(\mathbb{P}(X_{t+s} \in A|\mathcal{F}_t^X) = \mathbb{P}(X_{t+s} \in A|X_t)\)</span>, so <span class="math inline">\((X_t,t \geq 0)\)</span> is a time-homogenous markov process. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="the-strong-markov-property" class="level2">
<h2 class="anchored" data-anchor-id="the-strong-markov-property">The Strong Markov Property</h2>
<p>The Doob’s Optional Stopping theorem extended some properties of martingales to stopping times. The Markov property can also be extended to stopping times for certain processes. These processes are called <em>strong Markov processes</em>.</p>
<p>We know, that the sigma-algebra <span class="math inline">\(\mathcal{F}_t\)</span> represents the set of all observable events upto time <span class="math inline">\(t\)</span>. What is the sigma-algebra of observable events at a random stopping time <span class="math inline">\(\tau\)</span>?</p>
<div id="def-sigma-algebra-of-the-past" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (<span class="math inline">\(\sigma\)</span>-algebra of <span class="math inline">\(\tau\)</span>-past)</strong></span> Let <span class="math inline">\((\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\geq 0},\mathbb{P})\)</span> be a filtered probability space. The sigma-algebra at the stopping time <span class="math inline">\(\tau\)</span> is then:</p>
<p><span id="eq-sigma-algebra-of-the-past"><span class="math display">\[
\mathcal{F}_{\tau} = \{A \in \mathcal{F}_\infty : A \cap \{\tau \leq t\} \in \mathcal{F}_t, \forall t \geq 0 \}
\tag{6}\]</span></span></p>
</div>
<p>In words, an event <span class="math inline">\(A\)</span> is in <span class="math inline">\(\mathcal{F}_\tau\)</span>, if we can determine if <span class="math inline">\(A\)</span> and <span class="math inline">\(\{\tau \leq t\}\)</span> both occurred or not based on the information <span class="math inline">\(\mathcal{F}_t\)</span> known at any arbitrary time <span class="math inline">\(t\)</span>. You should be able to tell the value of the random variable <span class="math inline">\(\mathbf{1}_A \cdot \mathbf{1}_{\{\tau \leq t\}}\)</span> given <span class="math inline">\(\mathcal{F}_t\)</span> for any arbitrary time <span class="math inline">\(t \geq 0\)</span>.</p>
<p>For example, if <span class="math inline">\(\tau &lt; \infty\)</span>, the event <span class="math inline">\(\{B_\tau &gt; 0\}\)</span> is in <span class="math inline">\(\mathcal{F}_\tau\)</span>. However, the event <span class="math inline">\(\{B_1 &gt; 0\}\)</span> is not in <span class="math inline">\(\mathcal{F}_\tau\)</span> in general, since <span class="math inline">\(A \cap \{\tau \leq t\}\)</span> is not in <span class="math inline">\(\mathcal{F}_t\)</span> for <span class="math inline">\(t &lt; 1\)</span>. Roughly speaking, a random variable that is <span class="math inline">\(\mathcal{F}_\tau\)</span>-measurable should be thought of as an explicit function of <span class="math inline">\(X_\tau\)</span>. With this new object, we are ready to define the <em>strong markov property</em>.</p>
<div id="def-strong-markov-property" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Strong Markov Property)</strong></span> Let <span class="math inline">\((X_t,t\geq 0)\)</span> be a stochastic process and let <span class="math inline">\((\mathcal{F}_t,t\geq 0)\)</span> be its natural filtration. The process <span class="math inline">\((X_t,t\geq 0)\)</span> is said to be <em>strong markov</em> if for any stopping time <span class="math inline">\(\tau\)</span> for the filtration of the process and any bounded function <span class="math inline">\(g\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}[g(X_{t+\tau})|\mathcal{F}_\tau] = \mathbb{E}[g(X_{t+\tau})|X_\tau]
\]</span></p>
</div>
<p>This means that <span class="math inline">\(X_{t+\tau}\)</span> depends on <span class="math inline">\(\mathcal{F}_\tau\)</span> solely through <span class="math inline">\(X_\tau\)</span> (whenever <span class="math inline">\(\tau &lt; \infty\)</span>). It turns out that Brownian motion is a strong markov process. In fact a stronger statement holds which generalizes <a href="#exr-shifted-brownian-motion" class="quarto-xref">Exercise&nbsp;1</a>.</p>
<div id="thm-shifted-brownian-motion-about-a-stopping-time" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> Let <span class="math inline">\(\tau\)</span> be a stopping time for the filtration of the Brownian motion <span class="math inline">\((B_t,t\geq 0)\)</span> such that <span class="math inline">\(\tau &lt; \infty\)</span>. Then, the process:</p>
<p><span class="math display">\[
(B_{t+\tau} - B_{\tau},t\geq 0)
\]</span></p>
<p>is a standard brownian motion independent of <span class="math inline">\(\mathcal{F}_\tau\)</span>.</p>
</div>
<div id="exm-brownian-motion-is-strong-markov" class="theorem example">
<p><span class="theorem-title"><strong>Example 3</strong></span> (Brownian motion is strong Markov) To see this, let’s compute the conditional MGF as in <a href="#eq-conditional-mgf-of-xt" class="quarto-xref">Equation&nbsp;3</a>. We have:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}[e^{aB_{t+\tau}}|\mathcal{F}_\tau] &amp;= \mathbb{E}[e^{a(B_{t+\tau} - B_\tau + B_\tau)}|\mathcal{F}_\tau]\\
&amp;= e^{aB_\tau} \mathbb{E}[e^{a(B_{t+\tau} - B_\tau)}|\mathcal{F}_\tau]\\
&amp; \{ B_\tau \text{ is }\mathcal{F}_\tau-\text{measurable }\}\\
&amp;= e^{aB_\tau}\mathbb{E}[e^{a(B_{t+\tau} - B_\tau)}]\\
&amp; \{ (B_{t+\tau} - B_\tau) \perp \mathcal{F}_\tau\}\\
&amp;= e^{aB_\tau}e^{\frac{1}{2}a^2 t}\\
\end{align*}
\]</span></p>
<p>Thus, the conditional MGF is an explicit function of <span class="math inline">\(B_\tau\)</span> and <span class="math inline">\(t\)</span>. This proves the proposition. <span class="math inline">\(\blacksquare\)</span></p>
</div>
<p><em>Proof</em> of <a href="#thm-shifted-brownian-motion-about-a-stopping-time" class="quarto-xref">Theorem&nbsp;2</a>.</p>
<p>We first consider for fixed <span class="math inline">\(n\)</span> the discrete valued stopping time:</p>
<p><span class="math display">\[
\tau_n = \frac{k + 1}{2^n}, \quad \text{ if } \frac{k}{2^n} \leq \tau &lt; \frac{k+1}{2^n}, k\in \mathbb{N}
\]</span></p>
<p>In other words, if <span class="math inline">\(\tau\)</span> occurs in the interval <span class="math inline">\([\frac{k}{2^n},\frac{k+1}{2^n})\)</span>, we stop at the next dyadic <span class="math inline">\(\frac{k+1}{2^n}\)</span>. By construction <span class="math inline">\(\tau_n\)</span> depends only on the process in the past. Consider the process <span class="math inline">\(W_t = B_{t + \tau_n} - B_{\tau_n}, t \geq 0\)</span>. We show it is a standard brownian motion independent of <span class="math inline">\(\tau_n\)</span>. This is feasible as we can decompose over the discrete values taken by <span class="math inline">\(\tau_n\)</span>. More, precisely, take <span class="math inline">\(E \in \mathcal{F}_{\tau_n}\)</span>, and some generic event <span class="math inline">\(\{W_t \in A\}\)</span> for the process <span class="math inline">\(W\)</span>. Then, by decomposing over the values of <span class="math inline">\(\tau_n\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{P}(\{W_t \in A\} \cap E) &amp;= \sum_{k=0}^\infty \mathbb{P}\left(\{W_t \in A\} \cap E \cap \{\tau_n = \frac{k}{2^n}\}\right)\\
&amp;= \sum_{k=0}^\infty \mathbb{P}\left(\{(B_{t+k/2^n} - B_{k/2^n}) \in A\} \cap E \cap \{\tau_n = \frac{k}{2^n}\}\right)\\
&amp;= \sum_{k=0}^\infty \mathbb{P}\left(\{(B_{t+k/2^n} - B_{k/2^n}) \in A\}\right) \times \mathbb{P}\left( E \cap \{\tau_n = \frac{k}{2^n}\}\right)
\end{align*}
\]</span></p>
<p>since <span class="math inline">\((B_{t+k/2^n} - B_{k/2^n})\)</span> is independent of <span class="math inline">\(\mathcal{F}_{k/2^n}\)</span> by <a href="#exr-shifted-brownian-motion" class="quarto-xref">Exercise&nbsp;1</a> and since <span class="math inline">\(E \cap \{\tau_n = \frac{k}{2^n}\} \in \mathcal{F}_{k/2^n}\)</span> by definition of stopping time. But, given <span class="math inline">\(\{\tau_n = k/2^n\}\)</span>, the event <span class="math inline">\(\{(B_{t+k/2^n} - B_{k/2^n}) \in A\}\)</span> is the same as <span class="math inline">\(\{B_t \in A\} = \{W_t \in A\}\)</span>, since this process is now a standard brownian motion. Thus, <span class="math inline">\(\mathbb{P}\{(B_{t+k/2^n} - B_{k/2^n}) \in A\} = \mathbb{P}\{B_t \in A\} = \mathbb{P}\{W_t \in A\}\)</span>, dropping the dependence on <span class="math inline">\(k\)</span>. The sum over <span class="math inline">\(k\)</span> then yields:</p>
<p><span class="math display">\[
\mathbb{P}\left(\{W_t \in A\}\cap E\right) = \mathbb{P}(W_t \in A) \mathbb{P}(E)
\]</span></p>
<p>as claimed. The extension to <span class="math inline">\(\tau\)</span> is done by using continuity of paths. We have:</p>
<p><span class="math display">\[
\lim_{n \to \infty} B_{t + \tau_n} - B_{\tau_n} = B_{t+\tau} - B_{\tau} \text{ almost surely}
\]</span></p>
<p>Note, that this only uses right continuity! Moreover, this implies that <span class="math inline">\(B_{t+\tau} - B_\tau\)</span> is independent of <span class="math inline">\(\mathcal{F}_{\tau_n}\)</span> for all <span class="math inline">\(n\)</span>. Again by (right-)continuity this extends to independence of <span class="math inline">\(\mathcal{F}_\tau\)</span>. The limiting distribution of the process is obtained by looking at the finite dimensional distributions of the increments of <span class="math inline">\(B_{t+\tau_n} - B_{\tau_n}\)</span> for a finite number of <span class="math inline">\(t\)</span>’s and taking the limit as above. <span class="math inline">\(\blacksquare\)</span></p>
<p>Most diffusions also enjoy the strong markov property, as long as the functions <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\mu\)</span> encoding the volatility and drift are nice enough. This is the case for the diffusions we have considered.</p>
<div id="thm-most-diffusions-are-strong-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (Most diffusions are strong markov)</strong></span> Consider a diffusion <span class="math inline">\((X_t,t\leq T)\)</span> as as in <a href="#thm-diffusions-are-markov-processes" class="quarto-xref">Theorem&nbsp;1</a>. Then, the diffusion has strong markov property.</p>
</div>
<p>The proof follows the line of the one of <a href="#thm-diffusions-are-markov-processes" class="quarto-xref">Theorem&nbsp;1</a></p>
<p><em>Proof.</em></p>
<p>Consider the time-homogenous diffusion:</p>
<p><span class="math display">\[
dX_t = \mu(X_t)dt + \sigma(X_t)dB_t
\]</span></p>
<p>By the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes">existence and uniqueness theorem</a>, this SIVP defines a unique continuous adapted process <span class="math inline">\((X_t,t \geq 0)\)</span>. Let <span class="math inline">\(\mathfrak{F}=(\mathcal{F}_t^X,t \geq 0)\)</span> be the natural filtration of <span class="math inline">\((X_t, t\leq T)\)</span>. Let <span class="math inline">\(\tau\)</span> be a stopping time for the filtration <span class="math inline">\(\mathfrak{F}\)</span> and consider the process <span class="math inline">\(W_t = B_{t+\tau} - B_\tau\)</span>. From <a href="#thm-shifted-brownian-motion-about-a-stopping-time" class="quarto-xref">Theorem&nbsp;2</a>, we know that the process <span class="math inline">\((W_t,t\geq 0)\)</span> is a standard brownian motion independent <span class="math inline">\(\mathcal{F}_\tau\)</span>. For <span class="math inline">\(s \geq 0\)</span>, we consider the SDE:</p>
<p><span id="eq-diffusion-of-Y"><span class="math display">\[
dY_s = \mu(Y_s)ds + \sigma(Y_s)dW_s, \quad Y_0 = X_\tau
\tag{7}\]</span></span></p>
<p>Again by the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes">existence and uniqueness theorem</a>, there exists a unique solution to the SIVP that is adapted to the natural filtration of <span class="math inline">\(W\)</span>. We claim that <span class="math inline">\((X_{s+\tau},s \geq 0)\)</span> is the solution to this equation, since:</p>
<p><span class="math display">\[
X_{s+\tau} = X_\tau + \int_\tau^{s+\tau} \mu(X_u)du + \int_{\tau}^{s+\tau} \sigma(X_u)dB_u
\]</span></p>
<p>Perform a change of variable <span class="math inline">\(v = u - \tau\)</span>. Then, the limits of integration bare, <span class="math inline">\(v = 0\)</span> and <span class="math inline">\(v = s\)</span>. And <span class="math inline">\(dv = du\)</span>.</p>
<p><span class="math inline">\(dB_u  \approx B_{u_2} - B_{u_1} = B(v_1 + \tau) - B(v_2 + \tau) = W(v_2) - W(v_1) =dW_v\)</span>.</p>
<p><span class="math display">\[
X_{s+\tau} = X_\tau + \int_0^{s} \mu(X_{v+\tau})dv + \int_{0}^{s} \sigma(X_{v+\tau})dW_v
\]</span></p>
<p>If we let <span class="math inline">\(Y_0 = X_\tau\)</span>, <span class="math inline">\(Y_v = X_{v+\tau}\)</span>, we recover the dynamics of <span class="math inline">\((Y_v,v \geq 0)\)</span> in <a href="#eq-diffusion-of-Y" class="quarto-xref">Equation&nbsp;7</a>. So, <span class="math inline">\((X_{s+\tau},s\geq 0)\)</span> is the solution to the SIVP in <a href="#eq-diffusion-of-Y" class="quarto-xref">Equation&nbsp;7</a>. Thus, we conclude for any interval <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
\mathbb{P}(X_{s+\tau} \in A | \mathcal{F}_\tau^X) = \mathbb{P}(Y_v \in A| \mathcal{F}_\tau^X)
\]</span></p>
<p>But, since <span class="math inline">\((Y_v,v\geq 0)\)</span> depends on <span class="math inline">\(\mathcal{F}_\tau^X\)</span> only through <span class="math inline">\(X_\tau\)</span>, we conclude that <span class="math inline">\(\mathbb{P}(X_{s + \tau} \in A | \mathcal{F}_\tau^X) = \mathbb{P}(X_{s + \tau} \in A| X_\tau)\)</span>. Consequently, <span class="math inline">\((X_t,t \geq 0)\)</span> is a strong-markov process. <span class="math inline">\(\blacksquare\)</span></p>
<div id="nte-extension-of-optional-sampling" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip&nbsp;2: Extension of optional sampling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a continuous martingale <span class="math inline">\((M_t, t\leq T)\)</span> for a filtration <span class="math inline">\((\mathcal{F}_t, t\geq 0)\)</span> and a stopping time <span class="math inline">\(\tau\)</span> for the same filtration. Suppose we would like to compute for some <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}[M_T \mathbf{1}_{\{\tau \leq T\}}]
\]</span></p>
<p>It would be tempting to condition on <span class="math inline">\(\mathcal{F}_\tau\)</span> and write <span class="math inline">\(\mathbb{E}[M_T |\mathcal{F}_\tau] = M_\tau\)</span> on the event <span class="math inline">\(\{\tau \leq T\}\)</span>. We would then conclude that:</p>
<p><span class="math display">\[
\mathbb{E}[M_T 1_{\{\tau \leq T\}}] = \mathbb{E}[1_{\{\tau \leq T\}} \mathbb{E}[M_T|\mathcal{F}_\tau] ] = \mathbb{E}[M_\tau 1_{\{\tau \leq T\}}]
\]</span></p>
<p>In some sense, we have extended the martingale property to stopping times. This property can be proved under reasonable assumptions on <span class="math inline">\((M_t,t\leq T)\)</span> (for example, if it is positive). Indeed, it suffices to approximate <span class="math inline">\(\tau\)</span> by discrete valued stopping time <span class="math inline">\(\tau_n\)</span> as in the proof of <a href="#thm-shifted-brownian-motion-about-a-stopping-time" class="quarto-xref">Theorem&nbsp;2</a>. One can then apply martingale property at a fixed time.</p>
</div>
</div>
</section>
<section id="the-heat-equation" class="level2">
<h2 class="anchored" data-anchor-id="the-heat-equation">The Heat Equation</h2>
<p>We look at more detail on how PDEs come up when computing quantities related to Markov processes.</p>
<div id="exm-heat-equation-and-brownian-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 4</strong></span> (Heat Equation and Brownian motion) Let <span class="math inline">\(f(t,x)\)</span> be a function of time and space. The heat equation in <span class="math inline">\(1+1\)</span>-dimension (one dimension of time, one dimension of space) is the PDE:</p>
<p><span id="eq-heat-equation-in-2d"><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t} &amp;= \frac{1}{2}\frac{\partial^2 f}{\partial x^2}
\end{align*}
\tag{8}\]</span></span></p>
<p>In <span class="math inline">\(1+d\)</span> (one dimension of time, <span class="math inline">\(d\)</span> dimensions of space), the heat equation is:</p>
<p><span id="eq-heat-equation-in-d-plus-one-dims"><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t} &amp;= \frac{1}{2}\nabla^2 f
\end{align*}
\tag{9}\]</span></span></p>
<p>where <span class="math inline">\(\nabla^2\)</span> is the Laplacian operator.</p>
<p>The solutions to these PDEs can be expressed as an expectation over Brownian motion paths.</p>
<p>Let <span class="math inline">\((X_t,t \geq 0)\)</span> be a brownian motion and let <span class="math inline">\(f(t,x)\)</span> represent the PDF of <span class="math inline">\(X_t\)</span>. Then (as we will see) satisfies the PDE:</p>
<p><span class="math display">\[
\begin{align*}
\partial_{t}f(t,x) = \frac{1}{2}\partial_x^2 f(t,x)
\end{align*}
\]</span></p>
<p>Suppose the Brownian motion is at <span class="math inline">\(y\)</span> in the present. Suppose <em>the present</em> is a specific time <span class="math inline">\(t_1\)</span>. The joint probability density that <span class="math inline">\(\{X_{t}=x,X_{t_1}=y\}\)</span> is:</p>
<p><span class="math display">\[
\begin{align*}
p(x, t, y, t_1) = f(y,t_1) \cdot p(x,t|y,t_1)
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(p(x,t|y,t_1)\)</span> is the transition probability density function.</p>
<p>Since <span class="math inline">\(f(t,x)\)</span> is the density of <span class="math inline">\(X_t\)</span> and since we have a formula for the joint density of <span class="math inline">\(X_{t_1}\)</span> and <span class="math inline">\(X_t\)</span>, we can view <span class="math inline">\(f(t,x)\)</span> as the marginal of the joint density. You find the marginal density by integrating out the variables you are not interested in, <span class="math inline">\(X_{t_1}\)</span> in this case. In the abstract, this is:</p>
<p><span class="math display">\[
\begin{align*}
f(t,x) &amp;= \int_{-\infty}^{\infty} p(x, t, y, t_1)dy\\
&amp;= \int_{-\infty}^{\infty} f(t_1,y) p(x,t_2 | y,t_1) dy
\end{align*}
\]</span></p>
<p>Our claim is that <span class="math inline">\(f\)</span> indeed satisfies the PDE (<a href="#eq-heat-equation-in-2d" class="quarto-xref">Equation&nbsp;8</a>).</p>
<p>The gaussian transition probability density function (heat kernel) <span class="math inline">\(p(x,t|y,0)\)</span> is given by:</p>
<p><span class="math display">\[
p(x,t|y,0) = \frac{1}{\sqrt{2\pi t}}\exp\left(-\frac{(x-y)^2}{2t}\right)
\]</span></p>
<p>Differentiating <span class="math inline">\(p\)</span> with respect to <span class="math inline">\(t\)</span>, we have:</p>
<p><span id="eq-partial-with-respect-to-time"><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial t} p(x,t|y,0) &amp;= \frac{\sqrt{2\pi t} \exp\left(-\frac{(x-y)^2}{2t}\right) \frac{\partial}{\partial t}\left(-\frac{(x-y)^2}{2t}\right) - \exp\left(-\frac{(x-y)^2}{2t}\right)\sqrt{2\pi}\left(\frac{1}{2\sqrt{t}}\right)}{2\pi t}\\
&amp;=\sqrt{2\pi}\exp\left(-\frac{(x-y)^2}{2t}\right) \frac{\frac{(x-y)^2}{2t^{3/2}} - \frac{t}{2t^{3/2}}}{2\pi t}\\
&amp;= \exp\left(-\frac{(x-y)^2}{2t}\right) \frac{(x-y)^2 - t}{\sqrt{2\pi} (2t^{5/2}) }
\end{align*}
\tag{10}\]</span></span></p>
<p>Differentiating <span class="math inline">\(p\)</span> with respect to <span class="math inline">\(x\)</span>, we have:</p>
<p><span id="eq-first-derivative-with-respect-to-space"><span class="math display">\[
\begin{align*}
\frac{\partial }{\partial x} p(x,t|y,0) &amp;= \frac{1}{\sqrt{2\pi t}}\exp\left[-\frac{(x-y)^2}{2t}\right]\frac{\partial}{\partial x}\left(-\frac{(x-y)^2}{2t}\right)\\
&amp;= \frac{1}{\sqrt{2\pi t}} \cdot \left(-\frac{1}{\cancel{2} t}\right) \exp\left[-\frac{(x-y)^2}{2t}\right] \cdot \cancel{2}(x-y)\\
&amp;= -\frac{1}{t\sqrt{2\pi t}} (x-y)\exp\left[-\frac{(x-y)^2}{2t}\right]
\end{align*}
\tag{11}\]</span></span></p>
<p>Differentiating again with respect to space, we have:</p>
<p><span id="eq-second-derivative-with-respect-to-space"><span class="math display">\[
\begin{align*}
\frac{\partial^2}{\partial x^2} p(x,t|y,0) &amp;= -\frac{1}{t\sqrt{2\pi t}} \left[\exp\left\{-\frac{(x-y)^2}{2}\right\} + (x-y)\exp\left\{-\frac{(x-y)^2}{2}\right\}\left(-\frac{2(x-y)}{2y}\right)\right]\\
&amp;=-\frac{1}{t\sqrt{2\pi t}}\exp\left\{-\frac{(x-y)^2}{2}\right\} \left[1 - \frac{(x-y)^2}{t}\right]\\
&amp;=\frac{1}{t\sqrt{2\pi t}}\exp\left\{-\frac{(x-y)^2}{2}\right\} \left[\frac{(x-y)^2 - t}{t}\right]\\
&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{(x-y)^2}{2}\right\} \cdot \frac{(x-y)^2 - t}{t^{5/2}}
\end{align*}
\tag{12}\]</span></span></p>
<p>From <a href="#eq-partial-with-respect-to-time" class="quarto-xref">Equation&nbsp;10</a> and <a href="#eq-second-derivative-with-respect-to-space" class="quarto-xref">Equation&nbsp;12</a>, it follows that:</p>
<p><span class="math display">\[
\frac{\partial}{\partial t} p(x,t|y,0) = \frac{1}{2}\frac{\partial ^2}{\partial x^2} p(x,t|y,0)
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial t} \int_{-\infty}^\infty g(y) p(x,t|y,0)dy &amp;= \frac{1}{2}\frac{\partial^2}{\partial x^2} \int_{-\infty}^\infty g(y) p(x,t|y,0)dy \\
\frac{\partial }{\partial t}f(t,x) &amp;= \frac{1}{2}\frac{\partial^2 }{\partial x^2} f(t,x)
\end{align*}
\]</span></p>
</div>
</section>
<section id="solution-to-the-heat-pde-as-an-expectation-over-brownian-motion-paths" class="level2">
<h2 class="anchored" data-anchor-id="solution-to-the-heat-pde-as-an-expectation-over-brownian-motion-paths">Solution to the heat PDE as an expectation over Brownian-motion paths</h2>
<p>Consider again the heat PDE:</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t} = \frac{1}{2}\frac{\partial^2 f }{\partial x^2}
\end{align*}
\]</span></p>
<p>with the initial condition</p>
<p><span class="math display">\[
f(0,x) = g(x)
\]</span></p>
</section>
<section id="robert-browns-erratic-motion-of-pollen" class="level2">
<h2 class="anchored" data-anchor-id="robert-browns-erratic-motion-of-pollen">Robert Brown’s erratic motion of pollen</h2>
<p>In the summer of 1827, the Scottish botanist Robert Brown observed that microscopic pollen grains suspended in water move in an erratic, highly irregular, zigzag pattern. It was only in 1905, that Albert Einstein could provide a satisfactory explanation of Brownian motion. He asserted that Brownian motion originates in the continual bombardment of the pollen grains by the molecules of the surrounding water. As a result of continual collisions, the particles themselves had the same kinetic energy as the water molecules. Thus, he showed that Brownian motion provided a solution (in a certain sense) to Fourier’s famous heat equation</p>
<p><span class="math display">\[
\frac{\partial u}{\partial t}(t,x) = \kappa \frac{\partial^2 u}{\partial x^2}(t,x)
\]</span></p>
<section id="albert-einsteins-proof-of-the-existence-of-brownian-motion" class="level3">
<h3 class="anchored" data-anchor-id="albert-einsteins-proof-of-the-existence-of-brownian-motion">Albert Einstein’s proof of the existence of Brownian motion</h3>
<p>We now summarize Einstein’s original 1905 argument. Let’s say that we are interested in the motion along the horizontal <span class="math inline">\(x\)</span>-axis. Let’s say we drop brownian particles in a liquid. Let <span class="math inline">\(f(t,x)\)</span> represent the number of particles per unit volume (density) at position <span class="math inline">\(x\)</span> at time <span class="math inline">\(t\)</span>. So, the number of particles in a small interval <span class="math inline">\(I=[x,x+dx]\)</span> of width <span class="math inline">\(dx\)</span> will be <span class="math inline">\(f(t,x)dx\)</span>.</p>
<p>Now, as time progresses, the number of particles in this interval <span class="math inline">\(I\)</span> will change. The brownian particles will zig-zag upon bombardment by the molecules of the liquid. Some particles will move out of the interval <span class="math inline">\(I\)</span>, while other particles will move in.</p>
<p>Let’s consider a timestep of length <span class="math inline">\(\tau\)</span>. Einstein’s probabilistic approach was to model the distance travelled by the particles or displacement of the particles as a random variable <span class="math inline">\(\Delta\)</span>. To determine how many particles end up in the interval <span class="math inline">\(I\)</span>, we start with the area to the right of the interval <span class="math inline">\(I\)</span>.</p>
<p>The density of particles at <span class="math inline">\(x+\Delta\)</span> is <span class="math inline">\(f(t,x+\Delta)\)</span>; the number of particles in a small interval of length <span class="math inline">\(dx\)</span> is <span class="math inline">\(f(t,x+\Delta)dx\)</span>. If we represent the probability density of the displacement by <span class="math inline">\(\phi(\Delta)\)</span>, then the number of particles at <span class="math inline">\(x+\Delta\)</span> that will move to <span class="math inline">\(x\)</span> will be <span class="math inline">\(dx \cdot f(t,x+\Delta)\phi(\Delta)\)</span>. We can apply the same logic to the left hand side. The number of particles at <span class="math inline">\(x - \Delta\)</span> that will move to <span class="math inline">\(x\)</span> will be <span class="math inline">\(dx \cdot f(t,x-\Delta)\phi(-\Delta)\)</span>. Assume that <span class="math inline">\(\phi(\Delta) = \phi(-\Delta)\)</span>.</p>
<p>Now, if we integrate these movements across the real line, then we get the number of particles at <span class="math inline">\(x\)</span> at a short time later <span class="math inline">\(t + \tau\)</span>.</p>
<p><span class="math display">\[
f(t+ \tau,x) dx = dx \int_{-\infty}^{\infty} f(t,x+\Delta) \phi(\Delta) d\Delta
\]</span></p>
<p>Now, we can get rid of <span class="math inline">\(dx\)</span>.</p>
<p><span id="eq-expression-for-density-at-later-time"><span class="math display">\[
f(t+ \tau,x) = \int_{-\infty}^{\infty} f(t,x+\Delta) \phi(\Delta) d\Delta
\tag{13}\]</span></span></p>
<p>The Taylor’s series expansion of <span class="math inline">\(f(t+\tau,x)\)</span> centered at <span class="math inline">\(t\)</span> (holding <span class="math inline">\(x\)</span> constant) is:</p>
<p><span class="math display">\[
f(t + \tau,x) = f(t,x) + \frac{\partial f}{\partial t}\tau + O(\tau^2)
\]</span></p>
<p>The Taylor’s series expansion of <span class="math inline">\(f(t,x+\Delta)\)</span> centered at <span class="math inline">\(x\)</span> (holding <span class="math inline">\(t\)</span> constant) is:</p>
<p><span class="math display">\[
f(t,x+\Delta) = f(t,x) + \frac{\partial f}{\partial x}\Delta + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\Delta^2 + O(\Delta^3)
\]</span></p>
<p>We can now substitute these into <a href="#eq-expression-for-density-at-later-time" class="quarto-xref">Equation&nbsp;13</a> to get:</p>
<p><span class="math display">\[
\begin{align*}
f(t,x) + \frac{\partial f}{\partial t}\tau &amp;= \int_{-\infty}^{\infty}\left(f(t,x) + \frac{\partial f}{\partial x}\Delta + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\Delta^2\right) \phi(\Delta)d\Delta\\
&amp;= f(t,x) \int_{-\infty}^{\infty} \phi(\Delta)d\Delta \\
&amp;+ \frac{\partial f} {\partial x} \int_{-\infty}^{\infty} \Delta \phi(\Delta)d\Delta \\
&amp;+ \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta
\end{align*}
\]</span></p>
<p>Now, since the probability distribution of displacement <span class="math inline">\(\phi(\cdot)\)</span> is symmetric around the origin, the second term is zero. And we know, that if we integrate the density over <span class="math inline">\(\mathbb{R}\)</span>, we should get one, so the first term equals one. So, we get:</p>
<p><span class="math display">\[
f(t,x) + \frac{\partial f}{\partial t}\tau = f(t,x) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta
\]</span></p>
<p>Now, we can cancel the <span class="math inline">\(f\)</span> on both sides and then shift <span class="math inline">\(\tau\)</span> to the right hand side:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} =  \left(\frac{1}{2\tau} \int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta \right)\frac{\partial^2 f}{\partial x^2}
\]</span></p>
<p>Define <span class="math inline">\(D:= \left(\frac{1}{2\tau} \int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta \right)\)</span>. Then, we have:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} =  D\frac{\partial^2 f}{\partial x^2}
\]</span></p>
<p>The microscopic interpretation of the diffusion coefficient is, that its just the average of the squared displacements. The larger the <span class="math inline">\(D\)</span>, the faster the brownian particles move.</p>
</section>
</section>
<section id="kolmogorovs-backward-equation" class="level2">
<h2 class="anchored" data-anchor-id="kolmogorovs-backward-equation">Kolmogorov’s Backward Equation</h2>
<p>Think of <span class="math inline">\(y\)</span> and <span class="math inline">\(t\)</span> as being current values and <span class="math inline">\(y'\)</span> and <span class="math inline">\(t'\)</span> being future values. The transition probability density function <span class="math inline">\(p(y',t'|y,t)\)</span> of a diffusion satisfies two equations - one involving derivatives with respect to a future state and time (<span class="math inline">\(y'\)</span> and <span class="math inline">\(t'\)</span>) called <em>forward equation</em> and the other involving derivatives with respect to the current state and current time (<span class="math inline">\(y\)</span> and <span class="math inline">\(t\)</span>) called the <em>backward equation</em>. These two equations are parabolic partial differential equations not dissimilar to the Black-Scholes equation.</p>
<div id="thm-backward-equation-with-initial-value" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4 (Backward equation with initial value)</strong></span> Let <span class="math inline">\((X_t,t\geq 0)\)</span> be a diffusion in <span class="math inline">\(\mathbb{R}\)</span> with the SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t)dB_t + \mu(X_t) dt
\]</span></p>
<p>Let <span class="math inline">\(g\in C^2(\mathbb{R})\)</span> be such that <span class="math inline">\(g\)</span> is <span class="math inline">\(0\)</span> outside an interval. Then, the solution of the PDE with initial value</p>
<p><span id="eq-backward-equation"><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t}(t,x) &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(x)\frac{\partial f}{\partial x}\\
f(0,x) &amp;= g(x)
\end{align*}
\tag{14}\]</span></span></p>
<p>has the representation:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}[g(X_t)|X_0 = x]
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p><strong>Step 1.</strong> Let’s fix <span class="math inline">\(t\)</span> and consider the function of space <span class="math inline">\(h(x)=f(t,x)=\mathbb{E}[g(X_t)|X_0=x]\)</span>. Applying Ito’s formula to <span class="math inline">\(h\)</span>, we have:</p>
<p><span class="math display">\[\begin{align}
dh(X_s) &amp;= h'(X_s) dX_s + \frac{1}{2}h''(X_s) (dX_s)^2\\
&amp;= h'(X_s) (\sigma(X_s)dB_s + \mu(X_s) ds) + \frac{\sigma(X_s)^2}{2}h''(X_s)ds\\
&amp;= \sigma(X_s)h'(X_s)dB_s + \left(\frac{\sigma(X_s)^2}{2}h''(X_s) + \mu(X_s)h'(X_s)\right)ds
\end{align}\]</span></p>
<p>In the integral form this is:</p>
<p><span class="math display">\[\begin{align*}
h(X_s) - h(X_0) &amp;= \int_0^s \sigma(X_u)h'(X_u)dB_u \\
&amp;+ \int_0^s \left(\frac{\sigma(X_u)^2}{2}h''(X_u) + \mu(X_u)h'(X_u)\right)du \tag{1}
\end{align*}\]</span></p>
<p><strong>Step 2.</strong> Take expectations on both sides, divide by <span class="math inline">\(s\)</span> and let <span class="math inline">\(s \to 0\)</span>. We are interested in taking the derivative with respect to <span class="math inline">\(s\)</span> at <span class="math inline">\(s_0=0\)</span>.</p>
<p>The expectation of the first term on the right hand side is zero, by the properties of the Ito integral.</p>
<p>The integrand of the second term (RHS) is a conditional expectation <span class="math inline">\(\mathbb{E}[\xi(X_u)|X_0 = x]\)</span>, it is an average at time <span class="math inline">\(u\)</span>, of the paths of the process starting at initial position <span class="math inline">\(X_0 = x\)</span>, so it is a function of <span class="math inline">\(u\)</span> and <span class="math inline">\(x\)</span>. So, <span class="math inline">\(\mathbb{E}[\xi(X_u)|X_0 = x] = p(u,x)\)</span>. Suppressing the argument <span class="math inline">\(x\)</span>, we have the representation:</p>
<p><span class="math display">\[\begin{align}
\int_0^s p(u) du
\end{align}\]</span></p>
<p>Recall that, if <span class="math inline">\(p\)</span> is a continuous function, then it is Riemann integrable. Further, since integration and differentiation are inverse operations, there exists a unique antiderivative <span class="math inline">\(P\)</span> given by</p>
<p><span class="math display">\[
P(s) = \int_{0}^{s}p(u)du
\]</span></p>
<p>satisfying <span class="math inline">\(P'(0) = p(0)\)</span>.</p>
<p>By the definition of the derivative:</p>
<p><span class="math display">\[P'(0) = \lim_{s \to 0} \frac{P(s) - P(0)}{s} = \lim_{s\to 0} \frac{P(s)}{s} = p(0) \quad \{ P(0)=0 \text{ by definition }\}\]</span></p>
<p>Thus, we have:</p>
<p><span class="math display">\[
p(0,x) = \mathbb{E}[\xi(X_0)|X_0 = x] = \frac{\sigma(x)^2}{2} h''(x) + \mu(x)h'(x)
\]</span></p>
<p><strong>Step 3.</strong> As for the left-hand side, we have:</p>
<p><span class="math display">\[
\lim_{s \to 0} \frac{\mathbb{E}[h(X_s)|X_0 = x] - h(X_0)}{s} = \lim_{s \to 0} \frac{\mathbb{E}[h(X_s)|X_0 = x] - f(t,x)}{s}
\]</span></p>
<p>To prove that this limit is <span class="math inline">\(\frac{\partial f}{\partial t}(t,x)\)</span>, it remains to show that <span class="math inline">\(\mathbb{E}[h(X_s)|X_0 = x]=\mathbb{E}[g(X_{t+s})|X_0 = x]=f(t+s,x)\)</span>.</p>
<p>To see this, note that <span class="math inline">\(h(X_s) = \mathbb{E}[g(X_{t+s})|X_s]\)</span>. We deduce:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[h(X_s)|X_0 = x] &amp;= \mathbb{E}[\mathbb{E}[g(X_{t+s})|X_s]|X_0 = x]\\
&amp;= \mathbb{E}[\mathbb{E}[g(X_{t+s})|\mathcal{F}_s]|X_0 = x]\\
&amp; \{ (X_t,t\geq 0) \text{ is Markov }\} \\
&amp;= \mathbb{E}[g(X_{t+s})|X_0 = x]\\
&amp; \{ \text{ Tower property }\} \\
&amp;= f(t+s,x)
\end{align*}\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>The backward equation (<a href="#eq-backward-equation" class="quarto-xref">Equation&nbsp;14</a>) can be conveniently written in terms of <em>the generator of the diffusion</em>.</p>
<div id="def-generator-of-the-diffusion" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Generator of a diffusion)</strong></span> The generator of a diffusion with SDE <span class="math inline">\(dX_t = \sigma(X_t) dB_t + \mu(X_t)dt\)</span> is the differential operator acting on functions of space defined by :</p>
<p><span class="math display">\[
A = \frac{\sigma(x)^2}{2}\frac{\partial }{\partial x^2} + \mu(x)\frac{\partial}{\partial x}
\]</span></p>
</div>
<p>With this notation, the backward equation for the function <span class="math inline">\(f(t,x)\)</span> takes the form:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x}(t,x) = Af(t,x)
\]</span></p>
<p>where it is understood that <span class="math inline">\(A\)</span> acts only on the space variable. <a href="#thm-backward-equation-with-initial-value" class="quarto-xref">Theorem&nbsp;4</a> gives a nice interpretation of the generator: it quantifies how much the function <span class="math inline">\(f(t,x) = \mathbb{E}[g(X_t)|X_0 = x]\)</span> changes in a small time interval.</p>
</section>
<section id="the-heat-equation-as-a-special-case-of-the-backward-equation" class="level2">
<h2 class="anchored" data-anchor-id="the-heat-equation-as-a-special-case-of-the-backward-equation">The heat equation as a special case of the Backward equation</h2>
<p>Let <span class="math inline">\((B_t,t\geq 0)\)</span> be a standard brownian motion. Then, the generator is:</p>
<p><span class="math display">\[
A = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}(t,x)
\]</span></p>
<p>Then, by <a href="#thm-backward-equation-with-initial-value" class="quarto-xref">Theorem&nbsp;4</a>, the solution of the heat PDE</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t}(t,x) = Af(x) = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}(t,x)
\end{align*}
\]</span></p>
<p>with initial value <span class="math inline">\(f(0,x)=g(x)\)</span> has the stochastic representation:</p>
<p><span class="math display">\[f(t,x) = \mathbb{E}[g(B_t)|B_0 = x]\]</span></p>
<p>It can be represented as an average of <span class="math inline">\(g(B_t)\)</span> over all Brownian motion paths starting at the location <span class="math inline">\(x\)</span>.</p>
<div id="exm-generator-of-the-ornstein-uhlenbeck-process" class="theorem example">
<p><span class="theorem-title"><strong>Example 5</strong></span> (Generator of the Ornstein Uhlenbeck Process) The SDE of the Ornstein-Uhlenbeck process is:</p>
<p><span class="math display">\[
dX_t = dB_t - X_t dt
\]</span></p>
<p>This means that its generator is:</p>
<p><span class="math display">\[
A = \frac{1}{2}\frac{\partial^2}{\partial x^2} - x \frac{\partial}{\partial x}
\]</span></p>
</div>
<div id="exm-generator-of-geometric-brownian-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 6</strong></span> (Generator of Geometric Brownian Motion) Recall that the geometric Brownian motion</p>
<p><span class="math display">\[
S_t = S_0 \exp(\sigma B_t + \mu t)
\]</span></p>
<p>satisfies the SDE:</p>
<p><span class="math display">\[
dS_t = \sigma S_t dB_t + \left(\mu + \frac{\sigma^2}{2}\right) S_t dt
\]</span></p>
<p>In particular, the generator of geometric Brownian motion is :</p>
<p><span class="math display">\[
A = \frac{\sigma^2 x^2}{2} x \frac{\partial^2}{\partial x^2} + \left(\mu + \frac{\sigma^2}{2}\right)\frac{\partial}{\partial x}
\]</span></p>
</div>
<p>For applications, in particular in mathematical finance, it is important to solve the backward equation with terminal value instead of with initial value. The reversal of time causes the appearance of an extra minus sign in the equation.</p>
<div id="thm-backward-equation-with-terminal-value" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (Backward equation with terminal value)</strong></span> Let <span class="math inline">\((X_t,t\leq T)\)</span> be a diffusion with the dynamics:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t) dB_t + \mu(X_t)dt
\]</span></p>
<p>Let <span class="math inline">\(g\in C^2(\mathbb{R})\)</span> be such that <span class="math inline">\(g\)</span> is <span class="math inline">\(0\)</span> outside an interval. Then, the solution of the PDE with terminal value at time <span class="math inline">\(T\)</span></p>
<p><span id="eq-backward-equation-with-terminal-value"><span class="math display">\[
\begin{align*}
-\frac{\partial f}{\partial t} &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(x)\frac{\partial f}{\partial x}\\
f(T,x) &amp;= g(x)
\end{align*}
\tag{15}\]</span></span></p>
<p>has the representation:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}[g(X_T)|X_t = x]
\]</span></p>
</div>
<div id="nte-functions-of-markov" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip&nbsp;3: Backward equation with terminal value appears in the martingale condition
</div>
</div>
<div class="callout-body-container callout-body">
<p>One way to construct a martingale for the filtration <span class="math inline">\((\mathcal{F}_t,t\geq 0)\)</span> is to take</p>
<p><span class="math display">\[
M_t = \mathbb{E}[Y | \mathcal{F}_t]
\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is some integrable random variable. The martingale property then follows from the tower property of the conditional expectation. In the setup of <a href="#thm-backward-equation-with-terminal-value" class="quarto-xref">Theorem&nbsp;5</a>, the random variable <span class="math inline">\(Y\)</span> is <span class="math inline">\(g(X_T)\)</span>. By the Markov property of diffusion, we therefore have:</p>
<p><span class="math display">\[
f(t,X_t) = \mathbb{E}[g(X_T)|X_t] = \mathbb{E}[g(X_T)|\mathcal{F}_t]
\]</span></p>
<p>In other words, the solution to the backward equation with terminal value evaluated at <span class="math inline">\(X_t = x\)</span> yields a martingale for the natural filtration of the process. This is a different point of view on the procedure we have used many times now: To get a martingale of the form <span class="math inline">\(f(t,X_t)\)</span>, apply the Ito’s formula to <span class="math inline">\(f(t,X_t)\)</span> and set the <span class="math inline">\(dt\)</span> term to zero. The PDE we obtain is the backward equation with terminal value. In fact, the proof of the theorem takes this exact route.</p>
</div>
</div>
<p><em>Proof.</em></p>
<p>Consider <span class="math inline">\(f(t,X_t)\)</span> and apply Ito’s formula.</p>
<p><span class="math display">\[
\begin{align*}
df(t,X_t) &amp;= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x}dX_t + \frac{1}{2}\frac{\partial^2 f}{\partial x^2} dX_t \cdot dX_t\\
&amp;= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x}(\sigma(X_t) dB_t + \mu(X_t)dt) + \frac{\sigma(X_t)^2}{2}\frac{\partial^2 f}{\partial x^2} dt\\
&amp;= \sigma(X_t) dB_t + \left(\frac{\partial f}{\partial t} + \frac{\sigma(X_t)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(X_t)\frac{\partial f}{\partial x}\right)dt
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(f(t,x)\)</span> is a solution to the equation, we get that the <span class="math inline">\(dt\)</span> term is <span class="math inline">\(0\)</span> and <span class="math inline">\(f(t,X_t)\)</span> is a martingale for the Brownian filtration (and thus also for the natural filtration of the diffusion, which contains less information). In particular we have:</p>
<p><span class="math display">\[
f(t,X_t) = \mathbb{E}[f(T,X_T)|\mathcal{F}_t] = \mathbb{E}[g(X_T)|\mathcal{F}_t]
\]</span></p>
<p>Since <span class="math inline">\((X_t,t\leq T)\)</span> is a Markov process, we finally get:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}[g(X_T)|X_t = x]
\]</span></p>
<div id="exm-martingales-of-geometric-brownian-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 7</strong></span> (Martingales of geometric Brownian motion) Let <span class="math inline">\((S_t, \geq 0)\)</span> be a geometric brownian motion with SDE:</p>
<p><span class="math display">\[
dS_t = \sigma S_t dB_t + \left(\mu + \frac{\sigma^2}{2}\right)dt
\]</span></p>
<p>As we saw in <a href="#exm-generator-of-geometric-brownian-motion" class="quarto-xref">Example&nbsp;6</a>, its generator is:</p>
<p><span class="math display">\[
A = \frac{\sigma^2 x^2}{2}\frac{\partial^2}{\partial x^2} + x\left(\mu+\frac{\sigma^2}{2}\right)\frac{\partial}{\partial x}
\]</span></p>
<p>In view of <a href="#thm-backward-equation-with-terminal-value" class="quarto-xref">Theorem&nbsp;5</a>, if <span class="math inline">\(f(t,x)\)</span> satisfies the PDE</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} + \frac{\sigma^2 x^2}{2}\frac{\partial^2 f}{\partial x^2} + x\left(\mu+\frac{\sigma^2}{2}\right)\frac{\partial f}{\partial x}
\]</span></p>
<p>then processes of the form <span class="math inline">\(f(t,S_t)\)</span> will be martingales for the natural filtration.</p>
</div>
</section>
<section id="kolmogorovs-forward-equation" class="level2">
<h2 class="anchored" data-anchor-id="kolmogorovs-forward-equation">Kolmogorov’s forward equation</h2>
<p>The companion equation to the backward equation is the <em>Kolmogorov forward equation</em> or <em>forward equation</em>. It is also known as the <em>Fokker-Planck</em> equation from its physics origin. The equation is very useful as it is satisfied by the transition density function <span class="math inline">\(p(y',t'|y,t)\)</span> of a time-homogenous diffusion. It involves the <em>adjoint of the generator</em>.</p>
<div id="def-adjoint-of-the-generator" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (Adjoint of the generator)</strong></span> The adjoint <span class="math inline">\(A^*\)</span> of the generator of a diffusion <span class="math inline">\((X_t,t\geq 0)\)</span> with SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t)dB_t + \mu(X_t)dt
\]</span></p>
<p>is the differential operator acting on a function of space <span class="math inline">\(f(x)\)</span> as follows:</p>
<p><span id="eq-adjoint-of-the-generator-of-a-diffusion"><span class="math display">\[
A^*f(x) = \frac{1}{2}\frac{\partial^2 }{\partial x^2} \frac{\sigma(x)^2}{2} f(x) - \frac{\partial }{\partial x}\mu(x)f(x)
\tag{16}\]</span></span></p>
</div>
<p>Note the differences with the generator in <a href="#def-generator-of-the-diffusion" class="quarto-xref">Definition&nbsp;4</a>: there is an extra minus sign and the derivatives also act on the volatility and the drift.</p>
<div id="exm-the-generator-brownian-motion-is-self-adjoint" class="theorem example">
<p><span class="theorem-title"><strong>Example 8</strong></span> (The generator of Brownian motion is self-adjoint) In the case of standard brownian motion, it is easy to check that:</p>
<p><span class="math display">\[
A^* = \frac{1}{2}\frac{\partial^2}{\partial x^2}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
A^* = \frac{1}{2}\nabla^2
\]</span></p>
<p>in the multivariate case. In other words, the generator and its adjoint are the same. In this case, the operator is <em>self-adjoint</em>.</p>
</div>
<div id="exm-the-adjoint-for-geometric-brownian-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 9</strong></span> We see that the adjoint of the generator acting on <span class="math inline">\(f(x)\)</span> for geometric Brownian motion is:</p>
<p><span class="math display">\[
A^*f(x) = \frac{1}{2}\frac{\partial^2}{\partial x^2} (\sigma^2 x^2 f(x)) - \frac{\partial}{\partial x} \left(\left(\mu + \frac{\sigma^2}{2}\right) x f(x)\right)
\]</span></p>
<p>Using the product rule in differentiating we get:</p>
<p><span class="math display">\[
A^*[f(x)] = \frac{\sigma^2}{2}\left(2x f(x) + x^2 f''(x)\right) - \left(\left(\mu + \frac{\sigma^2}{2}\right)\left(f(x) + x f'(x)\right)\right)
\]</span></p>
</div>
<div id="exm-adjoint-for-the-ornstein-uhlenbeck-process" class="theorem example">
<p><span class="theorem-title"><strong>Example 10</strong></span> The generator for the Ornstein-Uhlenbeck process was given in <a href="#exm-generator-of-the-ornstein-uhlenbeck-process" class="quarto-xref">Example&nbsp;5</a>. The adjoint acting on <span class="math inline">\(f\)</span> is therefore:</p>
<p><span class="math display">\[
\begin{align*}
A^*f(x) &amp;= \frac{1}{2}\frac{\partial^2}{\partial x^2}(f(x)) - \frac{\partial}{\partial x}(- x f(x))\\
&amp;= \frac{f''(x)}{2} + (f(x)+xf'(x))
\end{align*}
\]</span></p>
</div>
<p>The forward equation takes the following form for a function <span class="math inline">\(f(t,x)\)</span> of time and space:</p>
<p><span id="eq-forward-equation"><span class="math display">\[
\frac{\partial f}{\partial t} = A^* f
\tag{17}\]</span></span></p>
<p>For brownian motion, since <span class="math inline">\(A^* = A\)</span>, the backward and forward equations are the same. As advertised earlier, the forward equation is satisfied by the transition <span class="math inline">\(p_t(y',t'|y,t)\)</span> of a diffusion. Before showing this in general, we verify it in the Brownian case.</p>
<div id="exm-the-heat-kernel-as-the-solution-of-the-forward-equation" class="theorem example">
<p><span class="theorem-title"><strong>Example 11</strong></span> Recall that the transition probability density <span class="math inline">\(p(y,t|x,0)\)</span> for Brownian motion, or heat kernel, is:</p>
<p><span class="math display">\[
p(y,t|x,0) = \frac{e^{-\frac{(y-x)^2}{2}}}{\sqrt{2\pi t}}
\]</span></p>
<p>Here, the space variable will be <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> will be fixed. The relevant function is thus <span class="math inline">\(f(t,y) = p(y,t|x,0)\)</span>. The adjoint operator acting on the space variable <span class="math inline">\(y\)</span> is <span class="math inline">\(A^* = A = \frac{1}{2}\frac{\partial^2}{\partial y^2}\)</span>. The relevant time and space derivatives are given by <a href="#eq-partial-with-respect-to-time" class="quarto-xref">Equation&nbsp;10</a> and <a href="#eq-second-derivative-with-respect-to-space" class="quarto-xref">Equation&nbsp;12</a>.</p>
<p>We conclude that <span class="math inline">\(f(t,y)=p(y,t|x,0)\)</span> is a solution of the forward equation.</p>
</div>
<p>Where does the form of the adjoint operator <a href="#eq-adjoint-of-the-generator-of-a-diffusion" class="quarto-xref">Equation&nbsp;16</a> come from? In some sense, the adjoint operator plays a role similar to that of the transpose of a matrix in linear algebra. The adjoint acts on the function on the left. To see this, consider two functions <span class="math inline">\(f,g\)</span> of space on which the generator <span class="math inline">\(A\)</span> of a diffusion is well-defined. In particular, let’s assume that the functions are zero outside an interval. Consider the quantity</p>
<p><span class="math display">\[
\int_{\mathbb{R}}g(x)A(f(x))dx = \int_{\mathbb{R}} g(x)\left(\frac{\sigma(x)^2 }{2}f''(x) + \mu(x)f'(x)\right)dx
\]</span></p>
<p>This quantity can represent for example the average of <span class="math inline">\(Af(x)\)</span> over some PDF <span class="math inline">\(g(x)\)</span>. In the above, <span class="math inline">\(A\)</span> acts on the function on the right. To make the operator act on <span class="math inline">\(g\)</span>, we integrate by parts. This gives for the second term:</p>
<p><span class="math display">\[
\int_{\mathbb{R}} g(x)\mu(x)f'(x)dx = g(x)\mu(x)f(x)\Bigg|_{-\infty}^{\infty}-\int_{\mathbb{R}}f(x)\frac{d}{dx}(g(x)\mu(x))dx
\]</span></p>
<p>The boundary term <span class="math inline">\(g(x)f(x)\mu(x)\Bigg|_{-\infty}^\infty\)</span> is <span class="math inline">\(0\)</span> by the assumptions on <span class="math inline">\(f,g\)</span>. This term on <span class="math inline">\(\sigma\)</span> is obtained by integrating by parts twice:</p>
<p><span class="math display">\[
\begin{align*}
\int_{\mathbb{R}} g(x) \frac{\sigma(x)^2}{2}f''(x)dx &amp;= g(x) \frac{\sigma(x)^2}{2}f'(x)\Bigg|_{-\infty}^{\infty} - \int_{\mathbb{R}}\frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right) f'(x)dx\\
-\int_{\mathbb{R}} \frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right)f'(x)dx &amp;= -\frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right)f(x) \Bigg|_{-\infty}^{\infty} + \int_{\mathbb{R}}\frac{d^2}{dx^2}\left(g(x) \frac{\sigma(x)^2}{2}\right)f(x)dx
\end{align*}
\]</span></p>
<p>Thus,</p>
<p><span id="eq-relation-between-generator-and-adjoint"><span class="math display">\[
\begin{align*}
\int_{\mathbb{R}}g(x) Af(x)dx &amp;= \int_{\mathbb{R}}\left(\frac{1}{2}\frac{d^2}{dx^2}(g(x) \sigma(x)^2) - \frac{d}{dx}(g(x)\mu(x))\right)f(x)dx\\
&amp;= \int_{\mathbb{R}}(A^*g(x))f(x)dx
\end{align*}
\tag{18}\]</span></span></p>
<div id="thm-forward-equation-and-transition-probability" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (Forward equation and transition probability)</strong></span> Let <span class="math inline">\((X_t,t\geq 0)\)</span> be a diffusion with SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t)dB_t + \mu(X_t)dt, \quad X_0 = x_0
\]</span></p>
<p>Let <span class="math inline">\(p(x,t|x_0,0)\)</span> be the transition probability density function for a fixed <span class="math inline">\(x_0\)</span>. Then, the function <span class="math inline">\(f(t,y) = p(y,t|x_0,0)\)</span> is a solution of the PDE</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} = A^* f
\]</span></p>
<p>where <span class="math inline">\(A^*\)</span> is the adjoint of <span class="math inline">\(A\)</span>.</p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(h(x)\)</span> be some arbitrary function of space that is <span class="math inline">\(0\)</span> outside an interval. We compute :</p>
<p><span class="math display">\[
\frac{1}{\epsilon}(\mathbb{E}[h(X_{t+\epsilon}) - \mathbb{E}[h(X_t)]])
\]</span></p>
<p>two different ways and take the limit as <span class="math inline">\(\epsilon \to 0\)</span>.</p>
<p>On one hand, we have by the definition of the transition density</p>
<p><span class="math display">\[
\frac{1}{\epsilon}\left(\mathbb{E}[h(X_{t+\epsilon})]-\mathbb{E}[h(X_t)]\right) = \int_{\mathbb{R}}\frac{1}{\epsilon}(p(x,t+\epsilon|x,0) - p(x,t|x_0,0))h(x)dx
\]</span></p>
<p>By taking the limit <span class="math inline">\(\epsilon \to 0\)</span> inside the integral (assuming this is fine), we get:</p>
<p><span id="eq-fwd-equation-partial-wrt-time"><span class="math display">\[
\int_{\mathbb{R}} \frac{\partial}{\partial t}p(x,t|x_0,0)h(x)dx
\tag{19}\]</span></span></p>
<p>On the other hand, Ito’s formula implies</p>
<p><span class="math display">\[
\begin{align*}
dh(X_s) &amp;= \frac{\partial h}{\partial x} dX_s + \frac{1}{2} \frac{\partial^2 h}{\partial x^2} (dX_s)^2\\
&amp;= \frac{\partial h}{\partial x} (\sigma(X_s) dB_s + \mu(X_s)ds) + \frac{1}{2} \frac{\partial^2 h}{\partial x^2} (\sigma(X_s)^2 ds)\\
&amp;= \sigma(X_s)\frac{\partial h}{\partial x} dB_s + \left(\mu(X_s) \frac{\partial h}{\partial x} + \frac{\sigma(X_s)^2}{2}\frac{\partial^2 h}{\partial x^2}\right)ds\\
h(X_{t+\epsilon}) - h(X_t) &amp;= \int_{t}^{t+\epsilon}\sigma(X_s)\frac{\partial h}{\partial x} dB_s + \int_{t}^{t+\epsilon}(Ah(x))ds\\
\mathbb{E}[h(X_{t+\epsilon})] - \mathbb{E}[h(X_t)] &amp;= \underbrace{\mathbb{E}\left[\int_{t}^{t+\epsilon}\sigma(X_s)\frac{\partial h}{\partial x} dB_s\right]}_{0} + \int_{t}^{t+\epsilon}\mathbb{E}[Ah(X_s)]ds
\end{align*}
\]</span></p>
<p>Dividing by <span class="math inline">\(\epsilon\)</span> and taking the limit as <span class="math inline">\(\epsilon \to 0\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
\lim_{\epsilon \to 0} \frac{1}{\epsilon} (\mathbb{E}[h(X_{t+\epsilon})] - \mathbb{E}[h(X_t)]) &amp;= \mathbb{E}[Ah(X_t)]\\
&amp;= \int_{\mathbb{R}} p(x,t|x_0,0) Ah(x) dx
\end{align*}
\]</span></p>
<p>This can be written using <a href="#eq-relation-between-generator-and-adjoint" class="quarto-xref">Equation&nbsp;18</a> as,</p>
<p><span class="math display">\[
\int_{\mathbb{R}}(A^* p(x,t|x_0,0)) h(x) dx
\]</span></p>
<p>Since <span class="math inline">\(h\)</span> is arbitrary, we conclude that:</p>
<p><span id="eq-forward-equation"><span class="math display">\[
\frac{\partial}{\partial t}p(x,t|x_0,0) = A^* p(x,t|x_0,0)
\tag{20}\]</span></span></p>
<div id="exm-forward-equation-and-invariant-probability" class="theorem example">
<p><span class="theorem-title"><strong>Example 12</strong></span> (Forward equation and invariant probability.) The Ornstein-Uhlenbeck process converges to a stationary distribution as noted in the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#exm-ornstein-uhlenbeck-process">example</a> here. For example, for the SDE of the form</p>
<p><span class="math display">\[
dX_t = -X_t dt + dB_t
\]</span></p>
<p>with <span class="math inline">\(X_0\)</span> a Gaussian of mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1/2\)</span>, the PDF of <span class="math inline">\(X_t\)</span>, is, for all <span class="math inline">\(t\)</span> is:</p>
<p><span id="eq-pdf-of-OU-process"><span class="math display">\[
f(x) = \frac{1}{\sqrt{\pi}} e^{-x^2}
\tag{21}\]</span></span></p>
<p>This <em>invariant distribution</em> can be seen from the point of view of the forward equation. Indeed since the PDF is constant in time, the forward equation simply becomes:</p>
<p><span id="eq-forward-equation-of-ou-process"><span class="math display">\[
A^* f = 0
\tag{22}\]</span></span></p>
</div>
<div id="exm-smoluchowski-equation" class="theorem example">
<p><span class="theorem-title"><strong>Example 13</strong></span> The SDE of the Ornstein-Uhlenbeck process can be generated as follows. Consider <span class="math inline">\(V(x)\)</span>, a smooth function of space such that <span class="math inline">\(\int_{\mathbb{R}} e^{-2V(x)}dx&lt;\infty\)</span>. The <em>Smoluchowski</em> equation is the SDE of the form:</p>
<p><span id="eq-smoluchowski-equation"><span class="math display">\[
dX_t = dB_t - V'(X_t) dt
\tag{23}\]</span></span></p>
<p>The SDE can be interpreted as follows: <span class="math inline">\(X_t\)</span> represents the position of a particle on <span class="math inline">\(\mathbb{R}\)</span>. The position varies due to the Brownian fluctuations and also due to a force <span class="math inline">\(V'(X_t)\)</span> that depends on the position. The function <span class="math inline">\(V(x)\)</span> should then be thought of as the potential with which the particle moves, since the force (field) is the (negative) derivative of the potential function in Newtonian physics. The generator of this diffusion is:</p>
<p><span class="math display">\[
A = \frac{1}{2}\frac{\partial^2}{\partial x^2} - V'(x)\frac{\partial}{\partial x}
\]</span></p>
<p>This diffusion admits an invariant distribution :</p>
<p><span class="math display">\[
f(x) = Ce^{-2V(x)}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> is such that <span class="math inline">\(\int_{\mathbb{R}}f(x)dx = 1\)</span>.</p>
</div>
</section>
<section id="the-feynman-kac-formula" class="level2">
<h2 class="anchored" data-anchor-id="the-feynman-kac-formula">The Feynman-Kac Formula</h2>
<p>We saw in <a href="#exm-heat-equation-and-brownian-motion" class="quarto-xref">Example&nbsp;4</a> that the solution of the heat equation:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}
\]</span></p>
<p>can be represented as an average over Brownian paths. This representation was extended to diffusions in theorem <a href="#thm-backward-equation-with-initial-value" class="quarto-xref">Theorem&nbsp;4</a> where the second derivative in the equation is replaced by the generator of the corresponding diffusion. How robust is this representation? In other words, is it possible to slightly change the PDE and still get a stochastic representation representation for the solution? The answer to this question is yes, when a term of the form <span class="math inline">\(r(x)f(t,x)\)</span> is added to the equation, where <span class="math inline">\(r(x)\)</span> is a well-behaved function of space (for example, piecewise continuous). The stochastic representation of the PDE in this case bears the name <em>Feynman-Kac</em> formula, making a fruitful collaboration between the physicist <a href="https://en.wikipedia.org/wiki/Richard_Feynman">Richard Feynman</a> and the mathematician <a href="https://en.wikipedia.org/wiki/Mark_Kac">Mark Kac</a>. By the way, you pronounce “Kac” as “cats”. His name is Polish. People who immigrated from Poland before him spelled their names as “Katz”. The case when <span class="math inline">\(r(x)\)</span> is linear will be important in the applications to mathematical finance, where it represents the contribution of the interest rate.</p>
<div id="thm-initial-value-problem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 (Initial Value Problem)</strong></span> Let <span class="math inline">\((X_t,t\geq 0)\)</span> be a diffusion in <span class="math inline">\(\mathbb{R}\)</span> with the SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t) dB_t + \mu(X_t)dt
\]</span></p>
<p>Let <span class="math inline">\(g\in C^2(\mathbb{R})\)</span> be such that <span class="math inline">\(g\)</span> is <span class="math inline">\(0\)</span> outside an interval. Then, the solution of the PDE with initial value</p>
<p><span id="eq-initial-value-problem"><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t}(t,x) &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2}(t,x) + \mu(x)\frac{\partial f}{\partial x}(t,x) - r(x)f(x)\\
f(0,x) &amp;= g(x)
\end{align*}
\tag{24}\]</span></span></p>
<p>has the stochastic representation:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}\left[g(X_t)\exp\left(-\int_0^t r(X_s) ds\right)\Bigg| X_0 = x\right]
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>The proof is again based on Ito’s formula. For a fixed <span class="math inline">\(t\)</span>, we consider the process:</p>
<p><span class="math display">\[
M_s = f(t-s, X_s) \exp\left(-\int_0^s r(X_u) du\right), \quad s \leq t
\]</span></p>
<p>Write <span class="math inline">\(Z_s = \exp\left(-\int_0^s r(X_u) du\right)\)</span> and <span class="math inline">\(V_s = f(t-s,X_s)\)</span>. A direct application of Ito’s formula yields:</p>
<p>Let <span class="math inline">\(R_s = -\int_0^s r(X_u) du\)</span>. So, <span class="math inline">\(dR_t = r(X_t) dt\)</span>. <span class="math inline">\((R_t,t\geq 0)\)</span> is a random variable, because <span class="math inline">\(r(X_s)\)</span> depends on how <span class="math inline">\((X_s, s \leq t)\)</span> evolves, it is <em>stochastic</em>, but for very small intervals of time <span class="math inline">\(r(X_s)\)</span> is a constant, and hence the process <span class="math inline">\((R_t,t\geq 0)\)</span> is said to be locally deterministic.</p>
<p><span class="math display">\[
\begin{align*}
Z_s &amp;= e^{-R_s}\\
dZ_s &amp;= -e^{-R_s} dR_s + \frac{1}{2}e^{R_s} (dR_s)^2\\
&amp;= -Z_s r(X_s) ds
\end{align*}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align*}
dV_s &amp;= \frac{\partial}{\partial s}f(t-s, X_s)ds + \frac{\partial}{\partial x}f(t-s, X_s)dX_s + \frac{1}{2}\frac{\partial^2}{\partial x^2}f(t-s,X_s)(dX_s)^2\\
&amp;= -f_s ds + f_x (\sigma(X_s)dB_s + \mu(X_s)ds) + \frac{1}{2}f_{xx} \sigma(X_s)^2 ds \\
&amp;= \sigma(X_s) f_x dB_s + \\
&amp;+ \left\{-f_s + \mu(X_s)f_x + \frac{\sigma(X_s)^2}{2}f_{xx}\right\}ds
\end{align*}
\]</span></p>
<p>Recall that <span class="math inline">\(t\)</span> is fixed here, and we differentiate with respect to <span class="math inline">\(s\)</span> in time. Since <span class="math inline">\(f(t,x)\)</span> is a solution of the PDE, we can write the second equation as:</p>
<p><span class="math display">\[
dV_s = \sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds
\]</span></p>
<p>Now, by Ito’s product rule, we finally have:</p>
<p><span class="math display">\[
\begin{align*}
dM_s &amp;= V_s dZ_s + Z_s dV_s + dZ_s dV_s\\
&amp;= -f(t-s,X_s)Z_s r(X_s) ds + Z_s (\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds) + 0\\
&amp;= \sigma(X_s)Z_s f_x dB_s
\end{align*}
\]</span></p>
<p>This proves that <span class="math inline">\((M_s, s \leq t)\)</span> is a martingale. We conclude that:</p>
<p><span class="math display">\[
\mathbb{E}[M_t] = \mathbb{E}[M_0]
\]</span></p>
<p>Using the definition of <span class="math inline">\(M_t\)</span>, this yields:</p>
<p><span class="math display">\[
\mathbb{E}[M_t] = \mathbb{E}\left[f(0,X_t)\exp\left(-\int_0^t r(X_u) du\right)\right] = \mathbb{E}\left[g(X_t)\exp\left(-\int_0^t r(X_u) du\right)\right] = \mathbb{E}[M_0] = f(t,x)
\]</span></p>
<p>This proves the theorem. <span class="math inline">\(\blacksquare\)</span></p>
<p>As for the backward equation, it is natural to consider the terminal value problem for the same PDE.</p>
<div id="thm-terminal-value-problem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8 (Terminal Value Problem)</strong></span> Let <span class="math inline">\((X_t,t \leq T)\)</span> be a diffusion in <span class="math inline">\(\mathbb{R}\)</span> with the SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t) dB_t + \mu(X_t) dt
\]</span></p>
<p>Let <span class="math inline">\(g\in C^2(\mathbb{R})\)</span> be such that <span class="math inline">\(g\)</span> is <span class="math inline">\(0\)</span> outside an interval. Then, the solution of the PDE with initial value</p>
<p><span class="math display">\[
\begin{align*}
-\frac{\partial f}{\partial t}(t,x) &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2}(t,x) + \mu(x)\frac{\partial f}{\partial x}(t,x) - r(x)f(t,x)\\
f(T,x) &amp;= g(x)
\end{align*}
\]</span></p>
<p>has the stochastic representation :</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}\left[g(X_T)\exp\left(-\int_t^T r(X_u) du\right)\Bigg|X_t = x\right]
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>The proof is similar by considering instead</p>
<p><span class="math display">\[
M_t = f(t,X_t)\exp\left(-\int_0^t r(X_u) du\right)
\]</span></p>
<div id="thm-generalized-feynman-kac" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9 (Generalized version.)</strong></span> Let <span class="math inline">\(V\in C^2(\mathbb{R})\)</span> be the payout function. Then, the solution to the PDE</p>
<p><span class="math display">\[
\begin{align*}
\left(\frac{\partial}{\partial t} + \mu(t,x)\frac{\partial}{\partial x} + \frac{1}{2}\sigma^2(t,x)\frac{\partial^2}{\partial x^2}\right)f = r(t,x)f(t,x) + B(t,x)
\end{align*}
\]</span></p>
<p>with the boundary condition:</p>
<p><span class="math display">\[
f(T,x) = V(x)
\]</span></p>
<p>has the stochastic representation:</p>
<p><span class="math display">\[
f(t,x)=\mathbb{E}_t\left[\exp\left(-\int_t^T r(u,X_u) du\right)V(X_T)\right] - \mathbb{E}_t\left[\int_{t}^T \exp\left(-\int_t^s r(u,X_u) du\right)B(s,X_s)ds\right]
\]</span></p>
<p>where <span class="math inline">\((X_t,t\leq T)\)</span> is a diffusion in <span class="math inline">\(\mathbb{R}\)</span> with the dynamics :</p>
<p><span class="math display">\[
dX_t = \sigma(t,X_t) dB_t + \mu(t,X_t)dt
\]</span></p>
</div>
<p><em>Proof</em></p>
<p>For brevity, I drop the space coordinates in the below derivations.</p>
<p>Define <span class="math inline">\(Z_s = \exp\left(-\int_t^s r_u du\right)\)</span>. Consider the process</p>
<p><span class="math display">\[
Y(s) = Z_s f(s,X_s) - \int_t^s Z_s B_s ds
\]</span></p>
<p>By Ito’s product rule:</p>
<p><span class="math display">\[
\begin{align*}
dY_s &amp;= dZ_s f + Z_s df + dZ_s df - Z_s B_s ds
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(dZ_s df = O(dt dt)\)</span> it can be dropped. We have:</p>
<p><span class="math display">\[
\begin{align*}
dY_s &amp;= -r_s Z_s f ds + Z_s \left(f_s ds + f_x dX_s + \frac{1}{2}f_{xx}(dX_s)^2\right) - Z_s B_s ds\\
&amp;= -r_s Z_s f ds + Z_s \left[f_s ds + f_x (\mu ds + \sigma dW_s) + \frac{1}{2}\sigma^2f_{xx}ds\right] - Z_s B_s ds \\
&amp;= -r_s Z_s f ds + Z_s \left[\left(f_s + \mu f_x  + \frac{1}{2}\sigma^2f_{xx}\right)ds + \sigma f_x dW_s \right]  - Z_s B_s ds
\end{align*}
\]</span></p>
<p>We can substitute the term in the round brackets <span class="math inline">\(\left(f_s + \mu f_x  + \frac{1}{2}\sigma^2f_{xx}\right) = r_s f + B_s\)</span>, since <span class="math inline">\(f\)</span> satisfies the PDE. So, we have:</p>
<p><span class="math display">\[
\begin{align*}
dY_s
&amp;= -r_s Z_s f ds + Z_s \left[\left(r_s f + B_s\right)ds + \sigma f_x dW_s \right]  - Z_s B_s ds\\
&amp;= Z_s \sigma f_x dW_s
\end{align*}
\]</span></p>
<p>So, the process <span class="math inline">\((Y_s,s\leq T)\)</span> is a martingale. Integrating the above equation from <span class="math inline">\(t\)</span> to <span class="math inline">\(T\)</span>, we have:</p>
<p><span class="math display">\[
Y(T) - Y(t) = \int_t^T Z_s \sigma f_x dW_s
\]</span></p>
<p>Upon taking expectations, conditioned on <span class="math inline">\(X_t = x\)</span> and observing that the RHS is an Ito integral, which has zero expectation, it follows that:</p>
<p><span class="math display">\[
\mathbb{E}_t[Y_T|X_t = x] =  \mathbb{E}_t[Y_t|X_t = x]
\]</span></p>
<p>On the right hand side, <span class="math inline">\(Y_t = f(t,X_t) =f(t,x)\)</span>. It follows that:</p>
<p><span class="math display">\[
\begin{align*}
f(t,x) &amp;= \mathbb{E}_t\left[Z_T f(T,X_T) - \int_{t}^T Z_s B_s ds \right]\\
&amp;= \mathbb{E}_t\left[Z_T V(X_T)\right] - \mathbb{E}_t\left[\int_{t}^T Z_s B_s ds \right]
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-shifted-brownian-motion" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1</strong></span> (Shifted Brownian Motion) Let <span class="math inline">\((B_t,t\geq 0)\)</span> be a standard brownian motion. Fix <span class="math inline">\(t &gt; 0\)</span>. Show that the process <span class="math inline">\((W_s,s \geq 0)\)</span> with <span class="math inline">\(W_s = B_{t+s} - B_t\)</span> is a standard brownian motion independent of <span class="math inline">\(\mathcal{F}_t\)</span>.</p>
</div>
<p><em>Solution</em>.</p>
<p>At <span class="math inline">\(s = 0\)</span>, <span class="math inline">\(W(0) = B(t) - B(t) = 0\)</span>.</p>
<p>Consider any arbitrary times <span class="math inline">\(t_1 &lt; t_2\)</span>. We have:</p>
<p><span class="math display">\[\begin{align*}
W(t_2) - W(t_1) &amp;= (B(t + t_2) - B(t)) - (B(t + t_1) - B(t))\\
&amp;= B(t + t_2) - B(t + t_1)
\end{align*}\]</span></p>
<p>Now, <span class="math inline">\(B(t + t_2) - B(t + t_1) \sim \mathcal{N}(0,t_2 - t_1)\)</span>. So, <span class="math inline">\(W(t_2) - W(t_1)\)</span> is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t_2 - t_1\)</span>.</p>
<p>Finally, consider any finite set of times <span class="math inline">\(0=t_0 &lt; t_1 &lt; t_2 &lt; \ldots &lt; t_n = T\)</span>. Then, <span class="math inline">\(t &lt; t + t_1 &lt; t + t_2 &lt; \ldots &lt; t + t_n\)</span>. We have that, <span class="math inline">\(B(t + t_1) - B(t)\)</span>, <span class="math inline">\(B(t + t_2) - B(t + t_1)\)</span>, <span class="math inline">\(B(t + t_3) - B(t + t_2)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(B(t+T) - B(t+t_{n-1})\)</span> are independent random variables. Consequently, <span class="math inline">\(W(t_1) - W(0)\)</span>, <span class="math inline">\(W(t_2) - W(t_1)\)</span>, <span class="math inline">\(W(t_3) - W(t_2)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(W(t_n) - W(t_{n-1})\)</span> are independent random variables. So, <span class="math inline">\((W_s,s\geq 0)\)</span> is a standard brownian motion.</p>
<p>Also, we have:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[W(s)|\mathcal{F}_t] &amp;= \mathbb{E}[B(t + s) - B(t)|\mathcal{F}_t]\\
&amp; \{ B(t+s) - B(t) \perp \mathcal{F}_t \}\\
&amp;= \mathbb{E}[B(t + s) - B(t)]\\
&amp;= \mathbb{E}[W(s)]
\end{align*}\]</span></p>
<p>Thus, <span class="math inline">\(W(s)\)</span> is independent of <span class="math inline">\(\mathcal{F}_t\)</span>, it does not depend upon the information available upto time <span class="math inline">\(t\)</span>.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "The Markov Property"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Quasar"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-07-12"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Stochastic Calculus]      </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "image.jpg"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Markov Property for Diffusions</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>Let's start by exhibiting the Markov property of Brownian motion. To see this, consider $(\mathcal{F}_t,t\geq 0)$, the natural filtration of the Brownian motion $(B_t,t\geq 0)$. Consider $g(B_t)$ for some time $t$ and bounded function $g$. (For example, $g$ could be an indicator function.) Consider also a random variable $W$ that is $\mathcal{F}_s$ measurable for $s &lt; t$. (For example, $W$ could be $B_s$ or $1_{B_s &gt; 0}$.) Let's compute $\mathbb{E}<span class="co">[</span><span class="ot">g(B_t)W</span><span class="co">]</span>$.</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(B_t)W</span><span class="co">]</span> &amp;= \mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}[Wg(B_t - B_s + B_s)|\mathcal{F}_s]</span><span class="co">]</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>The random variable $(B_t - B_s)$ follows a $\mathcal{N}(0,t-s)$ distribution. By LOTUS,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(B_t)W</span><span class="co">]</span> </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{\mathbb{R}} \mathbb{E}<span class="co">[</span><span class="ot">W g(y + B_s)|\mathcal{F_s}</span><span class="co">]</span>f_{(B_t - B_s)|B_s}(y) dy<span class="sc">\\</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>&amp;= <span class="sc">\{</span>\text{ Using the fact that }B_t - B_s \perp B_s<span class="sc">\}\\</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{\mathbb{R}} \mathbb{E}<span class="co">[</span><span class="ot">W g(y + B_s)</span><span class="co">]</span>f_{(B_t - B_s)}(y)dy<span class="sc">\\</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{\mathbb{R}} \mathbb{E}<span class="co">[</span><span class="ot">W g(y + B_s)</span><span class="co">]</span>\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>By Fubini's theorem, the integral and the expectation operator can be interchanged, and since $W$ is $\mathcal{F}_s$ measurable, it follows from the definition of conditional expectations that:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">Wg(B_t)</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">W\mathbb{E}[g(B_t)|\mathcal{F}_s]</span><span class="co">]</span> = \mathbb{E}\left<span class="co">[</span><span class="ot">W\int_{\mathbb{R}}g(y + B_s)</span><span class="co">]</span>\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy\right]</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>It follows that:</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(B_t)|\mathcal{F}_s</span><span class="co">]</span> = \int_{\mathbb{R}}g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>$${#eq-conditional-expectation-of-function-brownian-motion}</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>We make two important observations. First, the right hand side is a function of $s,t$ and $B_s$ only (and not of the Brownian motion before time s). In particular, we have:</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(B_t)|\mathcal{F}_s</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">g(B_t)|B_s</span><span class="co">]</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>This holds for any bounded function $g$. In particular, it holds for all indicator functions. This implies that the conditional distribution of $B_t$ given $\mathcal{F}_s$ depends solely on $B_s$, and not on other values before time $s$. Second, the right-hand side is *time-homogenous* in the sense that it depends on the time difference $t-s$. </span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>We have just shown that Brownian motion is a *time-homogenous Markov process*. </span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>::: {#def-markov-process}</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="fu">### Markov process.</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>Consider a stochastic process $(X_t,t\geq 0)$ and its natural filtration $(\mathcal{F}_t,t\geq 0)$. It is said to be a *Markov process* if and only if for any (bounded) function $g: \mathbb{R} \to \mathbb{R}$, we have:</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(X_t) | \mathcal{F}_s</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">g(X_t) | X_s</span><span class="co">]</span>, \quad \forall t \geq 0, \forall s \leq t</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>$$ {#eq-markov-process}</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>This implies that $\mathbb{E}<span class="co">[</span><span class="ot">g(X_t)|\mathcal{F}_s</span><span class="co">]</span>$ is an explicit function of $s$, $t$ and $X_s$. It is said to be *time-homogenous*, if it is a function of $t-s$ and $X_s$. Since the above holds for all bounded $g$, the conditional distribution of $X_t$ given $\mathcal{F}_s$ is the same as the conditional distribution of $X_t$ given $X_s$. </span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>One way to compute the conditional distribution of $X_t$ given $\mathcal{F}_s$ is to compute the conditional MGF given $\mathcal{F}_s$, that is:</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">e^{a X_t}|\mathcal{F}_s</span><span class="co">]</span>, \quad a \geq 0</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>$$ {#eq-conditional-mgf-of-xt}</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>The process would be Markov, if the conditional MGF is an explicit function of $s$, $t$ and $X_s$. </span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>::: {#exm-brownian-motion-is-markov}</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>(Brownian Motion is Markov) Let $(B_t,t\geq 0)$ be a standard brownian motion. Our claim is that the brownian motion is a markov process.</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>We have:</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">e^{a B_t}|\mathcal{F}_s</span><span class="co">]</span> &amp;= \mathbb{E}<span class="co">[</span><span class="ot">e^{a (B_t - B_s + B_s)}|\mathcal{F}_s</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>&amp; <span class="sc">\{</span> \text{ since }B_s \text{ is }\mathcal{F}_s-\text{ measurable }<span class="sc">\}\\</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>&amp;= e^{a B_s} \mathbb{E}<span class="co">[</span><span class="ot">e^{a (B_t - B_s)}|\mathcal{F}_s</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>&amp; <span class="sc">\{</span> \text{ since }B_t - B_s \perp \mathcal{F}_s <span class="sc">\}\\</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>&amp;= e^{a B_s} \mathbb{E}<span class="co">[</span><span class="ot">e^{a (B_t - B_s)}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>&amp;= e^{a B_s} e^{\frac{1}{2}a^2(t-s)}</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>This closes the proof. $\blacksquare$</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>An equivalent (but more symmetric) way to express the Markov property is to say that *the future of the process is independent of the past, when conditioned on the present*. Concretely, this means that for any $r &lt; s&lt; t$, we have that $X_t$ is independent of $X_r$, when we condition on $X_s$.</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>The conditional distribution of $X_t$ given $X_s$ is well described using *transition probabilities*. We will more interested in a case well these probabilities admit a density $f_{X_t|X_s=x}(y)$. More precisely, for such a Markov process, we have:</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(X_t)|X_s = x</span><span class="co">]</span> &amp;= \int_{\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy<span class="sc">\\</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>&amp;=\int_{\mathbb{R}} g(y) p(y,t|x,s) dy</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>Here, we explicitly write the left-hand side as a function of space, that is, the position $X_s$, by fixing $X_s = x$. In words, the *transition probability density* $p(y,t|x,s)$ represents the probability density that starting from $X_s = x$ at time $s$, the process ends up at $X_t = y$ at time $t &gt; s$. If the process is time-homogenous, this only depends on the time difference $(t-s)$ and we write $p(y,t|x,s)$. From @eq-conditional-expectation-of-function-brownian-motion, we can write:</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(B_t)|B_s = x</span><span class="co">]</span> = \int_{\mathbb{R}} g(u + x) \frac{e^{-\frac{u^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} du</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>In the above expression, the random variable $B_t - B_s$ takes some value $u \in \mathbb{R}$ and $B_s = x$ is fixed. Then, $B_t$ takes the value $u + x$. Let $y = u + x$. Then, $u = y - x$. Consequently, we may write:</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(B_t)|B_s = x</span><span class="co">]</span> = \int_{\mathbb{R}} g(y) \frac{e^{-\frac{(y-x)^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} dy</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>So, the transition density function for standard Brownian motion is:</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>p(y,t|x,0)= \frac{e^{-\frac{(y-x)^2}{2s}}}{\sqrt{2\pi s}}, \quad s&gt;0, x,y\in\mathbb{R}</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>$$ {#eq-brownian-motion-transition-density-function}</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>This function is sometimes called the *heat kernel*, as it relates to the *heat equation*. </span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>The Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process $(X_t,t\geq 0)$ at different times. Consider the functions $f = \mathbf{1}_A$ and $g = \mathbf{1}_B$ from $\mathbb{R} \to \mathbb{R}$, where $A$ and $B$ are two intervals in $\mathbb{R}$. Let's compute $\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) = \mathbb{E}[\mathbf{1}_{A} \mathbf{1}_{B}] = \mathbb{E}[f(X_{t_1}) g(X_{t_2})]$ for $t_1 &lt; t_2$. By the properties of conditional expectation and the Markov property, we have:</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">f(X_{t_1})g(X_{t_2})</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">f(X_{t_1})\mathbb{E}[g(X_{t_2})|\mathcal{F}_{t_1}]</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">f(X_{t_1})\mathbb{E}[g(X_{t_2})|X_{t_1}]</span><span class="co">]</span></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>Assuming that the process is time-homogenous and admits a transition density $p(y,t|x,0)$ as for Brownian motion, this becomes:</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &amp;= \int_{\mathbb{R}} f(x_1) \left(\int_{\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \right) p(x_1,t_1|x_0,0) dx_1<span class="sc">\\</span></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{A} \left(\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \right) p(x_1,t_1|x_0,0) dx_1</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>This easily generalizes to any finite-dimensional distribution of $(X_t, t\geq 0)$.</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>::: {#exm-markov-versus-martingale}</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>(Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift $(X_t, t \geq 0)$, where $X_t = \sigma B_t + \mu t$. Conversely, take $Y_t = \int_0^t X_s dB_s$, where $X_s = \int_0^s B_u dB_u$. The integrand $X_s$ depends on whole Brownian motion path upto time $s$ and not just on $B_s$. </span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>::: {#nte-functions-of-markov .callout-tip}</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="fu">### Functions of Markov Processes</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>It might be tempting to think that if $(X_t,t\geq 0)$ is a Markov process, then the process defined by $Y_t = f(X_t)$ for some reasonable function $f$ is also Markov. Indeed, one could hope to write for an arbitrary bounded function $g$:</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(Y_t)|\mathcal{F}_s</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">g(f(X_t))|\mathcal{F}_s</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">g(f(X_t))|\mathcal{X}_s</span><span class="co">]</span> </span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>$$ {#eq-functions-of-markov-process}</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>by using the Markov property of $(X_t,t\geq 0)$. The flaw in this reasoning is that the Markov property should hold for the natural fitration $(\mathcal{F}_t^Y,t\geq 0)$ of the process $(Y_t,t\geq 0)$ and not the one of $(X_t,t\geq 0)$, $(\mathcal{F}_t^X,t\geq 0)$. It might be that the filtration $(\mathcal{F}_t^Y,t\geq 0)$ has less information that $(\mathcal{F}_t^X,t\geq 0)$, especially, if the function $f$ is not one-to-one. For example, if $f(x)=x^2$, then $\mathcal{F}_t^Y$ has less information than $\mathcal{F}_t^X$ as we cannot recover the sign of $X_t$ knowing $Y_t$. In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when $f$ is not one-to-one. </span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>It turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>::: {#thm-diffusions-are-markov-processes}</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a><span class="fu">### Diffusions are Markov processes. </span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>Let $(B_t,t\geq 0)$ be a standard Brownian motion. Let $\mu : \mathbb{R} \to \mathbb{R}$ and $\sigma: \mathbb{R} \to \mathbb{R}$ be differentiable functions with bounded derivatives on $<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>$. Then, the diffusion with the SDE </span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>dX_t = \mu(X_t) dt + \sigma(X_t)dB_t, \quad X_0 = x_0</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>defines a time-homogenous markov process on $<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>$. </span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>An analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments. </span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>By the <span class="co">[</span><span class="ot">existence and uniqueness theorem</span><span class="co">](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes)</span>, this stochastic initial value problem(SIVP) defines a unique continous adapted process $(X_t,t\leq T)$. Let $(\mathcal{F}_t^X,t\geq 0)$ be the natural filtration of $(X_t,t\leq T)$. For a fixed $t &gt; 0$, consider the process $W_s = B_{t+s} - B_t, s \geq 0$. Let $(\mathcal{F}_t,t \geq 0)$ be the natural filtration of $(B_t,t \geq 0)$. It turns out that the process $(W_s,s \geq 0)$ is a standard brownian motion independent of $\mathcal{F}_t$ (@exr-shifted-brownian-motion). For $s \geq 0$, we consider the SDE:</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>dY_s = \mu (Y_s) ds + \sigma(Y_s) dW_s, \quad Y_0 = X_t</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>Again by the <span class="co">[</span><span class="ot">existence and uniqueness theorem</span><span class="co">](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes)</span>, there exists a unique solution to the SIVP that is adapted to the natural filtration of $W$. Note that, the shifted process $(X_{t+s},s\geq 0)$ is *the* solution to this SIVP since:</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>X_{t+s} &amp;= X_{t} + \int_{t}^{t+s}\mu(X_u) du + \int_{t}^{t+s}\sigma(X_u) dB_u</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>Perform a change of variable $v = u - t$. Then, $dv = du$, $dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v$. So,</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>X_{t+s} &amp;= X_{t} + \int_{0}^{s}\mu(X_{t+v}) dv + \int_{0}^{s}\sigma(X_{t+v}) dW_v</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>Let $Y_v= X_{t+v}$, $Y_0 = X_t$. Then,</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>Y_s &amp;= Y_0 + \int_{0}^{s}\mu(Y_v) dv + \int_{0}^{s}\sigma(Y_v) dW_v</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>Thus, we conclude that for any interval $A$:</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(X_{t+s} \in A|\mathcal{F}_t^X) = \mathbb{P}(Y_s \in A | \mathcal{F}_t^X)</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>But, since $(Y_s,s \geq 0)$ depends on $\mathcal{F}_t^X$ only through $X_t$ (because $(W_s,s \geq 0)$ is independent of $\mathcal{F}_t$), we conclude that $\mathbb{P}(X_{t+s} \in A|\mathcal{F}_t^X) = \mathbb{P}(X_{t+s} \in A|X_t)$, so $(X_t,t \geq 0)$ is a time-homogenous markov process. $\blacksquare$</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Strong Markov Property</span></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>The Doob's Optional Stopping theorem extended some properties of martingales to stopping times. The Markov property can also be extended to stopping times for certain processes. These processes are called *strong Markov processes*. </span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>We know, that the sigma-algebra $\mathcal{F}_t$ represents the set of all observable events upto time $t$. What is the sigma-algebra of observable events at a random stopping time $\tau$? </span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>::: {#def-sigma-algebra-of-the-past}</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="fu">### $\sigma$-algebra of $\tau$-past</span></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>Let $(\Omega,\mathcal{F},<span class="sc">\{</span>\mathcal{F}_t\}_{t\geq 0},\mathbb{P})$ be a filtered probability space. The sigma-algebra at the stopping time $\tau$ is then:</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>\mathcal{F}_{\tau} = \{A \in \mathcal{F}_\infty : A \cap <span class="sc">\{</span>\tau \leq t<span class="sc">\}</span> \in \mathcal{F}_t, \forall t \geq 0 <span class="sc">\}</span></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>$$ {#eq-sigma-algebra-of-the-past}</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>In words, an event $A$ is in $\mathcal{F}_\tau$, if we can determine if $A$ and $\{\tau \leq t\}$ both occurred or not based on the information $\mathcal{F}_t$ known at any arbitrary time $t$. You should be able to tell the value of the random variable $\mathbf{1}_A \cdot \mathbf{1}_{<span class="sc">\{</span>\tau \leq t<span class="sc">\}</span>}$ given $\mathcal{F}_t$ for any arbitrary time $t \geq 0$.</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>For example, if $\tau &lt; \infty$, the event $<span class="sc">\{</span>B_\tau &gt; 0<span class="sc">\}</span>$ is in $\mathcal{F}_\tau$. However, the event $\{B_1 &gt; 0\}$ is not in $\mathcal{F}_\tau$ in general, since $A \cap \{\tau \leq t\}$ is not in $\mathcal{F}_t$ for $t &lt; 1$. Roughly speaking, a random variable that is $\mathcal{F}_\tau$-measurable should be thought of as an explicit function of $X_\tau$. With this new object, we are ready to define the *strong markov property*.</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>::: {#def-strong-markov-property}</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a><span class="fu">### Strong Markov Property</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>Let $(X_t,t\geq 0)$ be a stochastic process and let $(\mathcal{F}_t,t\geq 0)$ be its natural filtration. The process $(X_t,t\geq 0)$ is said to be *strong markov* if for any stopping time $\tau$ for the filtration of the process and any bounded function $g$:</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(X_{t+\tau})|\mathcal{F}_\tau</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">g(X_{t+\tau})|X_\tau</span><span class="co">]</span></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>This means that $X_{t+\tau}$ depends on $\mathcal{F}_\tau$ solely through $X_\tau$ (whenever $\tau &lt; \infty$). It turns out that Brownian motion is a strong markov process. In fact a stronger statement holds which generalizes @exr-shifted-brownian-motion. </span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>::: {#thm-shifted-brownian-motion-about-a-stopping-time}</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>Let $\tau$ be a stopping time for the filtration of the Brownian motion $(B_t,t\geq 0)$ such that $\tau &lt; \infty$. Then, the process:</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>(B_{t+\tau} - B_{\tau},t\geq 0)</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>is a standard brownian motion independent of $\mathcal{F}_\tau$.</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>::: {#exm-brownian-motion-is-strong-markov}</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>(Brownian motion is strong Markov) To see this, let's compute the conditional MGF as in @eq-conditional-mgf-of-xt. We have:</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">e^{aB_{t+\tau}}|\mathcal{F}_\tau</span><span class="co">]</span> &amp;= \mathbb{E}<span class="co">[</span><span class="ot">e^{a(B_{t+\tau} - B_\tau + B_\tau)}|\mathcal{F}_\tau</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>&amp;= e^{aB_\tau} \mathbb{E}<span class="co">[</span><span class="ot">e^{a(B_{t+\tau} - B_\tau)}|\mathcal{F}_\tau</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>&amp; <span class="sc">\{</span> B_\tau \text{ is }\mathcal{F}_\tau-\text{measurable }<span class="sc">\}\\</span></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>&amp;= e^{aB_\tau}\mathbb{E}<span class="co">[</span><span class="ot">e^{a(B_{t+\tau} - B_\tau)}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>&amp; <span class="sc">\{</span> (B_{t+\tau} - B_\tau) \perp \mathcal{F}_\tau<span class="sc">\}\\</span></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>&amp;= e^{aB_\tau}e^{\frac{1}{2}a^2 t}<span class="sc">\\</span></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>Thus, the conditional MGF is an explicit function of $B_\tau$ and $t$. This proves the proposition. $\blacksquare$</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>*Proof* of @thm-shifted-brownian-motion-about-a-stopping-time.</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>We first consider for fixed $n$ the discrete valued stopping time:</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>\tau_n = \frac{k + 1}{2^n}, \quad \text{ if } \frac{k}{2^n} \leq \tau &lt; \frac{k+1}{2^n}, k\in \mathbb{N}</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>In other words, if $\tau$ occurs in the interval $[\frac{k}{2^n},\frac{k+1}{2^n})$, we stop at the next dyadic $\frac{k+1}{2^n}$. By construction $\tau_n$ depends only on the process in the past. Consider the process $W_t = B_{t + \tau_n} - B_{\tau_n}, t \geq 0$. We show it is a standard brownian motion independent of $\tau_n$. This is feasible as we can decompose over the discrete values taken by $\tau_n$. More, precisely, take $E \in \mathcal{F}_{\tau_n}$, and some generic event $<span class="sc">\{</span>W_t \in A<span class="sc">\}</span>$ for the process $W$. Then, by decomposing over the values of $\tau_n$, we have:</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(<span class="sc">\{</span>W_t \in A<span class="sc">\}</span> \cap E) &amp;= \sum_{k=0}^\infty \mathbb{P}\left(<span class="sc">\{</span>W_t \in A<span class="sc">\}</span> \cap E \cap <span class="sc">\{</span>\tau_n = \frac{k}{2^n}<span class="sc">\}</span>\right)<span class="sc">\\</span></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{k=0}^\infty \mathbb{P}\left(<span class="sc">\{</span>(B_{t+k/2^n} - B_{k/2^n}) \in A<span class="sc">\}</span> \cap E \cap <span class="sc">\{</span>\tau_n = \frac{k}{2^n}<span class="sc">\}</span>\right)<span class="sc">\\</span></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{k=0}^\infty \mathbb{P}\left(<span class="sc">\{</span>(B_{t+k/2^n} - B_{k/2^n}) \in A<span class="sc">\}</span>\right) \times \mathbb{P}\left( E \cap <span class="sc">\{</span>\tau_n = \frac{k}{2^n}<span class="sc">\}</span>\right)</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>since $(B_{t+k/2^n} - B_{k/2^n})$ is independent of $\mathcal{F}_{k/2^n}$ by @exr-shifted-brownian-motion and since $E \cap \{\tau_n = \frac{k}{2^n}\} \in \mathcal{F}_{k/2^n}$ by definition of stopping time. But, given $\{\tau_n = k/2^n\}$, the event $\{(B_{t+k/2^n} - B_{k/2^n}) \in A\}$ is the same as $\{B_t \in A\} = \{W_t \in A\}$, since this process is now a standard brownian motion. Thus, $\mathbb{P}\{(B_{t+k/2^n} - B_{k/2^n}) \in A<span class="sc">\}</span> = \mathbb{P}<span class="sc">\{</span>B_t \in A<span class="sc">\}</span> = \mathbb{P}<span class="sc">\{</span>W_t \in A<span class="sc">\}</span>$, dropping the dependence on $k$. The sum over $k$ then yields:</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>\mathbb{P}\left(<span class="sc">\{</span>W_t \in A<span class="sc">\}</span>\cap E\right) = \mathbb{P}(W_t \in A) \mathbb{P}(E)</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>as claimed. The extension to $\tau$ is done by using continuity of paths. We have:</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>\lim_{n \to \infty} B_{t + \tau_n} - B_{\tau_n} = B_{t+\tau} - B_{\tau} \text{ almost surely}</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>Note, that this only uses right continuity! Moreover, this implies that $B_{t+\tau} - B_\tau$ is independent of $\mathcal{F}_{\tau_n}$ for all $n$. Again by (right-)continuity this extends to independence of $\mathcal{F}_\tau$. The limiting distribution of the process is obtained by looking at the finite dimensional distributions of the increments of $B_{t+\tau_n} - B_{\tau_n}$ for a finite number of $t$'s and taking the limit as above. $\blacksquare$</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>Most diffusions also enjoy the strong markov property, as long as the functions $\sigma$ and $\mu$ encoding the volatility and drift are nice enough. This is the case for the diffusions we have considered. </span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>::: {#thm-most-diffusions-are-strong-markov}</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a><span class="fu">### Most diffusions are strong markov</span></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>Consider a diffusion $(X_t,t\leq T)$ as as in @thm-diffusions-are-markov-processes. Then, the diffusion has strong markov property. </span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>The proof follows the line of the one of @thm-diffusions-are-markov-processes</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>Consider the time-homogenous diffusion:</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>dX_t = \mu(X_t)dt + \sigma(X_t)dB_t</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>By the <span class="co">[</span><span class="ot">existence and uniqueness theorem</span><span class="co">](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes)</span>, this SIVP defines a unique continuous adapted process $(X_t,t \geq 0)$. Let $\mathfrak{F}=(\mathcal{F}_t^X,t \geq 0)$ be the natural filtration of $(X_t, t\leq T)$. Let $\tau$ be a stopping time for the filtration $\mathfrak{F}$ and consider the process $W_t = B_{t+\tau} - B_\tau$. From @thm-shifted-brownian-motion-about-a-stopping-time, we know that the process $(W_t,t\geq 0)$ is a standard brownian motion independent $\mathcal{F}_\tau$. For $s \geq 0$, we consider the SDE:</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>dY_s = \mu(Y_s)ds + \sigma(Y_s)dW_s, \quad Y_0 = X_\tau</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>$$ {#eq-diffusion-of-Y}</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>Again by the <span class="co">[</span><span class="ot">existence and uniqueness theorem</span><span class="co">](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes)</span>, there exists a unique solution to the SIVP that is adapted to the natural filtration of $W$. We claim that $(X_{s+\tau},s \geq 0)$ is the solution to this equation, since:</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>X_{s+\tau} = X_\tau + \int_\tau^{s+\tau} \mu(X_u)du + \int_{\tau}^{s+\tau} \sigma(X_u)dB_u</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>Perform a change of variable $v = u - \tau$. Then, the limits of integration bare, $v = 0$ and $v = s$. And $dv = du$. </span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>$dB_u  \approx B_{u_2} - B_{u_1} = B(v_1 + \tau) - B(v_2 + \tau) = W(v_2) - W(v_1) =dW_v$. </span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>X_{s+\tau} = X_\tau + \int_0^{s} \mu(X_{v+\tau})dv + \int_{0}^{s} \sigma(X_{v+\tau})dW_v</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>If we let $Y_0 = X_\tau$, $Y_v = X_{v+\tau}$, we recover the dynamics of $(Y_v,v \geq 0)$ in @eq-diffusion-of-Y. So, $(X_{s+\tau},s\geq 0)$ is the solution to the SIVP in @eq-diffusion-of-Y. Thus, we conclude for any interval $A$:</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(X_{s+\tau} \in A | \mathcal{F}_\tau^X) = \mathbb{P}(Y_v \in A| \mathcal{F}_\tau^X)</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>But, since $(Y_v,v\geq 0)$ depends on $\mathcal{F}_\tau^X$ only through $X_\tau$, we conclude that $\mathbb{P}(X_{s + \tau} \in A | \mathcal{F}_\tau^X) = \mathbb{P}(X_{s + \tau} \in A| X_\tau)$. Consequently, $(X_t,t \geq 0)$ is a strong-markov process. $\blacksquare$</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>::: {#nte-extension-of-optional-sampling .callout-tip}</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a><span class="fu">### Extension of optional sampling</span></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a>Consider a continuous martingale $(M_t, t\leq T)$ for a filtration $(\mathcal{F}_t, t\geq 0)$ and a stopping time $\tau$ for the same filtration. Suppose we would like to compute for some $T$:</span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">M_T \mathbf{1}_{\{\tau \leq T\}}</span><span class="co">]</span></span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>It would be tempting to condition on $\mathcal{F}_\tau$ and write $\mathbb{E}[M_T |\mathcal{F}_\tau] = M_\tau$ on the event $<span class="sc">\{</span>\tau \leq T<span class="sc">\}</span>$. We would then conclude that:</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">M_T 1_{\{\tau \leq T\}}</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">1_{\{\tau \leq T\}} \mathbb{E}[M_T|\mathcal{F}_\tau] </span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">M_\tau 1_{\{\tau \leq T\}}</span><span class="co">]</span></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>In some sense, we have extended the martingale property to stopping times. This property can be proved under reasonable assumptions on $(M_t,t\leq T)$ (for example, if it is positive). Indeed, it suffices to approximate $\tau$ by discrete valued stopping time $\tau_n$ as in the proof of @thm-shifted-brownian-motion-about-a-stopping-time. One can then apply martingale property at a fixed time. </span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Heat Equation</span></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a>We look at more detail on how PDEs come up when computing quantities related to Markov processes.</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>::: {#exm-heat-equation-and-brownian-motion}</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>(Heat Equation and Brownian motion) Let $f(t,x)$ be a function of time and space. The heat equation in $1+1$-dimension (one dimension of time, one dimension of space) is the PDE:</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} &amp;= \frac{1}{2}\frac{\partial^2 f}{\partial x^2}</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a>$$ {#eq-heat-equation-in-2d}</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a>In $1+d$ (one dimension of time, $d$ dimensions of space), the heat equation is:</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} &amp;= \frac{1}{2}\nabla^2 f</span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>$$ {#eq-heat-equation-in-d-plus-one-dims}</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>where $\nabla^2$ is the Laplacian operator. </span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a>The solutions to these PDEs can be expressed as an expectation over Brownian motion paths. </span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a>Let $(X_t,t \geq 0)$ be a brownian motion and let $f(t,x)$ represent the PDF of $X_t$. Then (as we will see) satisfies the PDE:</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a>\partial_{t}f(t,x) = \frac{1}{2}\partial_x^2 f(t,x)</span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a>Suppose the Brownian motion is at $y$ in the present. Suppose *the present* is a specific time $t_1$. The joint probability density that $<span class="sc">\{</span>X_{t}=x,X_{t_1}=y<span class="sc">\}</span>$ is:</span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a>p(x, t, y, t_1) = f(y,t_1) \cdot p(x,t|y,t_1)</span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>where $p(x,t|y,t_1)$ is the transition probability density function. </span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>Since $f(t,x)$ is the density of $X_t$ and since we have a formula for the joint density of $X_{t_1}$ and $X_t$, we can view $f(t,x)$ as the marginal of the joint density. You find the marginal density by integrating out the variables you are not interested in, $X_{t_1}$ in this case. In the abstract, this is:</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>f(t,x) &amp;= \int_{-\infty}^{\infty} p(x, t, y, t_1)dy<span class="sc">\\</span></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{-\infty}^{\infty} f(t_1,y) p(x,t_2 | y,t_1) dy</span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>Our claim is that $f$ indeed satisfies the PDE (@eq-heat-equation-in-2d). </span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a>The gaussian transition probability density function (heat kernel) $p(x,t|y,0)$ is given by:</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a>p(x,t|y,0) = \frac{1}{\sqrt{2\pi t}}\exp\left(-\frac{(x-y)^2}{2t}\right)</span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>Differentiating $p$ with respect to $t$, we have:</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial t} p(x,t|y,0) &amp;= \frac{\sqrt{2\pi t} \exp\left(-\frac{(x-y)^2}{2t}\right) \frac{\partial}{\partial t}\left(-\frac{(x-y)^2}{2t}\right) - \exp\left(-\frac{(x-y)^2}{2t}\right)\sqrt{2\pi}\left(\frac{1}{2\sqrt{t}}\right)}{2\pi t}<span class="sc">\\</span></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>&amp;=\sqrt{2\pi}\exp\left(-\frac{(x-y)^2}{2t}\right) \frac{\frac{(x-y)^2}{2t^{3/2}} - \frac{t}{2t^{3/2}}}{2\pi t}<span class="sc">\\</span></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>&amp;= \exp\left(-\frac{(x-y)^2}{2t}\right) \frac{(x-y)^2 - t}{\sqrt{2\pi} (2t^{5/2}) }</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>$$ {#eq-partial-with-respect-to-time}</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>Differentiating $p$ with respect to $x$, we have:</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>\frac{\partial }{\partial x} p(x,t|y,0) &amp;= \frac{1}{\sqrt{2\pi t}}\exp\left<span class="co">[</span><span class="ot">-\frac{(x-y)^2}{2t}\right</span><span class="co">]</span>\frac{\partial}{\partial x}\left(-\frac{(x-y)^2}{2t}\right)<span class="sc">\\</span></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{\sqrt{2\pi t}} \cdot \left(-\frac{1}{\cancel{2} t}\right) \exp\left<span class="co">[</span><span class="ot">-\frac{(x-y)^2}{2t}\right</span><span class="co">]</span> \cdot \cancel{2}(x-y)<span class="sc">\\</span></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>&amp;= -\frac{1}{t\sqrt{2\pi t}} (x-y)\exp\left<span class="co">[</span><span class="ot">-\frac{(x-y)^2}{2t}\right</span><span class="co">]</span></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>$$ {#eq-first-derivative-with-respect-to-space}</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>Differentiating again with respect to space, we have:</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a>\frac{\partial^2}{\partial x^2} p(x,t|y,0) &amp;= -\frac{1}{t\sqrt{2\pi t}} \left<span class="co">[</span><span class="ot">\exp\left\{-\frac{(x-y)^2}{2}\right\} + (x-y)\exp\left\{-\frac{(x-y)^2}{2}\right\}\left(-\frac{2(x-y)}{2y}\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a>&amp;=-\frac{1}{t\sqrt{2\pi t}}\exp\left<span class="sc">\{</span>-\frac{(x-y)^2}{2}\right<span class="sc">\}</span> \left<span class="co">[</span><span class="ot">1 - \frac{(x-y)^2}{t}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a>&amp;=\frac{1}{t\sqrt{2\pi t}}\exp\left<span class="sc">\{</span>-\frac{(x-y)^2}{2}\right<span class="sc">\}</span> \left<span class="co">[</span><span class="ot">\frac{(x-y)^2 - t}{t}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a>&amp;=\frac{1}{\sqrt{2\pi}}\exp\left<span class="sc">\{</span>-\frac{(x-y)^2}{2}\right<span class="sc">\}</span> \cdot \frac{(x-y)^2 - t}{t^{5/2}}</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a>$$ {#eq-second-derivative-with-respect-to-space}</span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>From @eq-partial-with-respect-to-time and @eq-second-derivative-with-respect-to-space, it follows that:</span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial t} p(x,t|y,0) = \frac{1}{2}\frac{\partial ^2}{\partial x^2} p(x,t|y,0)</span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a>Thus,</span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial t} \int_{-\infty}^\infty g(y) p(x,t|y,0)dy &amp;= \frac{1}{2}\frac{\partial^2}{\partial x^2} \int_{-\infty}^\infty g(y) p(x,t|y,0)dy <span class="sc">\\</span></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a>\frac{\partial }{\partial t}f(t,x) &amp;= \frac{1}{2}\frac{\partial^2 }{\partial x^2} f(t,x)</span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a><span class="fu">## Solution to the heat PDE as an expectation over Brownian-motion paths</span></span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>Consider again the heat PDE:</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} = \frac{1}{2}\frac{\partial^2 f }{\partial x^2}</span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a>with the initial condition </span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a>f(0,x) = g(x)</span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a><span class="fu">## Robert Brown's erratic motion of pollen</span></span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>In the summer of 1827, the Scottish botanist Robert Brown observed that microscopic pollen grains suspended in water move in an erratic, highly irregular, zigzag pattern. It was only in 1905, that Albert Einstein could provide a satisfactory explanation of Brownian motion. He asserted that Brownian motion originates in the continual bombardment of the pollen grains by the molecules of the surrounding water. As a result of continual collisions, the particles themselves had the same kinetic energy as the water molecules. Thus, he showed that Brownian motion provided a solution (in a certain sense) to Fourier's famous heat equation</span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a>\frac{\partial u}{\partial t}(t,x) = \kappa \frac{\partial^2 u}{\partial x^2}(t,x)</span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a><span class="fu">### Albert Einstein's proof of the existence of Brownian motion</span></span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a>We now summarize Einstein's original 1905 argument. Let's say that we are interested in the motion along the horizontal $x$-axis. Let's say we drop brownian particles in a liquid. Let $f(t,x)$ represent the number of particles per unit volume (density) at position $x$ at time $t$. So, the number of particles in a small interval $I=<span class="co">[</span><span class="ot">x,x+dx</span><span class="co">]</span>$ of width $dx$ will be $f(t,x)dx$. </span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>Now, as time progresses, the number of particles in this interval $I$ will change. The brownian particles will zig-zag upon bombardment by the molecules of the liquid. Some particles will move out of the interval $I$, while other particles will move in. </span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a>Let's consider a timestep of length $\tau$. Einstein's probabilistic approach was to model the distance travelled by the particles or displacement of the particles as a random variable $\Delta$. To determine how many particles end up in the interval $I$, we start with the area to the right of the interval $I$.</span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a>The density of particles at $x+\Delta$ is $f(t,x+\Delta)$; the number of particles in a small interval of length $dx$ is $f(t,x+\Delta)dx$. If we represent the probability density of the displacement by $\phi(\Delta)$, then the number of particles at $x+\Delta$ that will move to $x$ will be $dx \cdot f(t,x+\Delta)\phi(\Delta)$. We can apply the same logic to the left hand side. The number of particles at $x - \Delta$ that will move to $x$ will be $dx \cdot f(t,x-\Delta)\phi(-\Delta)$. Assume that $\phi(\Delta) = \phi(-\Delta)$.</span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a>Now, if we integrate these movements across the real line, then we get the number of particles at $x$ at a short time later $t + \tau$. </span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a>f(t+ \tau,x) dx = dx \int_{-\infty}^{\infty} f(t,x+\Delta) \phi(\Delta) d\Delta</span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a>Now, we can get rid of $dx$.</span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a>f(t+ \tau,x) = \int_{-\infty}^{\infty} f(t,x+\Delta) \phi(\Delta) d\Delta</span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a>$$ {#eq-expression-for-density-at-later-time}</span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a>The Taylor's series expansion of $f(t+\tau,x)$ centered at $t$ (holding $x$ constant) is:</span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a>f(t + \tau,x) = f(t,x) + \frac{\partial f}{\partial t}\tau + O(\tau^2)</span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a>The Taylor's series expansion of $f(t,x+\Delta)$ centered at $x$ (holding $t$ constant) is:</span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a>f(t,x+\Delta) = f(t,x) + \frac{\partial f}{\partial x}\Delta + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\Delta^2 + O(\Delta^3)</span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a>We can now substitute these into @eq-expression-for-density-at-later-time to get:</span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a>f(t,x) + \frac{\partial f}{\partial t}\tau &amp;= \int_{-\infty}^{\infty}\left(f(t,x) + \frac{\partial f}{\partial x}\Delta + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\Delta^2\right) \phi(\Delta)d\Delta<span class="sc">\\</span></span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a>&amp;= f(t,x) \int_{-\infty}^{\infty} \phi(\Delta)d\Delta <span class="sc">\\</span></span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a>&amp;+ \frac{\partial f} {\partial x} \int_{-\infty}^{\infty} \Delta \phi(\Delta)d\Delta <span class="sc">\\</span></span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a>&amp;+ \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta</span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a>Now, since the probability distribution of displacement $\phi(\cdot)$ is symmetric around the origin, the second term is zero. And we know, that if we integrate the density over $\mathbb{R}$, we should get one, so the first term equals one. So, we get:</span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a>f(t,x) + \frac{\partial f}{\partial t}\tau = f(t,x) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta</span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a>Now, we can cancel the $f$ on both sides and then shift $\tau$ to the right hand side:</span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} =  \left(\frac{1}{2\tau} \int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta \right)\frac{\partial^2 f}{\partial x^2}</span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a>Define $D:= \left(\frac{1}{2\tau} \int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta \right)$. Then, we have: </span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} =  D\frac{\partial^2 f}{\partial x^2}</span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a>The microscopic interpretation of the diffusion coefficient is, that its just the average of the squared displacements. The larger the $D$, the faster the brownian particles move.</span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a><span class="fu">## Kolmogorov's Backward Equation</span></span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a>Think of $y$ and $t$ as being current values and $y'$ and $t'$ being future values. The transition probability density function $p(y',t'|y,t)$ of a diffusion satisfies two equations - one involving derivatives with respect to a future state and time ($y'$ and $t'$) called *forward equation* and the other involving derivatives with respect to the current state and current time ($y$ and $t$) called the *backward equation*. These two equations are parabolic partial differential equations not dissimilar to the Black-Scholes equation. </span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a>::: {#thm-backward-equation-with-initial-value}</span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a><span class="fu">### Backward equation with initial value</span></span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a>Let $(X_t,t\geq 0)$ be a diffusion in $\mathbb{R}$ with the SDE:</span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a>dX_t = \sigma(X_t)dB_t + \mu(X_t) dt</span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a>Let $g\in C^2(\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value </span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t}(t,x) &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(x)\frac{\partial f}{\partial x}<span class="sc">\\</span></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a>f(0,x) &amp;= g(x)</span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a>$$ {#eq-backward-equation}</span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a>has the representation:</span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a>f(t,x) = \mathbb{E}<span class="co">[</span><span class="ot">g(X_t)|X_0 = x</span><span class="co">]</span></span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a>**Step 1.** </span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a>Let's fix $t$ and consider the function of space $h(x)=f(t,x)=\mathbb{E}<span class="co">[</span><span class="ot">g(X_t)|X_0=x</span><span class="co">]</span>$. Applying Ito's formula to $h$, we have:</span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a>dh(X_s) &amp;= h'(X_s) dX_s + \frac{1}{2}h''(X_s) (dX_s)^2<span class="sc">\\</span></span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a>&amp;= h'(X_s) (\sigma(X_s)dB_s + \mu(X_s) ds) + \frac{\sigma(X_s)^2}{2}h''(X_s)ds<span class="sc">\\</span></span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a>&amp;= \sigma(X_s)h'(X_s)dB_s + \left(\frac{\sigma(X_s)^2}{2}h''(X_s) + \mu(X_s)h'(X_s)\right)ds</span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a>In the integral form this is:</span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a>h(X_s) - h(X_0) &amp;= \int_0^s \sigma(X_u)h'(X_u)dB_u <span class="sc">\\</span></span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a>&amp;+ \int_0^s \left(\frac{\sigma(X_u)^2}{2}h''(X_u) + \mu(X_u)h'(X_u)\right)du \tag{1}</span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a>**Step 2.** Take expectations on both sides, divide by $s$ and let $s \to 0$. We are interested in taking the derivative with respect to $s$ at $s_0=0$.</span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a>The expectation of the first term on the right hand side is zero, by the properties of the Ito integral. </span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a>The integrand of the second term (RHS) is a conditional expectation $\mathbb{E}<span class="co">[</span><span class="ot">\xi(X_u)|X_0 = x</span><span class="co">]</span>$, it is an average at time $u$, of the paths of the process starting at initial position $X_0 = x$, so it is a function of $u$ and $x$. So, $\mathbb{E}<span class="co">[</span><span class="ot">\xi(X_u)|X_0 = x</span><span class="co">]</span> = p(u,x)$. Suppressing the argument $x$, we have the representation: </span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a>\int_0^s p(u) du</span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a>Recall that, if $p$ is a continuous function, then it is Riemann integrable. Further, since integration and differentiation are inverse operations, there exists a unique antiderivative $P$ given by</span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a>P(s) = \int_{0}^{s}p(u)du</span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>satisfying $P'(0) = p(0)$. </span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>By the definition of the derivative:</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a>$$P'(0) = \lim_{s \to 0} \frac{P(s) - P(0)}{s} = \lim_{s\to 0} \frac{P(s)}{s} = p(0) \quad <span class="sc">\{</span> P(0)=0 \text{ by definition }<span class="sc">\}</span>$$</span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a>Thus, we have:</span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a>p(0,x) = \mathbb{E}<span class="co">[</span><span class="ot">\xi(X_0)|X_0 = x</span><span class="co">]</span> = \frac{\sigma(x)^2}{2} h''(x) + \mu(x)h'(x)</span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a>**Step 3.** As for the left-hand side, we have:</span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a>\lim_{s \to 0} \frac{\mathbb{E}<span class="co">[</span><span class="ot">h(X_s)|X_0 = x</span><span class="co">]</span> - h(X_0)}{s} = \lim_{s \to 0} \frac{\mathbb{E}<span class="co">[</span><span class="ot">h(X_s)|X_0 = x</span><span class="co">]</span> - f(t,x)}{s} </span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a>To prove that this limit is $\frac{\partial f}{\partial t}(t,x)$, it remains to show that $\mathbb{E}<span class="co">[</span><span class="ot">h(X_s)|X_0 = x</span><span class="co">]</span>=\mathbb{E}<span class="co">[</span><span class="ot">g(X_{t+s})|X_0 = x</span><span class="co">]</span>=f(t+s,x)$. </span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a>To see this, note that $h(X_s) = \mathbb{E}<span class="co">[</span><span class="ot">g(X_{t+s})|X_s</span><span class="co">]</span>$. We deduce:</span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">h(X_s)|X_0 = x</span><span class="co">]</span> &amp;= \mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}[g(X_{t+s})|X_s]|X_0 = x</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}[g(X_{t+s})|\mathcal{F}_s]|X_0 = x</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a>&amp; <span class="sc">\{</span> (X_t,t\geq 0) \text{ is Markov }<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">g(X_{t+s})|X_0 = x</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a>&amp; <span class="sc">\{</span> \text{ Tower property }<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a>&amp;= f(t+s,x)</span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a>This closes the proof. $\blacksquare$</span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a>The backward equation (@eq-backward-equation) can be conveniently written in terms of *the generator of the diffusion*.</span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a>::: {#def-generator-of-the-diffusion}</span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generator of a diffusion</span></span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a>The generator of a diffusion with SDE $dX_t = \sigma(X_t) dB_t + \mu(X_t)dt$ is the differential operator acting on functions of space defined by :</span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a>A = \frac{\sigma(x)^2}{2}\frac{\partial }{\partial x^2} + \mu(x)\frac{\partial}{\partial x}</span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a>With this notation, the backward equation for the function $f(t,x)$ takes the form:</span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial x}(t,x) = Af(t,x)</span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a>where it is understood that $A$ acts only on the space variable. @thm-backward-equation-with-initial-value gives a nice interpretation of the generator: it quantifies how much the function $f(t,x) = \mathbb{E}<span class="co">[</span><span class="ot">g(X_t)|X_0 = x</span><span class="co">]</span>$ changes in a small time interval.</span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a><span class="fu">## The heat equation as a special case of the Backward equation</span></span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a>Let $(B_t,t\geq 0)$ be a standard brownian motion. Then, the generator is:</span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a>A = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}(t,x)</span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a>Then, by @thm-backward-equation-with-initial-value, the solution of the heat PDE</span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t}(t,x) = Af(x) = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}(t,x)</span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a>with initial value $f(0,x)=g(x)$ has the stochastic representation:</span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a>$$f(t,x) = \mathbb{E}<span class="co">[</span><span class="ot">g(B_t)|B_0 = x</span><span class="co">]</span>$$</span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a>It can be represented as an average of $g(B_t)$ over all Brownian motion paths starting at the location $x$.</span>
<span id="cb1-722"><a href="#cb1-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-723"><a href="#cb1-723" aria-hidden="true" tabindex="-1"></a>::: {#exm-generator-of-the-ornstein-uhlenbeck-process}</span>
<span id="cb1-724"><a href="#cb1-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-725"><a href="#cb1-725" aria-hidden="true" tabindex="-1"></a>(Generator of the Ornstein Uhlenbeck Process) The SDE of the Ornstein-Uhlenbeck process is:</span>
<span id="cb1-726"><a href="#cb1-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-727"><a href="#cb1-727" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-728"><a href="#cb1-728" aria-hidden="true" tabindex="-1"></a>dX_t = dB_t - X_t dt</span>
<span id="cb1-729"><a href="#cb1-729" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-730"><a href="#cb1-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-731"><a href="#cb1-731" aria-hidden="true" tabindex="-1"></a>This means that its generator is:</span>
<span id="cb1-732"><a href="#cb1-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-733"><a href="#cb1-733" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-734"><a href="#cb1-734" aria-hidden="true" tabindex="-1"></a>A = \frac{1}{2}\frac{\partial^2}{\partial x^2} - x \frac{\partial}{\partial x}</span>
<span id="cb1-735"><a href="#cb1-735" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-736"><a href="#cb1-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-737"><a href="#cb1-737" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-738"><a href="#cb1-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-739"><a href="#cb1-739" aria-hidden="true" tabindex="-1"></a>::: {#exm-generator-of-geometric-brownian-motion}</span>
<span id="cb1-740"><a href="#cb1-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-741"><a href="#cb1-741" aria-hidden="true" tabindex="-1"></a>(Generator of Geometric Brownian Motion) Recall that the geometric Brownian motion </span>
<span id="cb1-742"><a href="#cb1-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-743"><a href="#cb1-743" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-744"><a href="#cb1-744" aria-hidden="true" tabindex="-1"></a>S_t = S_0 \exp(\sigma B_t + \mu t)</span>
<span id="cb1-745"><a href="#cb1-745" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-746"><a href="#cb1-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-747"><a href="#cb1-747" aria-hidden="true" tabindex="-1"></a>satisfies the SDE:</span>
<span id="cb1-748"><a href="#cb1-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-749"><a href="#cb1-749" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-750"><a href="#cb1-750" aria-hidden="true" tabindex="-1"></a>dS_t = \sigma S_t dB_t + \left(\mu + \frac{\sigma^2}{2}\right) S_t dt</span>
<span id="cb1-751"><a href="#cb1-751" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-752"><a href="#cb1-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-753"><a href="#cb1-753" aria-hidden="true" tabindex="-1"></a>In particular, the generator of geometric Brownian motion is :</span>
<span id="cb1-754"><a href="#cb1-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-755"><a href="#cb1-755" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-756"><a href="#cb1-756" aria-hidden="true" tabindex="-1"></a>A = \frac{\sigma^2 x^2}{2} x \frac{\partial^2}{\partial x^2} + \left(\mu + \frac{\sigma^2}{2}\right)\frac{\partial}{\partial x}</span>
<span id="cb1-757"><a href="#cb1-757" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-758"><a href="#cb1-758" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-759"><a href="#cb1-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-760"><a href="#cb1-760" aria-hidden="true" tabindex="-1"></a>For applications, in particular in mathematical finance, it is important to solve the backward equation with terminal value instead of with initial value. The reversal of time causes the appearance of an extra minus sign in the equation.</span>
<span id="cb1-761"><a href="#cb1-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-762"><a href="#cb1-762" aria-hidden="true" tabindex="-1"></a>::: {#thm-backward-equation-with-terminal-value}</span>
<span id="cb1-763"><a href="#cb1-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-764"><a href="#cb1-764" aria-hidden="true" tabindex="-1"></a><span class="fu">### Backward equation with terminal value</span></span>
<span id="cb1-765"><a href="#cb1-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-766"><a href="#cb1-766" aria-hidden="true" tabindex="-1"></a>Let $(X_t,t\leq T)$ be a diffusion with the dynamics:</span>
<span id="cb1-767"><a href="#cb1-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-768"><a href="#cb1-768" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-769"><a href="#cb1-769" aria-hidden="true" tabindex="-1"></a>dX_t = \sigma(X_t) dB_t + \mu(X_t)dt</span>
<span id="cb1-770"><a href="#cb1-770" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-771"><a href="#cb1-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-772"><a href="#cb1-772" aria-hidden="true" tabindex="-1"></a>Let $g\in C^2(\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with terminal value at time $T$</span>
<span id="cb1-773"><a href="#cb1-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-774"><a href="#cb1-774" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-775"><a href="#cb1-775" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-776"><a href="#cb1-776" aria-hidden="true" tabindex="-1"></a>-\frac{\partial f}{\partial t} &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(x)\frac{\partial f}{\partial x}<span class="sc">\\</span></span>
<span id="cb1-777"><a href="#cb1-777" aria-hidden="true" tabindex="-1"></a>f(T,x) &amp;= g(x)</span>
<span id="cb1-778"><a href="#cb1-778" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-779"><a href="#cb1-779" aria-hidden="true" tabindex="-1"></a>$$ {#eq-backward-equation-with-terminal-value}</span>
<span id="cb1-780"><a href="#cb1-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-781"><a href="#cb1-781" aria-hidden="true" tabindex="-1"></a>has the representation:</span>
<span id="cb1-782"><a href="#cb1-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-783"><a href="#cb1-783" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-784"><a href="#cb1-784" aria-hidden="true" tabindex="-1"></a>f(t,x) = \mathbb{E}<span class="co">[</span><span class="ot">g(X_T)|X_t = x</span><span class="co">]</span></span>
<span id="cb1-785"><a href="#cb1-785" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-786"><a href="#cb1-786" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-787"><a href="#cb1-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-788"><a href="#cb1-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-789"><a href="#cb1-789" aria-hidden="true" tabindex="-1"></a>::: {#nte-functions-of-markov .callout-tip}</span>
<span id="cb1-790"><a href="#cb1-790" aria-hidden="true" tabindex="-1"></a><span class="fu">### Backward equation with terminal value appears in the martingale condition</span></span>
<span id="cb1-791"><a href="#cb1-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-792"><a href="#cb1-792" aria-hidden="true" tabindex="-1"></a>One way to construct a martingale for the filtration $(\mathcal{F}_t,t\geq 0)$ is to take </span>
<span id="cb1-793"><a href="#cb1-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-794"><a href="#cb1-794" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-795"><a href="#cb1-795" aria-hidden="true" tabindex="-1"></a>M_t = \mathbb{E}<span class="co">[</span><span class="ot">Y | \mathcal{F}_t</span><span class="co">]</span></span>
<span id="cb1-796"><a href="#cb1-796" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-797"><a href="#cb1-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-798"><a href="#cb1-798" aria-hidden="true" tabindex="-1"></a>where $Y$ is some integrable random variable. The martingale property then follows from the tower property of the conditional expectation. In the setup of @thm-backward-equation-with-terminal-value, the random variable $Y$ is $g(X_T)$. By the Markov property of diffusion, we therefore have:</span>
<span id="cb1-799"><a href="#cb1-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-800"><a href="#cb1-800" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-801"><a href="#cb1-801" aria-hidden="true" tabindex="-1"></a>f(t,X_t) = \mathbb{E}<span class="co">[</span><span class="ot">g(X_T)|X_t</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">g(X_T)|\mathcal{F}_t</span><span class="co">]</span> </span>
<span id="cb1-802"><a href="#cb1-802" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-803"><a href="#cb1-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-804"><a href="#cb1-804" aria-hidden="true" tabindex="-1"></a>In other words, the solution to the backward equation with terminal value evaluated at $X_t = x$ yields a martingale for the natural filtration of the process. This is a different point of view on the procedure we have used many times now: To get a martingale of the form $f(t,X_t)$, apply the Ito's formula to $f(t,X_t)$ and set the $dt$ term to zero. The PDE we obtain is the backward equation with terminal value. In fact, the proof of the theorem takes this exact route.</span>
<span id="cb1-805"><a href="#cb1-805" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-806"><a href="#cb1-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-807"><a href="#cb1-807" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-808"><a href="#cb1-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-809"><a href="#cb1-809" aria-hidden="true" tabindex="-1"></a>Consider $f(t,X_t)$ and apply Ito's formula.</span>
<span id="cb1-810"><a href="#cb1-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-811"><a href="#cb1-811" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-812"><a href="#cb1-812" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-813"><a href="#cb1-813" aria-hidden="true" tabindex="-1"></a>df(t,X_t) &amp;= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x}dX_t + \frac{1}{2}\frac{\partial^2 f}{\partial x^2} dX_t \cdot dX_t<span class="sc">\\</span></span>
<span id="cb1-814"><a href="#cb1-814" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x}(\sigma(X_t) dB_t + \mu(X_t)dt) + \frac{\sigma(X_t)^2}{2}\frac{\partial^2 f}{\partial x^2} dt<span class="sc">\\</span></span>
<span id="cb1-815"><a href="#cb1-815" aria-hidden="true" tabindex="-1"></a>&amp;= \sigma(X_t) dB_t + \left(\frac{\partial f}{\partial t} + \frac{\sigma(X_t)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(X_t)\frac{\partial f}{\partial x}\right)dt</span>
<span id="cb1-816"><a href="#cb1-816" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-817"><a href="#cb1-817" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-818"><a href="#cb1-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-819"><a href="#cb1-819" aria-hidden="true" tabindex="-1"></a>Since $f(t,x)$ is a solution to the equation, we get that the $dt$ term is $0$ and $f(t,X_t)$ is a martingale for the Brownian filtration (and thus also for the natural filtration of the diffusion, which contains less information). In particular we have:</span>
<span id="cb1-820"><a href="#cb1-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-821"><a href="#cb1-821" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-822"><a href="#cb1-822" aria-hidden="true" tabindex="-1"></a>f(t,X_t) = \mathbb{E}<span class="co">[</span><span class="ot">f(T,X_T)|\mathcal{F}_t</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">g(X_T)|\mathcal{F}_t</span><span class="co">]</span></span>
<span id="cb1-823"><a href="#cb1-823" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-824"><a href="#cb1-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-825"><a href="#cb1-825" aria-hidden="true" tabindex="-1"></a>Since $(X_t,t\leq T)$ is a Markov process, we finally get:</span>
<span id="cb1-826"><a href="#cb1-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-827"><a href="#cb1-827" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-828"><a href="#cb1-828" aria-hidden="true" tabindex="-1"></a>f(t,x) = \mathbb{E}<span class="co">[</span><span class="ot">g(X_T)|X_t = x</span><span class="co">]</span></span>
<span id="cb1-829"><a href="#cb1-829" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-830"><a href="#cb1-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-831"><a href="#cb1-831" aria-hidden="true" tabindex="-1"></a>::: {#exm-martingales-of-geometric-brownian-motion}</span>
<span id="cb1-832"><a href="#cb1-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-833"><a href="#cb1-833" aria-hidden="true" tabindex="-1"></a>(Martingales of geometric Brownian motion) Let $(S_t, \geq 0)$ be a geometric brownian motion with SDE:</span>
<span id="cb1-834"><a href="#cb1-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-835"><a href="#cb1-835" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-836"><a href="#cb1-836" aria-hidden="true" tabindex="-1"></a>dS_t = \sigma S_t dB_t + \left(\mu + \frac{\sigma^2}{2}\right)dt</span>
<span id="cb1-837"><a href="#cb1-837" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-838"><a href="#cb1-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-839"><a href="#cb1-839" aria-hidden="true" tabindex="-1"></a>As we saw in @exm-generator-of-geometric-brownian-motion, its generator is:</span>
<span id="cb1-840"><a href="#cb1-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-841"><a href="#cb1-841" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-842"><a href="#cb1-842" aria-hidden="true" tabindex="-1"></a>A = \frac{\sigma^2 x^2}{2}\frac{\partial^2}{\partial x^2} + x\left(\mu+\frac{\sigma^2}{2}\right)\frac{\partial}{\partial x}</span>
<span id="cb1-843"><a href="#cb1-843" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-844"><a href="#cb1-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-845"><a href="#cb1-845" aria-hidden="true" tabindex="-1"></a>In view of @thm-backward-equation-with-terminal-value, if $f(t,x)$ satisfies the PDE </span>
<span id="cb1-846"><a href="#cb1-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-847"><a href="#cb1-847" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-848"><a href="#cb1-848" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} + \frac{\sigma^2 x^2}{2}\frac{\partial^2 f}{\partial x^2} + x\left(\mu+\frac{\sigma^2}{2}\right)\frac{\partial f}{\partial x}</span>
<span id="cb1-849"><a href="#cb1-849" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-850"><a href="#cb1-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-851"><a href="#cb1-851" aria-hidden="true" tabindex="-1"></a>then processes of the form $f(t,S_t)$ will be martingales for the natural filtration. </span>
<span id="cb1-852"><a href="#cb1-852" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-853"><a href="#cb1-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-854"><a href="#cb1-854" aria-hidden="true" tabindex="-1"></a><span class="fu">## Kolmogorov's forward equation</span></span>
<span id="cb1-855"><a href="#cb1-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-856"><a href="#cb1-856" aria-hidden="true" tabindex="-1"></a>The companion equation to the backward equation is the *Kolmogorov forward equation* or *forward equation*. It is also known as the *Fokker-Planck* equation from its physics origin. The equation is very useful as it is satisfied by the transition density function $p(y',t'|y,t)$ of a time-homogenous diffusion. It involves the *adjoint of the generator*. </span>
<span id="cb1-857"><a href="#cb1-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-858"><a href="#cb1-858" aria-hidden="true" tabindex="-1"></a>::: {#def-adjoint-of-the-generator}</span>
<span id="cb1-859"><a href="#cb1-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-860"><a href="#cb1-860" aria-hidden="true" tabindex="-1"></a><span class="fu">### Adjoint of the generator</span></span>
<span id="cb1-861"><a href="#cb1-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-862"><a href="#cb1-862" aria-hidden="true" tabindex="-1"></a>The adjoint $A^*$ of the generator of a diffusion $(X_t,t\geq 0)$ with SDE:</span>
<span id="cb1-863"><a href="#cb1-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-864"><a href="#cb1-864" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-865"><a href="#cb1-865" aria-hidden="true" tabindex="-1"></a>dX_t = \sigma(X_t)dB_t + \mu(X_t)dt</span>
<span id="cb1-866"><a href="#cb1-866" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-867"><a href="#cb1-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-868"><a href="#cb1-868" aria-hidden="true" tabindex="-1"></a>is the differential operator acting on a function of space $f(x)$ as follows:</span>
<span id="cb1-869"><a href="#cb1-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-870"><a href="#cb1-870" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-871"><a href="#cb1-871" aria-hidden="true" tabindex="-1"></a>A^*f(x) = \frac{1}{2}\frac{\partial^2 }{\partial x^2} \frac{\sigma(x)^2}{2} f(x) - \frac{\partial }{\partial x}\mu(x)f(x)</span>
<span id="cb1-872"><a href="#cb1-872" aria-hidden="true" tabindex="-1"></a>$$ {#eq-adjoint-of-the-generator-of-a-diffusion}</span>
<span id="cb1-873"><a href="#cb1-873" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-874"><a href="#cb1-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-875"><a href="#cb1-875" aria-hidden="true" tabindex="-1"></a>Note the differences with the generator in @def-generator-of-the-diffusion: there is an extra minus sign and the derivatives also act on the volatility and the drift.</span>
<span id="cb1-876"><a href="#cb1-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-877"><a href="#cb1-877" aria-hidden="true" tabindex="-1"></a>::: {#exm-the-generator-brownian-motion-is-self-adjoint}</span>
<span id="cb1-878"><a href="#cb1-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-879"><a href="#cb1-879" aria-hidden="true" tabindex="-1"></a>(The generator of Brownian motion is self-adjoint) In the case of standard brownian motion, it is easy to check that:</span>
<span id="cb1-880"><a href="#cb1-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-881"><a href="#cb1-881" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-882"><a href="#cb1-882" aria-hidden="true" tabindex="-1"></a>A^* = \frac{1}{2}\frac{\partial^2}{\partial x^2} </span>
<span id="cb1-883"><a href="#cb1-883" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-884"><a href="#cb1-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-885"><a href="#cb1-885" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb1-886"><a href="#cb1-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-887"><a href="#cb1-887" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-888"><a href="#cb1-888" aria-hidden="true" tabindex="-1"></a>A^* = \frac{1}{2}\nabla^2</span>
<span id="cb1-889"><a href="#cb1-889" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-890"><a href="#cb1-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-891"><a href="#cb1-891" aria-hidden="true" tabindex="-1"></a>in the multivariate case. In other words, the generator and its adjoint are the same. In this case, the operator is *self-adjoint*. </span>
<span id="cb1-892"><a href="#cb1-892" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-893"><a href="#cb1-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-894"><a href="#cb1-894" aria-hidden="true" tabindex="-1"></a>::: {#exm-the-adjoint-for-geometric-brownian-motion}</span>
<span id="cb1-895"><a href="#cb1-895" aria-hidden="true" tabindex="-1"></a>We see that the adjoint of the generator acting on $f(x)$ for geometric Brownian motion is:</span>
<span id="cb1-896"><a href="#cb1-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-897"><a href="#cb1-897" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-898"><a href="#cb1-898" aria-hidden="true" tabindex="-1"></a>A^*f(x) = \frac{1}{2}\frac{\partial^2}{\partial x^2} (\sigma^2 x^2 f(x)) - \frac{\partial}{\partial x} \left(\left(\mu + \frac{\sigma^2}{2}\right) x f(x)\right)</span>
<span id="cb1-899"><a href="#cb1-899" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-900"><a href="#cb1-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-901"><a href="#cb1-901" aria-hidden="true" tabindex="-1"></a>Using the product rule in differentiating we get:</span>
<span id="cb1-902"><a href="#cb1-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-903"><a href="#cb1-903" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-904"><a href="#cb1-904" aria-hidden="true" tabindex="-1"></a>A^*<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span> = \frac{\sigma^2}{2}\left(2x f(x) + x^2 f''(x)\right) - \left(\left(\mu + \frac{\sigma^2}{2}\right)\left(f(x) + x f'(x)\right)\right)</span>
<span id="cb1-905"><a href="#cb1-905" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-906"><a href="#cb1-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-907"><a href="#cb1-907" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-908"><a href="#cb1-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-909"><a href="#cb1-909" aria-hidden="true" tabindex="-1"></a>::: {#exm-adjoint-for-the-ornstein-uhlenbeck-process}</span>
<span id="cb1-910"><a href="#cb1-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-911"><a href="#cb1-911" aria-hidden="true" tabindex="-1"></a>The generator for the Ornstein-Uhlenbeck process was given in @exm-generator-of-the-ornstein-uhlenbeck-process. The adjoint acting on $f$ is therefore:</span>
<span id="cb1-912"><a href="#cb1-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-913"><a href="#cb1-913" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-914"><a href="#cb1-914" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-915"><a href="#cb1-915" aria-hidden="true" tabindex="-1"></a>A^*f(x) &amp;= \frac{1}{2}\frac{\partial^2}{\partial x^2}(f(x)) - \frac{\partial}{\partial x}(- x f(x))<span class="sc">\\</span></span>
<span id="cb1-916"><a href="#cb1-916" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{f''(x)}{2} + (f(x)+xf'(x))</span>
<span id="cb1-917"><a href="#cb1-917" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-918"><a href="#cb1-918" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-919"><a href="#cb1-919" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-920"><a href="#cb1-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-921"><a href="#cb1-921" aria-hidden="true" tabindex="-1"></a>The forward equation takes the following form for a function $f(t,x)$ of time and space:</span>
<span id="cb1-922"><a href="#cb1-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-923"><a href="#cb1-923" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-924"><a href="#cb1-924" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} = A^* f</span>
<span id="cb1-925"><a href="#cb1-925" aria-hidden="true" tabindex="-1"></a>$$ {#eq-forward-equation}</span>
<span id="cb1-926"><a href="#cb1-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-927"><a href="#cb1-927" aria-hidden="true" tabindex="-1"></a>For brownian motion, since $A^* = A$, the backward and forward equations are the same. As advertised earlier, the forward equation is satisfied by the transition $p_t(y',t'|y,t)$ of a diffusion. Before showing this in general, we verify it in the Brownian case.</span>
<span id="cb1-928"><a href="#cb1-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-929"><a href="#cb1-929" aria-hidden="true" tabindex="-1"></a>::: {#exm-the-heat-kernel-as-the-solution-of-the-forward-equation}</span>
<span id="cb1-930"><a href="#cb1-930" aria-hidden="true" tabindex="-1"></a>Recall that the transition probability density $p(y,t|x,0)$ for Brownian motion, or heat kernel, is:</span>
<span id="cb1-931"><a href="#cb1-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-932"><a href="#cb1-932" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-933"><a href="#cb1-933" aria-hidden="true" tabindex="-1"></a>p(y,t|x,0) = \frac{e^{-\frac{(y-x)^2}{2}}}{\sqrt{2\pi t}}</span>
<span id="cb1-934"><a href="#cb1-934" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-935"><a href="#cb1-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-936"><a href="#cb1-936" aria-hidden="true" tabindex="-1"></a>Here, the space variable will be $y$ and $x$ will be fixed. The relevant function is thus $f(t,y) = p(y,t|x,0)$. The adjoint operator acting on the space variable $y$ is $A^* = A = \frac{1}{2}\frac{\partial^2}{\partial y^2}$. The relevant time and space derivatives are given by @eq-partial-with-respect-to-time and @eq-second-derivative-with-respect-to-space. </span>
<span id="cb1-937"><a href="#cb1-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-938"><a href="#cb1-938" aria-hidden="true" tabindex="-1"></a>We conclude that $f(t,y)=p(y,t|x,0)$ is a solution of the forward equation. </span>
<span id="cb1-939"><a href="#cb1-939" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-940"><a href="#cb1-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-941"><a href="#cb1-941" aria-hidden="true" tabindex="-1"></a>Where does the form of the adjoint operator @eq-adjoint-of-the-generator-of-a-diffusion come from? In some sense, the adjoint operator plays a role similar to that of the transpose of a matrix in linear algebra. The adjoint acts on the function on the left. To see this, consider two functions $f,g$ of space on which the generator $A$ of a diffusion is well-defined. In particular, let's assume that the functions are zero outside an interval. Consider the quantity </span>
<span id="cb1-942"><a href="#cb1-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-943"><a href="#cb1-943" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-944"><a href="#cb1-944" aria-hidden="true" tabindex="-1"></a>\int_{\mathbb{R}}g(x)A(f(x))dx = \int_{\mathbb{R}} g(x)\left(\frac{\sigma(x)^2 }{2}f''(x) + \mu(x)f'(x)\right)dx</span>
<span id="cb1-945"><a href="#cb1-945" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-946"><a href="#cb1-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-947"><a href="#cb1-947" aria-hidden="true" tabindex="-1"></a>This quantity can represent for example the average of $Af(x)$ over some PDF $g(x)$. In the above, $A$ acts on the function on the right. To make the operator act on $g$, we integrate by parts. This gives for the second term:</span>
<span id="cb1-948"><a href="#cb1-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-949"><a href="#cb1-949" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-950"><a href="#cb1-950" aria-hidden="true" tabindex="-1"></a>\int_{\mathbb{R}} g(x)\mu(x)f'(x)dx = g(x)\mu(x)f(x)\Bigg|_{-\infty}^{\infty}-\int_{\mathbb{R}}f(x)\frac{d}{dx}(g(x)\mu(x))dx</span>
<span id="cb1-951"><a href="#cb1-951" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-952"><a href="#cb1-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-953"><a href="#cb1-953" aria-hidden="true" tabindex="-1"></a>The boundary term $g(x)f(x)\mu(x)\Bigg|_{-\infty}^\infty$ is $0$ by the assumptions on $f,g$. This term on $\sigma$ is obtained by integrating by parts twice:</span>
<span id="cb1-954"><a href="#cb1-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-955"><a href="#cb1-955" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-956"><a href="#cb1-956" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-957"><a href="#cb1-957" aria-hidden="true" tabindex="-1"></a>\int_{\mathbb{R}} g(x) \frac{\sigma(x)^2}{2}f''(x)dx &amp;= g(x) \frac{\sigma(x)^2}{2}f'(x)\Bigg|_{-\infty}^{\infty} - \int_{\mathbb{R}}\frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right) f'(x)dx<span class="sc">\\</span></span>
<span id="cb1-958"><a href="#cb1-958" aria-hidden="true" tabindex="-1"></a>-\int_{\mathbb{R}} \frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right)f'(x)dx &amp;= -\frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right)f(x) \Bigg|_{-\infty}^{\infty} + \int_{\mathbb{R}}\frac{d^2}{dx^2}\left(g(x) \frac{\sigma(x)^2}{2}\right)f(x)dx</span>
<span id="cb1-959"><a href="#cb1-959" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-960"><a href="#cb1-960" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-961"><a href="#cb1-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-962"><a href="#cb1-962" aria-hidden="true" tabindex="-1"></a>Thus,</span>
<span id="cb1-963"><a href="#cb1-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-964"><a href="#cb1-964" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-965"><a href="#cb1-965" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-966"><a href="#cb1-966" aria-hidden="true" tabindex="-1"></a>\int_{\mathbb{R}}g(x) Af(x)dx &amp;= \int_{\mathbb{R}}\left(\frac{1}{2}\frac{d^2}{dx^2}(g(x) \sigma(x)^2) - \frac{d}{dx}(g(x)\mu(x))\right)f(x)dx<span class="sc">\\</span></span>
<span id="cb1-967"><a href="#cb1-967" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{\mathbb{R}}(A^*g(x))f(x)dx</span>
<span id="cb1-968"><a href="#cb1-968" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-969"><a href="#cb1-969" aria-hidden="true" tabindex="-1"></a>$$ {#eq-relation-between-generator-and-adjoint}</span>
<span id="cb1-970"><a href="#cb1-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-971"><a href="#cb1-971" aria-hidden="true" tabindex="-1"></a>::: {#thm-forward-equation-and-transition-probability}</span>
<span id="cb1-972"><a href="#cb1-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-973"><a href="#cb1-973" aria-hidden="true" tabindex="-1"></a><span class="fu">### Forward equation and transition probability </span></span>
<span id="cb1-974"><a href="#cb1-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-975"><a href="#cb1-975" aria-hidden="true" tabindex="-1"></a>Let $(X_t,t\geq 0)$ be a diffusion with SDE:</span>
<span id="cb1-976"><a href="#cb1-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-977"><a href="#cb1-977" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-978"><a href="#cb1-978" aria-hidden="true" tabindex="-1"></a>dX_t = \sigma(X_t)dB_t + \mu(X_t)dt, \quad X_0 = x_0</span>
<span id="cb1-979"><a href="#cb1-979" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-980"><a href="#cb1-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-981"><a href="#cb1-981" aria-hidden="true" tabindex="-1"></a>Let $p(x,t|x_0,0)$ be the transition probability density function for a fixed $x_0$. Then, the function $f(t,y) = p(y,t|x_0,0)$ is a solution of the PDE </span>
<span id="cb1-982"><a href="#cb1-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-983"><a href="#cb1-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-984"><a href="#cb1-984" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-985"><a href="#cb1-985" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} = A^* f</span>
<span id="cb1-986"><a href="#cb1-986" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-987"><a href="#cb1-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-988"><a href="#cb1-988" aria-hidden="true" tabindex="-1"></a>where $A^*$ is the adjoint of $A$. </span>
<span id="cb1-989"><a href="#cb1-989" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb1-990"><a href="#cb1-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-991"><a href="#cb1-991" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-992"><a href="#cb1-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-993"><a href="#cb1-993" aria-hidden="true" tabindex="-1"></a>Let $h(x)$ be some arbitrary function of space that is $0$ outside an interval. We compute :</span>
<span id="cb1-994"><a href="#cb1-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-995"><a href="#cb1-995" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-996"><a href="#cb1-996" aria-hidden="true" tabindex="-1"></a>\frac{1}{\epsilon}(\mathbb{E}<span class="co">[</span><span class="ot">h(X_{t+\epsilon}) - \mathbb{E}[h(X_t)]</span><span class="co">]</span>)</span>
<span id="cb1-997"><a href="#cb1-997" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-998"><a href="#cb1-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-999"><a href="#cb1-999" aria-hidden="true" tabindex="-1"></a>two different ways and take the limit as $\epsilon \to 0$. </span>
<span id="cb1-1000"><a href="#cb1-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1001"><a href="#cb1-1001" aria-hidden="true" tabindex="-1"></a>On one hand, we have by the definition of the transition density </span>
<span id="cb1-1002"><a href="#cb1-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1003"><a href="#cb1-1003" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1004"><a href="#cb1-1004" aria-hidden="true" tabindex="-1"></a>\frac{1}{\epsilon}\left(\mathbb{E}<span class="co">[</span><span class="ot">h(X_{t+\epsilon})</span><span class="co">]</span>-\mathbb{E}<span class="co">[</span><span class="ot">h(X_t)</span><span class="co">]</span>\right) = \int_{\mathbb{R}}\frac{1}{\epsilon}(p(x,t+\epsilon|x,0) - p(x,t|x_0,0))h(x)dx</span>
<span id="cb1-1005"><a href="#cb1-1005" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1006"><a href="#cb1-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1007"><a href="#cb1-1007" aria-hidden="true" tabindex="-1"></a>By taking the limit $\epsilon \to 0$ inside the integral (assuming this is fine), we get:</span>
<span id="cb1-1008"><a href="#cb1-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1009"><a href="#cb1-1009" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1010"><a href="#cb1-1010" aria-hidden="true" tabindex="-1"></a>\int_{\mathbb{R}} \frac{\partial}{\partial t}p(x,t|x_0,0)h(x)dx</span>
<span id="cb1-1011"><a href="#cb1-1011" aria-hidden="true" tabindex="-1"></a>$$ {#eq-fwd-equation-partial-wrt-time}</span>
<span id="cb1-1012"><a href="#cb1-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1013"><a href="#cb1-1013" aria-hidden="true" tabindex="-1"></a>On the other hand, Ito's formula implies </span>
<span id="cb1-1014"><a href="#cb1-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1015"><a href="#cb1-1015" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1016"><a href="#cb1-1016" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1017"><a href="#cb1-1017" aria-hidden="true" tabindex="-1"></a>dh(X_s) &amp;= \frac{\partial h}{\partial x} dX_s + \frac{1}{2} \frac{\partial^2 h}{\partial x^2} (dX_s)^2<span class="sc">\\</span></span>
<span id="cb1-1018"><a href="#cb1-1018" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\partial h}{\partial x} (\sigma(X_s) dB_s + \mu(X_s)ds) + \frac{1}{2} \frac{\partial^2 h}{\partial x^2} (\sigma(X_s)^2 ds)<span class="sc">\\</span></span>
<span id="cb1-1019"><a href="#cb1-1019" aria-hidden="true" tabindex="-1"></a>&amp;= \sigma(X_s)\frac{\partial h}{\partial x} dB_s + \left(\mu(X_s) \frac{\partial h}{\partial x} + \frac{\sigma(X_s)^2}{2}\frac{\partial^2 h}{\partial x^2}\right)ds<span class="sc">\\</span></span>
<span id="cb1-1020"><a href="#cb1-1020" aria-hidden="true" tabindex="-1"></a>h(X_{t+\epsilon}) - h(X_t) &amp;= \int_{t}^{t+\epsilon}\sigma(X_s)\frac{\partial h}{\partial x} dB_s + \int_{t}^{t+\epsilon}(Ah(x))ds<span class="sc">\\</span></span>
<span id="cb1-1021"><a href="#cb1-1021" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">h(X_{t+\epsilon})</span><span class="co">]</span> - \mathbb{E}<span class="co">[</span><span class="ot">h(X_t)</span><span class="co">]</span> &amp;= \underbrace{\mathbb{E}\left<span class="co">[</span><span class="ot">\int_{t}^{t+\epsilon}\sigma(X_s)\frac{\partial h}{\partial x} dB_s\right</span><span class="co">]</span>}_{0} + \int_{t}^{t+\epsilon}\mathbb{E}<span class="co">[</span><span class="ot">Ah(X_s)</span><span class="co">]</span>ds</span>
<span id="cb1-1022"><a href="#cb1-1022" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1023"><a href="#cb1-1023" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1024"><a href="#cb1-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1025"><a href="#cb1-1025" aria-hidden="true" tabindex="-1"></a>Dividing by $\epsilon$ and taking the limit as $\epsilon \to 0$, we have:</span>
<span id="cb1-1026"><a href="#cb1-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1027"><a href="#cb1-1027" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1028"><a href="#cb1-1028" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1029"><a href="#cb1-1029" aria-hidden="true" tabindex="-1"></a>\lim_{\epsilon \to 0} \frac{1}{\epsilon} (\mathbb{E}<span class="co">[</span><span class="ot">h(X_{t+\epsilon})</span><span class="co">]</span> - \mathbb{E}<span class="co">[</span><span class="ot">h(X_t)</span><span class="co">]</span>) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">Ah(X_t)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-1030"><a href="#cb1-1030" aria-hidden="true" tabindex="-1"></a>&amp;= \int_{\mathbb{R}} p(x,t|x_0,0) Ah(x) dx</span>
<span id="cb1-1031"><a href="#cb1-1031" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1032"><a href="#cb1-1032" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1033"><a href="#cb1-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1034"><a href="#cb1-1034" aria-hidden="true" tabindex="-1"></a>This can be written using @eq-relation-between-generator-and-adjoint as,</span>
<span id="cb1-1035"><a href="#cb1-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1036"><a href="#cb1-1036" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1037"><a href="#cb1-1037" aria-hidden="true" tabindex="-1"></a>\int_{\mathbb{R}}(A^* p(x,t|x_0,0)) h(x) dx</span>
<span id="cb1-1038"><a href="#cb1-1038" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1039"><a href="#cb1-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1040"><a href="#cb1-1040" aria-hidden="true" tabindex="-1"></a>Since $h$ is arbitrary, we conclude that:</span>
<span id="cb1-1041"><a href="#cb1-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1042"><a href="#cb1-1042" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1043"><a href="#cb1-1043" aria-hidden="true" tabindex="-1"></a>\frac{\partial}{\partial t}p(x,t|x_0,0) = A^* p(x,t|x_0,0)</span>
<span id="cb1-1044"><a href="#cb1-1044" aria-hidden="true" tabindex="-1"></a>$$ {#eq-forward-equation}</span>
<span id="cb1-1045"><a href="#cb1-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1046"><a href="#cb1-1046" aria-hidden="true" tabindex="-1"></a>::: {#exm-forward-equation-and-invariant-probability}</span>
<span id="cb1-1047"><a href="#cb1-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1048"><a href="#cb1-1048" aria-hidden="true" tabindex="-1"></a>(Forward equation and invariant probability.) The Ornstein-Uhlenbeck process converges to a stationary distribution as noted in the <span class="co">[</span><span class="ot">example</span><span class="co">](https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#exm-ornstein-uhlenbeck-process)</span> here. For example, for the SDE of the form</span>
<span id="cb1-1049"><a href="#cb1-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1050"><a href="#cb1-1050" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1051"><a href="#cb1-1051" aria-hidden="true" tabindex="-1"></a>dX_t = -X_t dt + dB_t</span>
<span id="cb1-1052"><a href="#cb1-1052" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1053"><a href="#cb1-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1054"><a href="#cb1-1054" aria-hidden="true" tabindex="-1"></a>with $X_0$ a Gaussian of mean $0$ and variance $1/2$, the PDF of $X_t$, is, for all $t$ is:</span>
<span id="cb1-1055"><a href="#cb1-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1056"><a href="#cb1-1056" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1057"><a href="#cb1-1057" aria-hidden="true" tabindex="-1"></a>f(x) = \frac{1}{\sqrt{\pi}} e^{-x^2}</span>
<span id="cb1-1058"><a href="#cb1-1058" aria-hidden="true" tabindex="-1"></a>$$ {#eq-pdf-of-OU-process}</span>
<span id="cb1-1059"><a href="#cb1-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1060"><a href="#cb1-1060" aria-hidden="true" tabindex="-1"></a>This *invariant distribution* can be seen from the point of view of the forward equation. Indeed since the PDF is constant in time, the forward equation simply becomes:</span>
<span id="cb1-1061"><a href="#cb1-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1062"><a href="#cb1-1062" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1063"><a href="#cb1-1063" aria-hidden="true" tabindex="-1"></a>A^* f = 0</span>
<span id="cb1-1064"><a href="#cb1-1064" aria-hidden="true" tabindex="-1"></a>$$ {#eq-forward-equation-of-ou-process}</span>
<span id="cb1-1065"><a href="#cb1-1065" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1066"><a href="#cb1-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1067"><a href="#cb1-1067" aria-hidden="true" tabindex="-1"></a>::: {#exm-smoluchowski-equation}</span>
<span id="cb1-1068"><a href="#cb1-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1069"><a href="#cb1-1069" aria-hidden="true" tabindex="-1"></a>The SDE of the Ornstein-Uhlenbeck process can be generated as follows. Consider $V(x)$, a smooth function of space such that $\int_{\mathbb{R}} e^{-2V(x)}dx&lt;\infty$. The *Smoluchowski* equation is the SDE of the form:</span>
<span id="cb1-1070"><a href="#cb1-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1071"><a href="#cb1-1071" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb1-1072"><a href="#cb1-1072" aria-hidden="true" tabindex="-1"></a>dX_t = dB_t - V'(X_t) dt</span>
<span id="cb1-1073"><a href="#cb1-1073" aria-hidden="true" tabindex="-1"></a>$$ {#eq-smoluchowski-equation}</span>
<span id="cb1-1074"><a href="#cb1-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1075"><a href="#cb1-1075" aria-hidden="true" tabindex="-1"></a>The SDE can be interpreted as follows: $X_t$ represents the position of a particle on $\mathbb{R}$. The position varies due to the Brownian fluctuations and also due to a force $V'(X_t)$ that depends on the position. The function $V(x)$ should then be thought of as the potential with which the particle moves, since the force (field) is the (negative) derivative of the potential function in Newtonian physics. The generator of this diffusion is:</span>
<span id="cb1-1076"><a href="#cb1-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1077"><a href="#cb1-1077" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1078"><a href="#cb1-1078" aria-hidden="true" tabindex="-1"></a>A = \frac{1}{2}\frac{\partial^2}{\partial x^2} - V'(x)\frac{\partial}{\partial x}</span>
<span id="cb1-1079"><a href="#cb1-1079" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1080"><a href="#cb1-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1081"><a href="#cb1-1081" aria-hidden="true" tabindex="-1"></a>This diffusion admits an invariant distribution :</span>
<span id="cb1-1082"><a href="#cb1-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1083"><a href="#cb1-1083" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1084"><a href="#cb1-1084" aria-hidden="true" tabindex="-1"></a>f(x) = Ce^{-2V(x)}</span>
<span id="cb1-1085"><a href="#cb1-1085" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1086"><a href="#cb1-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1087"><a href="#cb1-1087" aria-hidden="true" tabindex="-1"></a>where $C$ is such that $\int_{\mathbb{R}}f(x)dx = 1$.</span>
<span id="cb1-1088"><a href="#cb1-1088" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1089"><a href="#cb1-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1090"><a href="#cb1-1090" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Feynman-Kac Formula</span></span>
<span id="cb1-1091"><a href="#cb1-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1092"><a href="#cb1-1092" aria-hidden="true" tabindex="-1"></a>We saw in @exm-heat-equation-and-brownian-motion that the solution of the heat equation:</span>
<span id="cb1-1093"><a href="#cb1-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1094"><a href="#cb1-1094" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1095"><a href="#cb1-1095" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t} = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}</span>
<span id="cb1-1096"><a href="#cb1-1096" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1097"><a href="#cb1-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1098"><a href="#cb1-1098" aria-hidden="true" tabindex="-1"></a>can be represented as an average over Brownian paths. This representation was extended to diffusions in theorem @thm-backward-equation-with-initial-value where the second derivative in the equation is replaced by the generator of the corresponding diffusion. How robust is this representation? In other words, is it possible to slightly change the PDE and still get a stochastic representation representation for the solution? The answer to this question is yes, when a term of the form $r(x)f(t,x)$ is added to the equation, where $r(x)$ is a well-behaved function of space (for example, piecewise continuous). The stochastic representation of the PDE in this case bears the name *Feynman-Kac* formula, making a fruitful collaboration between the physicist <span class="co">[</span><span class="ot">Richard Feynman</span><span class="co">](https://en.wikipedia.org/wiki/Richard_Feynman)</span> and the mathematician <span class="co">[</span><span class="ot">Mark Kac</span><span class="co">](https://en.wikipedia.org/wiki/Mark_Kac)</span>. By the way, you pronounce "Kac" as "cats". His name is Polish. People who immigrated from Poland before him spelled their names as "Katz". The case when $r(x)$ is linear will be important in the applications to mathematical finance, where it represents the contribution of the interest rate. </span>
<span id="cb1-1099"><a href="#cb1-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1100"><a href="#cb1-1100" aria-hidden="true" tabindex="-1"></a>::: {#thm-initial-value-problem}</span>
<span id="cb1-1101"><a href="#cb1-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1102"><a href="#cb1-1102" aria-hidden="true" tabindex="-1"></a><span class="fu">### Initial Value Problem </span></span>
<span id="cb1-1103"><a href="#cb1-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1104"><a href="#cb1-1104" aria-hidden="true" tabindex="-1"></a>Let $(X_t,t\geq 0)$ be a diffusion in $\mathbb{R}$ with the SDE:</span>
<span id="cb1-1105"><a href="#cb1-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1106"><a href="#cb1-1106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1107"><a href="#cb1-1107" aria-hidden="true" tabindex="-1"></a>dX_t = \sigma(X_t) dB_t + \mu(X_t)dt</span>
<span id="cb1-1108"><a href="#cb1-1108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1109"><a href="#cb1-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1110"><a href="#cb1-1110" aria-hidden="true" tabindex="-1"></a>Let $g\in C^2(\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value </span>
<span id="cb1-1111"><a href="#cb1-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1112"><a href="#cb1-1112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1113"><a href="#cb1-1113" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1114"><a href="#cb1-1114" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial t}(t,x) &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2}(t,x) + \mu(x)\frac{\partial f}{\partial x}(t,x) - r(x)f(x)<span class="sc">\\</span></span>
<span id="cb1-1115"><a href="#cb1-1115" aria-hidden="true" tabindex="-1"></a>f(0,x) &amp;= g(x)</span>
<span id="cb1-1116"><a href="#cb1-1116" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1117"><a href="#cb1-1117" aria-hidden="true" tabindex="-1"></a>$$ {#eq-initial-value-problem}</span>
<span id="cb1-1118"><a href="#cb1-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1119"><a href="#cb1-1119" aria-hidden="true" tabindex="-1"></a>has the stochastic representation:</span>
<span id="cb1-1120"><a href="#cb1-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1121"><a href="#cb1-1121" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1122"><a href="#cb1-1122" aria-hidden="true" tabindex="-1"></a>f(t,x) = \mathbb{E}\left<span class="co">[</span><span class="ot">g(X_t)\exp\left(-\int_0^t r(X_s) ds\right)\Bigg| X_0 = x\right</span><span class="co">]</span></span>
<span id="cb1-1123"><a href="#cb1-1123" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1124"><a href="#cb1-1124" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1125"><a href="#cb1-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1126"><a href="#cb1-1126" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-1127"><a href="#cb1-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1128"><a href="#cb1-1128" aria-hidden="true" tabindex="-1"></a>The proof is again based on Ito's formula. For a fixed $t$, we consider the process:</span>
<span id="cb1-1129"><a href="#cb1-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1130"><a href="#cb1-1130" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1131"><a href="#cb1-1131" aria-hidden="true" tabindex="-1"></a>M_s = f(t-s, X_s) \exp\left(-\int_0^s r(X_u) du\right), \quad s \leq t</span>
<span id="cb1-1132"><a href="#cb1-1132" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1133"><a href="#cb1-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1134"><a href="#cb1-1134" aria-hidden="true" tabindex="-1"></a>Write $Z_s = \exp\left(-\int_0^s r(X_u) du\right)$ and $V_s = f(t-s,X_s)$. A direct application of Ito's formula yields:</span>
<span id="cb1-1135"><a href="#cb1-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1136"><a href="#cb1-1136" aria-hidden="true" tabindex="-1"></a>Let $R_s = -\int_0^s r(X_u) du$. So, $dR_t = r(X_t) dt$. $(R_t,t\geq 0)$ is a random variable, because $r(X_s)$ depends on how $(X_s, s \leq t)$ evolves, it is *stochastic*, but for very small intervals of time $r(X_s)$ is a constant, and hence the process $(R_t,t\geq 0)$ is said to be locally deterministic.</span>
<span id="cb1-1137"><a href="#cb1-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1138"><a href="#cb1-1138" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1139"><a href="#cb1-1139" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1140"><a href="#cb1-1140" aria-hidden="true" tabindex="-1"></a>Z_s &amp;= e^{-R_s}<span class="sc">\\</span></span>
<span id="cb1-1141"><a href="#cb1-1141" aria-hidden="true" tabindex="-1"></a>dZ_s &amp;= -e^{-R_s} dR_s + \frac{1}{2}e^{R_s} (dR_s)^2<span class="sc">\\</span></span>
<span id="cb1-1142"><a href="#cb1-1142" aria-hidden="true" tabindex="-1"></a>&amp;= -Z_s r(X_s) ds</span>
<span id="cb1-1143"><a href="#cb1-1143" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1144"><a href="#cb1-1144" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1145"><a href="#cb1-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1146"><a href="#cb1-1146" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb1-1147"><a href="#cb1-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1148"><a href="#cb1-1148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1149"><a href="#cb1-1149" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1150"><a href="#cb1-1150" aria-hidden="true" tabindex="-1"></a>dV_s &amp;= \frac{\partial}{\partial s}f(t-s, X_s)ds + \frac{\partial}{\partial x}f(t-s, X_s)dX_s + \frac{1}{2}\frac{\partial^2}{\partial x^2}f(t-s,X_s)(dX_s)^2<span class="sc">\\</span></span>
<span id="cb1-1151"><a href="#cb1-1151" aria-hidden="true" tabindex="-1"></a>&amp;= -f_s ds + f_x (\sigma(X_s)dB_s + \mu(X_s)ds) + \frac{1}{2}f_{xx} \sigma(X_s)^2 ds <span class="sc">\\</span></span>
<span id="cb1-1152"><a href="#cb1-1152" aria-hidden="true" tabindex="-1"></a>&amp;= \sigma(X_s) f_x dB_s + <span class="sc">\\</span></span>
<span id="cb1-1153"><a href="#cb1-1153" aria-hidden="true" tabindex="-1"></a>&amp;+ \left<span class="sc">\{</span>-f_s + \mu(X_s)f_x + \frac{\sigma(X_s)^2}{2}f_{xx}\right<span class="sc">\}</span>ds</span>
<span id="cb1-1154"><a href="#cb1-1154" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1155"><a href="#cb1-1155" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1156"><a href="#cb1-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1157"><a href="#cb1-1157" aria-hidden="true" tabindex="-1"></a>Recall that $t$ is fixed here, and we differentiate with respect to $s$ in time. Since $f(t,x)$ is a solution of the PDE, we can write the second equation as:</span>
<span id="cb1-1158"><a href="#cb1-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1159"><a href="#cb1-1159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1160"><a href="#cb1-1160" aria-hidden="true" tabindex="-1"></a>dV_s = \sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds</span>
<span id="cb1-1161"><a href="#cb1-1161" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1162"><a href="#cb1-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1163"><a href="#cb1-1163" aria-hidden="true" tabindex="-1"></a>Now, by Ito's product rule, we finally have:</span>
<span id="cb1-1164"><a href="#cb1-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1165"><a href="#cb1-1165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1166"><a href="#cb1-1166" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1167"><a href="#cb1-1167" aria-hidden="true" tabindex="-1"></a>dM_s &amp;= V_s dZ_s + Z_s dV_s + dZ_s dV_s<span class="sc">\\</span></span>
<span id="cb1-1168"><a href="#cb1-1168" aria-hidden="true" tabindex="-1"></a>&amp;= -f(t-s,X_s)Z_s r(X_s) ds + Z_s (\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds) + 0<span class="sc">\\</span></span>
<span id="cb1-1169"><a href="#cb1-1169" aria-hidden="true" tabindex="-1"></a>&amp;= \sigma(X_s)Z_s f_x dB_s</span>
<span id="cb1-1170"><a href="#cb1-1170" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1171"><a href="#cb1-1171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1172"><a href="#cb1-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1173"><a href="#cb1-1173" aria-hidden="true" tabindex="-1"></a>This proves that $(M_s, s \leq t)$ is a martingale. We conclude that:</span>
<span id="cb1-1174"><a href="#cb1-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1175"><a href="#cb1-1175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1176"><a href="#cb1-1176" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">M_t</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">M_0</span><span class="co">]</span></span>
<span id="cb1-1177"><a href="#cb1-1177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1178"><a href="#cb1-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1179"><a href="#cb1-1179" aria-hidden="true" tabindex="-1"></a>Using the definition of $M_t$, this yields:</span>
<span id="cb1-1180"><a href="#cb1-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1181"><a href="#cb1-1181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1182"><a href="#cb1-1182" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">M_t</span><span class="co">]</span> = \mathbb{E}\left<span class="co">[</span><span class="ot">f(0,X_t)\exp\left(-\int_0^t r(X_u) du\right)\right</span><span class="co">]</span> = \mathbb{E}\left<span class="co">[</span><span class="ot">g(X_t)\exp\left(-\int_0^t r(X_u) du\right)\right</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">M_0</span><span class="co">]</span> = f(t,x)</span>
<span id="cb1-1183"><a href="#cb1-1183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1184"><a href="#cb1-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1185"><a href="#cb1-1185" aria-hidden="true" tabindex="-1"></a>This proves the theorem. $\blacksquare$</span>
<span id="cb1-1186"><a href="#cb1-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1187"><a href="#cb1-1187" aria-hidden="true" tabindex="-1"></a>As for the backward equation, it is natural to consider the terminal value problem for the same PDE.</span>
<span id="cb1-1188"><a href="#cb1-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1189"><a href="#cb1-1189" aria-hidden="true" tabindex="-1"></a>::: {#thm-terminal-value-problem}</span>
<span id="cb1-1190"><a href="#cb1-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1191"><a href="#cb1-1191" aria-hidden="true" tabindex="-1"></a><span class="fu">### Terminal Value Problem</span></span>
<span id="cb1-1192"><a href="#cb1-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1193"><a href="#cb1-1193" aria-hidden="true" tabindex="-1"></a>Let $(X_t,t \leq T)$ be a diffusion in $\mathbb{R}$ with the SDE:</span>
<span id="cb1-1194"><a href="#cb1-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1195"><a href="#cb1-1195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1196"><a href="#cb1-1196" aria-hidden="true" tabindex="-1"></a>dX_t = \sigma(X_t) dB_t + \mu(X_t) dt</span>
<span id="cb1-1197"><a href="#cb1-1197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1198"><a href="#cb1-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1199"><a href="#cb1-1199" aria-hidden="true" tabindex="-1"></a>Let $g\in C^2(\mathbb{R})$ be such that $g$ is $0$ outside an interval. Then, the solution of the PDE with initial value </span>
<span id="cb1-1200"><a href="#cb1-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1201"><a href="#cb1-1201" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1202"><a href="#cb1-1202" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1203"><a href="#cb1-1203" aria-hidden="true" tabindex="-1"></a>-\frac{\partial f}{\partial t}(t,x) &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2}(t,x) + \mu(x)\frac{\partial f}{\partial x}(t,x) - r(x)f(t,x)<span class="sc">\\</span></span>
<span id="cb1-1204"><a href="#cb1-1204" aria-hidden="true" tabindex="-1"></a>f(T,x) &amp;= g(x)</span>
<span id="cb1-1205"><a href="#cb1-1205" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1206"><a href="#cb1-1206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1207"><a href="#cb1-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1208"><a href="#cb1-1208" aria-hidden="true" tabindex="-1"></a>has the stochastic representation :</span>
<span id="cb1-1209"><a href="#cb1-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1210"><a href="#cb1-1210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1211"><a href="#cb1-1211" aria-hidden="true" tabindex="-1"></a>f(t,x) = \mathbb{E}\left<span class="co">[</span><span class="ot">g(X_T)\exp\left(-\int_t^T r(X_u) du\right)\Bigg|X_t = x\right</span><span class="co">]</span></span>
<span id="cb1-1212"><a href="#cb1-1212" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1213"><a href="#cb1-1213" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1214"><a href="#cb1-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1215"><a href="#cb1-1215" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-1216"><a href="#cb1-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1217"><a href="#cb1-1217" aria-hidden="true" tabindex="-1"></a>The proof is similar by considering instead</span>
<span id="cb1-1218"><a href="#cb1-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1219"><a href="#cb1-1219" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1220"><a href="#cb1-1220" aria-hidden="true" tabindex="-1"></a>M_t = f(t,X_t)\exp\left(-\int_0^t r(X_u) du\right)</span>
<span id="cb1-1221"><a href="#cb1-1221" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1222"><a href="#cb1-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1223"><a href="#cb1-1223" aria-hidden="true" tabindex="-1"></a>:::{#thm-generalized-feynman-kac}</span>
<span id="cb1-1224"><a href="#cb1-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1225"><a href="#cb1-1225" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generalized version. </span></span>
<span id="cb1-1226"><a href="#cb1-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1227"><a href="#cb1-1227" aria-hidden="true" tabindex="-1"></a>Let $V\in C^2(\mathbb{R})$ be the payout function. Then, the solution to the PDE </span>
<span id="cb1-1228"><a href="#cb1-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1229"><a href="#cb1-1229" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1230"><a href="#cb1-1230" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1231"><a href="#cb1-1231" aria-hidden="true" tabindex="-1"></a>\left(\frac{\partial}{\partial t} + \mu(t,x)\frac{\partial}{\partial x} + \frac{1}{2}\sigma^2(t,x)\frac{\partial^2}{\partial x^2}\right)f = r(t,x)f(t,x) + B(t,x)</span>
<span id="cb1-1232"><a href="#cb1-1232" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1233"><a href="#cb1-1233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1234"><a href="#cb1-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1235"><a href="#cb1-1235" aria-hidden="true" tabindex="-1"></a>with the boundary condition:</span>
<span id="cb1-1236"><a href="#cb1-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1237"><a href="#cb1-1237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1238"><a href="#cb1-1238" aria-hidden="true" tabindex="-1"></a>f(T,x) = V(x)</span>
<span id="cb1-1239"><a href="#cb1-1239" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1240"><a href="#cb1-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1241"><a href="#cb1-1241" aria-hidden="true" tabindex="-1"></a>has the stochastic representation:</span>
<span id="cb1-1242"><a href="#cb1-1242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1243"><a href="#cb1-1243" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1244"><a href="#cb1-1244" aria-hidden="true" tabindex="-1"></a>f(t,x)=\mathbb{E}_t\left[\exp\left(-\int_t^T r(u,X_u) du\right)V(X_T)\right] - \mathbb{E}_t\left[\int_{t}^T \exp\left(-\int_t^s r(u,X_u) du\right)B(s,X_s)ds\right]</span>
<span id="cb1-1245"><a href="#cb1-1245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1246"><a href="#cb1-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1247"><a href="#cb1-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1248"><a href="#cb1-1248" aria-hidden="true" tabindex="-1"></a>where $(X_t,t\leq T)$ is a diffusion in $\mathbb{R}$ with the dynamics :</span>
<span id="cb1-1249"><a href="#cb1-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1250"><a href="#cb1-1250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1251"><a href="#cb1-1251" aria-hidden="true" tabindex="-1"></a>dX_t = \sigma(t,X_t) dB_t + \mu(t,X_t)dt</span>
<span id="cb1-1252"><a href="#cb1-1252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1253"><a href="#cb1-1253" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1254"><a href="#cb1-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1255"><a href="#cb1-1255" aria-hidden="true" tabindex="-1"></a>*Proof*</span>
<span id="cb1-1256"><a href="#cb1-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1257"><a href="#cb1-1257" aria-hidden="true" tabindex="-1"></a>For brevity, I drop the space coordinates in the below derivations.</span>
<span id="cb1-1258"><a href="#cb1-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1259"><a href="#cb1-1259" aria-hidden="true" tabindex="-1"></a>Define $Z_s = \exp\left(-\int_t^s r_u du\right)$. Consider the process</span>
<span id="cb1-1260"><a href="#cb1-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1261"><a href="#cb1-1261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1262"><a href="#cb1-1262" aria-hidden="true" tabindex="-1"></a>Y(s) = Z_s f(s,X_s) - \int_t^s Z_s B_s ds</span>
<span id="cb1-1263"><a href="#cb1-1263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1264"><a href="#cb1-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1265"><a href="#cb1-1265" aria-hidden="true" tabindex="-1"></a>By Ito's product rule:</span>
<span id="cb1-1266"><a href="#cb1-1266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1267"><a href="#cb1-1267" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1268"><a href="#cb1-1268" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1269"><a href="#cb1-1269" aria-hidden="true" tabindex="-1"></a>dY_s &amp;= dZ_s f + Z_s df + dZ_s df - Z_s B_s ds</span>
<span id="cb1-1270"><a href="#cb1-1270" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1271"><a href="#cb1-1271" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1272"><a href="#cb1-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1273"><a href="#cb1-1273" aria-hidden="true" tabindex="-1"></a>Since $dZ_s df = O(dt dt)$ it can be dropped. We have:</span>
<span id="cb1-1274"><a href="#cb1-1274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1275"><a href="#cb1-1275" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1276"><a href="#cb1-1276" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1277"><a href="#cb1-1277" aria-hidden="true" tabindex="-1"></a>dY_s &amp;= -r_s Z_s f ds + Z_s \left(f_s ds + f_x dX_s + \frac{1}{2}f_{xx}(dX_s)^2\right) - Z_s B_s ds<span class="sc">\\</span></span>
<span id="cb1-1278"><a href="#cb1-1278" aria-hidden="true" tabindex="-1"></a>&amp;= -r_s Z_s f ds + Z_s \left<span class="co">[</span><span class="ot">f_s ds + f_x (\mu ds + \sigma dW_s) + \frac{1}{2}\sigma^2f_{xx}ds\right</span><span class="co">]</span> - Z_s B_s ds <span class="sc">\\</span></span>
<span id="cb1-1279"><a href="#cb1-1279" aria-hidden="true" tabindex="-1"></a>&amp;= -r_s Z_s f ds + Z_s \left<span class="co">[</span><span class="ot">\left(f_s + \mu f_x  + \frac{1}{2}\sigma^2f_{xx}\right)ds + \sigma f_x dW_s \right</span><span class="co">]</span>  - Z_s B_s ds</span>
<span id="cb1-1280"><a href="#cb1-1280" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1281"><a href="#cb1-1281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1282"><a href="#cb1-1282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1283"><a href="#cb1-1283" aria-hidden="true" tabindex="-1"></a>We can substitute the term in the round brackets $\left(f_s + \mu f_x  + \frac{1}{2}\sigma^2f_{xx}\right) = r_s f + B_s$, since $f$ satisfies the PDE. So, we have:</span>
<span id="cb1-1284"><a href="#cb1-1284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1285"><a href="#cb1-1285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1286"><a href="#cb1-1286" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1287"><a href="#cb1-1287" aria-hidden="true" tabindex="-1"></a>dY_s </span>
<span id="cb1-1288"><a href="#cb1-1288" aria-hidden="true" tabindex="-1"></a>&amp;= -r_s Z_s f ds + Z_s \left<span class="co">[</span><span class="ot">\left(r_s f + B_s\right)ds + \sigma f_x dW_s \right</span><span class="co">]</span>  - Z_s B_s ds<span class="sc">\\</span></span>
<span id="cb1-1289"><a href="#cb1-1289" aria-hidden="true" tabindex="-1"></a>&amp;= Z_s \sigma f_x dW_s</span>
<span id="cb1-1290"><a href="#cb1-1290" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1291"><a href="#cb1-1291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1292"><a href="#cb1-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1293"><a href="#cb1-1293" aria-hidden="true" tabindex="-1"></a>So, the process $(Y_s,s\leq T)$ is a martingale. Integrating the above equation from $t$ to $T$, we have:</span>
<span id="cb1-1294"><a href="#cb1-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1295"><a href="#cb1-1295" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1296"><a href="#cb1-1296" aria-hidden="true" tabindex="-1"></a>Y(T) - Y(t) = \int_t^T Z_s \sigma f_x dW_s</span>
<span id="cb1-1297"><a href="#cb1-1297" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1298"><a href="#cb1-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1299"><a href="#cb1-1299" aria-hidden="true" tabindex="-1"></a>Upon taking expectations, conditioned on $X_t = x$ and observing that the RHS is an Ito integral, which has zero expectation, it follows that:</span>
<span id="cb1-1300"><a href="#cb1-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1301"><a href="#cb1-1301" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1302"><a href="#cb1-1302" aria-hidden="true" tabindex="-1"></a>\mathbb{E}_t<span class="co">[</span><span class="ot">Y_T|X_t = x</span><span class="co">]</span> =  \mathbb{E}_t<span class="co">[</span><span class="ot">Y_t|X_t = x</span><span class="co">]</span></span>
<span id="cb1-1303"><a href="#cb1-1303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1304"><a href="#cb1-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1305"><a href="#cb1-1305" aria-hidden="true" tabindex="-1"></a>On the right hand side, $Y_t = f(t,X_t) =f(t,x)$. It follows that:</span>
<span id="cb1-1306"><a href="#cb1-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1307"><a href="#cb1-1307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1308"><a href="#cb1-1308" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1309"><a href="#cb1-1309" aria-hidden="true" tabindex="-1"></a>f(t,x) &amp;= \mathbb{E}_t\left[Z_T f(T,X_T) - \int_{t}^T Z_s B_s ds \right]<span class="sc">\\</span></span>
<span id="cb1-1310"><a href="#cb1-1310" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}_t\left[Z_T V(X_T)\right] - \mathbb{E}_t\left[\int_{t}^T Z_s B_s ds \right]</span>
<span id="cb1-1311"><a href="#cb1-1311" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1312"><a href="#cb1-1312" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1313"><a href="#cb1-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1314"><a href="#cb1-1314" aria-hidden="true" tabindex="-1"></a>This closes the proof. $\blacksquare$</span>
<span id="cb1-1315"><a href="#cb1-1315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1316"><a href="#cb1-1316" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb1-1317"><a href="#cb1-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1318"><a href="#cb1-1318" aria-hidden="true" tabindex="-1"></a>::: {#exr-shifted-brownian-motion}</span>
<span id="cb1-1319"><a href="#cb1-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1320"><a href="#cb1-1320" aria-hidden="true" tabindex="-1"></a>(Shifted Brownian Motion) Let $(B_t,t\geq 0)$ be a standard brownian motion. Fix $t &gt; 0$. Show that the process $(W_s,s \geq 0)$ with $W_s = B_{t+s} - B_t$ is a standard brownian motion independent of $\mathcal{F}_t$.</span>
<span id="cb1-1321"><a href="#cb1-1321" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1322"><a href="#cb1-1322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1323"><a href="#cb1-1323" aria-hidden="true" tabindex="-1"></a>*Solution*.</span>
<span id="cb1-1324"><a href="#cb1-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1325"><a href="#cb1-1325" aria-hidden="true" tabindex="-1"></a>At $s = 0$, $W(0) = B(t) - B(t) = 0$. </span>
<span id="cb1-1326"><a href="#cb1-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1327"><a href="#cb1-1327" aria-hidden="true" tabindex="-1"></a>Consider any arbitrary times $t_1 &lt; t_2$. We have:</span>
<span id="cb1-1328"><a href="#cb1-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1329"><a href="#cb1-1329" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1330"><a href="#cb1-1330" aria-hidden="true" tabindex="-1"></a>W(t_2) - W(t_1) &amp;= (B(t + t_2) - B(t)) - (B(t + t_1) - B(t))<span class="sc">\\</span></span>
<span id="cb1-1331"><a href="#cb1-1331" aria-hidden="true" tabindex="-1"></a>&amp;= B(t + t_2) - B(t + t_1)</span>
<span id="cb1-1332"><a href="#cb1-1332" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1333"><a href="#cb1-1333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1334"><a href="#cb1-1334" aria-hidden="true" tabindex="-1"></a>Now, $B(t + t_2) - B(t + t_1) \sim \mathcal{N}(0,t_2 - t_1)$. So, $W(t_2) - W(t_1)$ is a Gaussian random variable with mean $0$ and variance $t_2 - t_1$.</span>
<span id="cb1-1335"><a href="#cb1-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1336"><a href="#cb1-1336" aria-hidden="true" tabindex="-1"></a>Finally, consider any finite set of times $0=t_0 &lt; t_1 &lt; t_2 &lt; \ldots &lt; t_n = T$. Then, $t &lt; t + t_1 &lt; t + t_2 &lt; \ldots &lt; t + t_n$. We have that, $B(t + t_1) - B(t)$, $B(t + t_2) - B(t + t_1)$, $B(t + t_3) - B(t + t_2)$, $\ldots$, $B(t+T) - B(t+t_{n-1})$ are independent random variables. Consequently, $W(t_1) - W(0)$, $W(t_2) - W(t_1)$, $W(t_3) - W(t_2)$, $\ldots$, $W(t_n) - W(t_{n-1})$ are independent random variables. So, $(W_s,s\geq 0)$ is a standard brownian motion.</span>
<span id="cb1-1337"><a href="#cb1-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1338"><a href="#cb1-1338" aria-hidden="true" tabindex="-1"></a>Also, we have:</span>
<span id="cb1-1339"><a href="#cb1-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1340"><a href="#cb1-1340" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-1341"><a href="#cb1-1341" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">W(s)|\mathcal{F}_t</span><span class="co">]</span> &amp;= \mathbb{E}<span class="co">[</span><span class="ot">B(t + s) - B(t)|\mathcal{F}_t</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-1342"><a href="#cb1-1342" aria-hidden="true" tabindex="-1"></a>&amp; <span class="sc">\{</span> B(t+s) - B(t) \perp \mathcal{F}_t <span class="sc">\}\\</span></span>
<span id="cb1-1343"><a href="#cb1-1343" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">B(t + s) - B(t)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-1344"><a href="#cb1-1344" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">W(s)</span><span class="co">]</span></span>
<span id="cb1-1345"><a href="#cb1-1345" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-1346"><a href="#cb1-1346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1347"><a href="#cb1-1347" aria-hidden="true" tabindex="-1"></a>Thus, $W(s)$ is independent of $\mathcal{F}_t$, it does not depend upon the information available upto time $t$.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>
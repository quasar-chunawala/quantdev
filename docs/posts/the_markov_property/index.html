<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-07-12">

<title>Quant Insights - The Markov Property</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Quant Insights</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Markov Property</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Stochastic Calculus</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 12, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-markov-property-for-diffusions" id="toc-the-markov-property-for-diffusions" class="nav-link active" data-scroll-target="#the-markov-property-for-diffusions">The Markov Property for Diffusions</a></li>
  <li><a href="#the-strong-markov-property" id="toc-the-strong-markov-property" class="nav-link" data-scroll-target="#the-strong-markov-property">The Strong Markov Property</a></li>
  <li><a href="#the-heat-equation" id="toc-the-heat-equation" class="nav-link" data-scroll-target="#the-heat-equation">The Heat Equation</a>
  <ul class="collapse">
  <li><a href="#robert-browns-erratic-motion-of-pollen" id="toc-robert-browns-erratic-motion-of-pollen" class="nav-link" data-scroll-target="#robert-browns-erratic-motion-of-pollen">Robert Brown’s erratic motion of pollen</a></li>
  <li><a href="#albert-einsteins-proof-of-the-existence-of-brownian-motion" id="toc-albert-einsteins-proof-of-the-existence-of-brownian-motion" class="nav-link" data-scroll-target="#albert-einsteins-proof-of-the-existence-of-brownian-motion">Albert Einstein’s proof of the existence of Brownian motion</a></li>
  </ul></li>
  <li><a href="#kolmogorovs-backward-equation" id="toc-kolmogorovs-backward-equation" class="nav-link" data-scroll-target="#kolmogorovs-backward-equation">Kolmogorov’s Backward Equation</a></li>
  <li><a href="#kolmogorovs-forward-equation" id="toc-kolmogorovs-forward-equation" class="nav-link" data-scroll-target="#kolmogorovs-forward-equation">Kolmogorov’s forward equation</a></li>
  <li><a href="#the-feynman-kac-formula" id="toc-the-feynman-kac-formula" class="nav-link" data-scroll-target="#the-feynman-kac-formula">The Feynman-Kac Formula</a></li>
  <li><a href="#numerical-projects" id="toc-numerical-projects" class="nav-link" data-scroll-target="#numerical-projects">Numerical Projects</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="the-markov-property-for-diffusions" class="level2">
<h2 class="anchored" data-anchor-id="the-markov-property-for-diffusions">The Markov Property for Diffusions</h2>
<p>Let’s start by exhibiting the Markov property of Brownian motion. To see this, consider <span class="math inline">\((\mathcal{F}_t,t\geq 0)\)</span>, the natural filtration of the Brownian motion <span class="math inline">\((B_t,t\geq 0)\)</span>. Consider <span class="math inline">\(g(B_t)\)</span> for some time <span class="math inline">\(t\)</span> and bounded function <span class="math inline">\(g\)</span>. (For example, <span class="math inline">\(g\)</span> could be an indicator function.) Consider also a random variable <span class="math inline">\(W\)</span> that is <span class="math inline">\(\mathcal{F}_s\)</span> measurable for <span class="math inline">\(s &lt; t\)</span>. (For example, <span class="math inline">\(W\)</span> could be <span class="math inline">\(B_s\)</span> or <span class="math inline">\(1_{B_s &gt; 0}\)</span>.) Let’s compute <span class="math inline">\(\mathbb{E}[g(B_t)W]\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[g(B_t)W] &amp;= \mathbb{E}[\mathbb{E}[Wg(B_t - B_s + B_s)|\mathcal{F}_s]]
\end{align*}\]</span></p>
<p>The random variable <span class="math inline">\((B_t - B_s)\)</span> follows a <span class="math inline">\(\mathcal{N}(0,t-s)\)</span> distribution. By LOTUS,</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[g(B_t)W]
&amp;= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)|\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\
&amp;= \{\text{ Using the fact that }B_t - B_s \perp B_s\}\\
&amp;= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\
&amp;= \int_{\mathbb{R}} \mathbb{E}[W g(y + B_s)]\frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy
\end{align*}\]</span></p>
<p>By Fubini’s theorem, the integral and the expectation operator can be interchanged, and since <span class="math inline">\(W\)</span> is <span class="math inline">\(\mathcal{F}_s\)</span> measurable, it follows from the definition of conditional expectations that:</p>
<p><span id="eq-conditional-expectation-of-function-brownian-motion"><span class="math display">\[
\begin{align*}
\mathbb{E}[g(B_t)|\mathcal{F}_s] &amp;= \int_{\mathbb{R}} g(y + B_s) \frac{e^{-\frac{y^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy
\end{align*}
\tag{1}\]</span></span></p>
<p>We make two important observations. First, the right hand side is a function of <span class="math inline">\(s,t\)</span> and <span class="math inline">\(B_s\)</span> only (and not of the Brownian motion before time s). In particular, we have:</p>
<p><span class="math display">\[
\mathbb{E}[g(B_t)|\mathcal{F}_s] = \mathbb{E}[g(B_t)|B_s]
\]</span></p>
<p>This holds for any bounded function <span class="math inline">\(g\)</span>. In particular, it holds for all indicator functions. This implies that the conditional distribution of <span class="math inline">\(B_t\)</span> given <span class="math inline">\(\mathcal{F}_s\)</span> depends solely on <span class="math inline">\(B_s\)</span>, and not on other values before time <span class="math inline">\(s\)</span>. Second, the right-hand side is <em>time-homogenous</em> in the sense that it depends on the time difference <span class="math inline">\(t-s\)</span>.</p>
<p>We have just shown that Brownian motion is a <em>time-homogenous Markov process</em>.</p>
<div id="def-markov-process" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Markov process.) </strong></span>Consider a stochastic process <span class="math inline">\((X_t,t\geq 0)\)</span> and its natural filtration <span class="math inline">\((\mathcal{F}_t,t\geq 0)\)</span>. It is said to be a <em>Markov process</em> if and only if for any (bounded) function <span class="math inline">\(g: \mathbb{R} \to \mathbb{R}\)</span>, we have:</p>
<p><span id="eq-markov-process"><span class="math display">\[
\mathbb{E}[g(X_t) | \mathcal{F}_s] = \mathbb{E}[g(X_t) | X_s], \quad \forall t \geq 0, \forall s \leq t
\tag{2}\]</span></span></p>
</div>
<p>This implies that <span class="math inline">\(\mathbb{E}[g(X_t)|\mathcal{F}_s]\)</span> is an explicit function of <span class="math inline">\(s\)</span>, <span class="math inline">\(t\)</span> and <span class="math inline">\(X_s\)</span>. It is said to be <em>time-homogenous</em>, if it is a function of <span class="math inline">\(t-s\)</span> and <span class="math inline">\(X_s\)</span>. Since the above holds for all bounded <span class="math inline">\(g\)</span>, the conditional distribution of <span class="math inline">\(X_t\)</span> given <span class="math inline">\(\mathcal{F}_s\)</span> is the same as the conditional distribution of <span class="math inline">\(X_t\)</span> given <span class="math inline">\(X_s\)</span>.</p>
<p>One way to compute the conditional distribution of <span class="math inline">\(X_t\)</span> given <span class="math inline">\(\mathcal{F}_s\)</span> is to compute the conditional MGF given <span class="math inline">\(\mathcal{F}_s\)</span>, that is:</p>
<p><span id="eq-conditional-mgf-of-xt"><span class="math display">\[
\mathbb{E}[e^{a X_t}|\mathcal{F}_s], \quad a \geq 0
\tag{3}\]</span></span></p>
<p>The process would be Markov, if the conditional MGF is an explicit function of <span class="math inline">\(s\)</span>, <span class="math inline">\(t\)</span> and <span class="math inline">\(X_s\)</span>.</p>
<div id="exm-brownian-motion-is-markov" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 </strong></span>(Brownian Motion is Markov) Let <span class="math inline">\((B_t,t\geq 0)\)</span> be a standard brownian motion. Our claim is that the brownian motion is a markov process.</p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[e^{a B_t}|\mathcal{F}_s] &amp;= \mathbb{E}[e^{a (B_t - B_s + B_s)}|\mathcal{F}_s]\\
&amp; \{ \text{ since }B_s \text{ is }\mathcal{F}_s-\text{ measurable }\}\\
&amp;= e^{a B_s} \mathbb{E}[e^{a (B_t - B_s)}|\mathcal{F}_s]\\
&amp; \{ \text{ since }B_t - B_s \perp \mathcal{F}_s \}\\
&amp;= e^{a B_s} \mathbb{E}[e^{a (B_t - B_s)}]\\
&amp;= e^{a B_s} e^{\frac{1}{2}a^2(t-s)}
\end{align*}\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>An equivalent (but more symmetric) way to express the Markov property is to say that <em>the future of the process is independent of the past, when conditioned on the present</em>. Concretely, this means that for any <span class="math inline">\(r &lt; s&lt; t\)</span>, we have that <span class="math inline">\(X_t\)</span> is independent of <span class="math inline">\(X_r\)</span>, when we condition on <span class="math inline">\(X_s\)</span>.</p>
<p>The conditional distribution of <span class="math inline">\(X_t\)</span> given <span class="math inline">\(X_s\)</span> is well described using <em>transition probabilities</em>. We will more interested in a case well these probabilities admit a density <span class="math inline">\(f_{X_t|X_s=x}(y)\)</span>. More precisely, for such a Markov process, we have:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}[g(X_t)|X_s = x] &amp;= \int_{\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\
&amp;=\int_{\mathbb{R}} g(y) p(y,t|x,s) dy
\end{align*}
\]</span></p>
<p>Here, we explicitly write the left-hand side as a function of space, that is, the position <span class="math inline">\(X_s\)</span>, by fixing <span class="math inline">\(X_s = x\)</span>. In words, the <em>transition probability density</em> <span class="math inline">\(p(y,t|x,s)\)</span> represents the probability density that starting from <span class="math inline">\(X_s = x\)</span> at time <span class="math inline">\(s\)</span>, the process ends up at <span class="math inline">\(X_t = y\)</span> at time <span class="math inline">\(t &gt; s\)</span>. If the process is time-homogenous, this only depends on the time difference <span class="math inline">\((t-s)\)</span> and we write <span class="math inline">\(p(y,t|x,s)\)</span>. From <a href="#eq-conditional-expectation-of-function-brownian-motion">Equation&nbsp;1</a>, we can write: <span class="math display">\[
\mathbb{E}[g(B_t)|B_s = x] = \int_{\mathbb{R}} g(u + x) \frac{e^{-\frac{u^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} du
\]</span></p>
<p>In the above expression, the random variable <span class="math inline">\(B_t - B_s\)</span> takes some value <span class="math inline">\(u \in \mathbb{R}\)</span> and <span class="math inline">\(B_s = x\)</span> is fixed. Then, <span class="math inline">\(B_t\)</span> takes the value <span class="math inline">\(u + x\)</span>. Let <span class="math inline">\(y = u + x\)</span>. Then, <span class="math inline">\(u = y - x\)</span>. Consequently, we may write:</p>
<p><span class="math display">\[
\mathbb{E}[g(B_t)|B_s = x] = \int_{\mathbb{R}} g(y) \frac{e^{-\frac{(y-x)^2}{2(t-s)}}}{\sqrt{2\pi(t-s)}} dy
\]</span></p>
<p>So, the transition density function for standard Brownian motion is:</p>
<p><span id="eq-brownian-motion-transition-density-function"><span class="math display">\[
p(y,t|x,0)= \frac{e^{-\frac{(y-x)^2}{2s}}}{\sqrt{2\pi s}}, \quad s&gt;0, x,y\in\mathbb{R}
\tag{4}\]</span></span></p>
<p>This function is sometimes called the <em>heat kernel</em>, as it relates to the <em>heat equation</em>.</p>
<p>The Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process <span class="math inline">\((X_t,t\geq 0)\)</span> at different times. Consider the functions <span class="math inline">\(f = \mathbf{1}_A\)</span> and <span class="math inline">\(g = \mathbf{1}_B\)</span> from <span class="math inline">\(\mathbb{R} \to \mathbb{R}\)</span>, where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are two intervals in <span class="math inline">\(\mathbb{R}\)</span>. Let’s compute <span class="math inline">\(\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) = \mathbb{E}[\mathbf{1}_{A} \mathbf{1}_{B}] = \mathbb{E}[f(X_{t_1}) g(X_{t_2})]\)</span> for <span class="math inline">\(t_1 &lt; t_2\)</span>. By the properties of conditional expectation and the Markov property, we have:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &amp;= \mathbb{E}[f(X_{t_1})g(X_{t_2})]\\
&amp;= \mathbb{E}[f(X_{t_1})\mathbb{E}[g(X_{t_2})|\mathcal{F}_{t_1}]]\\
&amp;= \mathbb{E}[f(X_{t_1})\mathbb{E}[g(X_{t_2})|X_{t_1}]]
\end{align*}\]</span></p>
<p>Assuming that the process is time-homogenous and admits a transition density <span class="math inline">\(p(y,t|x,0)\)</span> as for Brownian motion, this becomes:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(X_{t_1} \in A, X_{t_2} \in B) &amp;= \int_{\mathbb{R}} f(x_1) \left(\int_{\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \right) p(x_1,t_1|x_0,0) dx_1\\
&amp;= \int_{A} \left(\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \right) p(x_1,t_1|x_0,0) dx_1
\end{align*}\]</span></p>
<p>This easily generalizes to any finite-dimensional distribution of <span class="math inline">\((X_t, t\geq 0)\)</span>.</p>
<div id="exm-markov-versus-martingale" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 </strong></span>(Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift <span class="math inline">\((X_t, t \geq 0)\)</span>, where <span class="math inline">\(X_t = \sigma B_t + \mu t\)</span>. Conversely, take <span class="math inline">\(Y_t = \int_0^t X_s dB_s\)</span>, where <span class="math inline">\(X_s = \int_0^s B_u dB_u\)</span>. The integrand <span class="math inline">\(X_s\)</span> depends on whole Brownian motion path upto time <span class="math inline">\(s\)</span> and not just on <span class="math inline">\(B_s\)</span>.</p>
</div>
<div id="nte-functions-of-markov" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Functions of Markov Processes
</div>
</div>
<div class="callout-body-container callout-body">
<p>It might be tempting to think that if <span class="math inline">\((X_t,t\geq 0)\)</span> is a Markov process, then the process defined by <span class="math inline">\(Y_t = f(X_t)\)</span> for some reasonable function <span class="math inline">\(f\)</span> is also Markov. Indeed, one could hope to write for an arbitrary bounded function <span class="math inline">\(g\)</span>:</p>
<p><span id="eq-functions-of-markov-process"><span class="math display">\[
\begin{align*}
\mathbb{E}[g(Y_t)|\mathcal{F}_s] = \mathbb{E}[g(f(X_t))|\mathcal{F}_s] = \mathbb{E}[g(f(X_t))|\mathcal{X}_s]
\end{align*}
\tag{5}\]</span></span></p>
<p>by using the Markov property of <span class="math inline">\((X_t,t\geq 0)\)</span>. The flaw in this reasoning is that the Markov property should hold for the natural fitration <span class="math inline">\((\mathcal{F}_t^Y,t\geq 0)\)</span> of the process <span class="math inline">\((Y_t,t\geq 0)\)</span> and not the one of <span class="math inline">\((X_t,t\geq 0)\)</span>, <span class="math inline">\((\mathcal{F}_t^X,t\geq 0)\)</span>. It might be that the filtration <span class="math inline">\((\mathcal{F}_t^Y,t\geq 0)\)</span> has less information that <span class="math inline">\((\mathcal{F}_t^X,t\geq 0)\)</span>, especially, if the function <span class="math inline">\(f\)</span> is not one-to-one. For example, if <span class="math inline">\(f(x)=x^2\)</span>, then <span class="math inline">\(\mathcal{F}_t^Y\)</span> has less information than <span class="math inline">\(\mathcal{F}_t^X\)</span> as we cannot recover the sign of <span class="math inline">\(X_t\)</span> knowing <span class="math inline">\(Y_t\)</span>. In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when <span class="math inline">\(f\)</span> is not one-to-one.</p>
</div>
</div>
<p>It turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.</p>
<div id="thm-diffusions-are-markov-processes" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Diffusions are Markov processes.) </strong></span>Let <span class="math inline">\((B_t,t\geq 0)\)</span> be a standard Brownian motion. Let <span class="math inline">\(\mu : \mathbb{R} \to \mathbb{R}\)</span> and <span class="math inline">\(\sigma: \mathbb{R} \to \mathbb{R}\)</span> be differentiable functions with bounded derivatives on <span class="math inline">\([0,T]\)</span>. Then, the diffusion with the SDE</p>
<p><span class="math display">\[
dX_t = \mu(X_t) dt + \sigma(X_t)dB_t, \quad X_0 = x_0
\]</span></p>
<p>defines a time-homogenous markov process on <span class="math inline">\([0,T]\)</span>.</p>
</div>
<p>An analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments.</p>
<p><em>Proof.</em></p>
<p>By the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes">existence and uniqueness theorem</a>, this stochastic initial value problem(SIVP) defines a unique continous adapted process <span class="math inline">\((X_t,t\leq T)\)</span>. Let <span class="math inline">\((\mathcal{F}_t^X,t\geq 0)\)</span> be the natural filtration of <span class="math inline">\((X_t,t\leq T)\)</span>. For a fixed <span class="math inline">\(t &gt; 0\)</span>, consider the process <span class="math inline">\(W_s = B_{t+s} - B_t, s \geq 0\)</span>. Let <span class="math inline">\((\mathcal{F}_t,t \geq 0)\)</span> be the natural filtration of <span class="math inline">\((B_t,t \geq 0)\)</span>. It turns out that the process <span class="math inline">\((W_s,s \geq 0)\)</span> is a standard brownian motion independent of <span class="math inline">\(\mathcal{F}_t\)</span> (<a href="#exr-shifted-brownian-motion">Exercise&nbsp;1</a>). For <span class="math inline">\(s \geq 0\)</span>, we consider the SDE:</p>
<p><span class="math display">\[
dY_s = \mu (Y_s) ds + \sigma(Y_s) dW_s, \quad Y_0 = X_t
\]</span></p>
<p>Again by the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes">existence and uniqueness theorem</a>, there exists a unique solution to the SIVP that is adapted to the natural filtration of <span class="math inline">\(W\)</span>. Note that, the shifted process <span class="math inline">\((X_{t+s},s\geq 0)\)</span> is <em>the</em> solution to this SIVP since:</p>
<p><span class="math display">\[\begin{align*}
X_{t+s} &amp;= X_{t} + \int_{t}^{t+s}\mu(X_u) du + \int_{t}^{t+s}\sigma(X_u) dB_u
\end{align*}\]</span></p>
<p>Perform a change of variable <span class="math inline">\(v = u - t\)</span>. Then, <span class="math inline">\(dv = du\)</span>, <span class="math inline">\(dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v\)</span>. So,</p>
<p><span class="math display">\[\begin{align*}
X_{t+s} &amp;= X_{t} + \int_{0}^{s}\mu(X_{t+v}) dv + \int_{0}^{s}\sigma(X_{t+v}) dW_v
\end{align*}\]</span></p>
<p>Let <span class="math inline">\(Y_v= X_{t+v}\)</span>, <span class="math inline">\(Y_0 = X_t\)</span>. Then,</p>
<p><span class="math display">\[\begin{align*}
Y_s &amp;= Y_0 + \int_{0}^{s}\mu(Y_v) dv + \int_{0}^{s}\sigma(Y_v) dW_v
\end{align*}\]</span></p>
<p>Thus, we conclude that for any interval <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
\mathbb{P}(X_{t+s} \in A|\mathcal{F}_t^X) = \mathbb{P}(Y_s \in A | \mathcal{F}_t^X)
\]</span></p>
<p>But, since <span class="math inline">\((Y_s,s \geq 0)\)</span> depends on <span class="math inline">\(\mathcal{F}_t^X\)</span> only through <span class="math inline">\(X_t\)</span> (because <span class="math inline">\((W_s,s \geq 0)\)</span> is independent of <span class="math inline">\(\mathcal{F}_t\)</span>), we conclude that <span class="math inline">\(\mathbb{P}(X_{t+s} \in A|\mathcal{F}_t^X) = \mathbb{P}(X_{t+s} \in A|X_t)\)</span>, so <span class="math inline">\((X_t,t \geq 0)\)</span> is a time-homogenous markov process. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="the-strong-markov-property" class="level2">
<h2 class="anchored" data-anchor-id="the-strong-markov-property">The Strong Markov Property</h2>
<p>The Doob’s Optional Stopping theorem extended some properties of martingales to stopping times. The Markov property can also be extended to stopping times for certain processes. These processes are called <em>strong Markov processes</em>.</p>
<p>We know, that the sigma-algebra <span class="math inline">\(\mathcal{F}_t\)</span> represents the set of all observable events upto time <span class="math inline">\(t\)</span>. What is the sigma-algebra of observable events at a random stopping time <span class="math inline">\(\tau\)</span>?</p>
<div id="def-sigma-algebra-of-the-past" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (<span class="math inline">\(\sigma\)</span>-algebra of <span class="math inline">\(\tau\)</span>-past) </strong></span>Let <span class="math inline">\((\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\geq 0},\mathbb{P})\)</span> be a filtered probability space. The sigma-algebra at the stopping time <span class="math inline">\(\tau\)</span> is then:</p>
<p><span id="eq-sigma-algebra-of-the-past"><span class="math display">\[
\mathcal{F}_{\tau} = \{A \in \mathcal{F}_\infty : A \cap \{\tau \leq t\} \in \mathcal{F}_t, \forall t \geq 0 \}
\tag{6}\]</span></span></p>
</div>
<p>In words, an event <span class="math inline">\(A\)</span> is in <span class="math inline">\(\mathcal{F}_\tau\)</span>, if we can determine if <span class="math inline">\(A\)</span> and <span class="math inline">\(\{\tau \leq t\}\)</span> both occurred or not based on the information <span class="math inline">\(\mathcal{F}_t\)</span> known at any arbitrary time <span class="math inline">\(t\)</span>. You should be able to tell the value of the random variable <span class="math inline">\(\mathbf{1}_A \cdot \mathbf{1}_{\{\tau \leq t\}}\)</span> given <span class="math inline">\(\mathcal{F}_t\)</span> for any arbitrary time <span class="math inline">\(t \geq 0\)</span>.</p>
<p>For example, if <span class="math inline">\(\tau &lt; \infty\)</span>, the event <span class="math inline">\(\{B_\tau &gt; 0\}\)</span> is in <span class="math inline">\(\mathcal{F}_\tau\)</span>. However, the event <span class="math inline">\(\{B_1 &gt; 0\}\)</span> is not in <span class="math inline">\(\mathcal{F}_\tau\)</span> in general, since <span class="math inline">\(A \cap \{\tau \leq t\}\)</span> is not in <span class="math inline">\(\mathcal{F}_t\)</span> for <span class="math inline">\(t &lt; 1\)</span>. Roughly speaking, a random variable that is <span class="math inline">\(\mathcal{F}_\tau\)</span>-measurable should be thought of as an explicit function of <span class="math inline">\(X_\tau\)</span>. With this new object, we are ready to define the <em>strong markov property</em>.</p>
<div id="def-strong-markov-property" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Strong Markov Property) </strong></span>Let <span class="math inline">\((X_t,t\geq 0)\)</span> be a stochastic process and let <span class="math inline">\((\mathcal{F}_t,t\geq 0)\)</span> be its natural filtration. The process <span class="math inline">\((X_t,t\geq 0)\)</span> is said to be <em>strong markov</em> if for any stopping time <span class="math inline">\(\tau\)</span> for the filtration of the process and any bounded function <span class="math inline">\(g\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}[g(X_{t+\tau})|\mathcal{F}_\tau] = \mathbb{E}[g(X_{t+\tau})|X_\tau]
\]</span></p>
</div>
<p>This means that <span class="math inline">\(X_{t+\tau}\)</span> depends on <span class="math inline">\(\mathcal{F}_\tau\)</span> solely through <span class="math inline">\(X_\tau\)</span> (whenever <span class="math inline">\(\tau &lt; \infty\)</span>). It turns out that Brownian motion is a strong markov process. In fact a stronger statement holds which generalizes <a href="#exr-shifted-brownian-motion">Exercise&nbsp;1</a>.</p>
<div id="thm-shifted-brownian-motion-about-a-stopping-time" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 </strong></span>Let <span class="math inline">\(\tau\)</span> be a stopping time for the filtration of the Brownian motion <span class="math inline">\((B_t,t\geq 0)\)</span> such that <span class="math inline">\(\tau &lt; \infty\)</span>. Then, the process:</p>
<p><span class="math display">\[
(B_{t+\tau} - B_{\tau},t\geq 0)
\]</span></p>
<p>is a standard brownian motion independent of <span class="math inline">\(\mathcal{F}_\tau\)</span>.</p>
</div>
<div id="exm-brownian-motion-is-strong-markov" class="theorem example">
<p><span class="theorem-title"><strong>Example 3 </strong></span>(Brownian motion is strong Markov) To see this, let’s compute the conditional MGF as in <a href="#eq-conditional-mgf-of-xt">Equation&nbsp;3</a>. We have:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}[e^{aB_{t+\tau}}|\mathcal{F}_\tau] &amp;= \mathbb{E}[e^{a(B_{t+\tau} - B_\tau + B_\tau)}|\mathcal{F}_\tau]\\
&amp;= e^{aB_\tau} \mathbb{E}[e^{a(B_{t+\tau} - B_\tau)}|\mathcal{F}_\tau]\\
&amp; \{ B_\tau \text{ is }\mathcal{F}_\tau-\text{measurable }\}\\
&amp;= e^{aB_\tau}\mathbb{E}[e^{a(B_{t+\tau} - B_\tau)}]\\
&amp; \{ (B_{t+\tau} - B_\tau) \perp \mathcal{F}_\tau\}\\
&amp;= e^{aB_\tau}e^{\frac{1}{2}a^2 t}\\
\end{align*}
\]</span></p>
<p>Thus, the conditional MGF is an explicit function of <span class="math inline">\(B_\tau\)</span> and <span class="math inline">\(t\)</span>. This proves the proposition. <span class="math inline">\(\blacksquare\)</span></p>
</div>
<p><em>Proof</em> of <a href="#thm-shifted-brownian-motion-about-a-stopping-time">Theorem&nbsp;2</a>.</p>
<p>We first consider for fixed <span class="math inline">\(n\)</span> the discrete valued stopping time:</p>
<p><span class="math display">\[
\tau_n = \frac{k + 1}{2^n}, \quad \text{ if } \frac{k}{2^n} \leq \tau &lt; \frac{k+1}{2^n}, k\in \mathbb{N}
\]</span></p>
<p>In other words, if <span class="math inline">\(\tau\)</span> occurs in the interval <span class="math inline">\([\frac{k}{2^n},\frac{k+1}{2^n})\)</span>, we stop at the next dyadic <span class="math inline">\(\frac{k+1}{2^n}\)</span>. By construction <span class="math inline">\(\tau_n\)</span> depends only on the process in the past. Consider the process <span class="math inline">\(W_t = B_{t + \tau_n} - B_{\tau_n}, t \geq 0\)</span>. We show it is a standard brownian motion independent of <span class="math inline">\(\tau_n\)</span>. This is feasible as we can decompose over the discrete values taken by <span class="math inline">\(\tau_n\)</span>. More, precisely, take <span class="math inline">\(E \in \mathcal{F}_{\tau_n}\)</span>, and some generic event <span class="math inline">\(\{W_t \in A\}\)</span> for the process <span class="math inline">\(W\)</span>. Then, by decomposing over the values of <span class="math inline">\(\tau_n\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{P}(\{W_t \in A\} \cap E) &amp;= \sum_{k=0}^\infty \mathbb{P}\left(\{W_t \in A\} \cap E \cap \{\tau_n = \frac{k}{2^n}\}\right)\\
&amp;= \sum_{k=0}^\infty \mathbb{P}\left(\{(B_{t+k/2^n} - B_{k/2^n}) \in A\} \cap E \cap \{\tau_n = \frac{k}{2^n}\}\right)\\
&amp;= \sum_{k=0}^\infty \mathbb{P}\left(\{(B_{t+k/2^n} - B_{k/2^n}) \in A\}\right) \times \mathbb{P}\left( E \cap \{\tau_n = \frac{k}{2^n}\}\right)
\end{align*}
\]</span></p>
<p>since <span class="math inline">\((B_{t+k/2^n} - B_{k/2^n})\)</span> is independent of <span class="math inline">\(\mathcal{F}_{k/2^n}\)</span> by <a href="#exr-shifted-brownian-motion">Exercise&nbsp;1</a> and since <span class="math inline">\(E \cap \{\tau_n = \frac{k}{2^n}\} \in \mathcal{F}_{k/2^n}\)</span> by definition of stopping time. But, given <span class="math inline">\(\{\tau_n = k/2^n\}\)</span>, the event <span class="math inline">\(\{(B_{t+k/2^n} - B_{k/2^n}) \in A\}\)</span> is the same as <span class="math inline">\(\{B_t \in A\} = \{W_t \in A\}\)</span>, since this process is now a standard brownian motion. Thus, <span class="math inline">\(\mathbb{P}\{(B_{t+k/2^n} - B_{k/2^n}) \in A\} = \mathbb{P}\{B_t \in A\} = \mathbb{P}\{W_t \in A\}\)</span>, dropping the dependence on <span class="math inline">\(k\)</span>. The sum over <span class="math inline">\(k\)</span> then yields:</p>
<p><span class="math display">\[
\mathbb{P}\left(\{W_t \in A\}\cap E\right) = \mathbb{P}(W_t \in A) \mathbb{P}(E)
\]</span></p>
<p>as claimed. The extension to <span class="math inline">\(\tau\)</span> is done by using continuity of paths. We have:</p>
<p><span class="math display">\[
\lim_{n \to \infty} B_{t + \tau_n} - B_{\tau_n} = B_{t+\tau} - B_{\tau} \text{ almost surely}
\]</span></p>
<p>Note, that this only uses right continuity! Moreover, this implies that <span class="math inline">\(B_{t+\tau} - B_\tau\)</span> is independent of <span class="math inline">\(\mathcal{F}_{\tau_n}\)</span> for all <span class="math inline">\(n\)</span>. Again by (right-)continuity this extends to independence of <span class="math inline">\(\mathcal{F}_\tau\)</span>. The limiting distribution of the process is obtained by looking at the finite dimensional distributions of the increments of <span class="math inline">\(B_{t+\tau_n} - B_{\tau_n}\)</span> for a finite number of <span class="math inline">\(t\)</span>’s and taking the limit as above. <span class="math inline">\(\blacksquare\)</span></p>
<p>Most diffusions also enjoy the strong markov property, as long as the functions <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\mu\)</span> encoding the volatility and drift are nice enough. This is the case for the diffusions we have considered.</p>
<div id="thm-most-diffusions-are-strong-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (Most diffusions are strong markov) </strong></span>Consider a diffusion <span class="math inline">\((X_t,t\leq T)\)</span> as as in <a href="#thm-diffusions-are-markov-processes">Theorem&nbsp;1</a>. Then, the diffusion has strong markov property.</p>
</div>
<p>The proof follows the line of the one of <a href="#thm-diffusions-are-markov-processes">Theorem&nbsp;1</a></p>
<p><em>Proof.</em></p>
<p>Consider the time-homogenous diffusion:</p>
<p><span class="math display">\[
dX_t = \mu(X_t)dt + \sigma(X_t)dB_t
\]</span></p>
<p>By the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes">existence and uniqueness theorem</a>, this SIVP defines a unique continuous adapted process <span class="math inline">\((X_t,t \geq 0)\)</span>. Let <span class="math inline">\(\mathfrak{F}=(\mathcal{F}_t^X,t \geq 0)\)</span> be the natural filtration of <span class="math inline">\((X_t, t\leq T)\)</span>. Let <span class="math inline">\(\tau\)</span> be a stopping time for the filtration <span class="math inline">\(\mathfrak{F}\)</span> and consider the process <span class="math inline">\(W_t = B_{t+\tau} - B_\tau\)</span>. From <a href="#thm-shifted-brownian-motion-about-a-stopping-time">Theorem&nbsp;2</a>, we know that the process <span class="math inline">\((W_t,t\geq 0)\)</span> is a standard brownian motion independent <span class="math inline">\(\mathcal{F}_\tau\)</span>. For <span class="math inline">\(s \geq 0\)</span>, we consider the SDE:</p>
<p><span id="eq-diffusion-of-Y"><span class="math display">\[
dY_s = \mu(Y_s)ds + \sigma(Y_s)dW_s, \quad Y_0 = X_\tau
\tag{7}\]</span></span></p>
<p>Again by the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#existence-and-uniqueness-of-sdes">existence and uniqueness theorem</a>, there exists a unique solution to the SIVP that is adapted to the natural filtration of <span class="math inline">\(W\)</span>. We claim that <span class="math inline">\((X_{s+\tau},s \geq 0)\)</span> is the solution to this equation, since:</p>
<p><span class="math display">\[
X_{s+\tau} = X_\tau + \int_\tau^{s+\tau} \mu(X_u)du + \int_{\tau}^{s+\tau} \sigma(X_u)dB_u
\]</span></p>
<p>Perform a change of variable <span class="math inline">\(v = u - \tau\)</span>. Then, the limits of integration bare, <span class="math inline">\(v = 0\)</span> and <span class="math inline">\(v = s\)</span>. And <span class="math inline">\(dv = du\)</span>.</p>
<p><span class="math inline">\(dB_u \approx B_{u_2} - B_{u_1} = B(v_1 + \tau) - B(v_2 + \tau) = W(v_2) - W(v_1) =dW_v\)</span>.</p>
<p><span class="math display">\[
X_{s+\tau} = X_\tau + \int_0^{s} \mu(X_{v+\tau})dv + \int_{0}^{s} \sigma(X_{v+\tau})dW_v
\]</span></p>
<p>If we let <span class="math inline">\(Y_0 = X_\tau\)</span>, <span class="math inline">\(Y_v = X_{v+\tau}\)</span>, we recover the dynamics of <span class="math inline">\((Y_v,v \geq 0)\)</span> in <a href="#eq-diffusion-of-Y">Equation&nbsp;7</a>. So, <span class="math inline">\((X_{s+\tau},s\geq 0)\)</span> is the solution to the SIVP in <a href="#eq-diffusion-of-Y">Equation&nbsp;7</a>. Thus, we conclude for any interval <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
\mathbb{P}(X_{s+\tau} \in A | \mathcal{F}_\tau^X) = \mathbb{P}(Y_v \in A| \mathcal{F}_\tau^X)
\]</span></p>
<p>But, since <span class="math inline">\((Y_v,v\geq 0)\)</span> depends on <span class="math inline">\(\mathcal{F}_\tau^X\)</span> only through <span class="math inline">\(X_\tau\)</span>, we conclude that <span class="math inline">\(\mathbb{P}(X_{s + \tau} \in A | \mathcal{F}_\tau^X) = \mathbb{P}(X_{s + \tau} \in A| X_\tau)\)</span>. Consequently, <span class="math inline">\((X_t,t \geq 0)\)</span> is a strong-markov process. <span class="math inline">\(\blacksquare\)</span></p>
<div id="nte-extension-of-optional-sampling" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Extension of optional sampling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a continuous martingale <span class="math inline">\((M_t, t\leq T)\)</span> for a filtration <span class="math inline">\((\mathcal{F}_t, t\geq 0)\)</span> and a stopping time <span class="math inline">\(\tau\)</span> for the same filtration. Suppose we would like to compute for some <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}[M_T \mathbf{1}_{\{\tau \leq T\}}]
\]</span></p>
<p>It would be tempting to condition on <span class="math inline">\(\mathcal{F}_\tau\)</span> and write <span class="math inline">\(\mathbb{E}[M_T |\mathcal{F}_\tau] = M_\tau\)</span> on the event <span class="math inline">\(\{\tau \leq T\}\)</span>. We would then conclude that:</p>
<p><span class="math display">\[
\mathbb{E}[M_T 1_{\{\tau \leq T\}}] = \mathbb{E}[1_{\{\tau \leq T\}} \mathbb{E}[M_T|\mathcal{F}_\tau] ] = \mathbb{E}[M_\tau 1_{\{\tau \leq T\}}]
\]</span></p>
<p>In some sense, we have extended the martingale property to stopping times. This property can be proved under reasonable assumptions on <span class="math inline">\((M_t,t\leq T)\)</span> (for example, if it is positive). Indeed, it suffices to approximate <span class="math inline">\(\tau\)</span> by discrete valued stopping time <span class="math inline">\(\tau_n\)</span> as in the proof of <a href="#thm-shifted-brownian-motion-about-a-stopping-time">Theorem&nbsp;2</a>. One can then apply martingale property at a fixed time.</p>
</div>
</div>
</section>
<section id="the-heat-equation" class="level2">
<h2 class="anchored" data-anchor-id="the-heat-equation">The Heat Equation</h2>
<p>We look at more detail on how PDEs come up when computing quantities related to Markov processes.</p>
<div id="exm-heat-equation-and-brownian-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 4 </strong></span>(Heat Equation and Brownian motion) Let <span class="math inline">\(f(t,x)\)</span> be a function of time and space. The heat equation in <span class="math inline">\(1+1\)</span>-dimension (one dimension of time, one dimension of space) is the PDE:</p>
<p><span id="eq-heat-equation-in-2d"><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t} &amp;= \frac{1}{2}\frac{\partial^2 f}{\partial x^2}
\end{align*}
\tag{8}\]</span></span></p>
<p>In <span class="math inline">\(1+d\)</span> (one dimension of time, <span class="math inline">\(d\)</span> dimensions of space), the heat equation is:</p>
<p><span id="eq-heat-equation-in-d-plus-one-dims"><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t} &amp;= \frac{1}{2}\nabla^2 f
\end{align*}
\tag{9}\]</span></span></p>
<p>where <span class="math inline">\(\nabla^2\)</span> is the Laplacian operator.</p>
<p>Let <span class="math inline">\((X_t,t \geq 0)\)</span> be a brownian motion starting at <span class="math inline">\(X_0 = x\)</span> with probability density:</p>
<p><span id="eq-initial-condition"><span class="math display">\[
f(0,x) = g(x)
\tag{10}\]</span></span></p>
<p>where <span class="math inline">\(g\)</span> is a function of space.</p>
<p>Let <span class="math inline">\(f(t,u)\)</span> be the probability density that the process ends up at <span class="math inline">\(X_t=u\)</span> at time <span class="math inline">\(t\)</span>. By the law of total probability, we have:</p>
<p><span class="math display">\[
\begin{align*}
f(t,x) &amp;\approx \sum_{y} \mathbb{P}\left\{X_0 = y \right\} \times \mathbb{P}\left\{X_t = x | X_0 = y \right\}\\
&amp;= \int_{-\infty}^\infty g(y)\cdot p(x,t|y,0)dy\\
\end{align*}
\]</span></p>
<p>Observe that:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}[g(X_t)|X_0 = x] &amp;= \int_{-\infty}^\infty g(y) \cdot p(y,t|x,0)dy\\
&amp;=\int_{-\infty}^\infty g(y) \cdot p(x,t|y,0)dy
\end{align*}
\]</span></p>
<p>Thus, the function <span class="math inline">\(f\)</span> can be represented as a specific type of space average. It can be represented as an average of <span class="math inline">\(g(B_t)\)</span> over brownian paths starting at <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}[g(B_t)|B_0 = x]
\]</span></p>
<p>Our claim is that <span class="math inline">\(f\)</span> indeed satisfies the PDE (<a href="#eq-heat-equation-in-2d">Equation&nbsp;8</a>).</p>
<p>The gaussian transition probability density function (heat kernel) <span class="math inline">\(p(x,t|y,0)\)</span> is given by:</p>
<p><span class="math display">\[
p(x,t|y,0) = \frac{1}{\sqrt{2\pi t}}\exp\left(-\frac{(x-y)^2}{2t}\right)
\]</span></p>
<p>Differentiating <span class="math inline">\(p\)</span> with respect to <span class="math inline">\(t\)</span>, we have:</p>
<p><span id="eq-partial-with-respect-to-time"><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial t} p(x,t|y,0) &amp;= \frac{\sqrt{2\pi t} \exp\left(-\frac{(x-y)^2}{2t}\right) \frac{\partial}{\partial t}\left(-\frac{(x-y)^2}{2t}\right) - \exp\left(-\frac{(x-y)^2}{2t}\right)\sqrt{2\pi}\left(\frac{1}{2\sqrt{t}}\right)}{2\pi t}\\
&amp;=\sqrt{2\pi}\exp\left(-\frac{(x-y)^2}{2t}\right) \frac{\frac{(x-y)^2}{2t^{3/2}} - \frac{t}{2t^{3/2}}}{2\pi t}\\
&amp;= \exp\left(-\frac{(x-y)^2}{2t}\right) \frac{(x-y)^2 - t}{\sqrt{2\pi} (2t^{5/2}) }
\end{align*}
\tag{11}\]</span></span></p>
<p>Differentiating <span class="math inline">\(p\)</span> with respect to <span class="math inline">\(x\)</span>, we have:</p>
<p><span id="eq-first-derivative-with-respect-to-space"><span class="math display">\[
\begin{align*}
\frac{\partial }{\partial x} p(x,t|y,0) &amp;= \frac{1}{\sqrt{2\pi t}}\exp\left[-\frac{(x-y)^2}{2t}\right]\frac{\partial}{\partial x}\left(-\frac{(x-y)^2}{2t}\right)\\
&amp;= \frac{1}{\sqrt{2\pi t}} \cdot \left(-\frac{1}{\cancel{2} t}\right) \exp\left[-\frac{(x-y)^2}{2t}\right] \cdot \cancel{2}(x-y)\\
&amp;= -\frac{1}{t\sqrt{2\pi t}} (x-y)\exp\left[-\frac{(x-y)^2}{2t}\right]
\end{align*}
\tag{12}\]</span></span></p>
<p>Differentiating again with respect to space, we have:</p>
<p><span id="eq-second-derivative-with-respect-to-space"><span class="math display">\[
\begin{align*}
\frac{\partial^2}{\partial x^2} p(x,t|y,0) &amp;= -\frac{1}{t\sqrt{2\pi t}} \left[\exp\left\{-\frac{(x-y)^2}{2}\right\} + (x-y)\exp\left\{-\frac{(x-y)^2}{2}\right\}\left(-\frac{2(x-y)}{2y}\right)\right]\\
&amp;=-\frac{1}{t\sqrt{2\pi t}}\exp\left\{-\frac{(x-y)^2}{2}\right\} \left[1 - \frac{(x-y)^2}{t}\right]\\
&amp;=\frac{1}{t\sqrt{2\pi t}}\exp\left\{-\frac{(x-y)^2}{2}\right\} \left[\frac{(x-y)^2 - t}{t}\right]\\
&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{(x-y)^2}{2}\right\} \cdot \frac{(x-y)^2 - t}{t^{5/2}}
\end{align*}
\tag{13}\]</span></span></p>
<p>From <a href="#eq-partial-with-respect-to-time">Equation&nbsp;11</a> and <a href="#eq-second-derivative-with-respect-to-space">Equation&nbsp;13</a>, it follows that:</p>
<p><span class="math display">\[
\frac{\partial}{\partial t} p(x,t|y,0) = \frac{1}{2}\frac{\partial ^2}{\partial x^2} p(x,t|y,0)
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial t} \int_{-\infty}^\infty g(y) p(x,t|y,0)dy &amp;= \frac{1}{2}\frac{\partial^2}{\partial x^2} \int_{-\infty}^\infty g(y) p(x,t|y,0)dy \\
\frac{\partial }{\partial t}f(t,x) &amp;= \frac{1}{2}\frac{\partial^2 }{\partial x^2} f(t,x)
\end{align*}
\]</span></p>
</div>
<section id="robert-browns-erratic-motion-of-pollen" class="level3">
<h3 class="anchored" data-anchor-id="robert-browns-erratic-motion-of-pollen">Robert Brown’s erratic motion of pollen</h3>
<p>In the summer of 1827, the Scottish botanist Robert Brown observed that microscopic pollen grains suspended in water move in an erratic, highly irregular, zigzag pattern. It was only in 1905, that Albert Einstein could provide a satisfactory explanation of Brownian motion. He asserted that Brownian motion originates in the continual bombardment of the pollen grains by the molecules of the surrounding water. As a result of continual collisions, the particles themselves had the same kinetic energy as the water molecules. Thus, he showed that Brownian motion provided a solution (in a certain sense) to Fourier’s famous heat equation</p>
<p><span class="math display">\[
\frac{\partial u}{\partial t}(t,x) = \kappa \frac{\partial^2 u}{\partial x^2}(t,x)
\]</span></p>
</section>
<section id="albert-einsteins-proof-of-the-existence-of-brownian-motion" class="level3">
<h3 class="anchored" data-anchor-id="albert-einsteins-proof-of-the-existence-of-brownian-motion">Albert Einstein’s proof of the existence of Brownian motion</h3>
<p>We now summarize Einstein’s original 1905 argument. Let’s say that we are interested in the motion along the horizontal <span class="math inline">\(x\)</span>-axis. Let’s say we drop brownian particles in a liquid. Let <span class="math inline">\(f(t,x)\)</span> represent the number of particles per unit volume (density) at position <span class="math inline">\(x\)</span> at time <span class="math inline">\(t\)</span>. So, the number of particles in a small interval <span class="math inline">\(I=[x,x+dx]\)</span> of width <span class="math inline">\(dx\)</span> will be <span class="math inline">\(f(t,x)dx\)</span>.</p>
<p>Now, as time progresses, the number of particles in this interval <span class="math inline">\(I\)</span> will change. The brownian particles will zig-zag upon bombardment by the molecules of the liquid. Some particles will move out of the interval <span class="math inline">\(I\)</span>, while other particles will move in.</p>
<p>Let’s consider a timestep of length <span class="math inline">\(\tau\)</span>. Einstein’s probabilistic approach was to model the distance travelled by the particles or displacement of the particles as a random variable <span class="math inline">\(\Delta\)</span>. To determine how many particles end up in the interval <span class="math inline">\(I\)</span>, we start with the area to the right of the interval <span class="math inline">\(I\)</span>.</p>
<p>The density of particles at <span class="math inline">\(x+\Delta\)</span> is <span class="math inline">\(f(t,x+\Delta)\)</span>; the number of particles in a small interval of length <span class="math inline">\(dx\)</span> is <span class="math inline">\(f(t,x+\Delta)dx\)</span>. If we represent the probability density of the displacement by <span class="math inline">\(\phi(\Delta)\)</span>, then the number of particles at <span class="math inline">\(x+\Delta\)</span> that will move to <span class="math inline">\(x\)</span> will be <span class="math inline">\(dx \cdot f(t,x+\Delta)\phi(\Delta)\)</span>. We can apply the same logic to the left hand side. The number of particles at <span class="math inline">\(x - \Delta\)</span> that will move to <span class="math inline">\(x\)</span> will be <span class="math inline">\(dx \cdot f(t,x-\Delta)\phi(-\Delta)\)</span>. Assume that <span class="math inline">\(\phi(\Delta) = \phi(-\Delta)\)</span>.</p>
<p>Now, if we integrate these movements across the real line, then we get the number of particles at <span class="math inline">\(x\)</span> at a short time later <span class="math inline">\(t + \tau\)</span>.</p>
<p><span class="math display">\[
f(t+ \tau,x) dx = dx \int_{-\infty}^{\infty} f(t,x+\Delta) \phi(\Delta) d\Delta
\]</span></p>
<p>Now, we can get rid of <span class="math inline">\(dx\)</span>.</p>
<p><span id="eq-expression-for-density-at-later-time"><span class="math display">\[
f(t+ \tau,x) = \int_{-\infty}^{\infty} f(t,x+\Delta) \phi(\Delta) d\Delta
\tag{14}\]</span></span></p>
<p>The Taylor’s series expansion of <span class="math inline">\(f(t+\tau,x)\)</span> centered at <span class="math inline">\(t\)</span> (holding <span class="math inline">\(x\)</span> constant) is:</p>
<p><span class="math display">\[
f(t + \tau,x) = f(t,x) + \frac{\partial f}{\partial t}\tau + O(\tau^2)
\]</span></p>
<p>The Taylor’s series expansion of <span class="math inline">\(f(t,x+\Delta)\)</span> centered at <span class="math inline">\(x\)</span> (holding <span class="math inline">\(t\)</span> constant) is:</p>
<p><span class="math display">\[
f(t,x+\Delta) = f(t,x) + \frac{\partial f}{\partial x}\Delta + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\Delta^2 + O(\Delta^3)
\]</span></p>
<p>We can now substitute these into <a href="#eq-expression-for-density-at-later-time">Equation&nbsp;14</a> to get:</p>
<p><span class="math display">\[
\begin{align*}
f(t,x) + \frac{\partial f}{\partial t}\tau &amp;= \int_{-\infty}^{\infty}\left(f(t,x) + \frac{\partial f}{\partial x}\Delta + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\Delta^2\right) \phi(\Delta)d\Delta\\
&amp;= f(t,x) \int_{-\infty}^{\infty} \phi(\Delta)d\Delta \\
&amp;+ \frac{\partial f} {\partial x} \int_{-\infty}^{\infty} \Delta \phi(\Delta)d\Delta \\
&amp;+ \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta
\end{align*}
\]</span></p>
<p>Now, since the probability distribution of displacement <span class="math inline">\(\phi(\cdot)\)</span> is symmetric around the origin, the second term is zero. And we know, that if we integrate the density over <span class="math inline">\(\mathbb{R}\)</span>, we should get one, so the first term equals one. So, we get:</p>
<p><span class="math display">\[
f(t,x) + \frac{\partial f}{\partial t}\tau = f(t,x) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta
\]</span></p>
<p>Now, we can cancel the <span class="math inline">\(f\)</span> on both sides and then shift <span class="math inline">\(\tau\)</span> to the right hand side:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} =  \left(\frac{1}{2\tau} \int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta \right)\frac{\partial^2 f}{\partial x^2}
\]</span></p>
<p>Define <span class="math inline">\(D:= \left(\frac{1}{2\tau} \int_{-\infty}^{\infty}\Delta^2 \phi(\Delta)d\Delta \right)\)</span>. Then, we have:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} =  D\frac{\partial^2 f}{\partial x^2}
\]</span></p>
<p>The microscopic interpretation of the diffusion coefficient is, that its just the average of the squared displacements. The larger the <span class="math inline">\(D\)</span>, the faster the brownian particles move.</p>
</section>
</section>
<section id="kolmogorovs-backward-equation" class="level2">
<h2 class="anchored" data-anchor-id="kolmogorovs-backward-equation">Kolmogorov’s Backward Equation</h2>
<p>Think of <span class="math inline">\(y\)</span> and <span class="math inline">\(t\)</span> as being current values and <span class="math inline">\(y'\)</span> and <span class="math inline">\(t'\)</span> being future values. The transition probability density function <span class="math inline">\(p(y',t'|y,t)\)</span> of a diffusion satisfies two equations - one involving derivatives with respect to a future state and time (<span class="math inline">\(y'\)</span> and <span class="math inline">\(t'\)</span>) called <em>forward equation</em> and the other involving derivatives with respect to the current state and current time (<span class="math inline">\(y\)</span> and <span class="math inline">\(t\)</span>) called the <em>backward equation</em>. These two equations are parabolic partial differential equations not dissimilar to the Black-Scholes equation.</p>
<div id="thm-backward-equation-with-initial-value" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4 (Backward equation with initial value) </strong></span>Let <span class="math inline">\((X_t,t\geq 0)\)</span> be a diffusion in <span class="math inline">\(\mathbb{R}\)</span> with the SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t)dB_t + \mu(X_t) dt
\]</span></p>
<p>Let <span class="math inline">\(g\in C^2(\mathbb{R})\)</span> be such that <span class="math inline">\(g\)</span> is <span class="math inline">\(0\)</span> outside an interval. Then, the solution of the PDE with initial value</p>
<p><span id="eq-backward-equation"><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t}(t,x) &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(x)\frac{\partial f}{\partial x}\\
f(0,x) &amp;= g(x)
\end{align*}
\tag{15}\]</span></span></p>
<p>has the representation:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}[g(X_t)|X_0 = x]
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p><strong>Step 1.</strong> Let’s fix <span class="math inline">\(t\)</span> and consider the function of space <span class="math inline">\(h(x)=f(t,x)=\mathbb{E}[g(X_t)|X_0=x]\)</span>. Applying Ito’s formula to <span class="math inline">\(h\)</span>, we have:</p>
<p><span class="math display">\[\begin{align}
dh(X_s) &amp;= h'(X_s) dX_s + \frac{1}{2}h''(X_s) (dX_s)^2\\
&amp;= h'(X_s) (\sigma(X_s)dB_s + \mu(X_s) ds) + \frac{\sigma(X_s)^2}{2}h''(X_s)ds\\
&amp;= \sigma(X_s)h'(X_s)dB_s + \left(\frac{\sigma(X_s)^2}{2}h''(X_s) + \mu(X_s)h'(X_s)\right)ds
\end{align}\]</span></p>
<p>In the integral form this is:</p>
<p><span class="math display">\[\begin{align*}
h(X_s) - h(X_0) &amp;= \int_0^s \sigma(X_u)h'(X_u)dB_u \\
&amp;+ \int_0^s \left(\frac{\sigma(X_u)^2}{2}h''(X_u) + \mu(X_u)h'(X_u)\right)du \tag{1}
\end{align*}\]</span></p>
<p><strong>Step 2.</strong> Take expectations on both sides, divide by <span class="math inline">\(s\)</span> and let <span class="math inline">\(s \to 0\)</span>. We are interested in taking the derivative with respect to <span class="math inline">\(s\)</span> at <span class="math inline">\(s_0=0\)</span>.</p>
<p>The expectation of the first term on the right hand side is zero, by the properties of the Ito integral.</p>
<p>The integrand of the second term (RHS) is a conditional expectation <span class="math inline">\(\mathbb{E}[\xi(X_u)|X_0 = x]\)</span>, it is an average at time <span class="math inline">\(u\)</span>, of the paths of the process starting at initial position <span class="math inline">\(X_0 = x\)</span>, so it is a function of <span class="math inline">\(u\)</span> and <span class="math inline">\(x\)</span>. So, <span class="math inline">\(\mathbb{E}[\xi(X_u)|X_0 = x] = p(u,x)\)</span>. Suppressing the argument <span class="math inline">\(x\)</span>, we have the representation:</p>
<p><span class="math display">\[\begin{align}
\int_0^s p(u) du
\end{align}\]</span></p>
<p>Recall that, if <span class="math inline">\(p\)</span> is a continuous function, then it is Riemann integrable. Further, since integration and differentiation are inverse operations, there exists a unique antiderivative <span class="math inline">\(P\)</span> given by</p>
<p><span class="math display">\[
P(s) = \int_{0}^{s}p(u)du
\]</span></p>
<p>satisfying <span class="math inline">\(P'(0) = p(0)\)</span>.</p>
<p>By the definition of the derivative:</p>
<p><span class="math display">\[P'(0) = \lim_{s \to 0} \frac{P(s) - P(0)}{s} = \lim_{s\to 0} \frac{P(s)}{s} = p(0) \quad \{ P(0)=0 \text{ by definition }\}\]</span></p>
<p>Thus, we have:</p>
<p><span class="math display">\[
p(0,x) = \mathbb{E}[\xi(X_0)|X_0 = x] = \frac{\sigma(x)^2}{2} h''(x) + \mu(x)h'(x)
\]</span></p>
<p><strong>Step 3.</strong> As for the left-hand side, we have:</p>
<p><span class="math display">\[
\lim_{s \to 0} \frac{\mathbb{E}[h(X_s)|X_0 = x] - h(X_0)}{s} = \lim_{s \to 0} \frac{\mathbb{E}[h(X_s)|X_0 = x] - f(t,x)}{s}
\]</span></p>
<p>To prove that this limit is <span class="math inline">\(\frac{\partial f}{\partial t}(t,x)\)</span>, it remains to show that <span class="math inline">\(\mathbb{E}[h(X_s)|X_0 = x]=\mathbb{E}[g(X_{t+s})|X_0 = x]=f(t+s,x)\)</span>.</p>
<p>To see this, note that <span class="math inline">\(h(X_s) = \mathbb{E}[g(X_{t+s})|X_s]\)</span>. We deduce:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[h(X_s)|X_0 = x] &amp;= \mathbb{E}[\mathbb{E}[g(X_{t+s})|X_s]|X_0 = x]\\
&amp;= \mathbb{E}[\mathbb{E}[g(X_{t+s})|\mathcal{F}_s]|X_0 = x]\\
&amp; \{ (X_t,t\geq 0) \text{ is Markov }\} \\
&amp;= \mathbb{E}[g(X_{t+s})|X_0 = x]\\
&amp; \{ \text{ Tower property }\} \\
&amp;= f(t+s,x)
\end{align*}\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>The backward equation (<a href="#eq-backward-equation">Equation&nbsp;15</a>) can be conveniently written in terms of <em>the generator of the diffusion</em>.</p>
<div id="def-generator-of-the-diffusion" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Generator of a diffusion) </strong></span>The generator of a diffusion with SDE <span class="math inline">\(dX_t = \sigma(X_t) dB_t + \mu(X_t)dt\)</span> is the differential operator acting on functions of space defined by :</p>
<p><span class="math display">\[
A = \frac{\sigma(x)^2}{2}\frac{\partial }{\partial x^2} + \mu(x)\frac{\partial}{\partial x}
\]</span></p>
</div>
<p>With this notation, the backward equation for the function <span class="math inline">\(f(t,x)\)</span> takes the form:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x}(t,x) = Af(t,x)
\]</span></p>
<p>where it is understood that <span class="math inline">\(A\)</span> acts only on the space variable. <a href="#thm-backward-equation-with-initial-value">Theorem&nbsp;4</a> gives a nice interpretation of the generator: it quantifies how much the function <span class="math inline">\(f(t,x) = \mathbb{E}[g(X_t)|X_0 = x]\)</span> changes in a small time interval.</p>
<div id="exm-generator-of-the-ornstein-uhlenbeck-process" class="theorem example">
<p><span class="theorem-title"><strong>Example 5 </strong></span>(Generator of the Ornstein Uhlenbeck Process) The SDE of the Ornstein-Uhlenbeck process is:</p>
<p><span class="math display">\[
dX_t = dB_t - X_t dt
\]</span></p>
<p>This means that its generator is:</p>
<p><span class="math display">\[
A = \frac{1}{2}\frac{\partial^2}{\partial x^2} - x \frac{\partial}{\partial x}
\]</span></p>
</div>
<div id="exm-generator-of-geometric-brownian-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 6 </strong></span>(Generator of Geometric Brownian Motion) Recall that the geometric Brownian motion</p>
<p><span class="math display">\[
S_t = S_0 \exp(\sigma B_t + \mu t)
\]</span></p>
<p>satisfies the SDE:</p>
<p><span class="math display">\[
dS_t = \sigma S_t dB_t + \left(\mu + \frac{\sigma^2}{2}\right) S_t dt
\]</span></p>
<p>In particular, the generator of geometric Brownian motion is :</p>
<p><span class="math display">\[
A = \frac{\sigma^2 x^2}{2} x \frac{\partial^2}{\partial x^2} + \left(\mu + \frac{\sigma^2}{2}\right)\frac{\partial}{\partial x}
\]</span></p>
</div>
<p>For applications, in particular in mathematical finance, it is important to solve the backward equation with terminal value instead of with initial value. The reversal of time causes the appearance of an extra minus sign in the equation.</p>
<div id="thm-backward-equation-with-terminal-value" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (Backward equation with terminal value) </strong></span>Let <span class="math inline">\((X_t,t\leq T)\)</span> be a diffusion with the dynamics:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t) dB_t + \mu(X_t)dt
\]</span></p>
<p>Let <span class="math inline">\(g\in C^2(\mathbb{R})\)</span> be such that <span class="math inline">\(g\)</span> is <span class="math inline">\(0\)</span> outside an interval. Then, the solution of the PDE with terminal value at time <span class="math inline">\(T\)</span></p>
<p><span id="eq-backward-equation-with-terminal-value"><span class="math display">\[
\begin{align*}
-\frac{\partial f}{\partial t} &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(x)\frac{\partial f}{\partial x}\\
f(T,x) &amp;= g(x)
\end{align*}
\tag{16}\]</span></span></p>
<p>has the representation:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}[g(X_T)|X_t = x]
\]</span></p>
</div>
<div id="nte-functions-of-markov" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Backward equation with terminal value appears in the martingale condition
</div>
</div>
<div class="callout-body-container callout-body">
<p>One way to construct a martingale for the filtration <span class="math inline">\((\mathcal{F}_t,t\geq 0)\)</span> is to take</p>
<p><span class="math display">\[
M_t = \mathbb{E}[Y | \mathcal{F}_t]
\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is some integrable random variable. The martingale property then follows from the tower property of the conditional expectation. In the setup of <a href="#thm-backward-equation-with-terminal-value">Theorem&nbsp;5</a>, the random variable <span class="math inline">\(Y\)</span> is <span class="math inline">\(g(X_T)\)</span>. By the Markov property of diffusion, we therefore have:</p>
<p><span class="math display">\[
f(t,X_t) = \mathbb{E}[g(X_T)|X_t] = \mathbb{E}[g(X_T)|\mathcal{F}_t]
\]</span></p>
<p>In other words, the solution to the backward equation with terminal value evaluated at <span class="math inline">\(X_t = x\)</span> yields a martingale for the natural filtration of the process. This is a different point of view on the procedure we have used many times now: To get a martingale of the form <span class="math inline">\(f(t,X_t)\)</span>, apply the Ito’s formula to <span class="math inline">\(f(t,X_t)\)</span> and set the <span class="math inline">\(dt\)</span> term to zero. The PDE we obtain is the backward equation with terminal value. In fact, the proof of the theorem takes this exact route.</p>
</div>
</div>
<p><em>Proof.</em></p>
<p>Consider <span class="math inline">\(f(t,X_t)\)</span> and apply Ito’s formula.</p>
<p><span class="math display">\[
\begin{align*}
df(t,X_t) &amp;= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x}dX_t + \frac{1}{2}\frac{\partial^2 f}{\partial x^2} dX_t \cdot dX_t\\
&amp;= \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial x}(\sigma(X_t) dB_t + \mu(X_t)dt) + \frac{\sigma(X_t)^2}{2}\frac{\partial^2 f}{\partial x^2} dt\\
&amp;= \sigma(X_t) dB_t + \left(\frac{\partial f}{\partial t} + \frac{\sigma(X_t)^2}{2}\frac{\partial^2 f}{\partial x^2} + \mu(X_t)\frac{\partial f}{\partial x}\right)dt
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(f(t,x)\)</span> is a solution to the equation, we get that the <span class="math inline">\(dt\)</span> term is <span class="math inline">\(0\)</span> and <span class="math inline">\(f(t,X_t)\)</span> is a martingale for the Brownian filtration (and thus also for the natural filtration of the diffusion, which contains less information). In particular we have:</p>
<p><span class="math display">\[
f(t,X_t) = \mathbb{E}[f(T,X_T)|\mathcal{F}_t] = \mathbb{E}[g(X_T)|\mathcal{F}_t]
\]</span></p>
<p>Since <span class="math inline">\((X_t,t\leq T)\)</span> is a Markov process, we finally get:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}[g(X_T)|X_t = x]
\]</span></p>
<div id="exm-martingales-of-geometric-brownian-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 7 </strong></span>(Martingales of geometric Brownian motion) Let <span class="math inline">\((S_t, \geq 0)\)</span> be a geometric brownian motion with SDE:</p>
<p><span class="math display">\[
dS_t = \sigma S_t dB_t + \left(\mu + \frac{\sigma^2}{2}\right)dt
\]</span></p>
<p>As we saw in <a href="#exm-generator-of-geometric-brownian-motion">Example&nbsp;6</a>, its generator is:</p>
<p><span class="math display">\[
A = \frac{\sigma^2 x^2}{2}\frac{\partial^2}{\partial x^2} + x\left(\mu+\frac{\sigma^2}{2}\right)\frac{\partial}{\partial x}
\]</span></p>
<p>In view of <a href="#thm-backward-equation-with-terminal-value">Theorem&nbsp;5</a>, if <span class="math inline">\(f(t,x)\)</span> satisfies the PDE</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} + \frac{\sigma^2 x^2}{2}\frac{\partial^2 f}{\partial x^2} + x\left(\mu+\frac{\sigma^2}{2}\right)\frac{\partial f}{\partial x}
\]</span></p>
<p>then processes of the form <span class="math inline">\(f(t,S_t)\)</span> will be martingales for the natural filtration.</p>
</div>
</section>
<section id="kolmogorovs-forward-equation" class="level2">
<h2 class="anchored" data-anchor-id="kolmogorovs-forward-equation">Kolmogorov’s forward equation</h2>
<p>The companion equation to the backward equation is the <em>Kolmogorov forward equation</em> or <em>forward equation</em>. It is also known as the <em>Fokker-Planck</em> equation from its physics origin. The equation is very useful as it is satisfied by the transition density function <span class="math inline">\(p(y',t'|y,t)\)</span> of a time-homogenous diffusion. It involves the <em>adjoint of the generator</em>.</p>
<div id="def-adjoint-of-the-generator" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (Adjoint of the generator) </strong></span>The adjoint <span class="math inline">\(A^*\)</span> of the generator of a diffusion <span class="math inline">\((X_t,t\geq 0)\)</span> with SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t)dB_t + \mu(X_t)dt
\]</span></p>
<p>is the differential operator acting on a function of space <span class="math inline">\(f(x)\)</span> as follows:</p>
<p><span id="eq-adjoint-of-the-generator-of-a-diffusion"><span class="math display">\[
A^*f(x) = \frac{1}{2}\frac{\partial^2 }{\partial x^2} \frac{\sigma(x)^2}{2} f(x) - \frac{\partial }{\partial x}\mu(x)f(x)
\tag{17}\]</span></span></p>
</div>
<p>Note the differences with the generator in <a href="#def-generator-of-the-diffusion">Definition&nbsp;4</a>: there is an extra minus sign and the derivatives also act on the volatility and the drift.</p>
<div id="exm-the-generator-brownian-motion-is-self-adjoint" class="theorem example">
<p><span class="theorem-title"><strong>Example 8 </strong></span>(The generator of Brownian motion is self-adjoint) In the case of standard brownian motion, it is easy to check that:</p>
<p><span class="math display">\[
A^* = \frac{1}{2}\frac{\partial^2}{\partial x^2}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
A^* = \frac{1}{2}\nabla^2
\]</span></p>
<p>in the multivariate case. In other words, the generator and its adjoint are the same. In this case, the operator is <em>self-adjoint</em>.</p>
</div>
<div id="exm-the-adjoint-for-geometric-brownian-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 9 </strong></span>We see that the adjoint of the generator acting on <span class="math inline">\(f(x)\)</span> for geometric Brownian motion is:</p>
<p><span class="math display">\[
A^*f(x) = \frac{1}{2}\frac{\partial^2}{\partial x^2} (\sigma^2 x^2 f(x)) - \frac{\partial}{\partial x} \left(\left(\mu + \frac{\sigma^2}{2}\right) x f(x)\right)
\]</span></p>
<p>Using the product rule in differentiating we get:</p>
<p><span class="math display">\[
A^*[f(x)] = \frac{\sigma^2}{2}\left(2x f(x) + x^2 f''(x)\right) - \left(\left(\mu + \frac{\sigma^2}{2}\right)\left(f(x) + x f'(x)\right)\right)
\]</span></p>
</div>
<div id="exm-adjoint-for-the-ornstein-uhlenbeck-process" class="theorem example">
<p><span class="theorem-title"><strong>Example 10 </strong></span>The generator for the Ornstein-Uhlenbeck process was given in <a href="#exm-generator-of-the-ornstein-uhlenbeck-process">Example&nbsp;5</a>. The adjoint acting on <span class="math inline">\(f\)</span> is therefore:</p>
<p><span class="math display">\[
\begin{align*}
A^*f(x) &amp;= \frac{1}{2}\frac{\partial^2}{\partial x^2}(f(x)) - \frac{\partial}{\partial x}(- x f(x))\\
&amp;= \frac{f''(x)}{2} + (f(x)+xf'(x))
\end{align*}
\]</span></p>
</div>
<p>The forward equation takes the following form for a function <span class="math inline">\(f(t,x)\)</span> of time and space:</p>
<p><span id="eq-forward-equation"><span class="math display">\[
\frac{\partial f}{\partial t} = A^* f
\tag{18}\]</span></span></p>
<p>For brownian motion, since <span class="math inline">\(A^* = A\)</span>, the backward and forward equations are the same. As advertised earlier, the forward equation is satisfied by the transition <span class="math inline">\(p_t(y',t'|y,t)\)</span> of a diffusion. Before showing this in general, we verify it in the Brownian case.</p>
<div id="exm-the-heat-kernel-as-the-solution-of-the-forward-equation" class="theorem example">
<p><span class="theorem-title"><strong>Example 11 </strong></span>Recall that the transition probability density <span class="math inline">\(p(y,t|x,0)\)</span> for Brownian motion, or heat kernel, is:</p>
<p><span class="math display">\[
p(y,t|x,0) = \frac{e^{-\frac{(y-x)^2}{2}}}{\sqrt{2\pi t}}
\]</span></p>
<p>Here, the space variable will be <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> will be fixed. The relevant function is thus <span class="math inline">\(f(t,y) = p(y,t|x,0)\)</span>. The adjoint operator acting on the space variable <span class="math inline">\(y\)</span> is <span class="math inline">\(A^* = A = \frac{1}{2}\frac{\partial^2}{\partial y^2}\)</span>. The relevant time and space derivatives are given by <a href="#eq-partial-with-respect-to-time">Equation&nbsp;11</a> and <a href="#eq-second-derivative-with-respect-to-space">Equation&nbsp;13</a>.</p>
<p>We conclude that <span class="math inline">\(f(t,y)=p(y,t|x,0)\)</span> is a solution of the forward equation.</p>
</div>
<p>Where does the form of the adjoint operator <a href="#eq-adjoint-of-the-generator-of-a-diffusion">Equation&nbsp;17</a> come from? In some sense, the adjoint operator plays a role similar to that of the transpose of a matrix in linear algebra. The adjoint acts on the function on the left. To see this, consider two functions <span class="math inline">\(f,g\)</span> of space on which the generator <span class="math inline">\(A\)</span> of a diffusion is well-defined. In particular, let’s assume that the functions are zero outside an interval. Consider the quantity</p>
<p><span class="math display">\[
\int_{\mathbb{R}}g(x)A(f(x))dx = \int_{\mathbb{R}} g(x)\left(\frac{\sigma(x)^2 }{2}f''(x) + \mu(x)f'(x)\right)dx
\]</span></p>
<p>This quantity can represent for example the average of <span class="math inline">\(Af(x)\)</span> over some PDF <span class="math inline">\(g(x)\)</span>. In the above, <span class="math inline">\(A\)</span> acts on the function on the right. To make the operator act on <span class="math inline">\(g\)</span>, we integrate by parts. This gives for the second term:</p>
<p><span class="math display">\[
\int_{\mathbb{R}} g(x)\mu(x)f'(x)dx = g(x)\mu(x)f(x)\Bigg|_{-\infty}^{\infty}-\int_{\mathbb{R}}f(x)\frac{d}{dx}(g(x)\mu(x))dx
\]</span></p>
<p>The boundary term <span class="math inline">\(g(x)f(x)\mu(x)\Bigg|_{-\infty}^\infty\)</span> is <span class="math inline">\(0\)</span> by the assumptions on <span class="math inline">\(f,g\)</span>. This term on <span class="math inline">\(\sigma\)</span> is obtained by integrating by parts twice:</p>
<p><span class="math display">\[
\begin{align*}
\int_{\mathbb{R}} g(x) \frac{\sigma(x)^2}{2}f''(x)dx &amp;= g(x) \frac{\sigma(x)^2}{2}f'(x)\Bigg|_{-\infty}^{\infty} - \int_{\mathbb{R}}\frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right) f'(x)dx\\
-\int_{\mathbb{R}} \frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right)f'(x)dx &amp;= -\frac{d}{dx}\left(g(x) \frac{\sigma(x)^2}{2}\right)f(x) \Bigg|_{-\infty}^{\infty} + \int_{\mathbb{R}}\frac{d^2}{dx^2}\left(g(x) \frac{\sigma(x)^2}{2}\right)f(x)dx
\end{align*}
\]</span></p>
<p>Thus,</p>
<p><span id="eq-relation-between-generator-and-adjoint"><span class="math display">\[
\begin{align*}
\int_{\mathbb{R}}g(x) Af(x)dx &amp;= \int_{\mathbb{R}}\left(\frac{1}{2}\frac{d^2}{dx^2}(g(x) \sigma(x)^2) - \frac{d}{dx}(g(x)\mu(x))\right)f(x)dx\\
&amp;= \int_{\mathbb{R}}(A^*g(x))f(x)dx
\end{align*}
\tag{19}\]</span></span></p>
<div id="thm-forward-equation-and-transition-probability" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (Forward equation and transition probability) </strong></span>Let <span class="math inline">\((X_t,t\geq 0)\)</span> be a diffusion with SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t)dB_t + \mu(X_t)dt, \quad X_0 = x_0
\]</span></p>
<p>Let <span class="math inline">\(p(x,t|x_0,0)\)</span> be the transition probability density function for a fixed <span class="math inline">\(x_0\)</span>. Then, the function <span class="math inline">\(f(t,y) = p(y,t|x_0,0)\)</span> is a solution of the PDE</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} = A^* f
\]</span></p>
<p>where <span class="math inline">\(A^*\)</span> is the adjoint of <span class="math inline">\(A\)</span>.</p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(h(x)\)</span> be some arbitrary function of space that is <span class="math inline">\(0\)</span> outside an interval. We compute :</p>
<p><span class="math display">\[
\frac{1}{\epsilon}(\mathbb{E}[h(X_{t+\epsilon}) - \mathbb{E}[h(X_t)]])
\]</span></p>
<p>two different ways and take the limit as <span class="math inline">\(\epsilon \to 0\)</span>.</p>
<p>On one hand, we have by the definition of the transition density</p>
<p><span class="math display">\[
\frac{1}{\epsilon}\left(\mathbb{E}[h(X_{t+\epsilon})]-\mathbb{E}[h(X_t)]\right) = \int_{\mathbb{R}}\frac{1}{\epsilon}(p(x,t+\epsilon|x,0) - p(x,t|x_0,0))h(x)dx
\]</span></p>
<p>By taking the limit <span class="math inline">\(\epsilon \to 0\)</span> inside the integral (assuming this is fine), we get:</p>
<p><span id="eq-fwd-equation-partial-wrt-time"><span class="math display">\[
\int_{\mathbb{R}} \frac{\partial}{\partial t}p(x,t|x_0,0)h(x)dx
\tag{20}\]</span></span></p>
<p>On the other hand, Ito’s formula implies</p>
<p><span class="math display">\[
\begin{align*}
dh(X_s) &amp;= \frac{\partial h}{\partial x} dX_s + \frac{1}{2} \frac{\partial^2 h}{\partial x^2} (dX_s)^2\\
&amp;= \frac{\partial h}{\partial x} (\sigma(X_s) dB_s + \mu(X_s)ds) + \frac{1}{2} \frac{\partial^2 h}{\partial x^2} (\sigma(X_s)^2 ds)\\
&amp;= \sigma(X_s)\frac{\partial h}{\partial x} dB_s + \left(\mu(X_s) \frac{\partial h}{\partial x} + \frac{\sigma(X_s)^2}{2}\frac{\partial^2 h}{\partial x^2}\right)ds\\
h(X_{t+\epsilon}) - h(X_t) &amp;= \int_{t}^{t+\epsilon}\sigma(X_s)\frac{\partial h}{\partial x} dB_s + \int_{t}^{t+\epsilon}(Ah(x))ds\\
\mathbb{E}[h(X_{t+\epsilon})] - \mathbb{E}[h(X_t)] &amp;= \underbrace{\mathbb{E}\left[\int_{t}^{t+\epsilon}\sigma(X_s)\frac{\partial h}{\partial x} dB_s\right]}_{0} + \int_{t}^{t+\epsilon}\mathbb{E}[Ah(X_s)]ds
\end{align*}
\]</span></p>
<p>Dividing by <span class="math inline">\(\epsilon\)</span> and taking the limit as <span class="math inline">\(\epsilon \to 0\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
\lim_{\epsilon \to 0} \frac{1}{\epsilon} (\mathbb{E}[h(X_{t+\epsilon})] - \mathbb{E}[h(X_t)]) &amp;= \mathbb{E}[Ah(X_t)]\\
&amp;= \int_{\mathbb{R}} p(x,t|x_0,0) Ah(x) dx
\end{align*}
\]</span></p>
<p>This can be written using <a href="#eq-relation-between-generator-and-adjoint">Equation&nbsp;19</a> as,</p>
<p><span class="math display">\[
\int_{\mathbb{R}}(A^* p(x,t|x_0,0)) h(x) dx
\]</span></p>
<p>Since <span class="math inline">\(h\)</span> is arbitrary, we conclude that:</p>
<p><span id="eq-forward-equation"><span class="math display">\[
\frac{\partial}{\partial t}p(x,t|x_0,0) = A^* p(x,t|x_0,0)
\tag{21}\]</span></span></p>
<div id="exm-forward-equation-and-invariant-probability" class="theorem example">
<p><span class="theorem-title"><strong>Example 12 </strong></span>(Forward equation and invariant probability.) The Ornstein-Uhlenbeck process converges to a stationary distribution as noted in the <a href="https://quantinsights.github.io/posts/ito-processes-and-stochastic-diff-eqs/#exm-ornstein-uhlenbeck-process">example</a> here. For example, for the SDE of the form</p>
<p><span class="math display">\[
dX_t = -X_t dt + dB_t
\]</span></p>
<p>with <span class="math inline">\(X_0\)</span> a Gaussian of mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1/2\)</span>, the PDF of <span class="math inline">\(X_t\)</span>, is, for all <span class="math inline">\(t\)</span> is:</p>
<p><span id="eq-pdf-of-OU-process"><span class="math display">\[
f(x) = \frac{1}{\sqrt{\pi}} e^{-x^2}
\tag{22}\]</span></span></p>
<p>This <em>invariant distribution</em> can be seen from the point of view of the forward equation. Indeed since the PDF is constant in time, the forward equation simply becomes:</p>
<p><span id="eq-forward-equation-of-ou-process"><span class="math display">\[
A^* f = 0
\tag{23}\]</span></span></p>
</div>
<div id="exm-smoluchowski-equation" class="theorem example">
<p><span class="theorem-title"><strong>Example 13 </strong></span>The SDE of the Ornstein-Uhlenbeck process can be generated as follows. Consider <span class="math inline">\(V(x)\)</span>, a smooth function of space such that <span class="math inline">\(\int_{\mathbb{R}} e^{-2V(x)}dx&lt;\infty\)</span>. The <em>Smoluchowski</em> equation is the SDE of the form:</p>
<p><span id="eq-smoluchowski-equation"><span class="math display">\[
dX_t = dB_t - V'(X_t) dt
\tag{24}\]</span></span></p>
<p>The SDE can be interpreted as follows: <span class="math inline">\(X_t\)</span> represents the position of a particle on <span class="math inline">\(\mathbb{R}\)</span>. The position varies due to the Brownian fluctuations and also due to a force <span class="math inline">\(V'(X_t)\)</span> that depends on the position. The function <span class="math inline">\(V(x)\)</span> should then be thought of as the potential with which the particle moves, since the force (field) is the (negative) derivative of the potential function in Newtonian physics. The generator of this diffusion is:</p>
<p><span class="math display">\[
A = \frac{1}{2}\frac{\partial^2}{\partial x^2} - V'(x)\frac{\partial}{\partial x}
\]</span></p>
<p>This diffusion admits an invariant distribution :</p>
<p><span class="math display">\[
f(x) = Ce^{-2V(x)}
\]</span></p>
<p>where <span class="math inline">\(C\)</span> is such that <span class="math inline">\(\int_{\mathbb{R}}f(x)dx = 1\)</span>.</p>
</div>
</section>
<section id="the-feynman-kac-formula" class="level2">
<h2 class="anchored" data-anchor-id="the-feynman-kac-formula">The Feynman-Kac Formula</h2>
<p>We saw in <a href="#exm-heat-equation-and-brownian-motion">Example&nbsp;4</a> that the solution of the heat equation:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial t} = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}
\]</span></p>
<p>can be represented as an average over Brownian paths. This representation was extended to diffusions in theorem <a href="#thm-backward-equation-with-initial-value">Theorem&nbsp;4</a> where the second derivative in the equation is replaced by the generator of the corresponding diffusion. How robust is this representation? In other words, is it possible to slightly change the PDE and still get a stochastic representation representation for the solution? The answer to this question is yes, when a term of the form <span class="math inline">\(r(x)f(t,x)\)</span> is added to the equation, where <span class="math inline">\(r(x)\)</span> is a well-behaved function of space (for example, piecewise continuous). The stochastic representation of the PDE in this case bears the name <em>Feynman-Kac</em> formula, making a fruitful collaboration between the physicist <a href="https://en.wikipedia.org/wiki/Richard_Feynman">Richard Feynman</a> and the mathematician <a href="https://en.wikipedia.org/wiki/Mark_Kac">Mark Kac</a>. By the way, you pronounce “Kac” as “cats”. His name is Polish. People who immigrated from Poland before him spelled their names as “Katz”. The case when <span class="math inline">\(r(x)\)</span> is linear will be important in the applications to mathematical finance, where it represents the contribution of the interest rate.</p>
<div id="thm-initial-value-problem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 (Initial Value Problem) </strong></span>Let <span class="math inline">\((X_t,t\geq 0)\)</span> be a diffusion in <span class="math inline">\(\mathbb{R}\)</span> with the SDE:</p>
<p><span class="math display">\[
dX_t = \sigma(X_t) dB_t + \mu(X_t)dt
\]</span></p>
<p>Let <span class="math inline">\(g\in C^2(\mathbb{R})\)</span> be such that <span class="math inline">\(g\)</span> is <span class="math inline">\(0\)</span> outside an interval. Then, the solution of the PDE with initial value</p>
<p><span id="eq-initial-value-problem"><span class="math display">\[
\begin{align*}
\frac{\partial f}{\partial t}(t,x) &amp;= \frac{\sigma(x)^2}{2}\frac{\partial^2 f}{\partial x^2}(t,x) + \mu(x)\frac{\partial f}{\partial x}(t,x) - r(x)f(x)\\
f(0,x) &amp;= g(x)
\end{align*}
\tag{25}\]</span></span></p>
<p>has the stochastic representation:</p>
<p><span class="math display">\[
f(t,x) = \mathbb{E}\left[g(X_t)\exp\left(-\int_0^t r(X_s) ds\right)\Bigg| X_0 = x\right]
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>The proof is again based on Ito’s formula. For a fixed <span class="math inline">\(t\)</span>, we consider the process:</p>
<p><span class="math display">\[
M_s = f(t-s, X_s) \exp\left(-\int_0^s r(X_u) du\right), \quad s \leq t
\]</span></p>
<p>Write <span class="math inline">\(Z_s = \exp\left(-\int_0^s r(X_u) du\right)\)</span> and <span class="math inline">\(V_s = f(t-s,X_s)\)</span>. A direct application of Ito’s formula yields:</p>
<p>Let <span class="math inline">\(R_s = -\int_0^s r(X_u) du\)</span>. So, <span class="math inline">\(dR_t = r(X_t) dt\)</span>. <span class="math inline">\((R_t,t\geq 0)\)</span> is a random variable, because <span class="math inline">\(r(X_s)\)</span> depends on how <span class="math inline">\((X_s, s \leq t)\)</span> evolves, it is <em>stochastic</em>, but for very small intervals of time <span class="math inline">\(r(X_s)\)</span> is a constant, and hence the process <span class="math inline">\((R_t,t\geq 0)\)</span> is said to be locally deterministic.</p>
<p><span class="math display">\[
\begin{align*}
Z_s &amp;= e^{-R_s}\\
dZ_s &amp;= -e^{-R_s} dR_s + \frac{1}{2}e^{R_s} (dR_s)^2\\
&amp;= -Z_s r(X_s) ds
\end{align*}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align*}
dV_s &amp;= \frac{\partial}{\partial s}f(t-s, X_s)ds + \frac{\partial}{\partial x}f(t-s, X_s)dX_s + \frac{1}{2}\frac{\partial^2}{\partial x^2}f(t-s,X_s)(dX_s)^2\\
&amp;= -f_s ds + f_x (\sigma(X_s)dB_s + \mu(X_s)ds) + \frac{1}{2}f_{xx} \sigma(X_s)^2 ds \\
&amp;= \sigma(X_s) f_x dB_s + \\
&amp;+ \left\{-f_s + \mu(X_s)f_x + \frac{\sigma(X_s)^2}{2}f_{xx}\right\}ds
\end{align*}
\]</span></p>
<p>Recall that <span class="math inline">\(t\)</span> is fixed here, and we differentiate with respect to <span class="math inline">\(s\)</span> in time. Since <span class="math inline">\(f(t,x)\)</span> is a solution of the PDE, we can write the second equation as:</p>
<p><span class="math display">\[
dV_s = \sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds
\]</span></p>
<p>Now, by Ito’s product rule, we finally have:</p>
<p><span class="math display">\[
\begin{align*}
dM_s &amp;= V_s dZ_s + Z_s dV_s + dZ_s dV_s\\
&amp;= -f(t-s,X_s)Z_s r(X_s) ds + Z_s (\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds) + 0\\
&amp;= \sigma(X_s)Z_s f_x dB_s
\end{align*}
\]</span></p>
<p>This proves that <span class="math inline">\((M_s, s \leq t)\)</span> is a martingale. We conclude that:</p>
<p><span class="math display">\[
\mathbb{E}[M_t] = M_0
\]</span></p>
<p>Using the definition of <span class="math inline">\((M_t\)</span></p>
</section>
<section id="numerical-projects" class="level2">
<h2 class="anchored" data-anchor-id="numerical-projects">Numerical Projects</h2>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-shifted-brownian-motion" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1 </strong></span>(Shifted Brownian Motion) Let <span class="math inline">\((B_t,t\geq 0)\)</span> be a standard brownian motion. Fix <span class="math inline">\(t &gt; 0\)</span>. Show that the process <span class="math inline">\((W_s,s \geq 0)\)</span> with <span class="math inline">\(W_s = B_{t+s} - B_t\)</span> is a standard brownian motion independent of <span class="math inline">\(\mathcal{F}_t\)</span>.</p>
</div>
<p><em>Solution</em>.</p>
<p>At <span class="math inline">\(s = 0\)</span>, <span class="math inline">\(W(0) = B(t) - B(t) = 0\)</span>.</p>
<p>Consider any arbitrary times <span class="math inline">\(t_1 &lt; t_2\)</span>. We have:</p>
<p><span class="math display">\[\begin{align*}
W(t_2) - W(t_1) &amp;= (B(t + t_2) - B(t)) - (B(t + t_1) - B(t))\\
&amp;= B(t + t_2) - B(t + t_1)
\end{align*}\]</span></p>
<p>Now, <span class="math inline">\(B(t + t_2) - B(t + t_1) \sim \mathcal{N}(0,t_2 - t_1)\)</span>. So, <span class="math inline">\(W(t_2) - W(t_1)\)</span> is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t_2 - t_1\)</span>.</p>
<p>Finally, consider any finite set of times <span class="math inline">\(0=t_0 &lt; t_1 &lt; t_2 &lt; \ldots &lt; t_n = T\)</span>. Then, <span class="math inline">\(t &lt; t + t_1 &lt; t + t_2 &lt; \ldots &lt; t + t_n\)</span>. We have that, <span class="math inline">\(B(t + t_1) - B(t)\)</span>, <span class="math inline">\(B(t + t_2) - B(t + t_1)\)</span>, <span class="math inline">\(B(t + t_3) - B(t + t_2)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(B(t+T) - B(t+t_{n-1})\)</span> are independent random variables. Consequently, <span class="math inline">\(W(t_1) - W(0)\)</span>, <span class="math inline">\(W(t_2) - W(t_1)\)</span>, <span class="math inline">\(W(t_3) - W(t_2)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(W(t_n) - W(t_{n-1})\)</span> are independent random variables. So, <span class="math inline">\((W_s,s\geq 0)\)</span> is a standard brownian motion.</p>
<p>Also, we have:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[W(s)|\mathcal{F}_t] &amp;= \mathbb{E}[B(t + s) - B(t)|\mathcal{F}_t]\\
&amp; \{ B(t+s) - B(t) \perp \mathcal{F}_t \}\\
&amp;= \mathbb{E}[B(t + s) - B(t)]\\
&amp;= \mathbb{E}[W(s)]
\end{align*}\]</span></p>
<p>Thus, <span class="math inline">\(W(s)\)</span> is independent of <span class="math inline">\(\mathcal{F}_t\)</span>, it does not depend upon the information available upto time <span class="math inline">\(t\)</span>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
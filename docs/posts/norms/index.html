<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-07-25">

<title>quantdev.blog - Norms</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sell_side_quant_critical_path.html" rel="" target="">
 <span class="menu-text">Sell-side Quant</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../roadmap.html" rel="" target="">
 <span class="menu-text">C++ Roadmap</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/quasar-chunawala" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://linkedin.com/in/quasar-chunawala" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Norms</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Numerical Methods</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 25, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#inner-product" id="toc-inner-product" class="nav-link active" data-scroll-target="#inner-product">Inner product</a></li>
  <li><a href="#norms" id="toc-norms" class="nav-link" data-scroll-target="#norms">Norms</a>
  <ul class="collapse">
  <li><a href="#the-vector-2-norm" id="toc-the-vector-2-norm" class="nav-link" data-scroll-target="#the-vector-2-norm">The vector <span class="math inline">\(2-\)</span>norm</a></li>
  </ul></li>
  <li><a href="#cauchy-schwarz-inequality" id="toc-cauchy-schwarz-inequality" class="nav-link" data-scroll-target="#cauchy-schwarz-inequality">Cauchy-Schwarz Inequality</a></li>
  <li><a href="#euclidean-norm" id="toc-euclidean-norm" class="nav-link" data-scroll-target="#euclidean-norm">Euclidean Norm</a></li>
  <li><a href="#the-vector-1-norm" id="toc-the-vector-1-norm" class="nav-link" data-scroll-target="#the-vector-1-norm">The vector <span class="math inline">\(1-\)</span>norm</a></li>
  <li><a href="#jensens-inequality" id="toc-jensens-inequality" class="nav-link" data-scroll-target="#jensens-inequality">Jensenâ€™s inequality</a>
  <ul class="collapse">
  <li><a href="#convex-functions-and-combinations" id="toc-convex-functions-and-combinations" class="nav-link" data-scroll-target="#convex-functions-and-combinations">Convex functions and combinations</a></li>
  <li><a href="#proving-jensens-inequality" id="toc-proving-jensens-inequality" class="nav-link" data-scroll-target="#proving-jensens-inequality">Proving Jensenâ€™s inequality</a></li>
  </ul></li>
  <li><a href="#youngs-inequality" id="toc-youngs-inequality" class="nav-link" data-scroll-target="#youngs-inequality">Youngâ€™s Inequality</a></li>
  <li><a href="#holders-inequality" id="toc-holders-inequality" class="nav-link" data-scroll-target="#holders-inequality">Holderâ€™s inequality</a></li>
  <li><a href="#the-vector-p-norm" id="toc-the-vector-p-norm" class="nav-link" data-scroll-target="#the-vector-p-norm">The vector <span class="math inline">\(p\)</span>-norm</a></li>
  <li><a href="#the-vector-infty-norm" id="toc-the-vector-infty-norm" class="nav-link" data-scroll-target="#the-vector-infty-norm">The vector <span class="math inline">\(\infty\)</span>-norm</a></li>
  <li><a href="#equivalence-of-vector-norms" id="toc-equivalence-of-vector-norms" class="nav-link" data-scroll-target="#equivalence-of-vector-norms">Equivalence of vector norms</a>
  <ul class="collapse">
  <li><a href="#deriving-the-constants-c_1infty-c_infty1" id="toc-deriving-the-constants-c_1infty-c_infty1" class="nav-link" data-scroll-target="#deriving-the-constants-c_1infty-c_infty1">Deriving the constants <span class="math inline">\(C_{1,\infty}\)</span>, <span class="math inline">\(C_{\infty,1}\)</span></a></li>
  <li><a href="#deriving-the-constants-c_12-c_21" id="toc-deriving-the-constants-c_12-c_21" class="nav-link" data-scroll-target="#deriving-the-constants-c_12-c_21">Deriving the constants <span class="math inline">\(C_{1,2}\)</span>, <span class="math inline">\(C_{2,1}\)</span></a></li>
  <li><a href="#deriving-the-constants-c_2infty-and-c_infty2" id="toc-deriving-the-constants-c_2infty-and-c_infty2" class="nav-link" data-scroll-target="#deriving-the-constants-c_2infty-and-c_infty2">Deriving the constants <span class="math inline">\(C_{2,\infty}\)</span> and <span class="math inline">\(C_{\infty,2}\)</span></a></li>
  </ul></li>
  <li><a href="#matrix-norms" id="toc-matrix-norms" class="nav-link" data-scroll-target="#matrix-norms">Matrix Norms</a>
  <ul class="collapse">
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions">Definitions</a></li>
  <li><a href="#computing-the-matrix-1-norm-and-infty-norm" id="toc-computing-the-matrix-1-norm-and-infty-norm" class="nav-link" data-scroll-target="#computing-the-matrix-1-norm-and-infty-norm">Computing the matrix <span class="math inline">\(1\)</span>-norm and <span class="math inline">\(\infty\)</span>-norm</a></li>
  <li><a href="#equivalence-of-matrix-norms" id="toc-equivalence-of-matrix-norms" class="nav-link" data-scroll-target="#equivalence-of-matrix-norms">Equivalence of matrix norms</a></li>
  <li><a href="#sub-multiplicative-norms" id="toc-sub-multiplicative-norms" class="nav-link" data-scroll-target="#sub-multiplicative-norms">Sub-multiplicative norms</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="inner-product" class="level2">
<h2 class="anchored" data-anchor-id="inner-product">Inner product</h2>
<p>Consider geometric vectors <span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbf{R}^2\)</span>. The scalar product(dot-product) of these two vectors is defined by:</p>
<p><span class="math display">\[
\mathbf{x} \cdot \mathbf{y} = x_1 y_1 + x_2 y_2
\]</span></p>
<p>An inner-product is a mathematical generalization of the dot-product.</p>
<div class="hidden">
<p><span class="math display">\[
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normp}[2]{\left\lVert\mathbf{#1}\right\rVert_{#2}}
\newcommand\inner[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\bf}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\RR}[1]{\mathbf{R}^2}
\newcommand{\RRR}[1]{\mathbf{R}^3}
\newcommand{\C}{\mathbf{C}}
\newcommand{\CC}[1]{\mathbf{C}^2}
\newcommand{\CCC}[1]{\mathbf{C}^3}
\]</span></p>
</div>
<div id="def-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Inner product) </strong></span>Let <span class="math inline">\(V\)</span> be a vector space and <span class="math inline">\(F\)</span> be a scalar field, which is either <span class="math inline">\(\bf{R}\)</span> or <span class="math inline">\(\bf{C}\)</span>. Let <span class="math inline">\(\inner{\cdot}{\cdot}\)</span> be a map from <span class="math inline">\(V\times V \to F\)</span>. Then, <span class="math inline">\(\inner{\cdot}{\cdot}\)</span> is an inner product if for all <span class="math inline">\(\bf{u},\bf{v}, \bf{w} \in V\)</span>, it satisfies:</p>
<section id="positive-semi-definite" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="positive-semi-definite">Positive semi-definite</h4>
<p><span class="math display">\[
\inner{\bf{v}}{\bf{v}} \geq 0 \quad \text { and } \quad  \inner{\bf{v}}{\bf{v}} = 0 \Longleftrightarrow \bf{v} = \bf{0}
\]</span></p>
</section>
<section id="additivity-in-the-first-slot" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="additivity-in-the-first-slot">Additivity in the first slot</h4>
<p><span class="math display">\[
\inner{\bf{u} + \bf{v}}{\bf{w}} = \inner{\bf{u}}{\bf{w}} + \inner{\bf{v}}{\bf{w}}
\]</span></p>
</section>
<section id="homogeneity" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="homogeneity">Homogeneity</h4>
<p><span class="math display">\[
\begin{align*}
\inner{\alpha \bf{v}}{\bf{w}} &amp;= \overline{\alpha} \inner{\bf{v}}{\bf{w}}\\
\inner{\bf{v}}{\alpha \bf{w}} &amp;= \alpha \inner{\bf{v}}{\bf{w}}
\end{align*}
\]</span></p>
</section>
<section id="conjugate-symmetry" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="conjugate-symmetry">Conjugate symmetry</h4>
<p><span class="math display">\[
\inner{\bf{v}}{\bf{w}} = \overline{\inner{\bf{w}}{\bf{v}}}
\]</span></p>
</section>
</div>
<p>The most important example of inner-product is the Euclidean inner product on <span class="math inline">\(\C^n\)</span>. Let <span class="math inline">\(\bf{w},\bf{z}\)</span> be (column) vectors in <span class="math inline">\(\C^n\)</span>.</p>
<p><span class="math display">\[
\inner{\bf{w}}{\bf{z}} = (\bf{w}^H \bf{z}) =  \overline{w_1}z_1 + \overline{w_2}z_2 + \ldots + \overline{w_n} z_n
\]</span></p>
<p>Firstly,</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{v} + \bf{w}}{\bf{z}} &amp;= (\bf{v} + \bf{w})^H \bf{z} &amp; \{ \text{ Definition }\}\\
&amp;= (\bf{v}^H + \bf{w}^H)\bf{z} &amp; \{ \overline{z_1 + z_2} = \overline{z_1} + \overline{z_2}; z_1,z_2\in \C \}\\
&amp;= \bf{v}^H \bf{z} + \bf{w}^H \bf{z}\\
&amp;= \inner{\bf{v}}{\bf{z}} + \inner{\bf{w}}{\bf{z}}
\end{align*}
\]</span></p>
<p>So, it is additive in the first slot.</p>
<p>Next, let <span class="math inline">\(\alpha \in \C\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
\inner{\alpha\bf{u}}{\bf{v}} &amp;= (\alpha \bf{u})^H \bf{v}  &amp; \{ \text{ Definition }\}\\
&amp;= \overline{\alpha} \bf{u}^H \bf{v} = \overline{\alpha} \inner{\bf{u}}{\bf{v}}
\end{align*}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{u}}{\alpha\bf{v}} &amp;= (\bf{u})^H \bf{ \alpha v}  &amp; \{ \text{ Definition }\}\\
&amp;= \alpha \bf{u}^H \bf{v} = \alpha \inner{\bf{u}}{\bf{v}}
\end{align*}
\]</span></p>
<p>It is homogenous.</p>
<p>Finally,</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{v}}{\bf{w}} &amp;= \sum_{i=1}^n \overline{v_i}w_i\\
&amp;= \sum_{i=1}^n \overline{v_i \overline{w_i}}\\
&amp;= \overline{\left(\sum_{i=1}^n \overline{w_i} v_i\right)}\\
&amp;= \overline{\inner{\bf{w}}{\bf{v}}}
\end{align*}
\]</span></p>
</section>
<section id="norms" class="level2">
<h2 class="anchored" data-anchor-id="norms">Norms</h2>
<p>Very often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length.</p>
<div id="def-vector-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Vector norm) </strong></span>Let <span class="math inline">\(\nu:V \to \mathbf{R}\)</span>. Then, <span class="math inline">\(\nu\)</span> is a (vector) norm if for all <span class="math inline">\(\mathbf{x},\mathbf{y}\in V\)</span> and for all <span class="math inline">\(\alpha \in \mathbf{C}\)</span>, <span class="math inline">\(\nu(\cdot)\)</span> satisfies:</p>
<section id="positive-semi-definiteness" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="positive-semi-definiteness">Positive Semi-Definiteness</h4>
<p><span class="math display">\[\nu(\mathbf{x}) \geq 0, \quad \forall \bf{x}\in V\]</span></p>
<p>and</p>
<p><span class="math display">\[\nu(\mathbf{x})=0 \Longleftrightarrow \mathbf{x}=\mathbf{0}\]</span></p>
</section>
<section id="homogeneity-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="homogeneity-1">Homogeneity</h4>
<p><span class="math display">\[\nu(\alpha \mathbf{x}) = |\alpha|\nu(\mathbf{x})\]</span></p>
</section>
<section id="triangle-inequality" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="triangle-inequality">Triangle inequality</h4>
<p><span class="math display">\[\nu(\mathbf{x} + \mathbf{y}) \leq \nu(\mathbf{x}) + \nu(\mathbf{y})\]</span></p>
</section>
</div>
<section id="the-vector-2-norm" class="level3">
<h3 class="anchored" data-anchor-id="the-vector-2-norm">The vector <span class="math inline">\(2-\)</span>norm</h3>
<p>The length of a vector is most commonly measured by the <em>square root of the sum of the squares of the components of the vector</em>, also known as the <em>euclidean norm</em>.</p>
<div id="def-euclidean-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Vector <span class="math inline">\(2-\)</span>norm) </strong></span>The vector <span class="math inline">\(2-\)</span> norm, <span class="math inline">\(||\cdot||:\mathbf{C}^n \to \mathbf{R}\)</span> is defined for <span class="math inline">\(\mathbf{x}\in\mathbf{C}^n\)</span> by:</p>
<p><span class="math display">\[
\norm{\bf{x}}_2 = \sqrt{|\chi_1|^2 + |\chi_2|^2 + |\chi_n|^2} = \sqrt{\sum_{i=1}^n |\chi_i^2|}
\]</span></p>
<p>Equivalently, it can be defined as:</p>
<p><span class="math display">\[
\norm{\bf{x}}_2 = \sqrt{\inner{\bf{x}}{\bf{x}}} =  (\bf{x}^H \bf{x})^{1/2} = \sqrt{\overline{\chi_1}\chi_1 +\overline{\chi_2}\chi_2+\ldots+\overline{\chi_n}\chi_n}
\]</span></p>
</div>
<p>To prove that the vector <span class="math inline">\(2-\)</span>norm is indeed a valid norm(just calling it a norm, doesnâ€™t mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if <span class="math inline">\(\bf{x},\bf{y} \in \R^n\)</span>, then <span class="math inline">\(|\bf{x}^T \bf{y}|\leq \norm{\bf{x}}_2\cdot\norm{\bf{y}}_2\)</span>.</p>
<p>Before we rigorously prove this result, letâ€™s review the idea of orthogonality.</p>
<div id="def-orthogonal-vectors" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Orthogonal vectors) </strong></span>Two vectors <span class="math inline">\(\bf{u},\bf{v} \in V\)</span> are said to be orthogonal to each other if and only if their inner product equals zero:</p>
<p><span class="math display">\[
\inner{\bf{u}}{\bf{v}} = 0
\]</span></p>
</div>
<div id="thm-pythagorean-theorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Pythagorean Theorem) </strong></span>If <span class="math inline">\(\bf{u}\)</span> and <span class="math inline">\(\bf{v}\)</span> are orthogonal vectors, then</p>
<p><span class="math display">\[
\inner{\bf{u} + \bf{v}}{\bf{u} + \bf{v}} = \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{u} + \bf{v}}{\bf{u}+\bf{v}} &amp;= \inner{\bf{u}}{\bf{u} + \bf{v}} + \inner{\bf{v}}{\bf{u} + \bf{v}} &amp; \{ \text{ Additivity in the first slot }\}\\
&amp;= \overline{\inner{\bf{u} + \bf{v}}{\bf{u}}} + \overline{\inner{\bf{u} + \bf{v}}{\bf{v}}} &amp; \{ \text{ Conjugate symmetry }\}\\
&amp;= \overline{\inner{\bf{u}}{\bf{u}}} + \overline{\inner{\bf{v}}{\bf{u}}} + \overline{\inner{\bf{u}}{\bf{v}}} + \overline{\inner{\bf{v}}{\bf{v}}} \\
&amp;= \inner{\bf{u}}{\bf{u}} + \inner{\bf{u}}{\bf{v}} + \inner{\bf{v}}{\bf{u}} + \inner{\bf{v}}{\bf{v}} \\
&amp;= \inner{\bf{u}}{\bf{u}} + 0 + 0 + \inner{\bf{v}}{\bf{v}} &amp; \{ \bf{u} \perp \bf{v}\}\\
&amp;= \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>In the special case that <span class="math inline">\(V=\C^n\)</span> or <span class="math inline">\(V=\R^n\)</span>, the pythagorean theorem reduces to:</p>
<p><span class="math display">\[
\norm{\bf{u} + \bf{v}}_2^2 = \norm{\bf{u}}_2^2 + \norm{\bf{v}}_2^2
\]</span></p>
</section>
</section>
<section id="cauchy-schwarz-inequality" class="level2">
<h2 class="anchored" data-anchor-id="cauchy-schwarz-inequality">Cauchy-Schwarz Inequality</h2>
<p>Suppose <span class="math inline">\(\bf{u},\bf{v}\in V\)</span>. We would like to write <span class="math inline">\(\bf{u}\)</span> as a scalar multiple of <span class="math inline">\(\bf{v}\)</span> plus a vector <span class="math inline">\(\bf{w}\)</span> orthogonal to <span class="math inline">\(\bf{v}\)</span>, as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of <span class="math inline">\(\bf{u}\)</span>.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>load_ext itikz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows,arrows.meta <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb2-2"><a href="#cb2-2"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">2.0</span>]</span>
<span id="cb2-3"><a href="#cb2-3"></a>    \draw [<span class="op">-</span>{Stealth[length<span class="op">=</span><span class="dv">5</span><span class="er">mm</span>]}](<span class="fl">0.0</span>,<span class="fl">0.0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>    \draw [<span class="op">-</span>{Stealth[length<span class="op">=</span><span class="dv">5</span><span class="er">mm</span>]}] (<span class="fl">0.0</span>,<span class="fl">0.0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>    \node []  at (<span class="fl">3.5</span>,<span class="fl">2.25</span>) {\large $\mathbf{u}$}<span class="op">;</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>    \draw [dashed] (<span class="dv">7</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>    \node [circle,fill,minimum size <span class="op">=</span> <span class="fl">0.5</span><span class="er">mm</span>] at (<span class="dv">5</span>,<span class="dv">0</span>) {}<span class="op">;</span></span>
<span id="cb2-8"><a href="#cb2-8"></a>    \node []  at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">0.40</span>) {\large $\mathbf{v}$}<span class="op">;</span></span>
<span id="cb2-9"><a href="#cb2-9"></a>    \node []  at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">0.40</span>) {\large $\alpha\mathbf{v}$}<span class="op">;</span></span>
<span id="cb2-10"><a href="#cb2-10"></a>    \node []  at (<span class="fl">7.4</span>,<span class="fl">2.0</span>) {\large $\mathbf{w}$}<span class="op">;</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<p><img src="index_files/figure-html/cell-3-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>To discover how to write <span class="math inline">\(\bf{u}\)</span> as a scalar multiple of <span class="math inline">\(\bf{v}\)</span> plus a vector orthogonal to <span class="math inline">\(\bf{v}\)</span>, let <span class="math inline">\(\alpha\)</span> denote a scalar. Then,</p>
<p><span class="math display">\[
\bf{u} = \alpha \bf{v} + (\bf{u} - \alpha \bf{v})
\]</span></p>
<p>Thus, we need to choose <span class="math inline">\(\alpha\)</span> so that <span class="math inline">\(\bf{v}\)</span> and <span class="math inline">\(\bf{w} = \bf{u} - \alpha{v}\)</span> are mutually orthogonal. Thus, we must set:</p>
<p><span class="math display">\[
\inner{\bf{u} - \alpha\bf{v}}{\bf{v}} = \inner{\bf{u}}{\bf{v}} - \alpha \inner{\bf{v}}{\bf{v}} = 0
\]</span></p>
<p>The equation above shows that we choose <span class="math inline">\(\alpha\)</span> to be <span class="math inline">\(\inner{\bf{u}}{\bf{v}}/\inner{\bf{v}}{\bf{v}}\)</span> (assume that <span class="math inline">\(\bf{v} \neq \bf{0}\)</span> to avoid division by 0). Making this choice of <span class="math inline">\(\alpha\)</span>, we can write:</p>
<p><span id="eq-orthogonal-decomposition"><span class="math display">\[
\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v} + \left(\bf{u} - \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v}\right)
\tag{1}\]</span></span></p>
<p>The equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics</p>
<div id="thm-cauchy-schwarz-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (Cauchy-Schwarz Inequality) </strong></span>Let <span class="math inline">\(\bf{x},\bf{y}\in V\)</span>. Then</p>
<p><span id="eq-cauchy-schwarz-inequality"><span class="math display">\[
|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}
\tag{2}\]</span></span></p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(\bf{u},\bf{v} \in V\)</span>. If <span class="math inline">\(\bf{v} = \bf{0}\)</span>, then both sides of <a href="#eq-cauchy-schwarz-inequality">Equation&nbsp;2</a> equal <span class="math inline">\(0\)</span> and the inequality holds. Thus, we assume that <span class="math inline">\(\bf{v}\neq \bf{0}\)</span>. Consider the orthogonal decomposition:</p>
<p><span class="math display">\[
\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v} + \bf{w}
\]</span></p>
<p>where <span class="math inline">\(\bf{w}\)</span> is orthogonal to <span class="math inline">\(\bf{v}\)</span> (<span class="math inline">\(\bf{w}\)</span> is taken to be the second term on the right hand side of <a href="#eq-orthogonal-decomposition">Equation&nbsp;1</a>). By the Pythagorean theorem:</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{u}}{\bf{u}} &amp;= \inner{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}+\inner{\bf{w}}{\bf{w}}\\
&amp;= \overline{\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)}\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)\inner{\bf{v}}{\bf{v}} + \inner{\bf{w}}{\bf{w}}\\
&amp;= \frac{\overline{\inner{\bf{u}}{\bf{v}}}\inner{\bf{u}}{\bf{v}}}{\overline{\inner{\bf{v}}{\bf{v}}}} + \inner{\bf{w}}{\bf{w}}\\
&amp;= \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}} + \inner{\bf{w}}{\bf{w}}
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(\inner{\bf{w}}{\bf{w}} \geq 0\)</span>, it follows that:</p>
<p><span class="math display">\[
\inner{\bf{u}}{\bf{u}} \geq \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}}
\]</span></p>
<p>Consequently, we have:</p>
<p><span class="math display">\[
|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>In the special case, that <span class="math inline">\(V=\R^n\)</span> or <span class="math inline">\(V=\C^n\)</span>, we have:</p>
<p><span class="math display">\[
|\inner{\bf{u}}{\bf{v}}| \leq \norm{\bf{u}}_2 \norm{\bf{v}}_2
\]</span></p>
</section>
<section id="euclidean-norm" class="level2">
<h2 class="anchored" data-anchor-id="euclidean-norm">Euclidean Norm</h2>
<div id="prp-euclidean-norm-is-well-defined" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 (Well-definedness of the Euclidean norm) </strong></span>Let <span class="math inline">\(\norm{\cdot}:\mathbf{C}^n \to \mathbf{C}\)</span> be the euclidean norm. Our claim is, it is well-defined.</p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(\bf{z} = (z_1,z_2,\ldots,z_n) \in \C^n\)</span>. Clearly, it is positive semi-definite.</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{z}}_2 = \bf{z}^H \bf{z} &amp;= \overline{z_1} z_1 +\overline{z_2}z_2 + \ldots + \overline{z_n} z_n\\
&amp;= \sum_{i=1}^n |z_i|^2 \geq 0
\end{align*}
\]</span></p>
<p>It is also homogenous. Let <span class="math inline">\(\alpha \in \C\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
\norm{\alpha \bf{z}}_2 &amp;= \norm{(\alpha z_1, \alpha z_2,\ldots,\alpha z_n)}_2\\
&amp;=\sqrt{\sum_{i=1}^n |\alpha z_i|^2}\\
&amp;=|\alpha|\sqrt{\sum_{i=1}^n |z_i|^2} \\
&amp;= |\alpha|\norm{\bf{z}}_2
\end{align*}
\]</span></p>
<p>Letâ€™s verify, if the triangle inequality is satisfied. Let <span class="math inline">\(\bf{x}, \bf{y}\in\C^n\)</span> be arbitrary vectors.</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x} + \bf{y}}_2^2 &amp;= |(\bf{x} + \bf{y})^H(\bf{x} + \bf{y})|\\
&amp;= |(\bf{x}^H + \bf{y}^H)(\bf{x} + \bf{y})|\\
&amp;= |\bf{x}^H \bf{x} + \bf{y}^H \bf{y} + \bf{y}^H \bf{x} + \bf{x}^H \bf{y}|\\
&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + |\inner{\bf{y}}{\bf{x}}| + |\inner{\bf{x}}{\bf{y}}|\\
&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + \norm{\bf{y}}_2 \norm{\bf{x}}_2  + \norm{\bf{x}}_2 \norm{\bf{y}}_2 &amp; \{ \text{ Cauchy-Schwarz } \}\\
&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 +  2\norm{\bf{x}}_2 \norm{\bf{y}}_2\\
&amp;= (\norm{\bf{x}}_2 + \norm{\bf{y}}_2)^2
\end{align*}
\]</span></p>
<p>Consequently, <span class="math inline">\(\norm{\bf{x} + \bf{y}}_2 \leq \norm{\bf{x}}_2 + \norm{\bf{y}}_2\)</span>.</p>
</section>
<section id="the-vector-1-norm" class="level2">
<h2 class="anchored" data-anchor-id="the-vector-1-norm">The vector <span class="math inline">\(1-\)</span>norm</h2>
<div id="def-the-vector-1-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (The vector <span class="math inline">\(1-\)</span>norm) </strong></span>The vector <span class="math inline">\(1\)</span>-norm, <span class="math inline">\(\norm{\cdot}_1 : \C^n \to \R\)</span> is defined for all <span class="math inline">\(\bf{x}\in\C^n\)</span> by:</p>
<p><span class="math display">\[
\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| =\sum_{i=1}^n |\chi_i|
\]</span></p>
</div>
<div id="thm-1-norm-is-a-norm" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 </strong></span>The vector <span class="math inline">\(1\)</span>-norm is well-defined.</p>
</div>
<p><em>Proof.</em></p>
<p><em>Positive semi-definitess.</em></p>
<p>The absolute value of complex numbers is non-negative.</p>
<p><span class="math display">\[
\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| \geq |\chi_i| \geq 0
\]</span></p>
<p><em>Homogeneity.</em></p>
<p><span class="math display">\[
\norm{\alpha\bf{x}}_1 = \sum_{i=1}^{n}|\alpha \chi_i| = |\alpha| \sum_{i=1}^{n}|\chi_i| = |\alpha| \norm{\bf{x}}_1
\]</span></p>
<p><em>Triangle Inequality.</em></p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x} + \bf{y}} &amp;= \norm{(\chi_1 + \psi_1, \ldots,\chi_n + \psi_n)}_1\\
&amp;= \sum_{i=1}^n |\chi_i + \psi_i|\\
&amp;\leq \sum_{i=1}^n |\chi_i| + |\psi_i| &amp; \{ \text{ Triangle inequality for complex numbers }\}\\
&amp;= \sum_{i=1}^n |\chi_i| + \sum_{i=1}^{n} |\psi_i| &amp; \{ \text{ Commutativity }\}\\
&amp;= \norm{\bf{x}}_1 + \norm{\bf{y}}_1
\end{align*}
\]</span></p>
<p>Hence, the three axioms are satisfied. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="jensens-inequality" class="level2">
<h2 class="anchored" data-anchor-id="jensens-inequality">Jensenâ€™s inequality</h2>
<section id="convex-functions-and-combinations" class="level3">
<h3 class="anchored" data-anchor-id="convex-functions-and-combinations">Convex functions and combinations</h3>
<p>A function <span class="math inline">\(f\)</span> is said to be <em>convex</em> on over an interval <span class="math inline">\(I\)</span>, if for all <span class="math inline">\(x_1,x_2 \in I\)</span>, and every <span class="math inline">\(p \in [0,1]\)</span>, we have:</p>
<p><span class="math display">\[
f(px_1 + (1-p)x_2) \leq pf(x_1) + (1-p)f(x_2)
\]</span></p>
<p>In other words, all chords(secants) joining any two points on <span class="math inline">\(f\)</span>, lie above the graph of <span class="math inline">\(f\)</span>. Note that, if <span class="math inline">\(0 \leq p \leq 1\)</span>, then <span class="math inline">\(\min(x_1,x_2) \leq px_1 + (1-p)x_2 \leq \max(x_1,x_2)\)</span>. More generally, for non-negative real numbers <span class="math inline">\(p_1, p_2, \ldots, p_n\)</span> summing to one, that is, satisfying <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>, and for any points <span class="math inline">\(x_1,\ldots,x_n \in I\)</span>, the point <span class="math inline">\(\sum_{i=1}^n \lambda_i x_i\)</span> is called a <em>convex combination</em> of <span class="math inline">\(x_1,\ldots,x_n\)</span>. Since:</p>
<p><span class="math display">\[ \min(x_1,\ldots,x_n) \leq \sum_{i=1}^n p_i x_i \leq \max(x_1,\ldots,x_n)\]</span></p>
<p>every convex combination of any finite number of points in <span class="math inline">\(I\)</span> is again a point of <span class="math inline">\(I\)</span>.</p>
<p>Intuitively, <span class="math inline">\(\sum_{i=1}^{n}p_i x_i\)</span> simply represents the center of mass of the points <span class="math inline">\(x_1,\ldots,x_n\)</span> with weights <span class="math inline">\(p_1,\ldots,p_n\)</span>.</p>
</section>
<section id="proving-jensens-inequality" class="level3">
<h3 class="anchored" data-anchor-id="proving-jensens-inequality">Proving Jensenâ€™s inequality</h3>
<p>Jensenâ€™s inequality named after the Danish engineer <a href="https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)">Johan Jensen</a> (1859-1925) can be stated as follows:</p>
<div id="thm-jensens-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4 </strong></span>Let <span class="math inline">\(n \in \bf{Z}_+\)</span> be a positive integer and let <span class="math inline">\(f:I \to \R\)</span> be a convex function over the interval <span class="math inline">\(I \subseteq \R\)</span>. For any (not necessarily distinct) points <span class="math inline">\(x_1,\ldots,x_n \in I\)</span>, and non-negative real numbers <span class="math inline">\(p_1,\ldots,p_n \in \R\)</span> summing to one,</p>
<p><span class="math display">\[
f(\sum_{i=1}^n p_i x_i) \leq \sum_{i=1}^n p_i f(x_i)
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We proceed by induction. Since <span class="math inline">\(f\)</span> is convex, by definition, <span class="math inline">\(\forall x_1,x_2 \in I\)</span>, and any <span class="math inline">\(p_1,p_2\in \R\)</span>, such that <span class="math inline">\(p_1 + p_2 = 1\)</span>, we have <span class="math inline">\(f(p_1 x_1 + p_2 x_2) \leq p_1 f(x_1) + p_2 f(x_2)\)</span>. So, the claim is true for <span class="math inline">\(n=2\)</span>.</p>
<p><em>Inductive hypothesis</em>. Assume that <span class="math inline">\(\forall x_1,\ldots,x_{k} \in I\)</span> and any <span class="math inline">\(p_1,\ldots,p_k \in \R\)</span>, such that <span class="math inline">\(\sum_{i=1}^k p_i = 1\)</span>, we have <span class="math inline">\(f(\sum_{i=1}^k p_i x_i) \leq \sum_{i=1}^k p_i f(x_i)\)</span>.</p>
<p><em>Claim</em>. The Jensenâ€™s inequality holds for <span class="math inline">\(k+1\)</span> points in <span class="math inline">\(I\)</span>.</p>
<p><em>Proof</em>.</p>
<p>Let <span class="math inline">\(x_1,\ldots,x_k, x_{k+1}\)</span> be arbitrary points in <span class="math inline">\(I\)</span> and consider any convex combination of these points <span class="math inline">\(\sum_{i=1}^{k+1}p_i x_i\)</span>, <span class="math inline">\(p_i \in [0,1], i \in \{1,2,3,\ldots,k+1\}, \sum_{i=1}^{k+1}p_i = 1\)</span>.</p>
<p>Define:</p>
<p><span class="math display">\[
z := \frac{p_1 x_1 + p_2 x_2 + \ldots + p_k x_k}{\sum_{i=1}^k p_i}
\]</span></p>
<p>Since, <span class="math inline">\(z\)</span> is a convex combination of <span class="math inline">\(\{x_1,\ldots,x_k\}\)</span>, <span class="math inline">\(z \in I\)</span>. Moreover, by the inductive hypothesis, since <span class="math inline">\(f\)</span> is convex,</p>
<p><span class="math display">\[
\begin{align*}
f(z) &amp;= f\left(\frac{p_1 x_1 + p_2 x_2 + \ldots + p_k x_k}{\sum_{i=1}^k p_i}\right)\\
&amp;\leq \frac{p_1}{\sum_{i=1}^k p_i}f(x_1) + \frac{p_2}{\sum_{i=1}^k p_i}f(x_2) + \ldots + \frac{p_k}{\sum_{i=1}^k p_i}f(x_k) \\
&amp;= \frac{p_1}{1-p_{k+1}}f(x_1) + \frac{p_2}{1-p_{k+1}}f(x_2) + \ldots + \frac{p_k}{1-p_{k+1}}f(x_k) \\
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(0 \leq 1 - p_{k+1} \leq 1\)</span>, we deduce that:</p>
<p><span class="math display">\[
(1 - p_{k+1})f(z) \leq p_1 f(x_1) + \ldots + p_k f(x_k)
\]</span></p>
<p>We have: <span class="math display">\[
\begin{align*}
f(p_1 x_1 + \ldots + p_k x_k + p_{k+1} x_{k+1}) &amp;= f((1-p_{k+1})z + p_{k+1}x_{k+1})\\
&amp;\leq (1-p_{k+1})f(z) + p_{k+1}f(x_{k+1}) &amp; \{ \text{ Jensen's inequality for }n=2\}\\
&amp;\leq p_1 f(x_1) + \ldots + p_k f(x_k) + p_{k+1}f(x_{k+1}) &amp; \{ \text{ Deduction from the inductive hypothesis }\}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
</section>
<section id="youngs-inequality" class="level2">
<h2 class="anchored" data-anchor-id="youngs-inequality">Youngâ€™s Inequality</h2>
<p>Youngâ€™s inequality is named after the English mathematician <a href="https://en.wikipedia.org/wiki/William_Henry_Young">William Henry Young</a> and can be stated as follows:</p>
<div id="thm-youngs-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (Youngâ€™s inequality) </strong></span>For any non-negative real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and any positive real numbers <span class="math inline">\(p,q\)</span> satisfying <span class="math inline">\(\frac{1}{p} + \frac{1}{q}=1\)</span>, we have:</p>
<p><span class="math display">\[
ab \leq \frac{a^p}{p} + \frac{b^q}{q}
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(f(x) = \log x\)</span>. Since <span class="math inline">\(f\)</span> is concave, we can reverse the Jensenâ€™s inequality. Consequently:</p>
<p><span class="math display">\[
\begin{align*}
\log(\frac{a^p}{p} + \frac{b^q}{q}) &amp;\geq \frac{1}{p}\log a^p + \frac{1}{q}\log b^q\\
&amp;= \frac{1}{p}\cdot p \log a + \frac{1}{q}\cdot q \log b\\
&amp;= \log (ab)
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(\log x\)</span> is monotonic increasing,</p>
<p><span class="math display">\[
\frac{a^p}{p} + \frac{b^q}{q} \geq ab
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="holders-inequality" class="level2">
<h2 class="anchored" data-anchor-id="holders-inequality">Holderâ€™s inequality</h2>
<p>We can use Youngâ€™s inequality to prove the Holderâ€™s inequality, named after the German mathematician <a href="https://en.wikipedia.org/wiki/Otto_H%C3%B6lder">Otto Ludwig Holder</a> (1859-1937).</p>
<div id="thm-holders-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (Holderâ€™s inequality) </strong></span>For any pair of vectors <span class="math inline">\(\bf{x},\bf{y}\in \C^n\)</span>, and for any positive real numbers satisfying <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, we have <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span> we have:</p>
<p><span class="math display">\[
\sum_{i=1}^{n}|x_i y_i| \leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p} \left(\sum_{i=1}^n |y_i|^q\right)^{1/q}
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>Apply Youngâ€™s inequality to <span class="math inline">\(a = \frac{|x_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}}\)</span> and <span class="math inline">\(b = \frac{|y_i|}{\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}}\)</span>. We get:</p>
<p><span class="math display">\[
\begin{align*}
\frac{|x_i||y_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}} &amp;\leq \frac{1}{p} \frac{|x_i|^p}{\sum_{i=1}^n |x_i|^p} + \frac{1}{q}\frac{|y_i|^q}{\sum_{i=1}^n |y_i|^q}
\end{align*}
\]</span></p>
<p>Summing on both sides, we get:</p>
<p><span class="math display">\[
\begin{align*}
\frac{\sum_{i=1}^n|x_i y_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}} &amp;\leq \frac{1}{p} \frac{\sum_{i=1}^n |x_i|^p}{\sum_{i=1}^n |x_i|^p} + \frac{1}{q}\frac{\sum_{i=1}^n|y_i|^q}{\sum_{i=1}^n |y_i|^q}\\
&amp;= \frac{1}{p} + \frac{1}{q}\\
&amp;= 1\\
\sum_{i=1}^n |x_i y_i| &amp;\leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="the-vector-p-norm" class="level2">
<h2 class="anchored" data-anchor-id="the-vector-p-norm">The vector <span class="math inline">\(p\)</span>-norm</h2>
<p>The vector <span class="math inline">\(1\)</span>-norm and <span class="math inline">\(2\)</span>-norm are special cases of the <span class="math inline">\(p\)</span>-norm.</p>
<div id="def-vector-p-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 (<span class="math inline">\(p\)</span>-norm) </strong></span>Given <span class="math inline">\(p \geq 1\)</span>, the vector <span class="math inline">\(p\)</span>-norm <span class="math inline">\(\norm{\cdot}_p : \C^n \to \R\)</span> is defined by :</p>
<p><span class="math display">\[
\norm{\bf{x}}_p = \left(\sum_{i=1}^n |\chi_i|^p\right)^{1/p}
\]</span></p>
</div>
<div id="thm-p-norm-is-a-norm" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 </strong></span>The vector <span class="math inline">\(p\)</span>-norm is a well-defined norm.</p>
</div>
<p><em>Proof.</em></p>
<p><em>Positive semi-definite</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x}}_p &amp;= \left(\sum_{i=1}^n |\chi_i|^p \right)^{1/p}\\
&amp;\geq \left(|\chi_i|^p \right)^{1/p}\\
&amp;= |\chi_i| \geq 0
\end{align*}
\]</span></p>
<p><em>Homogeneity</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\alpha \bf{x}}_p &amp;= \left(\sum_{i=1}^n |\alpha \chi_i|^p \right)^{1/p}\\
&amp;= \left(\sum_{i=1}^n |\alpha|^p |\chi_i|^p \right)^{1/p}\\
&amp;= |\alpha|\left(\sum_{i=1}^n |\chi_i|^p \right)^{1/p} &amp;= |\alpha|\norm{\bf{x}}_p
\end{align*}
\]</span></p>
<p><em>Triangle Inequality</em></p>
<p>Define <span class="math inline">\(\frac{1}{q} := 1 - \frac{1}{p}\)</span>. <span class="math inline">\(\Longrightarrow (p-1)q = p\)</span>.</p>
<p>By the Holderâ€™s inequality: <span class="math display">\[
\begin{align*}
\sum_{i=1}^n |x_i||x_i + y_i|^{p-1} &amp;\leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p} \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}\\
\sum_{i=1}^n |y_i||x_i + y_i|^{p-1} &amp;\leq \left(\sum_{i=1}^n |y_i|^p\right)^{1/p} \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}
\end{align*}
\]</span></p>
<p>Summing, we get:</p>
<p><span class="math display">\[
\begin{align*}
\sum_{i=1}^n |x_i + y_i|^{p} &amp;\leq \left\{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right\} \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}\\
&amp;= \left\{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right\}\left(\sum_{i=1}^n |x_i + y_i|^{p}\right)^{1-\frac{1}{p}}\\
\Longrightarrow \left(\sum_{i=1}^n |x_i + y_i|^{p}\right)^{1/p} &amp;\leq \left\{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right\}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="the-vector-infty-norm" class="level2">
<h2 class="anchored" data-anchor-id="the-vector-infty-norm">The vector <span class="math inline">\(\infty\)</span>-norm</h2>
<div id="def-infinity-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (<span class="math inline">\(\infty\)</span>-norm) </strong></span>The vector <span class="math inline">\(\infty\)</span>-norm, <span class="math inline">\(\norm{\cdot}:\C^n \to \R\)</span> is defined for <span class="math inline">\(\bf{x} \in \C^n\)</span> by:</p>
<p><span class="math display">\[
\norm{\bf{x}}_\infty = \max\{|\chi_1|,|\chi_2|,\ldots,|\chi_n|\}
\]</span></p>
<p>The <span class="math inline">\(\infty\)</span>-norm simply measures how long the vector is by the magnitude of its largest entry.</p>
</div>
<div id="thm-infty-norm-is-a-norm" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8 </strong></span>The vector <span class="math inline">\(\infty\)</span>-norm is well-defined.</p>
</div>
<p><em>Proof.</em></p>
<p><em>Positive semi-definiteness</em></p>
<p>We have:</p>
<p><span class="math display">\[
\norm{\bf{x}}_{\infty} = \max_{1\leq i \leq n} |\chi_i| \geq |\xi_i| \geq 0
\]</span></p>
<p><em>Homogeneity</em></p>
<p>We have:</p>
<p><span class="math display">\[
\norm{\alpha \bf{x}}_{\infty} = \max_{1\leq i \leq n}|\alpha \chi_i| =\max_{1\leq i \leq n}|\alpha|| \chi_i| = |\alpha| \max_{1\leq i \leq n}|\chi_i| = |\alpha|\norm{\bf{x}}_{\infty}
\]</span></p>
<p><em>Triangle Inequality</em></p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x} + \bf{y}}_\infty &amp;= \max_{i=1}^m |\chi_i + \xi_i|\\
&amp;\leq \max_{i=1}^m (|\chi_i| + |\xi_i|)\\
&amp;\leq \max_{i=1}^m |\chi_i| + \max_{i=1}^m |\xi_i|\\
&amp;= \norm{\bf{x}}_\infty + \norm{\bf{y}}_\infty
\end{align*}
\]</span></p>
</section>
<section id="equivalence-of-vector-norms" class="level2">
<h2 class="anchored" data-anchor-id="equivalence-of-vector-norms">Equivalence of vector norms</h2>
<p>As I was saying earlier, we often measure if a vector is <em>small</em> or <em>large</em> or the distance between two vectors by computing norms. It would be unfortunate, if a vector were <em>small</em> in one norm, yet <em>large</em> in another. Fortunately, the next theorem excludes this possibility.</p>
<div id="thm-equivalence-of-vector-norms" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9 (Equivalence of vector norms) </strong></span>Let <span class="math inline">\(\norm{\cdot}_a:\C^n \to \R\)</span> and <span class="math inline">\(\norm{\cdot}_b:\C^n\to \R\)</span> both be vector norms. Then there exist positive scalars <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> such that for <span class="math inline">\(\bf{x}\in \C^n\)</span>,</p>
<p><span class="math display">\[
C_1 \norm{\bf{x}}_b \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_b
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We can prove equivalence of norms in four steps, the last which uses the extreme value theorem from Real Analysis.</p>
<section id="step-1-it-is-sufficient-to-consider-normcdot_b-normcdot_1-transitivity." class="level4">
<h4 class="anchored" data-anchor-id="step-1-it-is-sufficient-to-consider-normcdot_b-normcdot_1-transitivity.">Step 1: It is sufficient to consider <span class="math inline">\(\norm{\cdot}_b = \norm{\cdot}_1\)</span> (transitivity).</h4>
<p>We will show that it is sufficient to prove that <span class="math inline">\(\norm{\cdot}_a\)</span> is equivalent to <span class="math inline">\(\norm{\cdot}_1\)</span> because norm equivalence is <em>transitive</em>: if two norms are equivalent to <span class="math inline">\(\norm{\cdot}_1\)</span>, then they are equivalent to each other. In particular, suppose both <span class="math inline">\(\norm{\cdot}_a\)</span> and <span class="math inline">\(\norm{\cdot}_{a'}\)</span> are equivalent to <span class="math inline">\(\norm{\cdot}_1\)</span> for constants <span class="math inline">\(0 \leq C_1 \leq C_2\)</span> and <span class="math inline">\(0 \leq C_1' \leq C_2'\)</span> respectively:</p>
<p><span class="math display">\[
C_1 \norm{\bf{x}}_1 \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_1
\]</span></p>
<p>and</p>
<p><span class="math display">\[
C_1' \norm{\bf{x}}_1 \leq \norm{\bf{x}}_{a'} \leq C_2' \norm{\bf{x}}_1
\]</span></p>
<p>Then, it immediately follows that:</p>
<p><span class="math display">\[
\norm{\bf{x}}_{a'} \leq C_2' \norm{\bf{x}}_1 \leq \frac{C_2'}{C_1} \norm{\bf{x}}_a
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\norm{\bf{x}}_{a'} \geq C_1' \norm{\bf{x}}_1 \geq \frac{C_1'}{C_2} \norm{\bf{x}}_a
\]</span></p>
<p>and hence <span class="math inline">\(\norm{\cdot}_a\)</span> and <span class="math inline">\(\norm{\cdot}_{a'}\)</span> are equivalent. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="step-2-it-is-sufficient-to-consider-only-bfx-with-normbfx_1-1." class="level4">
<h4 class="anchored" data-anchor-id="step-2-it-is-sufficient-to-consider-only-bfx-with-normbfx_1-1.">Step 2: It is sufficient to consider only <span class="math inline">\(\bf{x}\)</span> with <span class="math inline">\(\norm{\bf{x}}_1 = 1\)</span>.</h4>
<p>We wish to show that</p>
<p><span class="math display">\[
C_1 \norm{\bf{x}}_1 \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_1
\]</span></p>
<p>is true for all <span class="math inline">\(\bf{x} \in V\)</span> for some <span class="math inline">\(C_1\)</span>, <span class="math inline">\(C_2\)</span>. It is trivially true for <span class="math inline">\(\bf{x}=\bf{0}\)</span>, so we only need to consider <span class="math inline">\(\bf{x}\neq\bf{0}\)</span>, in which case, we can divide by <span class="math inline">\(\norm{\bf{x}}_1\)</span>, to obtain the condition:</p>
<p><span class="math display">\[
C_1 \leq \norm{\frac{\bf{x}}{\norm{\bf{x}}_1 }}_a \leq C_2
\]</span></p>
<p>The vector <span class="math inline">\(\bf{u} = \frac{\bf{x}}{\norm{\bf{x}}_1}\)</span> is a unit vector in the <span class="math inline">\(1\)</span>-norm, <span class="math inline">\(\norm{\bf{u}}_1 = 1\)</span>. So, we can write:</p>
<p><span class="math display">\[
C_1 \leq \norm{\bf{u}}_a \leq C_2
\]</span></p>
<p>We have the desired result. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="step-3-any-norm-normcdot_a-is-continuous-under-normcdot_1." class="level4">
<h4 class="anchored" data-anchor-id="step-3-any-norm-normcdot_a-is-continuous-under-normcdot_1.">Step 3: Any norm <span class="math inline">\(\norm{\cdot}_a\)</span> is continuous under <span class="math inline">\(\norm{\cdot}_1\)</span>.</h4>
<p>We wish to show that any norm <span class="math inline">\(\norm{\cdot}_a\)</span> is a continuous function on <span class="math inline">\(V\)</span> under the topology induced by <span class="math inline">\(\norm{\cdot}_1\)</span>. That is, we wish to show that for any <span class="math inline">\(\epsilon &gt; 0\)</span>, there exists <span class="math inline">\(\delta &gt; 0\)</span>, such that for all <span class="math inline">\(\norm{\bf{x} - \bf{c}}_1 &lt; \delta\)</span>, we have <span class="math inline">\(\norm{\norm{\bf{x}}_a - \norm{\bf{c}}_a}_1 &lt; \epsilon\)</span>.</p>
<p>We prove this into two steps. First, by the triangle inequality on <span class="math inline">\(\norm{\cdot}_a\)</span>, it follows that:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x}}_a - \norm{\bf{c}}_a &amp;= \norm{\bf{c} + (\bf{x} - \bf{c})}_a - \norm{\bf{c}}_a \\
&amp;\leq \norm{\bf{c}}_a + \norm{(\bf{x} - \bf{c})}_a - \norm{\bf{c}}_a\\
&amp;= \norm{(\bf{x} - \bf{c})}_a
\end{align*}
\]</span></p>
<p>And</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{c}}_a - \norm{\bf{x}}_a &amp;\leq \norm{(\bf{x} - \bf{c})}_a
\end{align*}
\]</span></p>
<p>and hence:</p>
<p><span class="math display">\[
|\norm{\bf{x}}_a - \norm{\bf{c}}_a| \leq \norm{(\bf{x} - \bf{c})}_a
\]</span></p>
<p>Second applying the triangle inequality again, and writing <span class="math inline">\(\bf{x} = \sum_{i=1}^n \alpha_i \bf{e}_i\)</span> and <span class="math inline">\(\bf{c} = \sum_{i=1}^n \alpha_i' \bf{e}_i\)</span> in our basis, we obtain:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x}-\bf{c}}_a &amp;= \norm{\sum_{i=1}^n (\alpha_i - \alpha_i')\bf{e}_i}_a\\
&amp;\leq \sum_{i=1}^n \norm{(\alpha_i - \alpha_i')\bf{e}_i}_a &amp; \{ \text{ Triangle Inequality }\}\\
&amp;= \sum_{i=1}^n |(\alpha_i - \alpha_i')|\norm{\bf{e}_i}_a \\
&amp;= \norm{\bf{x} - \bf{c}}_1 \left(\max_i \norm{\bf{e}_i}_a \right)
\end{align*}
\]</span></p>
<p>Therefore, if we choose:</p>
<p><span class="math display">\[
\delta = \frac{\epsilon}{\left(\max_i \norm{\bf{e}_i}_a \right)}
\]</span></p>
<p>it immediate follows that:</p>
<p><span class="math display">\[\begin{align*}
\norm{\bf{x} - \bf{c}}_1 &amp;&lt; \delta \\
\Longrightarrow |\norm{\bf{x}}_a - \norm{\bf{c}}_a| &amp;\leq \norm{\bf{x} - \bf{c}}_a \\ &amp;\leq \norm{\bf{x} - \bf{c}}_1 \left(\max_i \norm{\bf{e}_i}_a \right) \\
&amp; \leq \frac{\epsilon}{\left(\max_i \norm{\bf{e}_i}_a \right)} \left(\max_i \norm{\bf{e}_i}_a \right) = \epsilon
\end{align*}
\]</span></p>
<p>This proves (uniform) continuity. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="step-4-the-maximum-and-minimum-of-normcdot_a-on-the-unit-ball" class="level4">
<h4 class="anchored" data-anchor-id="step-4-the-maximum-and-minimum-of-normcdot_a-on-the-unit-ball">Step 4: The maximum and minimum of <span class="math inline">\(\norm{\cdot}_a\)</span> on the unit ball</h4>
<p>Let <span class="math inline">\(K:=\{\bf{u}:\norm{\bf{u}}_1 = 1\}\)</span>. Then, <span class="math inline">\(K\)</span> is a compact set. Since <span class="math inline">\(\norm{\cdot}_a\)</span> is continuous on <span class="math inline">\(K\)</span>, by the extreme value theorem, <span class="math inline">\(\norm{\cdot}_a\)</span> must achieve a supremum and infimum on the set. So, for all <span class="math inline">\(\bf{u}\)</span> with <span class="math inline">\(\norm{\bf{u}}_1 = 1\)</span>, there exists <span class="math inline">\(C_1,C_2 &gt; 0\)</span>, such that:</p>
<p><span class="math display">\[ C_1 \leq \norm{\bf{u}}_a \leq C_2\]</span></p>
<p>as required by step 2. And we are done! <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="deriving-the-constants-c_1infty-c_infty1" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-constants-c_1infty-c_infty1">Deriving the constants <span class="math inline">\(C_{1,\infty}\)</span>, <span class="math inline">\(C_{\infty,1}\)</span></h3>
<p>Letâ€™s write a python implementation of the various norms.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> itertools</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-5"><a href="#cb3-5"></a></span>
<span id="cb3-6"><a href="#cb3-6"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="kw">def</span> one_norm(x):</span>
<span id="cb3-9"><a href="#cb3-9"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(x))</span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="kw">def</span> two_norm(x):</span>
<span id="cb3-12"><a href="#cb3-12"></a>    <span class="cf">return</span> np.sqrt(np.<span class="bu">sum</span>(x<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb3-13"><a href="#cb3-13"></a></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="kw">def</span> p_norm(x,p):</span>
<span id="cb3-15"><a href="#cb3-15"></a>    <span class="cf">return</span> np.<span class="bu">pow</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(x)<span class="op">**</span>p),<span class="fl">1.0</span><span class="op">/</span>p)</span>
<span id="cb3-16"><a href="#cb3-16"></a></span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="kw">def</span> infty_norm(x):</span>
<span id="cb3-18"><a href="#cb3-18"></a>    <span class="cf">return</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(x))</span>
<span id="cb3-19"><a href="#cb3-19"></a></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="kw">def</span> get_vectors_eq_norm_val(func, val, lower_bound, upper_bound):</span>
<span id="cb3-21"><a href="#cb3-21"></a>    x_1 <span class="op">=</span> np.linspace(lower_bound, upper_bound, </span>
<span id="cb3-22"><a href="#cb3-22"></a>    <span class="bu">int</span>((upper_bound <span class="op">-</span> lower_bound)<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb3-23"><a href="#cb3-23"></a>    x_2 <span class="op">=</span> np.linspace(lower_bound, upper_bound, </span>
<span id="cb3-24"><a href="#cb3-24"></a>    <span class="bu">int</span>((upper_bound <span class="op">-</span> lower_bound)<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb3-25"><a href="#cb3-25"></a></span>
<span id="cb3-26"><a href="#cb3-26"></a>    pts <span class="op">=</span> np.array(<span class="bu">list</span>(itertools.product(x_1, x_2)))</span>
<span id="cb3-27"><a href="#cb3-27"></a>    norm_arr <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(func, pts)))</span>
<span id="cb3-28"><a href="#cb3-28"></a></span>
<span id="cb3-29"><a href="#cb3-29"></a>    pts_norm_list <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(pts,norm_arr))</span>
<span id="cb3-30"><a href="#cb3-30"></a></span>
<span id="cb3-31"><a href="#cb3-31"></a>    pts_with_norm_eq_val <span class="op">=</span> []</span>
<span id="cb3-32"><a href="#cb3-32"></a>    <span class="cf">for</span> pt <span class="kw">in</span> pts_norm_list:</span>
<span id="cb3-33"><a href="#cb3-33"></a>        <span class="cf">if</span> pt[<span class="dv">1</span>] <span class="op">==</span> val:</span>
<span id="cb3-34"><a href="#cb3-34"></a>            pts_with_norm_eq_val.append(pt[<span class="dv">0</span>])</span>
<span id="cb3-35"><a href="#cb3-35"></a></span>
<span id="cb3-36"><a href="#cb3-36"></a>    <span class="cf">return</span> np.array(pts_with_norm_eq_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can glean useful information by visualizing the set of points(vectors) with a given norm.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>pts1 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb4-2"><a href="#cb4-2"></a>    func<span class="op">=</span>infty_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">1.0</span>, upper_bound<span class="op">=</span><span class="fl">1.0</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>)</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>pts2 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb4-6"><a href="#cb4-6"></a>    func<span class="op">=</span>one_norm, val<span class="op">=</span><span class="fl">2.0</span>, lower_bound<span class="op">=-</span><span class="fl">2.0</span>, upper_bound<span class="op">=</span><span class="fl">2.0</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>)</span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-10"><a href="#cb4-10"></a>plt.xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb4-11"><a href="#cb4-11"></a>plt.ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb4-12"><a href="#cb4-12"></a>a <span class="op">=</span> plt.scatter(pts1[:, <span class="dv">0</span>], pts1[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-13"><a href="#cb4-13"></a>b <span class="op">=</span> plt.scatter(pts2[:, <span class="dv">0</span>], pts2[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co"># c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)</span></span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a>plt.legend(</span>
<span id="cb4-17"><a href="#cb4-17"></a>    (a, b), (<span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_\infty = 1$"</span>, <span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_1=2$"</span>), loc<span class="op">=</span><span class="st">"lower left"</span></span>
<span id="cb4-18"><a href="#cb4-18"></a>)</span>
<span id="cb4-19"><a href="#cb4-19"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="593" height="427"></p>
</div>
</div>
<p>The blue rectangle represents all vectors <span class="math inline">\(\bf{x}\in\R^2\)</span> with unit <span class="math inline">\(\infty\)</span>-norm, <span class="math inline">\(\norm{\bf{x}}_\infty = 1\)</span>. The orange rhombus represents all vectors <span class="math inline">\(\bf{x}\)</span> with <span class="math inline">\(\norm{\bf{x}}_1 = 2\)</span>. All points on or outside the blue square represent vectors <span class="math inline">\(\bf{y}\)</span>, such that <span class="math inline">\(\norm{\bf{y}}_\infty \geq 1\)</span>. Hence, if <span class="math inline">\(\norm{\bf{y}}_1 = 2\)</span>, <span class="math inline">\(\norm{\bf{y}}_\infty \geq 1\)</span>.</p>
<p>Now, pick any <span class="math inline">\(\bf{z}\neq \bf{0}\)</span>. Then, <span class="math inline">\(2\norm{\frac{\bf{z}}{\norm{\bf{z}}_1}}_1 =2\)</span>. Thus, <span class="math inline">\(\norm{\frac{2\bf{z}}{\norm{\bf{z}}_1}}_\infty \geq 1\)</span>. So, it follows that if <span class="math inline">\(\bf{z}\in\R^2\)</span> is any arbitrary vector, <span class="math inline">\(\norm{\bf{z}}_1 \leq 2 \norm{\bf{z}}_\infty\)</span>.</p>
<p>In general, if <span class="math inline">\(\bf{x}\in\C^n\)</span>, then:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x}}_1 &amp;= \sum_{i=1}^n |x_i|\\
&amp;\leq \sum_{i=1}^n \max\{|x_i|:i=1,2,\ldots,n\}\\
&amp;= n \norm{\bf{x}}_\infty
\end{align*}
\]</span></p>
<p>Next, in the below plot, the orange rhombus represents vectors <span class="math inline">\(\bf{x}\in\R^2\)</span>, such that <span class="math inline">\(\normp{x}{1} = 1\)</span> and all points on or outside the orange rhombus are such that <span class="math inline">\(\normp{y}{1} \geq 1\)</span>. The blue square represents vectors <span class="math inline">\(\normp{y}{\infty} = 1\)</span>. Consequently, if <span class="math inline">\(\normp{y}{1} = 1\)</span>, then <span class="math inline">\(\normp{y}{\infty} \leq \normp{y}{1}\)</span>. In general, if <span class="math inline">\(\bf{x}\in C^n\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
\normp{x}{\infty} &amp;= \max\{|x_1|,\ldots,|x_n|\}\\
&amp;\leq \sum_{i=1}^n |x_i|=\normp{x}{1}
\end{align*}
\]</span></p>
<p>Putting together, we have:</p>
<p><span class="math display">\[
\begin{align*}
\normp{x}{\infty} \leq C_{\infty,1} \normp{x}{1} \\
\normp{x}{1} \leq C_{1,\infty} \normp{x}{\infty}
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(C_{\infty,1} = 1\)</span> and <span class="math inline">\(C_{1,\infty}=n\)</span>.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>pts1 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb5-2"><a href="#cb5-2"></a>    func<span class="op">=</span>infty_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">1.0</span>, upper_bound<span class="op">=</span><span class="fl">1.0</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>)</span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a>pts2 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb5-6"><a href="#cb5-6"></a>    func<span class="op">=</span>one_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">2.0</span>, upper_bound<span class="op">=</span><span class="fl">2.0</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>)</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-10"><a href="#cb5-10"></a>plt.xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb5-11"><a href="#cb5-11"></a>plt.ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb5-12"><a href="#cb5-12"></a>a <span class="op">=</span> plt.scatter(pts1[:, <span class="dv">0</span>], pts1[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-13"><a href="#cb5-13"></a>b <span class="op">=</span> plt.scatter(pts2[:, <span class="dv">0</span>], pts2[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co"># c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)</span></span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a>plt.legend(</span>
<span id="cb5-17"><a href="#cb5-17"></a>    (a, b), (<span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_\infty = 1$"</span>, <span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_1=1$"</span>), loc<span class="op">=</span><span class="st">"lower left"</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>)</span>
<span id="cb5-19"><a href="#cb5-19"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="600" height="427"></p>
</div>
</div>
</section>
<section id="deriving-the-constants-c_12-c_21" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-constants-c_12-c_21">Deriving the constants <span class="math inline">\(C_{1,2}\)</span>, <span class="math inline">\(C_{2,1}\)</span></h3>
<p>We can also derive the constants <span class="math inline">\(C_{1,2}\)</span> and <span class="math inline">\(C_{2,1}\)</span>. We have:</p>
<p>Let <span class="math inline">\(\bf{x}\in\C^n\)</span> be an arbitrary vector. And let <span class="math inline">\(\bf{y}=(1+0i,\ldots,1+0i)\)</span>. By the Cauchy-Schwarz inequality,</p>
<p><span class="math display">\[
\begin{align*}
\sum_{i=1}^n |x_i| \leq \left(\sum_{i=1}^n |x_i|^2\right)^{1/2}\sqrt{n}
\end{align*}
\]</span></p>
<p>So, our claim is <span class="math inline">\(\normp{x}{1} \leq \sqrt{n}\normp{x}{2}\)</span>.</p>
<p>Also, consider the vector <span class="math inline">\(\bf{v}=\left(\frac{1}{\sqrt{n}},\ldots,\frac{1}{\sqrt{n}}\right)\)</span>. <span class="math inline">\(\norm{\bf{v}}_1 = \sqrt{n}\norm{\bf{v}}_2\)</span>. So, the bound is tight.</p>
<p>Moreover:</p>
<p><span class="math display">\[
\begin{align*}
\normp{x}{2}^2 &amp;= \sum_{i=1}^n |x_i|^2 \\
&amp;\leq \sum_{i=1}^n |x_i|^2 + \sum_{i \neq j}|x_i||x_j|\\
&amp;= \sum_{i=1}^n |x_i|^2 + \sum_{i &lt; j}2|x_i||x_j|\\
&amp;= \left(\sum_{i=1}^n |x_i|\right)^2
\end{align*}
\]</span></p>
<p>So, <span class="math inline">\(\normp{x}{2} \leq \normp{x}{1}\)</span>. Consider the standard basis vector <span class="math inline">\(\bf{e}_1 = (1,0,0,\ldots,0)\)</span>. <span class="math inline">\(\norm{\bf{e}_1}_2 = \norm{\bf{e}_1}_1\)</span>. Hence, the bound is tight. We conclude that:</p>
<p><span class="math display">\[
\begin{align*}
\normp{x}{1} \leq C_{1,2} \normp{x}{2}\\
\normp{x}{2} \leq C_{2,1} \normp{x}{1}
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(C_{1,2} = \sqrt{n}\)</span> and <span class="math inline">\(C_{2,1} = 1\)</span>.</p>
</section>
<section id="deriving-the-constants-c_2infty-and-c_infty2" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-constants-c_2infty-and-c_infty2">Deriving the constants <span class="math inline">\(C_{2,\infty}\)</span> and <span class="math inline">\(C_{\infty,2}\)</span></h3>
<p>Let <span class="math inline">\(x \in \C^n\)</span>. We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{x}_2^2 &amp; = \sum_{i=0}^{n-1}|\chi_i|^2\\
&amp;\leq\sum_{i=0}^{n-1} (\max_{i=0}^{n-1}|\chi_i|)^2\\
&amp;= n \norm{x}_\infty
\end{align}
\]</span></p>
<p>So, <span class="math inline">\(\norm{x}_2 \leq \sqrt{n} \norm{x}_\infty\)</span>.</p>
<p>Moreover, let <span class="math inline">\(x = (1, 1, \ldots, 1)^T\)</span>. Then, <span class="math inline">\(\norm{x}_2 = \sqrt{n}\)</span> and <span class="math inline">\(\norm{x}_\infty = 1\)</span>, so <span class="math inline">\(\norm{x}_2 = \sqrt{n}\norm{x}_\infty\)</span>. Hence, it is a tight inequality.</p>
<p>Also, we have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{x}_\infty^2 &amp;= \max \{|\chi_0|^2,|\chi_1|^2,\ldots,|\chi_{n-1}^2|\}\\
&amp;\leq \max \{\sum_{i=0}^{n-1}|\chi_i|^2,\sum_{i=0}^{n-1}|\chi_i|^2,\ldots,\sum_{i=0}^{n-1}|\chi_i|^2|\}\\
&amp;= \norm{x}_2^2
\end{align*}
\]</span></p>
<p>Moreover, let <span class="math inline">\(x = (1, 0)\)</span>. Then, <span class="math inline">\(\norm{x}_2 = 1\)</span> and <span class="math inline">\(\norm{x}_\infty = 1\)</span>. So, <span class="math inline">\(\norm{x}_\infty = \norm{x}_2\)</span>. Hence, the inequality is tight.</p>
</section>
</section>
<section id="matrix-norms" class="level2">
<h2 class="anchored" data-anchor-id="matrix-norms">Matrix Norms</h2>
<p>The analysis of matrix algorithms requires the use of matrix norms. For example, the quality of a linear system solution may be poor, if the matrix of coefficients is <em>nearly singular</em>. To quantify the notion of singularity, we need a measure of the distance on the space of matrices. Matrix norms can be used to provide that measure.</p>
<section id="definitions" class="level3">
<h3 class="anchored" data-anchor-id="definitions">Definitions</h3>
<p>Since <span class="math inline">\(\R^{m \times n}\)</span> is isomorphic <span class="math inline">\(\R^{mn}\)</span>, the definition of a matrix norm is equivalent to the definition of a vector norm. In particular, <span class="math inline">\(f:\R^{m \times n} \to \R\)</span> is a matrix norm, if the following three properties holds:</p>
<p><span class="math display">\[
\begin{align*}
f(A) \geq 0, &amp; &amp; A \in \R^{m \times n}\\
f(A + B) \leq f(A) + f(B), &amp; &amp; A,B \in \R^{m \times n}\\
f(\alpha A) = |\alpha|f(A), &amp; &amp; \alpha \in \R, A \in \R^{m \times n}
\end{align*}
\]</span></p>
<p>The most frequently used matrix norms in numerical linear algebra are the Frobenius norm and the <span class="math inline">\(p\)</span>-norms.</p>
<div id="def-the-frobenius-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8 (Frobenius Norm) </strong></span>The Frobenius norm <span class="math inline">\(\norm{\cdot}_F : \C^{m \times n} \to \R\)</span> is defined for <span class="math inline">\(A \in \C^{m \times n}\)</span> by:</p>
<p><span class="math display">\[
\norm{A}_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}
\]</span></p>
</div>
<div id="thm-frobenius-norm-is-well-defined" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10 </strong></span>The Frobenius norm is a well-defined norm.</p>
</div>
<p><em>Proof.</em></p>
<p><em>Positive Semi-definite</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_F &amp;= \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}\\
&amp;\geq \left( |a_{ij}|^2\right)^{1/2} = |a_{ij}|\\
&amp;\geq 0
\end{align*}
\]</span></p>
<p><em>Triangle Inequality</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A + B}_F^2 &amp;= \sum_{i=1}^m \sum_{j=1}^n |a_{ij} + b_{ij}|^2 \\
&amp;\leq \sum_{i=1}^m \sum_{j=1}^n \left(|a_{ij}|^2 + |b_{ij}|^2 + 2|a_{ij}||b_{ij}|\right)\\
&amp;= \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 + \sum_{i=1}^m \sum_{j=1}^n |b_{ij}|^2 + 2\sum_{i=1}^m \sum_{j=1}^n|a_{ij}||b_{ij}|\\
&amp;\leq \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 + \sum_{i=1}^m \sum_{j=1}^n |b_{ij}|^2 + 2\left(\sum_{i=1}^m \sum_{j=1}^n|a_{ij}|^2\right)^{1/2}\left(\sum_{i=1}^m \sum_{j=1}^n|b_{ij}|^2\right)^{1/2} &amp; \{\text{ Cauchy-Schwarz }\}\\
&amp;= \norm{A}_F^2 + \norm{B}_F^2 + 2\norm{A}_F \norm{B}_F\\
&amp;= (\norm{A}_F + \norm{B}_F)^2\\\\
\Longrightarrow \norm{A + B}_F &amp;\leq \norm{A}_F + \norm{B}_F
\end{align*}
\]</span></p>
<p><em>Homogeneity</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\alpha A}_F &amp;= \left(\sum_{i=1}^m \sum_{j=1}^n |\alpha a_{ij}|^2\right)^{1/2}\\
&amp;=\left(\sum_{i=1}^m \sum_{j=1}^n |\alpha|^2 |a_{ij}|^2\right)^{1/2}\\
&amp;= |\alpha| \norm{A}_F
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="def-induced-matrix-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9 (Induced matrix norm) </strong></span>Let <span class="math inline">\(\norm{\cdot}_\mu : \C^m \to \R\)</span> and <span class="math inline">\(\norm{\cdot}_\nu : \C^n \to R\)</span> be vector norms. Define <span class="math inline">\(\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to R\)</span> by:</p>
<p><span class="math display">\[
\norm{A}_{\mu,\nu} = \sup_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu}
\]</span></p>
<p>Matrix norms that are defined in this way are called <em>induced</em> matrix norms.</p>
</div>
<p>Let us start by interpreting this. How <em>large</em> <span class="math inline">\(A\)</span> is, as measured by <span class="math inline">\(\norm{A}_{\mu,\nu}\)</span> is defined as the most that <span class="math inline">\(A\)</span> magnifies the length of non-zero vectors, where the length of the <span class="math inline">\(\bf{x}\)</span> is measured with the norm <span class="math inline">\(\norm{\cdot}_\nu\)</span> and the length of the transformed vector <span class="math inline">\(A\bf{x}\)</span> is measured with the norm <span class="math inline">\(\norm{\cdot}_\mu\)</span>.</p>
<p>Two comments are in order. First,</p>
<p><span class="math display">\[
\begin{align*}
\sup_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} = \sup_{\bf{x} \neq \bf{0}} \norm{A\frac{\bf{x}}{\norm{\bf{x}}_\nu}}_\mu = \sup_{\norm{\bf{u}}_\nu = 1} \norm{A\bf{u}}_\mu
\end{align*}
\]</span></p>
<p>Second, it is not immediately obvious, that there is a vector <span class="math inline">\(\bf{x}\)</span> for which a supremum is attained. The fact is there is always such a vector <span class="math inline">\(\bf{x}\)</span>. The <span class="math inline">\(K=\{\bf{u}:\norm{\bf{u}}_\nu = 1\}\)</span> is a compact set, and <span class="math inline">\(\norm{\cdot}_\mu : \C^m \to \R\)</span> is a continuous function. Continuous functions preserve compact sets. So, the supremum exists and further it belongs to <span class="math inline">\(\{A\bf{x}:\norm{\bf{x}}_\nu = 1\}\)</span>.</p>
<div id="thm-the-induced-matrix-norm-is-well-defined" class="theorem">
<p><span class="theorem-title"><strong>Theorem 11 </strong></span>The induced matrix norm <span class="math inline">\(\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to \R\)</span> is a well-defined norm.</p>
</div>
<p><em>Proof</em></p>
<p>To prove this, we merely check if the three conditions are met:</p>
<p>Let <span class="math inline">\(A,B \in \C^{m \times n}\)</span> and <span class="math inline">\(\alpha \in \C\)</span> be arbitrarily chosen. Then:</p>
<p><em>Positive definite</em></p>
<p>Let <span class="math inline">\(A \neq 0\)</span>. That means, at least one of the columns of <span class="math inline">\(A\)</span> is not a zero-vector. Partition <span class="math inline">\(A\)</span> by columns:</p>
<p><span class="math display">\[
\left[
    \begin{array}{c|c|c|c}
        a_{1} &amp; a_2 &amp; \ldots &amp; a_{n}
    \end{array}
\right]
\]</span></p>
<p>Let us assume that, it is the <span class="math inline">\(j\)</span>-th column <span class="math inline">\(a_j\)</span>, that is non-zero. Let <span class="math inline">\(\bf{e}_j\)</span> be the column of <span class="math inline">\(I\)</span>(the identity matrix) indexed with <span class="math inline">\(j\)</span>. Then:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_{\mu,\nu} &amp;= \sup \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Definition }\}\\
&amp;\geq \frac{\norm{A\bf{e}_j}_\mu}{\norm{\bf{e}_j}_\nu}\\
&amp;= \frac{\norm{a_j}_\mu}{\norm{\bf{e}_j}_\nu} &amp; \{ A\bf{e}_j = a_j \}\\
&amp;&gt; 0 &amp; \{ \text{ we assumed } a_j \neq \bf{0}\}
\end{align*}
\]</span></p>
<p><em>Homogeneity</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\alpha A}_{\mu,\nu} &amp;= \sup_{\bf{x}\neq \bf{0}} \frac{\norm{\alpha A \bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Definition }\}\\
&amp;= \sup_{\bf{x}\neq \bf{0}} \frac{|\alpha|\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Homogeneity of vector norm }\norm{\cdot}_\mu\}\\
&amp;= |\alpha|\sup_{\bf{x}\neq \bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Algebra }\}\\
&amp;= |\alpha|\norm{A}_{\mu,\nu}
\end{align*}
\]</span></p>
<p><em>Triangle Inequality</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A + B}_{\mu,\nu} &amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A + B) \bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Definition }\}\\
&amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x} + B\bf{x})}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Distribute }\}\\
&amp;\leq \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu + \norm{B\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Triangle inequality for vector norms }\}\\
&amp;= \max_{\bf{x}\neq \bf{0}} \left(\frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} + \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} \right) &amp; \{ \text{ Algebra }\}\\
&amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} + \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} \\
&amp;= \norm{A}_{\mu,\nu} + \norm{B}_{\mu,\nu} &amp; \{ \text{ Definition }\}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>When <span class="math inline">\(\norm{\cdot}_\mu\)</span> and <span class="math inline">\(\norm{\cdot}_\nu\)</span> are the same norm, the induced norm becomes:</p>
<p><span class="math display">\[
\norm{A}_\mu = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\mu}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\norm{A}_\mu = \max_{\norm{\bf{u}}_\mu = 1} \norm{A\bf{u}}_\mu
\]</span></p>
<div id="exm-p-norm-is-the-same" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 </strong></span>Consider the vector <span class="math inline">\(p\)</span>-norm <span class="math inline">\(\norm{\cdot}_p:\C^n \to \R\)</span> and let us denote the induced matrix norm <span class="math inline">\(|||\cdot|||:\C^{m \times n} \to \R\)</span> by <span class="math inline">\(|||A||| = \max_{\bf{x}\neq\bf{0}}\frac{\norm{A\bf{x}}_p}{\norm{\bf{x}}_p}\)</span>. Prove that <span class="math inline">\(|||\bf{y}||| = \norm{\bf{y}}_p\)</span> for all <span class="math inline">\(\bf{y}\in\C^m\)</span>.</p>
</div>
<p><em>Proof</em>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
|||\bf{y}||| &amp;= \frac{\norm{\bf{y}x}_p}{\norm{x}_p} &amp; \{ \text{ Definition }\}\\
&amp;= \frac{|x_1| \norm{\bf{y}}_p}{|x_1|} &amp; \{ x \text{ has to be } 1 \times 1, \text{ a scalar }\}\\
&amp;= \norm{\bf{y}}_p
\end{align*}
\]</span></p>
<p>The last example is important. One can view a vector <span class="math inline">\(\bf{y}\in \C^m\)</span> as an <span class="math inline">\(m \times 1\)</span> matrix. What this last exercise tells us is that regardless of whether we view <span class="math inline">\(\bf{y}\)</span> as a matrix or a vector, <span class="math inline">\(\norm{y}_p\)</span> is the same.</p>
<p>We already encountered the vector <span class="math inline">\(p\)</span>-norms as an important class of vector norms. The matrix <span class="math inline">\(p\)</span>-norm is induced by the corresponding vector norm.</p>
<div id="def-the-matrix-p-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10 (The matrix <span class="math inline">\(p\)</span>-norm) </strong></span>For any vector <span class="math inline">\(p\)</span>-norm, define the corresponding matrix <span class="math inline">\(p\)</span>-norm <span class="math inline">\(\norm{\cdot}_p : \C^{m \times n} \to \R\)</span> by:</p>
<p><span class="math display">\[
\norm{A}_p = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_p}{\norm{\bf{x}}_p} \quad \text{ or equivalently } \quad \norm{A}_p = \max_{\norm{\bf{x}}_p = 1} \norm{A\bf{x}}_p
\]</span></p>
</div>
<p>In practice, the matrix <span class="math inline">\(2\)</span>-norm is of great theoretical importance, but difficult to evaluate, except for special matrices. The <span class="math inline">\(1\)</span>-norm, the <span class="math inline">\(\infty\)</span>-norm and Frobenius norms are straightforward and relatively cheap to compute.</p>
<p>Let us instantiate the definition of the vector <span class="math inline">\(p\)</span>-norm where <span class="math inline">\(p=2\)</span>, giving us a matrix norm induced by the vector <span class="math inline">\(2\)</span>-norm or the Euclidean norm:</p>
<div id="def-the-matrix-2-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 11 (The matrix <span class="math inline">\(2\)</span>-norm) </strong></span>Define the matrix <span class="math inline">\(2\)</span>-norm <span class="math inline">\(\norm{\cdot}_2:\C^{m \times n} \to \R\)</span> by :</p>
<p><span class="math display">\[
\norm{A}_2 = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_2}{\norm{\bf{x}}_2} = \max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2
\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The problem with the matrix <span class="math inline">\(2\)</span>-norm is that it is hard to compute. In future posts, we shall find out that if <span class="math inline">\(A\)</span> is a Hermitian matrix (<span class="math inline">\(A = A^H\)</span>), then <span class="math inline">\(\norm{A}_2 = |\lambda_1|\)</span> where <span class="math inline">\(\lambda_1\)</span> is the eigenvalue of <span class="math inline">\(A\)</span> that is largest in magnitude.</p>
<p>Recall from basic linear algebra, that computing eigenvalues involves computing the roots of polynomials, and for polynomials of degree three or greater, this is a non-trivial task. We shall see that the matrix <span class="math inline">\(2\)</span>-norm plays an important part in theory, but less so in practical computation.</p>
</div>
</div>
<div id="exm-matrix-2-norm-of-diagonal-matrix" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 </strong></span>Show that:</p>
<p><span class="math display">\[
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2 = \max(|d_1|,|d_2|)
\]</span></p>
</div>
<p><em>Solution</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}}_2^2 &amp; \{ \text{ Definition }\}\\
&amp;= \max_{\norm{\bf{x}}_2 = 1}|d_1x_1|^2 + |d_2 x_2|^2\\
&amp;\leq \max_{\norm{\bf{x}}_2 = 1} [\max(|d_1|,|d_2|)^2 |x_1|^2 + \max(|d_1|,|d_2|)^2 |x_2|^2]\\
&amp;= \max(|d_1|,|d_2|)^2 \max_{\norm{\bf{x}}_2 = 1} (|x_1|^2 + |x_2|^2)\\
&amp;= \max(|d_1|,|d_2|)^2
\end{align*}
\]</span></p>
<p>Moreover, if we take <span class="math inline">\(\bf{x} = \bf{e}_1\)</span> and <span class="math inline">\(\bf{x}=\bf{e}_2\)</span>, we get:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2^2  &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}}_2 &amp; \{ \text{ Definition }\}\\
&amp;\geq  \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}1 \\ 0\end{bmatrix}}_2^2 \\
&amp;= |d_1|^2
\end{align*}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align*}
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2^2  &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}}_2 &amp; \{ \text{ Definition }\}\\
&amp;\geq  \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}0 \\ 1\end{bmatrix}}_2 \\
&amp;= |d_2|^2
\end{align*}
\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2^2 \geq \max(|d_1|,|d_2|)^2
\]</span></p>
<p>We conclude that</p>
<p><span class="math display">\[
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2 = \max(|d_1|,|d_2|)
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The proof of the last example builds on a general principle: Showing that <span class="math inline">\(\max_{x \in D} f(x) = \alpha\)</span> for some function <span class="math inline">\(f:D \to \R\)</span> can be broken down into showing that both:</p>
<p><span class="math display">\[
\max_{x \in D} f(x) \leq \alpha
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\max_{x \in D} f(x) \geq \alpha
\]</span></p>
<p>In turn, showing that <span class="math inline">\(\max_{x \in D}f(x) \geq \alpha\)</span> can often be accomplished by showing that there exists a vector <span class="math inline">\(y \in D\)</span> such that <span class="math inline">\(f(y) = \alpha\)</span> since then</p>
<p><span class="math display">\[
\max_{x \in D}f(x) \geq f(y) = \alpha
\]</span></p>
<p>We will use this technique in future proofs involving matrix norms.</p>
</div>
</div>
<div id="exr-2-norm-of-a-diag-matrix" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1 </strong></span>Let <span class="math inline">\(D \in C^{m \times m}\)</span> be a diagonal matrix <span class="math inline">\(diag(d_1,d_2,\ldots,d_m)\)</span>. Show that:</p>
<p><span class="math display">\[
\norm{D}_2 = \max_{j=1}^{m} |d_j|
\]</span></p>
</div>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{D}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{
    \begin{bmatrix}
    d_1 \\
    &amp; d_2 \\
    &amp; &amp; \ddots\\
    &amp; &amp; &amp; d_m
    \end{bmatrix}
    \begin{bmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_m
    \end{bmatrix}
}_2^2 \{ \text{ Definition }\}\\
&amp;=\max_{\norm{\bf{x}}_2 = 1} \norm{
    \begin{bmatrix}
    d_1 x_1\\
    d_2 x_2\\
    \vdots\\
    d_m x_m
    \end{bmatrix}
}_2^2\\
&amp;= \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m |d_j x_j|^2\\
&amp;\leq \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m \max(|d_1|,\ldots,|d_m|)^2 |x_j|^2\\
&amp;= \max(|d_1|,\ldots,|d_m|)^2 \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m |x_j|^2 \\
&amp;= \max(|d_1|,\ldots,|d_m|)^2
\end{align*}
\]</span></p>
<p>Moreover, if we take take <span class="math inline">\(\bf{x} = \bf{e}_j\)</span>, the standard basis vector with its <span class="math inline">\(j\)</span>-th coordinate equal to one, we find that</p>
<p><span class="math display">\[
\norm{D}_2^2 \geq |d_j|^2
\]</span></p>
<p>Consequently, <span class="math inline">\(\norm{D}_2^2 \geq \max(|d_1|,\ldots,|d_m|)^2\)</span>.</p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-properties-of-two-norm-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2 </strong></span>Let <span class="math inline">\(\bf{y}\in\C^m\)</span> and <span class="math inline">\(\bf{x} \in \C^n\)</span>. Show that:</p>
<p><span class="math display">\[
\norm{\bf{y}\bf{x}^H}_2 = \norm{\bf{y}}_2 \norm{\bf{x}}_2
\]</span></p>
</div>
<p><em>Proof</em>.</p>
<p>From the Cauchy-Schwarz inequality, we know that:</p>
<p><span class="math display">\[
|x^H z| \leq \norm{\bf{x}^H}_2 \norm{\bf{z}}_2
\]</span></p>
<p>Now, <span class="math inline">\(\bf{x}^H \in \C^{1 \times n}\)</span> and <span class="math inline">\(\bf{z} \in \C^{n \times 1}\)</span>. So, <span class="math inline">\(\bf{x}^H \bf{z} \in \C^{1 \times 1}\)</span>, and it is a scalar.</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{y}\bf{x}^H}_2 &amp;= \max_{\norm{\bf{z}}_2 = 1} \norm{\bf{y}\bf{x}^H \bf{z}}_2 \\
&amp;= \max_{\norm{\bf{z}}_2 = 1} |\bf{x}^H \bf{z}| \norm{\bf{y}}_2 \{ \bf{x}^H\bf{z}\text{ is scalar }\}\\
&amp;\leq \max_{\norm{\bf{z}}_2 = 1} \norm{\bf{x}^H}_2 \norm{\bf{z}}_2 \norm{\bf{y}}_2 \\
&amp;= \norm{\bf{x}}_2 \norm{\bf{y}}_2
\end{align*}
\]</span></p>
<p>On the other hand,</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{y}\bf{x}^H}_2 &amp;= \max_{\bf{z}\neq \bf{0}} \frac{\norm{\bf{y}\bf{x}^H \bf{z}}_2}{\norm{\bf{z}}_2}\\
&amp;\geq \frac{\norm{\bf{y}\bf{x}^H \bf{x}}_2}{\norm{\bf{x}}_2} &amp; \{ \text{ Specific }\bf{z} \}\\
&amp;= \frac{\norm{\bf{y}\norm{\bf{x}}_2^2}_2}{\norm{\bf{x}}_2} &amp; \{ \bf{x}^H \bf{x} = \norm{\bf{x}}_2^2\}\\
&amp;= \norm{\bf{y}}_2 \norm{\bf{x}}_2
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-properties-of-two-norm-2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3 </strong></span>Let <span class="math inline">\(A \in \C^{m \times n}\)</span> and <span class="math inline">\(a_j\)</span> be its column indexed with <span class="math inline">\(j\)</span>. Prove that:</p>
<p><span class="math display">\[
\norm{a_j}_2 \leq \norm{A}_2
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_2 &amp;= \max_{\norm{\bf{z}}_2 = 1} \norm{A\bf{z}}_2 \\
&amp;\geq  \norm{A\bf{e}_j}_2\\
&amp;= \norm{a_j}_2
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-properties-of-two-norm-3" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4 </strong></span>Let <span class="math inline">\(A \in \C^{m \times n}\)</span>. Prove that:</p>
<ol type="i">
<li><p><span class="math display">\[
\norm{A}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|
\]</span></p></li>
<li><p><span class="math display">\[
\norm{A^H}_2 = \norm{A}_2
\]</span></p></li>
<li><p><span class="math display">\[
\norm{A^H A}_2 = \norm{A}_2^2
\]</span></p></li>
</ol>
</div>
<p><em>Claim</em>.</p>
<p><span class="math display">\[
\norm{A}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|
\]</span></p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}| &amp;\leq \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} \norm{\bf{y}^H}_2 \norm{A\bf{x}}_2 &amp; \{ \text{ Cauchy-Schwarz }\} \\
&amp;= \max_{\norm{\bf{x}}_2 } \norm{A\bf{x}}_2\\
&amp;= \norm{A}_2
\end{align*}
\]</span></p>
<p>On the other hand:</p>
<p><span class="math display">\[
\begin{align*}
\max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}| &amp;\geq \max_{\norm{\bf{x}}_2 = 1} |\left(\frac{A\bf{x}}{\norm{A\bf{x}}_2}\right)^H A \bf{x}| &amp; \{\text{ Specific vector }\}\\
&amp;= \max_{\norm{\bf{x}}_2 = 1} \frac{\norm{A\bf{x}}_2^2}{\norm{A\bf{x}}_2}\\
&amp;=\max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2\\
&amp;= \norm{A}_2
\end{align*}
\]</span></p>
<p>We have the desired result.</p>
<p><em>Claim</em>.</p>
<p><span class="math display">\[
\norm{A^H}_2 = \norm{A}_2
\]</span></p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A^H}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{A^H\bf{x}}_2^2 \\
&amp;= \max_{\norm{\bf{x}}_2 = 1} |(A^H \bf{x})^H (A^H \bf{x})|
\end{align*}
\]</span></p>
<p><em>Claim.</em></p>
<p><span class="math display">\[
\norm{A^H}_2 = \norm{A}_2
\]</span></p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A^H}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H \bf{x}| \\
&amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{x}^H A \bf{y}| &amp; \{ |\overline \alpha| = |\alpha| \}\\
&amp;= \max_{\norm{\bf{y}}_2 = 1} \norm{A \bf{y}}_2 \\
&amp;= \norm{A}_2
\end{align*}
\]</span></p>
<p><em>Claim</em></p>
<p><span class="math display">\[
\norm{A^H A}_2 = \norm{A}_2^2
\]</span></p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A^H A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H A \bf{x}|\\
&amp;\leq \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} \norm{\bf{y}^H A^H}_2 \norm{A\bf{x}}_2 &amp; \{ \text{ Cauchy-Schwarz }\}\\
&amp;= \max_{\norm{\bf{y}}_2 = 1} \norm{A\bf{y}}_2 \max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2  &amp; \{ \norm{A^H}_2 = \norm{A}_2 \}\\
&amp;= \norm{A}_2^2
\end{align*}
\]</span></p>
<p>Moreover:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A^H A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H A \bf{x}|\\
&amp;\geq \max_{\norm{\bf{x}}_2 = 1}  |\bf{x}^H A^H A \bf{x}| \{ \text{ Restrict the choices of }\bf{y}\}\\
&amp;=  \max_{\norm{\bf{x}}_2 = 1}  |(A\bf{x})^H (A \bf{x})| \\
&amp;=  \max_{\norm{\bf{x}}_2 = 1}  \norm{A\bf{x}}_2^2\\
&amp;= \norm{A}_2^2
\end{align*}
\]</span></p>
<div id="exr-properties-of-two-norm-4" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5 </strong></span>Partition</p>
<p><span class="math display">\[
A = \left[
    \begin{array}{c|c|c}
        A_{1,1} &amp; \ldots &amp; A_{1,N}\\
        \hline
        \vdots &amp; &amp; \vdots\\
        \hline
        A_{M,1} &amp; \ldots &amp; A_{M,N}
    \end{array}
\right]
\]</span></p>
<p>Prove that <span class="math inline">\(\norm{A_{i,j}}_2 \leq \norm{A}_2\)</span>.</p>
</div>
<p><em>Proof.</em></p>
<p>By definition,</p>
<p><span class="math display">\[
\begin{align*}
\norm{A_{i,j}}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A_{i,j} \bf{x}|
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1\)</span> is a compact set, the above maximum exists. There exists <span class="math inline">\(\bf{w}_i\)</span> and <span class="math inline">\(\bf{v}_j\)</span>, satisfying <span class="math inline">\(\norm{\bf{w}_i}_2 = \norm{\bf{v}_j}_2 = 1\)</span> such that:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A_{i,j}}_2 = |\bf{w}_i^H A_{i,j} \bf{v}_j|
\end{align*}
\]</span></p>
<p>Next, we choose</p>
<p><span class="math display">\[
\bf{w} = \left[
    \begin{array}{c}
    0 \\
    \hline
    0 \\
    \hline
    \vdots \\
    \hline
    \bf{w}_i\\
    \hline
    0 \\
    \hline
    \vdots\\
    0
    \end{array}
\right] \quad
\bf{v} = \left[
    \begin{array}{c}
    0 \\
    \hline
    0 \\
    \hline
    \vdots \\
    \hline
    \bf{v}_j\\
    \hline
    0 \\
    \hline
    \vdots\\
    0
    \end{array}
\right]
\]</span></p>
<p>Consider:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|\\
&amp; \geq |\bf{w}^H A \bf{v}|\\
&amp;= |\bf{w}_j^H A_{i,j} \bf{v}_i|\\
&amp;= \norm{A_{i,j}}_2
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="computing-the-matrix-1-norm-and-infty-norm" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-matrix-1-norm-and-infty-norm">Computing the matrix <span class="math inline">\(1\)</span>-norm and <span class="math inline">\(\infty\)</span>-norm</h3>
<p>The matrix <span class="math inline">\(1\)</span>-norm and the matrix <span class="math inline">\(\infty\)</span>-norm are of great importance, because, unlike the matrix <span class="math inline">\(2\)</span>-norm, they are easy and relatively cheap to compute. The following exercises show how to practically compute the matrix <span class="math inline">\(1\)</span>-norm and <span class="math inline">\(\infty\)</span>-norm.</p>
<div id="exr-exercise-on-matrix-1-norm-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6 </strong></span>Let <span class="math inline">\(A = \C^{m \times n}\)</span> and partition <span class="math inline">\(A = [a_1 | a_2 | \ldots | a_n]\)</span>. Prove that</p>
<p><span class="math display">\[
\norm{A}_1 = \max_{1 \leq j \leq n}\norm{a_j}_1
\]</span></p>
</div>
<p><em>Proof</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_1 &amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{A\bf{x}}_1 &amp; \{ \text{ Definition }\}\\
&amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{a_1 x_1 + a_2 x_2 + \ldots + a_n x_n}_1 &amp; \{ \text{ Algebra }\}\\
&amp;\leq \max_{\norm{\bf{x}}_1 = 1} \norm{a_1 x_1}_1 + \norm{a_2 x_2}_1 + \ldots + \norm{a_n x_n}_1 &amp; \{ \text{ Triangle Inequality }\}\\
&amp;= \max_{\norm{\bf{x}}_1 = 1} |x_1| \norm{a_1}_1 + |x_2| \norm{a_2}_1 + \ldots + |x_n| \norm{a_n}_1  &amp; \{ \text{ Homogeneity }\}\\
&amp;= \max_{\norm{\bf{x}}_1 = 1}  |x_1| (\max_{1 \leq j \leq n} \norm{a_j}_1) + |x_2| (\max_{1 \leq j \leq n} \norm{a_j}_1)+ \ldots + |x_n| (\max_{1 \leq j \leq n} \norm{a_j}_1)\\
&amp;= \max_{1 \leq j \leq n} \norm{a_j}_1 \max_{\norm{\bf{x}}_1 = 1} \sum_{j=1}^n |x_j|\\
&amp;= \max_{1 \leq j \leq n} \norm{a_j}_1
\end{align*}
\]</span></p>
<p>On the other hand,</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_1  &amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{A\bf{x}}_1 &amp; \{ \text{ Definition }\}\\
&amp;\geq \norm{A\bf{e}_j}_1 &amp; \{ \text{ Specific vector }\}\\
&amp;= \norm{a_j}_1
\end{align*}
\]</span></p>
<p>This concludes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-exercise-on-matrix-infty-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7 </strong></span>Let <span class="math inline">\(A = \C^{m \times n}\)</span> and partition</p>
<p><span class="math display">\[
A = \left[
    \begin{array}{c}
        \tilde{a}_0^T\\
        \hline
        \tilde{a}_1^T\\
        \hline
        \vdots\\
        \hline
        \tilde{a}_{m-1}^T\\
    \end{array}
\right]
\]</span></p>
<p>Prove that</p>
<p><span class="math display">\[
\norm{A}_\infty = \max_{0\leq i &lt; m} \norm{\tilde{a}_i}_1 = \max_{0 \leq i &lt; m} (|\alpha_{i,0}| + |\alpha_{i,1}| + \ldots + |\alpha_{i,n-1}|)
\]</span></p>
</div>
<p>*Notice that in this exercise, <span class="math inline">\(\tilde{a}_i\)</span> is really <span class="math inline">\((\tilde{a}_i^T)^T\)</span>, since <span class="math inline">\(\tilde{a}_i^T\)</span> is the label for the <span class="math inline">\(i\)</span>-th row of the matrix.</p>
<p><em>Proof</em>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_\infty &amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{A\bf{x}}_\infty &amp; \{ \text{ Definition }\} \\
&amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{\left[
    \begin{array}{c}
        \tilde{a}_0^T \bf{x}\\
        \hline
        \tilde{a}_1^T \bf{x}\\
        \hline
        \vdots\\
        \hline
        \tilde{a}_{m-1}^T \bf{x}\\
    \end{array}
\right]}_\infty &amp; \{ \text{ Algebra }\}\\
&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} |\tilde{a}_i^T \bf{x}|\\
&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} |\alpha_{i,0}x_0 + \ldots + \alpha_{i,n-1}x_{n-1}|\\
&amp;\leq  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} \left( |\alpha_{i,0}x_0 | + \ldots + |\alpha_{i,n-1}x_{n-1}| \right) &amp; \{ \text{ Triangle Inequality }\}\\
&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} \left( |\alpha_{i,0}||x_0 | + \ldots + |\alpha_{i,n-1}||x_{n-1}| \right) &amp; \{ \text{ Algebra }\}\\
&amp;\leq \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m}\left( |\alpha_{i,0}|\norm{\bf{x}}_\infty + \ldots + |\alpha_{i,n-1}|\norm{\bf{x}}_\infty \right) &amp; \{ |x_i| \leq \norm{\bf{x}}_\infty \}\\
&amp;= \max_{0 \leq i &lt; m} ( |\alpha_{i,0}| + \ldots + |\alpha_{i,n-1}|) \max_{\norm{\bf{x}}_\infty = 1} \norm{\bf{x}}_\infty\\
&amp;= \max_{0 \leq i &lt; m} ( |\alpha_{i,0}| + \ldots + |\alpha_{i,n-1}|)\\
&amp;= \max_{0 \leq i &lt; m}
\norm{\tilde{a}_i}_1
\end{align*}
\]</span></p>
<p>We also want to show that <span class="math inline">\(\norm{A}_\infty \geq \max_{0 \leq i &lt; m} \norm{\tilde{a}_i}_1\)</span>. Let <span class="math inline">\(k\)</span> be such that <span class="math inline">\(\max_{0 \leq i &lt; m}\norm{\tilde{a}_i}_1 = \norm{\tilde{a}_k}_1\)</span> and pick <span class="math inline">\(\bf{y} = \left(\begin{array}{c}\psi_0\\ \vdots\\ \psi_{n-1}\end{array}\right)\)</span> so that <span class="math inline">\(\tilde{a}_k^T \bf{y} = |\alpha_{k,0}| + |\alpha_{k,1}| + \ldots + |\alpha_{k,n-1}|=\norm{\tilde{a}_k}_1\)</span>. This is a matter of picking <span class="math inline">\(\psi_j = |\alpha_{k,j}|/\alpha_{k,j}\)</span>. Then, <span class="math inline">\(|\psi_j| = 1\)</span> and hence, <span class="math inline">\(\norm{\bf{y}}_\infty = 1\)</span> and <span class="math inline">\(\psi_j \alpha_{k,j} = |\alpha_{k,j}|\)</span>. Then:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_\infty &amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{A\bf{x}}_\infty &amp; \{ \text{ Definition }\} \\
&amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{\left[
    \begin{array}{c}
        \tilde{a}_0^T \bf{x}\\
        \hline
        \tilde{a}_1^T \bf{x}\\
        \hline
        \vdots\\
        \hline
        \tilde{a}_{m-1}^T \bf{x}\\
    \end{array}
\right]}_\infty &amp; \{ \text{ Expose rows }\}\\
&amp;\geq  \norm{\left[
    \begin{array}{c}
        \tilde{a}_0^T \bf{y}\\
        \hline
        \tilde{a}_1^T \bf{y}\\
        \hline
        \vdots\\
        \hline
        \tilde{a}_{m-1}^T \bf{y}\\
    \end{array}
\right]}_\infty &amp; \{ \text{ Specific vector }\}\\
&amp;\geq |\tilde{a}_k^T \bf{y}|\\
&amp;= \norm{\tilde{a}_k}_1 \\
&amp;= \max_{0 \leq i &lt; m} \norm{\tilde{a}_i}_1
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-computing-matrix-norms" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 8 </strong></span>Fill out the following table:</p>
<p><span class="math display">\[
\begin{array}{|c|c|c|c|}
\hline
A &amp; \norm{A}_1 &amp; \norm{A}_\infty &amp; \norm{A}_F &amp; \norm{A}_2\\
\hline
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}\\
\hline
\begin{bmatrix}
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1
\end{bmatrix}\\
\hline
\begin{bmatrix}
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0
\end{bmatrix}\\
\hline
\end{array}
\]</span></p>
</div>
<p><em>Solution</em>.</p>
<p>Let</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<p>We have, <span class="math inline">\(\norm{A}_1 = 1\)</span>, <span class="math inline">\(\norm{A}_\infty = 1\)</span>, <span class="math inline">\(\norm{A}_F = \sqrt{3}\)</span>. Since this is a diagonal matrix, <span class="math inline">\(\norm{A}_2 = \max_{0 \leq i \leq 2} |d_{i}|\)</span> = 1.</p>
<p>Next, consider:</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1
\end{bmatrix}
\]</span></p>
<p>We have, <span class="math inline">\(\norm{A}_1 = 4\)</span>, <span class="math inline">\(\norm{A}_\infty = 3\)</span>, <span class="math inline">\(\norm{A}_F = \sqrt{12}\)</span>.</p>
<p>Note that, we can write</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 \\
1 \\
1 \\
1 \\
\end{bmatrix} [1, 1, 1, 1] = \bf{x}\bf{y}^H
\]</span></p>
<p>where <span class="math inline">\(\bf{x} = \bf{y} = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \\ \end{bmatrix}\)</span>. Using the property that, <span class="math inline">\(\norm{\bf{x}\bf{y}^H}_2 = \norm{\bf{x}}_2 \norm{\bf{y}}_2\)</span>, we have that, <span class="math inline">\(\norm{A}_2 = 4\)</span>.</p>
<p>Finally, if</p>
<p><span class="math display">\[
A = \begin{bmatrix}
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0
\end{bmatrix}
\]</span></p>
<p>we find that <span class="math inline">\(\norm{A}_1 = 3\)</span>, <span class="math inline">\(\norm{A}_\infty = 1\)</span>, <span class="math inline">\(\norm{A}_F = \sqrt{3}\)</span>. Finally, let <span class="math inline">\(\bf{x} = \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}\)</span> and <span class="math inline">\(\bf{y} = \begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix}\)</span>. Then, <span class="math inline">\(A = \bf{x}\bf{y}^H\)</span>. So, <span class="math inline">\(\norm{A}_2 = \norm{\bf{x}}_2 \norm{\bf{y}}_2 = \sqrt{3}\)</span>.</p>
</section>
<section id="equivalence-of-matrix-norms" class="level3">
<h3 class="anchored" data-anchor-id="equivalence-of-matrix-norms">Equivalence of matrix norms</h3>
<p>We saw that vector norms are equivalent in the sense that if a vector is <em>small</em> in one norm, it is small in all other norms and if it is large in one norm, it is large in all other norms. The same is true for matrix norms.</p>
<div id="thm-equivalence-of-matrix-norms" class="theorem">
<p><span class="theorem-title"><strong>Theorem 12 (Equivalence of matrix norms) </strong></span>Let <span class="math inline">\(\norm{\cdot} : \C^{m \times n} \to \R\)</span> and <span class="math inline">\(|||\cdot|||:\C^{m \times n} \to \R\)</span> both be matrix norms. Then, there exist positive scalars <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tau\)</span> such that for all <span class="math inline">\(A \in \C^{m \times n}\)</span></p>
<p><span class="math display">\[
\sigma \norm{A} \leq |||A||| \leq \tau \norm{A}
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>The proof again builds on the fact that the supremum over a compact set is achieved and can be replaced by the maximum. We will prove that there exists <span class="math inline">\(\tau\)</span> such that for all <span class="math inline">\(A \in \C^{m \times n}\)</span></p>
<p><span class="math display">\[
|||A||| \leq \tau \norm{A}
\]</span></p>
<p>Let <span class="math inline">\(A \in \C^{m \times n}\)</span> be an arbitrary matrix. Assume that <span class="math inline">\(A \neq 0\)</span> (the zero matrix). Then:</p>
<p><span class="math display">\[
\begin{align*}
|||A||| &amp;= \frac{|||A|||}{\norm{A}} \cdot \norm{A} &amp; \{\text{Algebra}\}\\
&amp;\leq \sup_{Z \neq 0} \left(\frac{|||Z|||}{\norm{Z}}\right) \norm{A} &amp; \{\text{Definition of supremum}\}\\
&amp;= \sup_{Z \neq 0} \left(\Biggl|\Biggl|\Biggl|\frac{Z}{\norm{Z}}\Biggr|\Biggr|\Biggr|\right) \norm{A} &amp; \{\text{Homogeneity}\}\\
&amp;= \left(\sup_{\norm{B} = 1} |||B||| \right) \norm{A} &amp;\{ \text{change of variables }B=Z/\norm{Z}\}\\
&amp;= \left(\max_{\norm{B}=1}|||B|||\right) \norm{A} &amp; \{\text{the set }\norm{B} = 1\text{ is compact}\}
\end{align*}
\]</span></p>
<p>So, we can choose <span class="math inline">\(\tau = \max_{\norm{B}=1} |||B|||\)</span>.</p>
<p>Also, from the above proof, we deduce that, there exists <span class="math inline">\(\sigma\)</span> given by:</p>
<p><span class="math display">\[
\sigma = \frac{1}{\max_{|||B|||=1}||B||}
\]</span></p>
<p>such that:</p>
<p><span class="math display">\[
\sigma \norm{A} \leq |||A|||
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-matrix-2-norm-bounded-by-frobenius-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 9 </strong></span>Given <span class="math inline">\(A \in \C^{m \times n}\)</span>, show that <span class="math inline">\(\norm{A}_2 \leq \norm{A}_F\)</span>. For what matrix, is the equality attained?</p>
</div>
<p><em>Solution</em>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_2^2 &amp;= \max_{\norm{x}_2 = 1} \norm{Ax}_2^2  &amp; \{\text{Definition}\}\\
&amp;= \max_{\norm{x}_2 = 1} \norm{\begin{bmatrix}
\sum_{j=0}^{n-1} a_{0,j} x_j \\
\sum_{j=0}^{n-1} a_{1,j} x_j \\
\vdots\\
\sum_{j=0}^{n-1} a_{m-1,j} x_j
\end{bmatrix}
}_2^2\\
&amp;= \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \Biggl|\sum_{j=0}^{n-1} a_{i,j} x_j\Biggr|^2\\
&amp;\leq \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j} x_j|\right)^2 &amp; \{\text{Triangle Inequality}\}\\
&amp;\leq \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left\{\left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right)^{1/2} \left(\sum_{j=0}^{n-1}|x_j|^2\right)^{1/2}\right\}^2 &amp; \{\text{Cauchy-Schwarz}\}\\
&amp;=\max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right) \left(\sum_{j=0}^{n-1}|x_j|^2\right) &amp; \{\text{Simplify}\}\\
&amp;=\sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right) &amp; \{\norm{x}_2 = 1\}\\
&amp;= \norm{A}_F
\end{align*}
\]</span></p>
<p>Also, consider</p>
<p><span class="math display">\[A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}\]</span></p>
<p>Then, <span class="math inline">\(\norm{A}_2 = \norm{A}_F = 1\)</span>. So, the inequality <span class="math inline">\(\norm{A}_2 \leq \norm{A}_F\)</span> is tight. This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-matrix-norm-equivalences" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 10 </strong></span>Let <span class="math inline">\(A \in \C^{m \times n}\)</span>. The following table summarizes the equivalences of various matrix norms:</p>
<p><span class="math display">\[
\begin{array}{c|c|c|c}
&amp; \norm{A}_1 \leq \sqrt{m}\norm{A}_2 &amp; \norm{A}_1 \leq m \norm{A}_\infty &amp; \norm{A}_1 \leq \sqrt{m}\norm{A}_F \\
\hline
\norm{A}_2 \leq \sqrt{n}\norm{A}_1 &amp; &amp; \norm{A}_2 \leq \sqrt{m}\norm{A}_\infty &amp; \norm{A}_2 \leq \norm{A}_F \\
\hline
\norm{A}_\infty \leq n \norm{A}_1 &amp; \norm{A}_\infty \leq \sqrt{n} \norm{A}_2 &amp; &amp; \norm{A}_\infty \leq \sqrt{n}\norm{A}_F\\
\hline
\norm{A}_F \leq \sqrt{n} \norm{A}_1 &amp; \norm{A}_F \leq \tau \norm{A}_2 &amp; \norm{A}_F \leq \sqrt{m}\norm{A}_\infty
\end{array}
\]</span></p>
<p>For each, prove the inequality, including that it is a tight inequality for some nonzero <span class="math inline">\(A\)</span>. (Skip <span class="math inline">\(\norm{A}_F \leq \tau \norm{A}_2\)</span>, we revisit it in a later post)</p>
</div>
<p><em>Solution</em>.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_1 \leq \sqrt{m} \norm{A}_2\)</span>.</p>
<p>Partition <span class="math inline">\(A = [a_0 | a_1 | \ldots | a_{n-1}]\)</span>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_1 &amp;= \max_{0 \leq j &lt; n} \norm{a_j}_1 &amp; \{\text{Definition}\}\\
&amp;= \max_{0 \leq j &lt; n} \norm{
    \begin{bmatrix}
    \alpha_{0,j}\\
    \alpha_{1,j}\\
    \vdots\\
    \alpha_{m-1,j}
    \end{bmatrix}
}_1\\
&amp;= \max_{0 \leq j &lt; n} \sum_{i=0}^{m-1}|\alpha_{i,j}| \cdot |1|\\
&amp;= \max_{0 \leq j &lt; n} \left(\sum_{i=0}^{m-1}|\alpha_{i,j}|^2\right)^{1/2} \left(\sum_{i=0}^{m-1}|1|^2\right)^{1/2} &amp; \{\text{Cauchy-Schwarz}\}\\
&amp;= \max_{0 \leq j &lt; n} \norm{a_j}_2 \sqrt{m}\\
&amp;= \max_{0 \leq j &lt; n} \norm{A}_2 \sqrt{m} &amp; \{\norm{A_{i,j}}_2 \leq \norm{A}_2\}\\
&amp;= \sqrt{m} \norm{A}_2
\end{align*}
\]</span></p>
<p>Moreover, consider the matrix</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 &amp; 0 \\
1 &amp; 0
\end{bmatrix} =
\begin{bmatrix}
1 \\
1
\end{bmatrix}
\begin{bmatrix}
1 &amp; 0
\end{bmatrix}
\]</span></p>
<p>We have, <span class="math inline">\(\norm{A}_2 = \sqrt{2}\)</span> and <span class="math inline">\(\norm{A}_1 = 2\)</span>, so <span class="math inline">\(\norm{A}_1 = \sqrt{2}\norm{A}_2\)</span>. Thus, the inequality is tight. This closes the proof.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_1 \leq m \norm{A}_\infty\)</span>.</p>
<p><em>Solution</em>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_1 &amp;= \max_{0 \leq j &lt; n} \norm{a_j}_1 &amp; \{\text{Definition}\}\\
&amp;= \max_{0 \leq j &lt; n} \sum_{i=0}^{m-1}|\alpha_{i,j}|\\
&amp;\leq \sum_{j=0}^{n-1} \sum_{i=0}^{m-1} |\alpha_{i,j}| = \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}| \\
&amp;=\sum_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1\\
&amp;= m \max_{0 \leq i &lt; m} \norm{\tilde{a}_i^T}_1\\
&amp;= m \norm{A}_\infty
\end{align}
\]</span></p>
<p>Again, consider the matrix</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 &amp; 0\\
1 &amp; 0
\end{bmatrix}
\]</span></p>
<p><span class="math inline">\(\norm{A}_1 = 2\)</span> and <span class="math inline">\(\norm{A}_\infty = 1\)</span>, so <span class="math inline">\(\norm{A}_1 = 2 \norm{A}_\infty\)</span>. Hence, the inequality is tight. This closes the proof.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_1 \leq \sqrt{m}\norm{A}_F\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have shown that:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_1 &amp;\leq \sqrt{m}\norm{A}_2 \\
\norm{A}_2 &amp;\leq \norm{A}_F
\end{align}
\]</span></p>
<p>So, we deduce that <span class="math inline">\(\norm{A}_1 \leq \sqrt{m}\norm{A}_F\)</span>. Moreover, consider</p>
<p><span class="math display">\[
A = \sqrt{2}I
\]</span></p>
<p>Then, <span class="math inline">\(\norm{A}_1 = \sqrt{2}\)</span> and <span class="math inline">\(\norm{A}_F = 2\)</span>, so <span class="math inline">\(\norm{A}_1 = \sqrt{2}\norm{A}_F\)</span>. Hence, the inequality is tight.</p>
<p><em>Claim.</em> Our claim is that <span class="math inline">\(\norm{A}_2 \leq \sqrt{n}\norm{A}_1\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_2 &amp;= \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} &amp; \{\text{Definition}\}\\
&amp;= \max_{x \neq 0} \frac{\norm{Ax}_1}{\norm{x}_2}  &amp; \{\norm{z}_2 \leq \norm{z}_1\} \\
&amp;= \max_{x \neq 0} \frac{\norm{Ax}_1}{\frac{1}{\sqrt{n}}\norm{x}_1}  &amp; \{\norm{z}_1 \leq \sqrt{n}\norm{z}_2\} \\
&amp;= \sqrt{n}\norm{A}_1
\end{align}
\]</span></p>
<p>Again, consider the matrix <span class="math inline">\(A = [1 | 1| \ldots | 1]\)</span>. Then, <span class="math inline">\(\norm{A}_2 = \sqrt{n}\)</span> and <span class="math inline">\(\norm{A}_1 = 1\)</span>. So, <span class="math inline">\(\norm{A}_2 = \sqrt{n}\norm{A}_1\)</span>.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_2 \leq \sqrt{m} \norm{A}_\infty\)</span>.</p>
<p><em>Solution.</em></p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_2 &amp;= \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} &amp;\{\text{Definition}\}\\
&amp;=\max_{x \neq 0} \frac{\norm{\begin{bmatrix}\tilde{a}_0^T \\ \tilde{a}_1^T \\ \vdots \\ \tilde{a}_{m-1}^T\end{bmatrix}x}_2}{\norm{x}_2} &amp;\{\text{Expose rows}\}\\
&amp;=\max_{x \neq 0} \frac{\norm{\begin{bmatrix}\tilde{a}_0^T x \\ \tilde{a}_1^T x\\ \vdots \\ \tilde{a}_{m-1}^T x\end{bmatrix}}_2}{\norm{x}_2} &amp;\{\text{Algebra}\}\\
&amp;\leq \max_{x \neq 0} \frac{\sqrt{m}\norm{Ax}_\infty}{\norm{x}_2} &amp;\{\norm{z}_2 \leq \sqrt{n}\norm{z}_\infty\}\\
&amp; \leq \max_{x \neq 0} \frac{\sqrt{m}\norm{Ax}_\infty}{\norm{x}_\infty} &amp;\{\norm{z}_\infty \leq \norm{z}_2\}\\
&amp;= \sqrt{m} \norm{A}_\infty
\end{align*}
\]</span></p>
<p>Moreover, consider the matrix</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 \\
1 \\
\vdots \\
1
\end{bmatrix}
\]</span></p>
<p>We have <span class="math inline">\(\norm{A}_2 = \sqrt{m}\)</span>, <span class="math inline">\(\norm{A}_\infty = 1\)</span>, so <span class="math inline">\(\norm{A}_2 = \sqrt{m}\norm{A}_\infty\)</span>. Hence, the inequality is tight.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_\infty \leq n \norm{A}_1\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_\infty &amp;= \max_{x \neq 0} \frac{\norm{Ax}_\infty}{\norm{x}_\infty} &amp; \{\text{Definition}\}\\
&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_1}{\norm{x}_\infty} &amp; \{\norm{x}_\infty \leq \norm{x}_1\}\\
&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_1}{\frac{1}{n}\norm{x}_1} &amp; \{\norm{x}_1 \leq n \norm{x}_\infty\}\\
&amp;= n \norm{A}_1
\end{align*}
\]</span></p>
<p>Moreover, let <span class="math inline">\(A = [1 | 1 | \ldots | 1]\)</span>. Then, <span class="math inline">\(\norm{A}_\infty = n\)</span> and <span class="math inline">\(\norm{A}_1 = 1\)</span>, so <span class="math inline">\(\norm{A}_\infty = n \norm{A}_1\)</span>. Hence, the inequality is tight.</p>
<p><em>Claim.</em> Our claim is that <span class="math inline">\(\norm{A}_\infty \leq \sqrt{n} \norm{A}_2\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_\infty &amp;= \max_{x \neq 0} \frac{\norm{Ax}_\infty}{\norm{x}_\infty} &amp; \{\text{Definition}\}\\
&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_\infty} &amp; \{\norm{x}_\infty \leq \norm{x}_2\}\\
&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_2}{\frac{1}{\sqrt{n}}\norm{x}_2} &amp; \{\norm{x}_2 \leq \sqrt{n} \norm{x}_\infty\}\\
&amp;= \sqrt{n} \norm{A}_2
\end{align*}
\]</span></p>
<p>Moreover, let <span class="math inline">\(A = [1|1|\ldots|1]\)</span></p>
<p>Then, <span class="math inline">\(\norm{A}_\infty = n\)</span>, <span class="math inline">\(\norm{A}_2 = \sqrt{n}\)</span> and <span class="math inline">\(\norm{A}_\infty = \sqrt{n} \norm{A}_2\)</span>. So, the bound is tight.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_\infty \leq \sqrt{n}\norm{A}_F\)</span>.</p>
<p><em>Solution.</em> This is true since <span class="math inline">\(\norm{A}_\infty \leq \sqrt{n}\norm{A}_2\)</span> and <span class="math inline">\(\norm{A}_2 \leq \norm{A}_F\)</span>.</p>
<p>Let <span class="math inline">\(A = [1 | 1 | \ldots | 1]\)</span>. Then, <span class="math inline">\(\norm{A}_\infty = n\)</span> and <span class="math inline">\(\norm{A}_F = \sqrt{n}\)</span>. So, <span class="math inline">\(\norm{A}_\infty = \sqrt{n}\norm{A}_F\)</span>. The bound is tight.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_F \leq \sqrt{n} \norm{A}_1\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_F^2 &amp;= \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}|^2 &amp; \{\text{Definition}\}\\
&amp;= \sum_{j=0}^{n-1} \sum_{i=0}^{m-1} |\alpha_{i,j}|^2 \\
&amp;\leq \sum_{j=0}^{n-1} \left(\sum_{i=0}^{m-1} |\alpha_{i,j}| \right)^2 \\
&amp;= \sum_{j=0}^{n-1} \norm{a_j}_1^2 \\
&amp;\leq \sum_{j=0}^{n-1} \max_{j=0}^{n-1} \norm{a_j}_1^2 \\
&amp;= n \norm{A}_1^2
\end{align}
\]</span></p>
<p>Consequently, <span class="math inline">\(\norm{A}_F \leq \sqrt{n}\norm{A}_1\)</span>. Let <span class="math inline">\(A = [1 |1 | \ldots| 1]\)</span>. Then, <span class="math inline">\(\norm{A}_F = \sqrt{n}\)</span> and <span class="math inline">\(\norm{A}_1 = 1\)</span>, so <span class="math inline">\(\norm{A}_F = \sqrt{n}\norm{A}_1\)</span>. Hence, the bound is tight.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_F \leq \sqrt{m} \norm{A}_\infty\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_F^2 &amp;= \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}|^2 &amp; \{\text{Definition}\}\\
&amp;\leq \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |\alpha_{i,j}| \right)^2 \\
&amp;= \sum_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1^2 \\
&amp;\leq \sum_{j=0}^{n-1} \max_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1^2 \\
&amp;= m \norm{A}_\infty^2
\end{align}
\]</span></p>
<p>Consequently, <span class="math inline">\(\norm{A}_F \leq \sqrt{m}\norm{A}_\infty\)</span>. Let <span class="math inline">\(A = [1, 1, \ldots, 1]^T\)</span>. Then, <span class="math inline">\(\norm{A}_F = \sqrt{m}\)</span> and <span class="math inline">\(\norm{A}_\infty = 1\)</span>, so <span class="math inline">\(\norm{A}_F = \sqrt{m}\norm{A}_1\)</span>. Hence, the bound is tight.</p>
</section>
<section id="sub-multiplicative-norms" class="level3">
<h3 class="anchored" data-anchor-id="sub-multiplicative-norms">Sub-multiplicative norms</h3>
<p>There are a number of properties that we would like a matrix norm to have(but not all matrix norms do). Given a matrix norm <span class="math inline">\(\norm{\cdot} : \C^{m \times n} \to \R\)</span>, we may ask the following question. Do there exist vector norms <span class="math inline">\(\norm{\cdot}_\mu : C^m \to \R\)</span> and <span class="math inline">\(\norm{\cdot}:\C^n \to R\)</span>, such that the matrix norm is an upper bound on how much the non-zero vector <span class="math inline">\(x\)</span> is stretched? That is, the following inequality is satisfied:</p>
<p><span class="math display">\[
\frac{\norm{Ax}_\mu}{\norm{x}_\nu} \leq \norm{A}
\]</span></p>
<p>or equivalently</p>
<p><span class="math display">\[
\norm{Ax}_\mu \leq \norm{A} \norm{x}_\nu
\]</span></p>
<p>where this second formulation has the benefit that it also holds if <span class="math inline">\(x = 0\)</span>.</p>
<div id="def-subordinate-matrix-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 12 (Subordinate matrix norm) </strong></span>A matrix norm <span class="math inline">\(\norm{\cdot}:\C^{m \times n} \to \R\)</span> is said to be subordinate to vector norms <span class="math inline">\(\norm{\cdot}_\mu :\C^m \to \R\)</span> and <span class="math inline">\(\norm{\cdot}_\nu : \C^n \to \R\)</span>, if for all, <span class="math inline">\(x \in \C^n\)</span>,</p>
<p><span class="math display">\[
\norm{Ax}_\mu \leq \norm{A} \norm{x}_\nu
\]</span></p>
<p>If <span class="math inline">\(\norm{\cdot}_\mu\)</span> and <span class="math inline">\(\norm{\cdot}_\nu\)</span> are the same norm (but perhaps for different <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>), then <span class="math inline">\(\norm{\cdot}\)</span> is said to be subordinate to the given vector norm.</p>
</div>
<div id="exr-matrix-2-norm-is-subordinate-to-vector-2-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 11 </strong></span>Prove that the matrix <span class="math inline">\(2\)</span>-norm is subordinate to the vector <span class="math inline">\(2\)</span>-norm.</p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(A \in C^{m \times n}\)</span> and let <span class="math inline">\(x \in \C^n\)</span>. Assume that <span class="math inline">\(x \neq 0\)</span>, for if <span class="math inline">\(x = 0\)</span>, then the inequality <span class="math inline">\(\norm{Ax}_2 \leq \norm{A}_2 \norm{x}_2\)</span> is vacuously true.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{Ax}_2&amp;= \left(\frac{\norm{Ax}_2}{\norm{x}_2}\right) \norm{x}_2 &amp; \{x \neq 0\} \\
&amp;\leq \left(\max_{y \neq 0} \frac{\norm{Ay}_2}{\norm{y}_2}\right)\norm{x}_2\\
&amp;= \norm{A}_2 \norm{x}_2
\end{align}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-frobenius-norm-is-subordinate-to-vector-2-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 12 </strong></span>Prove that the Frobenius norm is subordinate to the vector <span class="math inline">\(2\)</span>-norm.</p>
</div>
<p><em>Proof.</em></p>
<p>We are interested to prove the claim that, <span class="math inline">\((\forall A \in \C^{m\times n})(\forall x \in \C^n)\)</span>:</p>
<p><span class="math display">\[ \norm{Ax}_2 \leq \norm{A}_F \norm{x}_2 \]</span></p>
<p>Again, without loss of generality, we have:</p>
<p><span class="math display">\[
\begin{align}
\norm{Ax}_2^2 &amp;= \norm{
    \begin{bmatrix}
        \sum_{j=0}^{n-1}\alpha_{0,j} x_j \\
        \sum_{j=0}^{n-1}\alpha_{1,j} x_j \\
        \vdots
        \sum_{j=0}^{n-1}\alpha_{m-1,j} x_j
    \end{bmatrix}
}_2^2 &amp; \{\text{Definition}\}\\
&amp;= \sum_{i=0}^{m-1} \Biggl| \sum_{j=0}^{n-1}\alpha_{i,j} x_j \Biggr|^2\\
&amp;= \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1}|\alpha_{i,j} x_j| \right)^2 &amp; \{\text{Triangle Inequality}\}\\
&amp;\leq \sum_{i=0}^{m-1} \left[\left(\sum_{j=0}^{n-1}|\alpha_{i,j}|^2 \right)  \left(\sum_{j=0}^{n-1} |x_j|^2\right)\right] &amp; \{\text{Cauchy-Schwarz}\}\\
&amp;= \left(\sum_{j=0}^{n-1} |x_j|^2\right) \left(\sum_{i=0}^{m-1} \sum_{j=0}^{n-1}|\alpha_{i,j}|^2 \right)   &amp; \{\text{Algebra}\}\\
&amp;= \norm{A}_F^2 \norm{x}_2^2
\end{align}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="thm-induced-matrix-norms-are-subordinate-norms" class="theorem">
<p><span class="theorem-title"><strong>Theorem 13 </strong></span>Induced matrix norms, <span class="math inline">\(\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to \R\)</span> are subordinate to the norms, <span class="math inline">\(\norm{\cdot}_\mu\)</span> and <span class="math inline">\(\norm{\cdot}_\nu\)</span> that induce them.</p>
</div>
<p><em>Proof</em>.</p>
<p>Without loss of generality, assume that <span class="math inline">\(x \neq 0\)</span>, otherwise the proposition is vacuously true.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{Ax}_\mu &amp;= \frac{\norm{Ax}_\mu}{\norm{x}_\nu} \norm{x}_\nu \\
&amp;\leq \left(\max_{x \neq 0} \frac{\norm{Ax}_\mu}{\norm{x}_\nu} \right) \norm{x}_\nu \\
&amp;= \left(\max_{y \neq 0} \frac{\norm{Ay}_\mu}{\norm{y}_\nu} \right) \norm{x}_\nu \\
&amp;= \norm{A} \norm{x}_\nu
\end{align}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="cor-matrix-p-norms-are-subordinate" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1 </strong></span>Any matrix <span class="math inline">\(p\)</span>-norm is subordinate to the corresponding vector norm.</p>
</div>
<p><em>Proof.</em></p>
<p>Without the loss of generality, assume that <span class="math inline">\(x \neq 0\)</span>. If <span class="math inline">\(x = 0\)</span>, the proposition is vacuously true.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{Ax}_p &amp;= \left(\frac{\norm{Ax}_p}{\norm{x}_p} \right) \norm{x}_p &amp; \{ x \neq 0\}\\
&amp;\leq  \left(\max_{x \neq 0} \frac{\norm{Ax}_p}{\norm{x}_p} \right) \norm{x}_p\\
&amp;=  \left(\max_{y \neq 0}\frac{\norm{Ay}_p}{\norm{y}_p} \right) \norm{x}_p\\
&amp;= \norm{A}_p \norm{x}_p
\end{align}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>Another desirable property that not all norms have is that:</p>
<p><span class="math display">\[
\norm{AB} \leq \norm{A} \norm{B}
\]</span></p>
<p>This requires the given norm to be defined for all matrix sizes.</p>
<div id="def-consistent-matrix-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 13 (Consistent matrix norm) </strong></span>A matrix norm <span class="math inline">\(\norm{\cdot} : \C^{m \times n} \to \R\)</span> is said to be consistent matrix norm if it is defined for all <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>, using the same formula for all <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>.</p>
</div>
<div id="def-submulticative-matrix-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 14 (Submultiplicative matrix norm) </strong></span>A consistent matrix norm <span class="math inline">\(\norm{\cdot} : \C^{m \times n} \to \R\)</span> is said to be submultiplicative if it satisfies:</p>
<p><span class="math display">\[
\norm{AB} \leq \norm{A} \norm{B}
\]</span></p>
</div>
<div id="thm-induced-matrix-norms-are-submultiplicative" class="theorem">
<p><span class="theorem-title"><strong>Theorem 14 </strong></span>Let <span class="math inline">\(\norm{\cdot} : \C^n \to \R\)</span> be a vector norm defined for all <span class="math inline">\(n\)</span>. Define the corresponding induced matrix norm as:</p>
<p><span class="math display">\[
\norm{A} = \max_{x \neq 0} \frac{\norm{Ax}}{\norm{x}} = \max_{\norm{x} = 1} \norm{Ax}
\]</span></p>
<p>Then, for any <span class="math inline">\(A \in \C^{m \times k}\)</span> and <span class="math inline">\(B^{k \times n}\)</span>, the inequality <span class="math inline">\(\norm{AB} \leq \norm{A} \norm{B}\)</span> holds.</p>
</div>
<p>In other words, induced matrix norms are submultiplicative.</p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{AB} &amp;= \max_{\norm{x}=1} \norm{ABx} &amp; \{\text{Definition}\}\\
&amp;= \max_{\norm{x}=1} \norm{A(Bx)} &amp; \{\text{Associativity}\}\\
&amp;\leq \max_{\norm{x}=1} \norm{A} \norm{Bx} &amp; \{\text{Subordinate property}\}\\
&amp;\leq \max_{\norm{x}=1} \norm{A} \norm{B} \norm{x} &amp; \{\text{Subordinate property}\}\\
&amp;= \norm{A} \norm{B} &amp; \{\norm{x}=1\}
\end{align}
\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>
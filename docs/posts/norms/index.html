<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-07-25">

<title>Norms – quantdev.blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-d129a44951930463e8a313df5966fbea.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-7c43ab0dc1b5750ae198bce08b29e1b3.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Norms</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Numerical Methods</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 25, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#inner-product" id="toc-inner-product" class="nav-link active" data-scroll-target="#inner-product">Inner product</a></li>
  <li><a href="#norms" id="toc-norms" class="nav-link" data-scroll-target="#norms">Norms</a>
  <ul class="collapse">
  <li><a href="#the-vector-2-norm" id="toc-the-vector-2-norm" class="nav-link" data-scroll-target="#the-vector-2-norm">The vector <span class="math inline">\(2-\)</span>norm</a></li>
  </ul></li>
  <li><a href="#cauchy-schwarz-inequality" id="toc-cauchy-schwarz-inequality" class="nav-link" data-scroll-target="#cauchy-schwarz-inequality">Cauchy-Schwarz Inequality</a></li>
  <li><a href="#euclidean-norm" id="toc-euclidean-norm" class="nav-link" data-scroll-target="#euclidean-norm">Euclidean Norm</a></li>
  <li><a href="#the-vector-1-norm" id="toc-the-vector-1-norm" class="nav-link" data-scroll-target="#the-vector-1-norm">The vector <span class="math inline">\(1-\)</span>norm</a></li>
  <li><a href="#jensens-inequality" id="toc-jensens-inequality" class="nav-link" data-scroll-target="#jensens-inequality">Jensen’s inequality</a>
  <ul class="collapse">
  <li><a href="#convex-functions-and-combinations" id="toc-convex-functions-and-combinations" class="nav-link" data-scroll-target="#convex-functions-and-combinations">Convex functions and combinations</a></li>
  <li><a href="#proving-jensens-inequality" id="toc-proving-jensens-inequality" class="nav-link" data-scroll-target="#proving-jensens-inequality">Proving Jensen’s inequality</a></li>
  </ul></li>
  <li><a href="#youngs-inequality" id="toc-youngs-inequality" class="nav-link" data-scroll-target="#youngs-inequality">Young’s Inequality</a></li>
  <li><a href="#holders-inequality" id="toc-holders-inequality" class="nav-link" data-scroll-target="#holders-inequality">Holder’s inequality</a></li>
  <li><a href="#the-vector-p-norm" id="toc-the-vector-p-norm" class="nav-link" data-scroll-target="#the-vector-p-norm">The vector <span class="math inline">\(p\)</span>-norm</a></li>
  <li><a href="#the-vector-infty-norm" id="toc-the-vector-infty-norm" class="nav-link" data-scroll-target="#the-vector-infty-norm">The vector <span class="math inline">\(\infty\)</span>-norm</a></li>
  <li><a href="#equivalence-of-vector-norms" id="toc-equivalence-of-vector-norms" class="nav-link" data-scroll-target="#equivalence-of-vector-norms">Equivalence of vector norms</a>
  <ul class="collapse">
  <li><a href="#deriving-the-constants-c_1infty-c_infty1" id="toc-deriving-the-constants-c_1infty-c_infty1" class="nav-link" data-scroll-target="#deriving-the-constants-c_1infty-c_infty1">Deriving the constants <span class="math inline">\(C_{1,\infty}\)</span>, <span class="math inline">\(C_{\infty,1}\)</span></a></li>
  <li><a href="#deriving-the-constants-c_12-c_21" id="toc-deriving-the-constants-c_12-c_21" class="nav-link" data-scroll-target="#deriving-the-constants-c_12-c_21">Deriving the constants <span class="math inline">\(C_{1,2}\)</span>, <span class="math inline">\(C_{2,1}\)</span></a></li>
  <li><a href="#deriving-the-constants-c_2infty-and-c_infty2" id="toc-deriving-the-constants-c_2infty-and-c_infty2" class="nav-link" data-scroll-target="#deriving-the-constants-c_2infty-and-c_infty2">Deriving the constants <span class="math inline">\(C_{2,\infty}\)</span> and <span class="math inline">\(C_{\infty,2}\)</span></a></li>
  </ul></li>
  <li><a href="#matrix-norms" id="toc-matrix-norms" class="nav-link" data-scroll-target="#matrix-norms">Matrix Norms</a>
  <ul class="collapse">
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions">Definitions</a></li>
  <li><a href="#computing-the-matrix-1-norm-and-infty-norm" id="toc-computing-the-matrix-1-norm-and-infty-norm" class="nav-link" data-scroll-target="#computing-the-matrix-1-norm-and-infty-norm">Computing the matrix <span class="math inline">\(1\)</span>-norm and <span class="math inline">\(\infty\)</span>-norm</a></li>
  <li><a href="#equivalence-of-matrix-norms" id="toc-equivalence-of-matrix-norms" class="nav-link" data-scroll-target="#equivalence-of-matrix-norms">Equivalence of matrix norms</a></li>
  <li><a href="#sub-multiplicative-norms" id="toc-sub-multiplicative-norms" class="nav-link" data-scroll-target="#sub-multiplicative-norms">Sub-multiplicative norms</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="inner-product" class="level2">
<h2 class="anchored" data-anchor-id="inner-product">Inner product</h2>
<p>Consider geometric vectors <span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbf{R}^2\)</span>. The scalar product(dot-product) of these two vectors is defined by:</p>
<p><span class="math display">\[
\mathbf{x} \cdot \mathbf{y} = x_1 y_1 + x_2 y_2
\]</span></p>
<p>An inner-product is a mathematical generalization of the dot-product.</p>
<div class="hidden">
<p><span class="math display">\[
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normp}[2]{\left\lVert\mathbf{#1}\right\rVert_{#2}}
\newcommand\inner[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\bf}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\RR}[1]{\mathbf{R}^2}
\newcommand{\RRR}[1]{\mathbf{R}^3}
\newcommand{\C}{\mathbf{C}}
\newcommand{\CC}[1]{\mathbf{C}^2}
\newcommand{\CCC}[1]{\mathbf{C}^3}
\]</span></p>
</div>
<div id="def-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Inner product)</strong></span> Let <span class="math inline">\(V\)</span> be a vector space and <span class="math inline">\(F\)</span> be a scalar field, which is either <span class="math inline">\(\bf{R}\)</span> or <span class="math inline">\(\bf{C}\)</span>. Let <span class="math inline">\(\inner{\cdot}{\cdot}\)</span> be a map from <span class="math inline">\(V\times V \to F\)</span>. Then, <span class="math inline">\(\inner{\cdot}{\cdot}\)</span> is an inner product if for all <span class="math inline">\(\bf{u},\bf{v}, \bf{w} \in V\)</span>, it satisfies:</p>
<section id="positive-semi-definite" class="level4">
<h4 class="anchored" data-anchor-id="positive-semi-definite">Positive semi-definite</h4>
<p><span class="math display">\[
\inner{\bf{v}}{\bf{v}} \geq 0 \quad \text { and } \quad  \inner{\bf{v}}{\bf{v}} = 0 \Longleftrightarrow \bf{v} = \bf{0}
\]</span></p>
</section>
<section id="additivity-in-the-first-slot" class="level4">
<h4 class="anchored" data-anchor-id="additivity-in-the-first-slot">Additivity in the first slot</h4>
<p><span class="math display">\[
\inner{\bf{u} + \bf{v}}{\bf{w}} = \inner{\bf{u}}{\bf{w}} + \inner{\bf{v}}{\bf{w}}
\]</span></p>
</section>
<section id="homogeneity" class="level4">
<h4 class="anchored" data-anchor-id="homogeneity">Homogeneity</h4>
<p><span class="math display">\[
\begin{align*}
\inner{\alpha \bf{v}}{\bf{w}} &amp;= \overline{\alpha} \inner{\bf{v}}{\bf{w}}\\
\inner{\bf{v}}{\alpha \bf{w}} &amp;= \alpha \inner{\bf{v}}{\bf{w}}
\end{align*}
\]</span></p>
</section>
<section id="conjugate-symmetry" class="level4">
<h4 class="anchored" data-anchor-id="conjugate-symmetry">Conjugate symmetry</h4>
<p><span class="math display">\[
\inner{\bf{v}}{\bf{w}} = \overline{\inner{\bf{w}}{\bf{v}}}
\]</span></p>
</section>
</div>
<p>The most important example of inner-product is the Euclidean inner product on <span class="math inline">\(\C^n\)</span>. Let <span class="math inline">\(\bf{w},\bf{z}\)</span> be (column) vectors in <span class="math inline">\(\C^n\)</span>.</p>
<p><span class="math display">\[
\inner{\bf{w}}{\bf{z}} = (\bf{w}^H \bf{z}) =  \overline{w_1}z_1 + \overline{w_2}z_2 + \ldots + \overline{w_n} z_n
\]</span></p>
<p>Firstly,</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{v} + \bf{w}}{\bf{z}} &amp;= (\bf{v} + \bf{w})^H \bf{z} &amp; \{ \text{ Definition }\}\\
&amp;= (\bf{v}^H + \bf{w}^H)\bf{z} &amp; \{ \overline{z_1 + z_2} = \overline{z_1} + \overline{z_2}; z_1,z_2\in \C \}\\
&amp;= \bf{v}^H \bf{z} + \bf{w}^H \bf{z}\\
&amp;= \inner{\bf{v}}{\bf{z}} + \inner{\bf{w}}{\bf{z}}
\end{align*}
\]</span></p>
<p>So, it is additive in the first slot.</p>
<p>Next, let <span class="math inline">\(\alpha \in \C\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
\inner{\alpha\bf{u}}{\bf{v}} &amp;= (\alpha \bf{u})^H \bf{v}  &amp; \{ \text{ Definition }\}\\
&amp;= \overline{\alpha} \bf{u}^H \bf{v} = \overline{\alpha} \inner{\bf{u}}{\bf{v}}
\end{align*}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{u}}{\alpha\bf{v}} &amp;= (\bf{u})^H \bf{ \alpha v}  &amp; \{ \text{ Definition }\}\\
&amp;= \alpha \bf{u}^H \bf{v} = \alpha \inner{\bf{u}}{\bf{v}}
\end{align*}
\]</span></p>
<p>It is homogenous.</p>
<p>Finally,</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{v}}{\bf{w}} &amp;= \sum_{i=1}^n \overline{v_i}w_i\\
&amp;= \sum_{i=1}^n \overline{v_i \overline{w_i}}\\
&amp;= \overline{\left(\sum_{i=1}^n \overline{w_i} v_i\right)}\\
&amp;= \overline{\inner{\bf{w}}{\bf{v}}}
\end{align*}
\]</span></p>
</section>
<section id="norms" class="level2">
<h2 class="anchored" data-anchor-id="norms">Norms</h2>
<p>Very often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length.</p>
<div id="def-vector-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Vector norm)</strong></span> Let <span class="math inline">\(\nu:V \to \mathbf{R}\)</span>. Then, <span class="math inline">\(\nu\)</span> is a (vector) norm if for all <span class="math inline">\(\mathbf{x},\mathbf{y}\in V\)</span> and for all <span class="math inline">\(\alpha \in \mathbf{C}\)</span>, <span class="math inline">\(\nu(\cdot)\)</span> satisfies:</p>
<section id="positive-semi-definiteness" class="level4">
<h4 class="anchored" data-anchor-id="positive-semi-definiteness">Positive Semi-Definiteness</h4>
<p><span class="math display">\[\nu(\mathbf{x}) \geq 0, \quad \forall \bf{x}\in V\]</span></p>
<p>and</p>
<p><span class="math display">\[\nu(\mathbf{x})=0 \Longleftrightarrow \mathbf{x}=\mathbf{0}\]</span></p>
</section>
<section id="homogeneity-1" class="level4">
<h4 class="anchored" data-anchor-id="homogeneity-1">Homogeneity</h4>
<p><span class="math display">\[\nu(\alpha \mathbf{x}) = |\alpha|\nu(\mathbf{x})\]</span></p>
</section>
<section id="triangle-inequality" class="level4">
<h4 class="anchored" data-anchor-id="triangle-inequality">Triangle inequality</h4>
<p><span class="math display">\[\nu(\mathbf{x} + \mathbf{y}) \leq \nu(\mathbf{x}) + \nu(\mathbf{y})\]</span></p>
</section>
</div>
<section id="the-vector-2-norm" class="level3">
<h3 class="anchored" data-anchor-id="the-vector-2-norm">The vector <span class="math inline">\(2-\)</span>norm</h3>
<p>The length of a vector is most commonly measured by the <em>square root of the sum of the squares of the components of the vector</em>, also known as the <em>euclidean norm</em>.</p>
<div id="def-euclidean-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Vector <span class="math inline">\(2-\)</span>norm)</strong></span> The vector <span class="math inline">\(2-\)</span> norm, <span class="math inline">\(||\cdot||:\mathbf{C}^n \to \mathbf{R}\)</span> is defined for <span class="math inline">\(\mathbf{x}\in\mathbf{C}^n\)</span> by:</p>
<p><span class="math display">\[
\norm{\bf{x}}_2 = \sqrt{|\chi_1|^2 + |\chi_2|^2 + |\chi_n|^2} = \sqrt{\sum_{i=1}^n |\chi_i^2|}
\]</span></p>
<p>Equivalently, it can be defined as:</p>
<p><span class="math display">\[
\norm{\bf{x}}_2 = \sqrt{\inner{\bf{x}}{\bf{x}}} =  (\bf{x}^H \bf{x})^{1/2} = \sqrt{\overline{\chi_1}\chi_1 +\overline{\chi_2}\chi_2+\ldots+\overline{\chi_n}\chi_n}
\]</span></p>
</div>
<p>To prove that the vector <span class="math inline">\(2-\)</span>norm is indeed a valid norm(just calling it a norm, doesn’t mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if <span class="math inline">\(\bf{x},\bf{y} \in \R^n\)</span>, then <span class="math inline">\(|\bf{x}^T \bf{y}|\leq \norm{\bf{x}}_2\cdot\norm{\bf{y}}_2\)</span>.</p>
<p>Before we rigorously prove this result, let’s review the idea of orthogonality.</p>
<div id="def-orthogonal-vectors" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Orthogonal vectors)</strong></span> Two vectors <span class="math inline">\(\bf{u},\bf{v} \in V\)</span> are said to be orthogonal to each other if and only if their inner product equals zero:</p>
<p><span class="math display">\[
\inner{\bf{u}}{\bf{v}} = 0
\]</span></p>
</div>
<div id="thm-pythagorean-theorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Pythagorean Theorem)</strong></span> If <span class="math inline">\(\bf{u}\)</span> and <span class="math inline">\(\bf{v}\)</span> are orthogonal vectors, then</p>
<p><span class="math display">\[
\inner{\bf{u} + \bf{v}}{\bf{u} + \bf{v}} = \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{u} + \bf{v}}{\bf{u}+\bf{v}} &amp;= \inner{\bf{u}}{\bf{u} + \bf{v}} + \inner{\bf{v}}{\bf{u} + \bf{v}} &amp; \{ \text{ Additivity in the first slot }\}\\
&amp;= \overline{\inner{\bf{u} + \bf{v}}{\bf{u}}} + \overline{\inner{\bf{u} + \bf{v}}{\bf{v}}} &amp; \{ \text{ Conjugate symmetry }\}\\
&amp;= \overline{\inner{\bf{u}}{\bf{u}}} + \overline{\inner{\bf{v}}{\bf{u}}} + \overline{\inner{\bf{u}}{\bf{v}}} + \overline{\inner{\bf{v}}{\bf{v}}} \\
&amp;= \inner{\bf{u}}{\bf{u}} + \inner{\bf{u}}{\bf{v}} + \inner{\bf{v}}{\bf{u}} + \inner{\bf{v}}{\bf{v}} \\
&amp;= \inner{\bf{u}}{\bf{u}} + 0 + 0 + \inner{\bf{v}}{\bf{v}} &amp; \{ \bf{u} \perp \bf{v}\}\\
&amp;= \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>In the special case that <span class="math inline">\(V=\C^n\)</span> or <span class="math inline">\(V=\R^n\)</span>, the pythagorean theorem reduces to:</p>
<p><span class="math display">\[
\norm{\bf{u} + \bf{v}}_2^2 = \norm{\bf{u}}_2^2 + \norm{\bf{v}}_2^2
\]</span></p>
</section>
</section>
<section id="cauchy-schwarz-inequality" class="level2">
<h2 class="anchored" data-anchor-id="cauchy-schwarz-inequality">Cauchy-Schwarz Inequality</h2>
<p>Suppose <span class="math inline">\(\bf{u},\bf{v}\in V\)</span>. We would like to write <span class="math inline">\(\bf{u}\)</span> as a scalar multiple of <span class="math inline">\(\bf{v}\)</span> plus a vector <span class="math inline">\(\bf{w}\)</span> orthogonal to <span class="math inline">\(\bf{v}\)</span>, as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of <span class="math inline">\(\bf{u}\)</span>.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>load_ext itikz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows,arrows.meta <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb2-2"><a href="#cb2-2"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">2.0</span>]</span>
<span id="cb2-3"><a href="#cb2-3"></a>    \draw [<span class="op">-</span>{Stealth[length<span class="op">=</span><span class="dv">5</span><span class="er">mm</span>]}](<span class="fl">0.0</span>,<span class="fl">0.0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>    \draw [<span class="op">-</span>{Stealth[length<span class="op">=</span><span class="dv">5</span><span class="er">mm</span>]}] (<span class="fl">0.0</span>,<span class="fl">0.0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>    \node []  at (<span class="fl">3.5</span>,<span class="fl">2.25</span>) {\large $\mathbf{u}$}<span class="op">;</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>    \draw [dashed] (<span class="dv">7</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>    \node [circle,fill,minimum size <span class="op">=</span> <span class="fl">0.5</span><span class="er">mm</span>] at (<span class="dv">5</span>,<span class="dv">0</span>) {}<span class="op">;</span></span>
<span id="cb2-8"><a href="#cb2-8"></a>    \node []  at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">0.40</span>) {\large $\mathbf{v}$}<span class="op">;</span></span>
<span id="cb2-9"><a href="#cb2-9"></a>    \node []  at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">0.40</span>) {\large $\alpha\mathbf{v}$}<span class="op">;</span></span>
<span id="cb2-10"><a href="#cb2-10"></a>    \node []  at (<span class="fl">7.4</span>,<span class="fl">2.0</span>) {\large $\mathbf{w}$}<span class="op">;</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-3-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>To discover how to write <span class="math inline">\(\bf{u}\)</span> as a scalar multiple of <span class="math inline">\(\bf{v}\)</span> plus a vector orthogonal to <span class="math inline">\(\bf{v}\)</span>, let <span class="math inline">\(\alpha\)</span> denote a scalar. Then,</p>
<p><span class="math display">\[
\bf{u} = \alpha \bf{v} + (\bf{u} - \alpha \bf{v})
\]</span></p>
<p>Thus, we need to choose <span class="math inline">\(\alpha\)</span> so that <span class="math inline">\(\bf{v}\)</span> and <span class="math inline">\(\bf{w} = \bf{u} - \alpha{v}\)</span> are mutually orthogonal. Thus, we must set:</p>
<p><span class="math display">\[
\inner{\bf{u} - \alpha\bf{v}}{\bf{v}} = \inner{\bf{u}}{\bf{v}} - \alpha \inner{\bf{v}}{\bf{v}} = 0
\]</span></p>
<p>The equation above shows that we choose <span class="math inline">\(\alpha\)</span> to be <span class="math inline">\(\inner{\bf{u}}{\bf{v}}/\inner{\bf{v}}{\bf{v}}\)</span> (assume that <span class="math inline">\(\bf{v} \neq \bf{0}\)</span> to avoid division by 0). Making this choice of <span class="math inline">\(\alpha\)</span>, we can write:</p>
<p><span id="eq-orthogonal-decomposition"><span class="math display">\[
\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v} + \left(\bf{u} - \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v}\right)
\tag{1}\]</span></span></p>
<p>The equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics</p>
<div id="thm-cauchy-schwarz-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (Cauchy-Schwarz Inequality)</strong></span> Let <span class="math inline">\(\bf{x},\bf{y}\in V\)</span>. Then</p>
<p><span id="eq-cauchy-schwarz-inequality"><span class="math display">\[
|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}
\tag{2}\]</span></span></p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(\bf{u},\bf{v} \in V\)</span>. If <span class="math inline">\(\bf{v} = \bf{0}\)</span>, then both sides of <a href="#eq-cauchy-schwarz-inequality" class="quarto-xref">Equation&nbsp;2</a> equal <span class="math inline">\(0\)</span> and the inequality holds. Thus, we assume that <span class="math inline">\(\bf{v}\neq \bf{0}\)</span>. Consider the orthogonal decomposition:</p>
<p><span class="math display">\[
\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v} + \bf{w}
\]</span></p>
<p>where <span class="math inline">\(\bf{w}\)</span> is orthogonal to <span class="math inline">\(\bf{v}\)</span> (<span class="math inline">\(\bf{w}\)</span> is taken to be the second term on the right hand side of <a href="#eq-orthogonal-decomposition" class="quarto-xref">Equation&nbsp;1</a>). By the Pythagorean theorem:</p>
<p><span class="math display">\[
\begin{align*}
\inner{\bf{u}}{\bf{u}} &amp;= \inner{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}+\inner{\bf{w}}{\bf{w}}\\
&amp;= \overline{\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)}\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)\inner{\bf{v}}{\bf{v}} + \inner{\bf{w}}{\bf{w}}\\
&amp;= \frac{\overline{\inner{\bf{u}}{\bf{v}}}\inner{\bf{u}}{\bf{v}}}{\overline{\inner{\bf{v}}{\bf{v}}}} + \inner{\bf{w}}{\bf{w}}\\
&amp;= \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}} + \inner{\bf{w}}{\bf{w}}
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(\inner{\bf{w}}{\bf{w}} \geq 0\)</span>, it follows that:</p>
<p><span class="math display">\[
\inner{\bf{u}}{\bf{u}} \geq \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}}
\]</span></p>
<p>Consequently, we have:</p>
<p><span class="math display">\[
|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>In the special case, that <span class="math inline">\(V=\R^n\)</span> or <span class="math inline">\(V=\C^n\)</span>, we have:</p>
<p><span class="math display">\[
|\inner{\bf{u}}{\bf{v}}| \leq \norm{\bf{u}}_2 \norm{\bf{v}}_2
\]</span></p>
</section>
<section id="euclidean-norm" class="level2">
<h2 class="anchored" data-anchor-id="euclidean-norm">Euclidean Norm</h2>
<div id="prp-euclidean-norm-is-well-defined" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 (Well-definedness of the Euclidean norm)</strong></span> Let <span class="math inline">\(\norm{\cdot}:\mathbf{C}^n \to \mathbf{C}\)</span> be the euclidean norm. Our claim is, it is well-defined.</p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(\bf{z} = (z_1,z_2,\ldots,z_n) \in \C^n\)</span>. Clearly, it is positive semi-definite.</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{z}}_2 = \bf{z}^H \bf{z} &amp;= \overline{z_1} z_1 +\overline{z_2}z_2 + \ldots + \overline{z_n} z_n\\
&amp;= \sum_{i=1}^n |z_i|^2 \geq 0
\end{align*}
\]</span></p>
<p>It is also homogenous. Let <span class="math inline">\(\alpha \in \C\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
\norm{\alpha \bf{z}}_2 &amp;= \norm{(\alpha z_1, \alpha z_2,\ldots,\alpha z_n)}_2\\
&amp;=\sqrt{\sum_{i=1}^n |\alpha z_i|^2}\\
&amp;=|\alpha|\sqrt{\sum_{i=1}^n |z_i|^2} \\
&amp;= |\alpha|\norm{\bf{z}}_2
\end{align*}
\]</span></p>
<p>Let’s verify, if the triangle inequality is satisfied. Let <span class="math inline">\(\bf{x}, \bf{y}\in\C^n\)</span> be arbitrary vectors.</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x} + \bf{y}}_2^2 &amp;= |(\bf{x} + \bf{y})^H(\bf{x} + \bf{y})|\\
&amp;= |(\bf{x}^H + \bf{y}^H)(\bf{x} + \bf{y})|\\
&amp;= |\bf{x}^H \bf{x} + \bf{y}^H \bf{y} + \bf{y}^H \bf{x} + \bf{x}^H \bf{y}|\\
&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + |\inner{\bf{y}}{\bf{x}}| + |\inner{\bf{x}}{\bf{y}}|\\
&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + \norm{\bf{y}}_2 \norm{\bf{x}}_2  + \norm{\bf{x}}_2 \norm{\bf{y}}_2 &amp; \{ \text{ Cauchy-Schwarz } \}\\
&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 +  2\norm{\bf{x}}_2 \norm{\bf{y}}_2\\
&amp;= (\norm{\bf{x}}_2 + \norm{\bf{y}}_2)^2
\end{align*}
\]</span></p>
<p>Consequently, <span class="math inline">\(\norm{\bf{x} + \bf{y}}_2 \leq \norm{\bf{x}}_2 + \norm{\bf{y}}_2\)</span>.</p>
</section>
<section id="the-vector-1-norm" class="level2">
<h2 class="anchored" data-anchor-id="the-vector-1-norm">The vector <span class="math inline">\(1-\)</span>norm</h2>
<div id="def-the-vector-1-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (The vector <span class="math inline">\(1-\)</span>norm)</strong></span> The vector <span class="math inline">\(1\)</span>-norm, <span class="math inline">\(\norm{\cdot}_1 : \C^n \to \R\)</span> is defined for all <span class="math inline">\(\bf{x}\in\C^n\)</span> by:</p>
<p><span class="math display">\[
\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| =\sum_{i=1}^n |\chi_i|
\]</span></p>
</div>
<div id="thm-1-norm-is-a-norm" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3</strong></span> The vector <span class="math inline">\(1\)</span>-norm is well-defined.</p>
</div>
<p><em>Proof.</em></p>
<p><em>Positive semi-definitess.</em></p>
<p>The absolute value of complex numbers is non-negative.</p>
<p><span class="math display">\[
\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| \geq |\chi_i| \geq 0
\]</span></p>
<p><em>Homogeneity.</em></p>
<p><span class="math display">\[
\norm{\alpha\bf{x}}_1 = \sum_{i=1}^{n}|\alpha \chi_i| = |\alpha| \sum_{i=1}^{n}|\chi_i| = |\alpha| \norm{\bf{x}}_1
\]</span></p>
<p><em>Triangle Inequality.</em></p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x} + \bf{y}} &amp;= \norm{(\chi_1 + \psi_1, \ldots,\chi_n + \psi_n)}_1\\
&amp;= \sum_{i=1}^n |\chi_i + \psi_i|\\
&amp;\leq \sum_{i=1}^n |\chi_i| + |\psi_i| &amp; \{ \text{ Triangle inequality for complex numbers }\}\\
&amp;= \sum_{i=1}^n |\chi_i| + \sum_{i=1}^{n} |\psi_i| &amp; \{ \text{ Commutativity }\}\\
&amp;= \norm{\bf{x}}_1 + \norm{\bf{y}}_1
\end{align*}
\]</span></p>
<p>Hence, the three axioms are satisfied. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="jensens-inequality" class="level2">
<h2 class="anchored" data-anchor-id="jensens-inequality">Jensen’s inequality</h2>
<section id="convex-functions-and-combinations" class="level3">
<h3 class="anchored" data-anchor-id="convex-functions-and-combinations">Convex functions and combinations</h3>
<p>A function <span class="math inline">\(f\)</span> is said to be <em>convex</em> on over an interval <span class="math inline">\(I\)</span>, if for all <span class="math inline">\(x_1,x_2 \in I\)</span>, and every <span class="math inline">\(p \in [0,1]\)</span>, we have:</p>
<p><span class="math display">\[
f(px_1 + (1-p)x_2) \leq pf(x_1) + (1-p)f(x_2)
\]</span></p>
<p>In other words, all chords(secants) joining any two points on <span class="math inline">\(f\)</span>, lie above the graph of <span class="math inline">\(f\)</span>. Note that, if <span class="math inline">\(0 \leq p \leq 1\)</span>, then <span class="math inline">\(\min(x_1,x_2) \leq px_1 + (1-p)x_2 \leq \max(x_1,x_2)\)</span>. More generally, for non-negative real numbers <span class="math inline">\(p_1, p_2, \ldots, p_n\)</span> summing to one, that is, satisfying <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>, and for any points <span class="math inline">\(x_1,\ldots,x_n \in I\)</span>, the point <span class="math inline">\(\sum_{i=1}^n \lambda_i x_i\)</span> is called a <em>convex combination</em> of <span class="math inline">\(x_1,\ldots,x_n\)</span>. Since:</p>
<p><span class="math display">\[ \min(x_1,\ldots,x_n) \leq \sum_{i=1}^n p_i x_i \leq \max(x_1,\ldots,x_n)\]</span></p>
<p>every convex combination of any finite number of points in <span class="math inline">\(I\)</span> is again a point of <span class="math inline">\(I\)</span>.</p>
<p>Intuitively, <span class="math inline">\(\sum_{i=1}^{n}p_i x_i\)</span> simply represents the center of mass of the points <span class="math inline">\(x_1,\ldots,x_n\)</span> with weights <span class="math inline">\(p_1,\ldots,p_n\)</span>.</p>
</section>
<section id="proving-jensens-inequality" class="level3">
<h3 class="anchored" data-anchor-id="proving-jensens-inequality">Proving Jensen’s inequality</h3>
<p>Jensen’s inequality named after the Danish engineer <a href="https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)">Johan Jensen</a> (1859-1925) can be stated as follows:</p>
<div id="thm-jensens-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4</strong></span> Let <span class="math inline">\(n \in \bf{Z}_+\)</span> be a positive integer and let <span class="math inline">\(f:I \to \R\)</span> be a convex function over the interval <span class="math inline">\(I \subseteq \R\)</span>. For any (not necessarily distinct) points <span class="math inline">\(x_1,\ldots,x_n \in I\)</span>, and non-negative real numbers <span class="math inline">\(p_1,\ldots,p_n \in \R\)</span> summing to one,</p>
<p><span class="math display">\[
f(\sum_{i=1}^n p_i x_i) \leq \sum_{i=1}^n p_i f(x_i)
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We proceed by induction. Since <span class="math inline">\(f\)</span> is convex, by definition, <span class="math inline">\(\forall x_1,x_2 \in I\)</span>, and any <span class="math inline">\(p_1,p_2\in \R\)</span>, such that <span class="math inline">\(p_1 + p_2 = 1\)</span>, we have <span class="math inline">\(f(p_1 x_1 + p_2 x_2) \leq p_1 f(x_1) + p_2 f(x_2)\)</span>. So, the claim is true for <span class="math inline">\(n=2\)</span>.</p>
<p><em>Inductive hypothesis</em>. Assume that <span class="math inline">\(\forall x_1,\ldots,x_{k} \in I\)</span> and any <span class="math inline">\(p_1,\ldots,p_k \in \R\)</span>, such that <span class="math inline">\(\sum_{i=1}^k p_i = 1\)</span>, we have <span class="math inline">\(f(\sum_{i=1}^k p_i x_i) \leq \sum_{i=1}^k p_i f(x_i)\)</span>.</p>
<p><em>Claim</em>. The Jensen’s inequality holds for <span class="math inline">\(k+1\)</span> points in <span class="math inline">\(I\)</span>.</p>
<p><em>Proof</em>.</p>
<p>Let <span class="math inline">\(x_1,\ldots,x_k, x_{k+1}\)</span> be arbitrary points in <span class="math inline">\(I\)</span> and consider any convex combination of these points <span class="math inline">\(\sum_{i=1}^{k+1}p_i x_i\)</span>, <span class="math inline">\(p_i \in [0,1], i \in \{1,2,3,\ldots,k+1\}, \sum_{i=1}^{k+1}p_i = 1\)</span>.</p>
<p>Define:</p>
<p><span class="math display">\[
z := \frac{p_1 x_1 + p_2 x_2 + \ldots + p_k x_k}{\sum_{i=1}^k p_i}
\]</span></p>
<p>Since, <span class="math inline">\(z\)</span> is a convex combination of <span class="math inline">\(\{x_1,\ldots,x_k\}\)</span>, <span class="math inline">\(z \in I\)</span>. Moreover, by the inductive hypothesis, since <span class="math inline">\(f\)</span> is convex,</p>
<p><span class="math display">\[
\begin{align*}
f(z) &amp;= f\left(\frac{p_1 x_1 + p_2 x_2 + \ldots + p_k x_k}{\sum_{i=1}^k p_i}\right)\\
&amp;\leq \frac{p_1}{\sum_{i=1}^k p_i}f(x_1) + \frac{p_2}{\sum_{i=1}^k p_i}f(x_2) + \ldots + \frac{p_k}{\sum_{i=1}^k p_i}f(x_k) \\
&amp;= \frac{p_1}{1-p_{k+1}}f(x_1) + \frac{p_2}{1-p_{k+1}}f(x_2) + \ldots + \frac{p_k}{1-p_{k+1}}f(x_k) \\
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(0 \leq 1 - p_{k+1} \leq 1\)</span>, we deduce that:</p>
<p><span class="math display">\[
(1 - p_{k+1})f(z) \leq p_1 f(x_1) + \ldots + p_k f(x_k)
\]</span></p>
<p>We have: <span class="math display">\[
\begin{align*}
f(p_1 x_1 + \ldots + p_k x_k + p_{k+1} x_{k+1}) &amp;= f((1-p_{k+1})z + p_{k+1}x_{k+1})\\
&amp;\leq (1-p_{k+1})f(z) + p_{k+1}f(x_{k+1}) &amp; \{ \text{ Jensen's inequality for }n=2\}\\
&amp;\leq p_1 f(x_1) + \ldots + p_k f(x_k) + p_{k+1}f(x_{k+1}) &amp; \{ \text{ Deduction from the inductive hypothesis }\}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
</section>
<section id="youngs-inequality" class="level2">
<h2 class="anchored" data-anchor-id="youngs-inequality">Young’s Inequality</h2>
<p>Young’s inequality is named after the English mathematician <a href="https://en.wikipedia.org/wiki/William_Henry_Young">William Henry Young</a> and can be stated as follows:</p>
<div id="thm-youngs-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (Young’s inequality)</strong></span> For any non-negative real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and any positive real numbers <span class="math inline">\(p,q\)</span> satisfying <span class="math inline">\(\frac{1}{p} + \frac{1}{q}=1\)</span>, we have:</p>
<p><span class="math display">\[
ab \leq \frac{a^p}{p} + \frac{b^q}{q}
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(f(x) = \log x\)</span>. Since <span class="math inline">\(f\)</span> is concave, we can reverse the Jensen’s inequality. Consequently:</p>
<p><span class="math display">\[
\begin{align*}
\log(\frac{a^p}{p} + \frac{b^q}{q}) &amp;\geq \frac{1}{p}\log a^p + \frac{1}{q}\log b^q\\
&amp;= \frac{1}{p}\cdot p \log a + \frac{1}{q}\cdot q \log b\\
&amp;= \log (ab)
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(\log x\)</span> is monotonic increasing,</p>
<p><span class="math display">\[
\frac{a^p}{p} + \frac{b^q}{q} \geq ab
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="holders-inequality" class="level2">
<h2 class="anchored" data-anchor-id="holders-inequality">Holder’s inequality</h2>
<p>We can use Young’s inequality to prove the Holder’s inequality, named after the German mathematician <a href="https://en.wikipedia.org/wiki/Otto_H%C3%B6lder">Otto Ludwig Holder</a> (1859-1937).</p>
<div id="thm-holders-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (Holder’s inequality)</strong></span> For any pair of vectors <span class="math inline">\(\bf{x},\bf{y}\in \C^n\)</span>, and for any positive real numbers satisfying <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, we have <span class="math inline">\(\frac{1}{p} + \frac{1}{q} = 1\)</span> we have:</p>
<p><span class="math display">\[
\sum_{i=1}^{n}|x_i y_i| \leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p} \left(\sum_{i=1}^n |y_i|^q\right)^{1/q}
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>Apply Young’s inequality to <span class="math inline">\(a = \frac{|x_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}}\)</span> and <span class="math inline">\(b = \frac{|y_i|}{\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}}\)</span>. We get:</p>
<p><span class="math display">\[
\begin{align*}
\frac{|x_i||y_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}} &amp;\leq \frac{1}{p} \frac{|x_i|^p}{\sum_{i=1}^n |x_i|^p} + \frac{1}{q}\frac{|y_i|^q}{\sum_{i=1}^n |y_i|^q}
\end{align*}
\]</span></p>
<p>Summing on both sides, we get:</p>
<p><span class="math display">\[
\begin{align*}
\frac{\sum_{i=1}^n|x_i y_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}} &amp;\leq \frac{1}{p} \frac{\sum_{i=1}^n |x_i|^p}{\sum_{i=1}^n |x_i|^p} + \frac{1}{q}\frac{\sum_{i=1}^n|y_i|^q}{\sum_{i=1}^n |y_i|^q}\\
&amp;= \frac{1}{p} + \frac{1}{q}\\
&amp;= 1\\
\sum_{i=1}^n |x_i y_i| &amp;\leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="the-vector-p-norm" class="level2">
<h2 class="anchored" data-anchor-id="the-vector-p-norm">The vector <span class="math inline">\(p\)</span>-norm</h2>
<p>The vector <span class="math inline">\(1\)</span>-norm and <span class="math inline">\(2\)</span>-norm are special cases of the <span class="math inline">\(p\)</span>-norm.</p>
<div id="def-vector-p-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 (<span class="math inline">\(p\)</span>-norm)</strong></span> Given <span class="math inline">\(p \geq 1\)</span>, the vector <span class="math inline">\(p\)</span>-norm <span class="math inline">\(\norm{\cdot}_p : \C^n \to \R\)</span> is defined by :</p>
<p><span class="math display">\[
\norm{\bf{x}}_p = \left(\sum_{i=1}^n |\chi_i|^p\right)^{1/p}
\]</span></p>
</div>
<div id="thm-p-norm-is-a-norm" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7</strong></span> The vector <span class="math inline">\(p\)</span>-norm is a well-defined norm.</p>
</div>
<p><em>Proof.</em></p>
<p><em>Positive semi-definite</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x}}_p &amp;= \left(\sum_{i=1}^n |\chi_i|^p \right)^{1/p}\\
&amp;\geq \left(|\chi_i|^p \right)^{1/p}\\
&amp;= |\chi_i| \geq 0
\end{align*}
\]</span></p>
<p><em>Homogeneity</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\alpha \bf{x}}_p &amp;= \left(\sum_{i=1}^n |\alpha \chi_i|^p \right)^{1/p}\\
&amp;= \left(\sum_{i=1}^n |\alpha|^p |\chi_i|^p \right)^{1/p}\\
&amp;= |\alpha|\left(\sum_{i=1}^n |\chi_i|^p \right)^{1/p} &amp;= |\alpha|\norm{\bf{x}}_p
\end{align*}
\]</span></p>
<p><em>Triangle Inequality</em></p>
<p>Define <span class="math inline">\(\frac{1}{q} := 1 - \frac{1}{p}\)</span>. <span class="math inline">\(\Longrightarrow (p-1)q = p\)</span>.</p>
<p>By the Holder’s inequality: <span class="math display">\[
\begin{align*}
\sum_{i=1}^n |x_i||x_i + y_i|^{p-1} &amp;\leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p} \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}\\
\sum_{i=1}^n |y_i||x_i + y_i|^{p-1} &amp;\leq \left(\sum_{i=1}^n |y_i|^p\right)^{1/p} \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}
\end{align*}
\]</span></p>
<p>Summing, we get:</p>
<p><span class="math display">\[
\begin{align*}
\sum_{i=1}^n |x_i + y_i|^{p} &amp;\leq \left\{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right\} \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}\\
&amp;= \left\{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right\}\left(\sum_{i=1}^n |x_i + y_i|^{p}\right)^{1-\frac{1}{p}}\\
\Longrightarrow \left(\sum_{i=1}^n |x_i + y_i|^{p}\right)^{1/p} &amp;\leq \left\{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right\}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="the-vector-infty-norm" class="level2">
<h2 class="anchored" data-anchor-id="the-vector-infty-norm">The vector <span class="math inline">\(\infty\)</span>-norm</h2>
<div id="def-infinity-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (<span class="math inline">\(\infty\)</span>-norm)</strong></span> The vector <span class="math inline">\(\infty\)</span>-norm, <span class="math inline">\(\norm{\cdot}:\C^n \to \R\)</span> is defined for <span class="math inline">\(\bf{x} \in \C^n\)</span> by:</p>
<p><span class="math display">\[
\norm{\bf{x}}_\infty = \max\{|\chi_1|,|\chi_2|,\ldots,|\chi_n|\}
\]</span></p>
<p>The <span class="math inline">\(\infty\)</span>-norm simply measures how long the vector is by the magnitude of its largest entry.</p>
</div>
<div id="thm-infty-norm-is-a-norm" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8</strong></span> The vector <span class="math inline">\(\infty\)</span>-norm is well-defined.</p>
</div>
<p><em>Proof.</em></p>
<p><em>Positive semi-definiteness</em></p>
<p>We have:</p>
<p><span class="math display">\[
\norm{\bf{x}}_{\infty} = \max_{1\leq i \leq n} |\chi_i| \geq |\xi_i| \geq 0
\]</span></p>
<p><em>Homogeneity</em></p>
<p>We have:</p>
<p><span class="math display">\[
\norm{\alpha \bf{x}}_{\infty} = \max_{1\leq i \leq n}|\alpha \chi_i| =\max_{1\leq i \leq n}|\alpha|| \chi_i| = |\alpha| \max_{1\leq i \leq n}|\chi_i| = |\alpha|\norm{\bf{x}}_{\infty}
\]</span></p>
<p><em>Triangle Inequality</em></p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x} + \bf{y}}_\infty &amp;= \max_{i=1}^m |\chi_i + \xi_i|\\
&amp;\leq \max_{i=1}^m (|\chi_i| + |\xi_i|)\\
&amp;\leq \max_{i=1}^m |\chi_i| + \max_{i=1}^m |\xi_i|\\
&amp;= \norm{\bf{x}}_\infty + \norm{\bf{y}}_\infty
\end{align*}
\]</span></p>
</section>
<section id="equivalence-of-vector-norms" class="level2">
<h2 class="anchored" data-anchor-id="equivalence-of-vector-norms">Equivalence of vector norms</h2>
<p>As I was saying earlier, we often measure if a vector is <em>small</em> or <em>large</em> or the distance between two vectors by computing norms. It would be unfortunate, if a vector were <em>small</em> in one norm, yet <em>large</em> in another. Fortunately, the next theorem excludes this possibility.</p>
<div id="thm-equivalence-of-vector-norms" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9 (Equivalence of vector norms)</strong></span> Let <span class="math inline">\(\norm{\cdot}_a:\C^n \to \R\)</span> and <span class="math inline">\(\norm{\cdot}_b:\C^n\to \R\)</span> both be vector norms. Then there exist positive scalars <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> such that for <span class="math inline">\(\bf{x}\in \C^n\)</span>,</p>
<p><span class="math display">\[
C_1 \norm{\bf{x}}_b \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_b
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We can prove equivalence of norms in four steps, the last which uses the extreme value theorem from Real Analysis.</p>
<section id="step-1-it-is-sufficient-to-consider-normcdot_b-normcdot_1-transitivity." class="level4">
<h4 class="anchored" data-anchor-id="step-1-it-is-sufficient-to-consider-normcdot_b-normcdot_1-transitivity.">Step 1: It is sufficient to consider <span class="math inline">\(\norm{\cdot}_b = \norm{\cdot}_1\)</span> (transitivity).</h4>
<p>We will show that it is sufficient to prove that <span class="math inline">\(\norm{\cdot}_a\)</span> is equivalent to <span class="math inline">\(\norm{\cdot}_1\)</span> because norm equivalence is <em>transitive</em>: if two norms are equivalent to <span class="math inline">\(\norm{\cdot}_1\)</span>, then they are equivalent to each other. In particular, suppose both <span class="math inline">\(\norm{\cdot}_a\)</span> and <span class="math inline">\(\norm{\cdot}_{a'}\)</span> are equivalent to <span class="math inline">\(\norm{\cdot}_1\)</span> for constants <span class="math inline">\(0 \leq C_1 \leq C_2\)</span> and <span class="math inline">\(0 \leq C_1' \leq C_2'\)</span> respectively:</p>
<p><span class="math display">\[
C_1 \norm{\bf{x}}_1 \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_1
\]</span></p>
<p>and</p>
<p><span class="math display">\[
C_1' \norm{\bf{x}}_1 \leq \norm{\bf{x}}_{a'} \leq C_2' \norm{\bf{x}}_1
\]</span></p>
<p>Then, it immediately follows that:</p>
<p><span class="math display">\[
\norm{\bf{x}}_{a'} \leq C_2' \norm{\bf{x}}_1 \leq \frac{C_2'}{C_1} \norm{\bf{x}}_a
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\norm{\bf{x}}_{a'} \geq C_1' \norm{\bf{x}}_1 \geq \frac{C_1'}{C_2} \norm{\bf{x}}_a
\]</span></p>
<p>and hence <span class="math inline">\(\norm{\cdot}_a\)</span> and <span class="math inline">\(\norm{\cdot}_{a'}\)</span> are equivalent. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="step-2-it-is-sufficient-to-consider-only-bfx-with-normbfx_1-1." class="level4">
<h4 class="anchored" data-anchor-id="step-2-it-is-sufficient-to-consider-only-bfx-with-normbfx_1-1.">Step 2: It is sufficient to consider only <span class="math inline">\(\bf{x}\)</span> with <span class="math inline">\(\norm{\bf{x}}_1 = 1\)</span>.</h4>
<p>We wish to show that</p>
<p><span class="math display">\[
C_1 \norm{\bf{x}}_1 \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_1
\]</span></p>
<p>is true for all <span class="math inline">\(\bf{x} \in V\)</span> for some <span class="math inline">\(C_1\)</span>, <span class="math inline">\(C_2\)</span>. It is trivially true for <span class="math inline">\(\bf{x}=\bf{0}\)</span>, so we only need to consider <span class="math inline">\(\bf{x}\neq\bf{0}\)</span>, in which case, we can divide by <span class="math inline">\(\norm{\bf{x}}_1\)</span>, to obtain the condition:</p>
<p><span class="math display">\[
C_1 \leq \norm{\frac{\bf{x}}{\norm{\bf{x}}_1 }}_a \leq C_2
\]</span></p>
<p>The vector <span class="math inline">\(\bf{u} = \frac{\bf{x}}{\norm{\bf{x}}_1}\)</span> is a unit vector in the <span class="math inline">\(1\)</span>-norm, <span class="math inline">\(\norm{\bf{u}}_1 = 1\)</span>. So, we can write:</p>
<p><span class="math display">\[
C_1 \leq \norm{\bf{u}}_a \leq C_2
\]</span></p>
<p>We have the desired result. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="step-3-any-norm-normcdot_a-is-continuous-under-normcdot_1." class="level4">
<h4 class="anchored" data-anchor-id="step-3-any-norm-normcdot_a-is-continuous-under-normcdot_1.">Step 3: Any norm <span class="math inline">\(\norm{\cdot}_a\)</span> is continuous under <span class="math inline">\(\norm{\cdot}_1\)</span>.</h4>
<p>We wish to show that any norm <span class="math inline">\(\norm{\cdot}_a\)</span> is a continuous function on <span class="math inline">\(V\)</span> under the topology induced by <span class="math inline">\(\norm{\cdot}_1\)</span>. That is, we wish to show that for any <span class="math inline">\(\epsilon &gt; 0\)</span>, there exists <span class="math inline">\(\delta &gt; 0\)</span>, such that for all <span class="math inline">\(\norm{\bf{x} - \bf{c}}_1 &lt; \delta\)</span>, we have <span class="math inline">\(\norm{\norm{\bf{x}}_a - \norm{\bf{c}}_a}_1 &lt; \epsilon\)</span>.</p>
<p>We prove this into two steps. First, by the triangle inequality on <span class="math inline">\(\norm{\cdot}_a\)</span>, it follows that:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x}}_a - \norm{\bf{c}}_a &amp;= \norm{\bf{c} + (\bf{x} - \bf{c})}_a - \norm{\bf{c}}_a \\
&amp;\leq \norm{\bf{c}}_a + \norm{(\bf{x} - \bf{c})}_a - \norm{\bf{c}}_a\\
&amp;= \norm{(\bf{x} - \bf{c})}_a
\end{align*}
\]</span></p>
<p>And</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{c}}_a - \norm{\bf{x}}_a &amp;\leq \norm{(\bf{x} - \bf{c})}_a
\end{align*}
\]</span></p>
<p>and hence:</p>
<p><span class="math display">\[
|\norm{\bf{x}}_a - \norm{\bf{c}}_a| \leq \norm{(\bf{x} - \bf{c})}_a
\]</span></p>
<p>Second applying the triangle inequality again, and writing <span class="math inline">\(\bf{x} = \sum_{i=1}^n \alpha_i \bf{e}_i\)</span> and <span class="math inline">\(\bf{c} = \sum_{i=1}^n \alpha_i' \bf{e}_i\)</span> in our basis, we obtain:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x}-\bf{c}}_a &amp;= \norm{\sum_{i=1}^n (\alpha_i - \alpha_i')\bf{e}_i}_a\\
&amp;\leq \sum_{i=1}^n \norm{(\alpha_i - \alpha_i')\bf{e}_i}_a &amp; \{ \text{ Triangle Inequality }\}\\
&amp;= \sum_{i=1}^n |(\alpha_i - \alpha_i')|\norm{\bf{e}_i}_a \\
&amp;= \norm{\bf{x} - \bf{c}}_1 \left(\max_i \norm{\bf{e}_i}_a \right)
\end{align*}
\]</span></p>
<p>Therefore, if we choose:</p>
<p><span class="math display">\[
\delta = \frac{\epsilon}{\left(\max_i \norm{\bf{e}_i}_a \right)}
\]</span></p>
<p>it immediate follows that:</p>
<p><span class="math display">\[\begin{align*}
\norm{\bf{x} - \bf{c}}_1 &amp;&lt; \delta \\
\Longrightarrow |\norm{\bf{x}}_a - \norm{\bf{c}}_a| &amp;\leq \norm{\bf{x} - \bf{c}}_a \\ &amp;\leq \norm{\bf{x} - \bf{c}}_1 \left(\max_i \norm{\bf{e}_i}_a \right) \\
&amp; \leq \frac{\epsilon}{\left(\max_i \norm{\bf{e}_i}_a \right)} \left(\max_i \norm{\bf{e}_i}_a \right) = \epsilon
\end{align*}
\]</span></p>
<p>This proves (uniform) continuity. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="step-4-the-maximum-and-minimum-of-normcdot_a-on-the-unit-ball" class="level4">
<h4 class="anchored" data-anchor-id="step-4-the-maximum-and-minimum-of-normcdot_a-on-the-unit-ball">Step 4: The maximum and minimum of <span class="math inline">\(\norm{\cdot}_a\)</span> on the unit ball</h4>
<p>Let <span class="math inline">\(K:=\{\bf{u}:\norm{\bf{u}}_1 = 1\}\)</span>. Then, <span class="math inline">\(K\)</span> is a compact set. Since <span class="math inline">\(\norm{\cdot}_a\)</span> is continuous on <span class="math inline">\(K\)</span>, by the extreme value theorem, <span class="math inline">\(\norm{\cdot}_a\)</span> must achieve a supremum and infimum on the set. So, for all <span class="math inline">\(\bf{u}\)</span> with <span class="math inline">\(\norm{\bf{u}}_1 = 1\)</span>, there exists <span class="math inline">\(C_1,C_2 &gt; 0\)</span>, such that:</p>
<p><span class="math display">\[ C_1 \leq \norm{\bf{u}}_a \leq C_2\]</span></p>
<p>as required by step 2. And we are done! <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="deriving-the-constants-c_1infty-c_infty1" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-constants-c_1infty-c_infty1">Deriving the constants <span class="math inline">\(C_{1,\infty}\)</span>, <span class="math inline">\(C_{\infty,1}\)</span></h3>
<p>Let’s write a python implementation of the various norms.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> itertools</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-5"><a href="#cb3-5"></a></span>
<span id="cb3-6"><a href="#cb3-6"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="kw">def</span> one_norm(x):</span>
<span id="cb3-9"><a href="#cb3-9"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(x))</span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="kw">def</span> two_norm(x):</span>
<span id="cb3-12"><a href="#cb3-12"></a>    <span class="cf">return</span> np.sqrt(np.<span class="bu">sum</span>(x<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb3-13"><a href="#cb3-13"></a></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="kw">def</span> p_norm(x,p):</span>
<span id="cb3-15"><a href="#cb3-15"></a>    <span class="cf">return</span> np.<span class="bu">pow</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(x)<span class="op">**</span>p),<span class="fl">1.0</span><span class="op">/</span>p)</span>
<span id="cb3-16"><a href="#cb3-16"></a></span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="kw">def</span> infty_norm(x):</span>
<span id="cb3-18"><a href="#cb3-18"></a>    <span class="cf">return</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(x))</span>
<span id="cb3-19"><a href="#cb3-19"></a></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="kw">def</span> get_vectors_eq_norm_val(func, val, lower_bound, upper_bound):</span>
<span id="cb3-21"><a href="#cb3-21"></a>    x_1 <span class="op">=</span> np.linspace(lower_bound, upper_bound, </span>
<span id="cb3-22"><a href="#cb3-22"></a>    <span class="bu">int</span>((upper_bound <span class="op">-</span> lower_bound)<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb3-23"><a href="#cb3-23"></a>    x_2 <span class="op">=</span> np.linspace(lower_bound, upper_bound, </span>
<span id="cb3-24"><a href="#cb3-24"></a>    <span class="bu">int</span>((upper_bound <span class="op">-</span> lower_bound)<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb3-25"><a href="#cb3-25"></a></span>
<span id="cb3-26"><a href="#cb3-26"></a>    pts <span class="op">=</span> np.array(<span class="bu">list</span>(itertools.product(x_1, x_2)))</span>
<span id="cb3-27"><a href="#cb3-27"></a>    norm_arr <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(func, pts)))</span>
<span id="cb3-28"><a href="#cb3-28"></a></span>
<span id="cb3-29"><a href="#cb3-29"></a>    pts_norm_list <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(pts,norm_arr))</span>
<span id="cb3-30"><a href="#cb3-30"></a></span>
<span id="cb3-31"><a href="#cb3-31"></a>    pts_with_norm_eq_val <span class="op">=</span> []</span>
<span id="cb3-32"><a href="#cb3-32"></a>    <span class="cf">for</span> pt <span class="kw">in</span> pts_norm_list:</span>
<span id="cb3-33"><a href="#cb3-33"></a>        <span class="cf">if</span> pt[<span class="dv">1</span>] <span class="op">==</span> val:</span>
<span id="cb3-34"><a href="#cb3-34"></a>            pts_with_norm_eq_val.append(pt[<span class="dv">0</span>])</span>
<span id="cb3-35"><a href="#cb3-35"></a></span>
<span id="cb3-36"><a href="#cb3-36"></a>    <span class="cf">return</span> np.array(pts_with_norm_eq_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can glean useful information by visualizing the set of points(vectors) with a given norm.</p>
<div class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>pts1 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb4-2"><a href="#cb4-2"></a>    func<span class="op">=</span>infty_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">1.0</span>, upper_bound<span class="op">=</span><span class="fl">1.0</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>)</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>pts2 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb4-6"><a href="#cb4-6"></a>    func<span class="op">=</span>one_norm, val<span class="op">=</span><span class="fl">2.0</span>, lower_bound<span class="op">=-</span><span class="fl">2.0</span>, upper_bound<span class="op">=</span><span class="fl">2.0</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>)</span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-10"><a href="#cb4-10"></a>plt.xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb4-11"><a href="#cb4-11"></a>plt.ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb4-12"><a href="#cb4-12"></a>a <span class="op">=</span> plt.scatter(pts1[:, <span class="dv">0</span>], pts1[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-13"><a href="#cb4-13"></a>b <span class="op">=</span> plt.scatter(pts2[:, <span class="dv">0</span>], pts2[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co"># c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)</span></span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a>plt.legend(</span>
<span id="cb4-17"><a href="#cb4-17"></a>    (a, b), (<span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_\infty = 1$"</span>, <span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_1=2$"</span>), loc<span class="op">=</span><span class="st">"lower left"</span></span>
<span id="cb4-18"><a href="#cb4-18"></a>)</span>
<span id="cb4-19"><a href="#cb4-19"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="593" height="427" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The blue rectangle represents all vectors <span class="math inline">\(\bf{x}\in\R^2\)</span> with unit <span class="math inline">\(\infty\)</span>-norm, <span class="math inline">\(\norm{\bf{x}}_\infty = 1\)</span>. The orange rhombus represents all vectors <span class="math inline">\(\bf{x}\)</span> with <span class="math inline">\(\norm{\bf{x}}_1 = 2\)</span>. All points on or outside the blue square represent vectors <span class="math inline">\(\bf{y}\)</span>, such that <span class="math inline">\(\norm{\bf{y}}_\infty \geq 1\)</span>. Hence, if <span class="math inline">\(\norm{\bf{y}}_1 = 2\)</span>, <span class="math inline">\(\norm{\bf{y}}_\infty \geq 1\)</span>.</p>
<p>Now, pick any <span class="math inline">\(\bf{z}\neq \bf{0}\)</span>. Then, <span class="math inline">\(2\norm{\frac{\bf{z}}{\norm{\bf{z}}_1}}_1 =2\)</span>. Thus, <span class="math inline">\(\norm{\frac{2\bf{z}}{\norm{\bf{z}}_1}}_\infty \geq 1\)</span>. So, it follows that if <span class="math inline">\(\bf{z}\in\R^2\)</span> is any arbitrary vector, <span class="math inline">\(\norm{\bf{z}}_1 \leq 2 \norm{\bf{z}}_\infty\)</span>.</p>
<p>In general, if <span class="math inline">\(\bf{x}\in\C^n\)</span>, then:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{x}}_1 &amp;= \sum_{i=1}^n |x_i|\\
&amp;\leq \sum_{i=1}^n \max\{|x_i|:i=1,2,\ldots,n\}\\
&amp;= n \norm{\bf{x}}_\infty
\end{align*}
\]</span></p>
<p>Next, in the below plot, the orange rhombus represents vectors <span class="math inline">\(\bf{x}\in\R^2\)</span>, such that <span class="math inline">\(\normp{x}{1} = 1\)</span> and all points on or outside the orange rhombus are such that <span class="math inline">\(\normp{y}{1} \geq 1\)</span>. The blue square represents vectors <span class="math inline">\(\normp{y}{\infty} = 1\)</span>. Consequently, if <span class="math inline">\(\normp{y}{1} = 1\)</span>, then <span class="math inline">\(\normp{y}{\infty} \leq \normp{y}{1}\)</span>. In general, if <span class="math inline">\(\bf{x}\in C^n\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
\normp{x}{\infty} &amp;= \max\{|x_1|,\ldots,|x_n|\}\\
&amp;\leq \sum_{i=1}^n |x_i|=\normp{x}{1}
\end{align*}
\]</span></p>
<p>Putting together, we have:</p>
<p><span class="math display">\[
\begin{align*}
\normp{x}{\infty} \leq C_{\infty,1} \normp{x}{1} \\
\normp{x}{1} \leq C_{1,\infty} \normp{x}{\infty}
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(C_{\infty,1} = 1\)</span> and <span class="math inline">\(C_{1,\infty}=n\)</span>.</p>
<div class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>pts1 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb5-2"><a href="#cb5-2"></a>    func<span class="op">=</span>infty_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">1.0</span>, upper_bound<span class="op">=</span><span class="fl">1.0</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>)</span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a>pts2 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb5-6"><a href="#cb5-6"></a>    func<span class="op">=</span>one_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">2.0</span>, upper_bound<span class="op">=</span><span class="fl">2.0</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>)</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-10"><a href="#cb5-10"></a>plt.xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb5-11"><a href="#cb5-11"></a>plt.ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb5-12"><a href="#cb5-12"></a>a <span class="op">=</span> plt.scatter(pts1[:, <span class="dv">0</span>], pts1[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-13"><a href="#cb5-13"></a>b <span class="op">=</span> plt.scatter(pts2[:, <span class="dv">0</span>], pts2[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co"># c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)</span></span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a>plt.legend(</span>
<span id="cb5-17"><a href="#cb5-17"></a>    (a, b), (<span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_\infty = 1$"</span>, <span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_1=1$"</span>), loc<span class="op">=</span><span class="st">"lower left"</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>)</span>
<span id="cb5-19"><a href="#cb5-19"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="600" height="427" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="deriving-the-constants-c_12-c_21" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-constants-c_12-c_21">Deriving the constants <span class="math inline">\(C_{1,2}\)</span>, <span class="math inline">\(C_{2,1}\)</span></h3>
<p>We can also derive the constants <span class="math inline">\(C_{1,2}\)</span> and <span class="math inline">\(C_{2,1}\)</span>. We have:</p>
<p>Let <span class="math inline">\(\bf{x}\in\C^n\)</span> be an arbitrary vector. And let <span class="math inline">\(\bf{y}=(1+0i,\ldots,1+0i)\)</span>. By the Cauchy-Schwarz inequality,</p>
<p><span class="math display">\[
\begin{align*}
\sum_{i=1}^n |x_i| \leq \left(\sum_{i=1}^n |x_i|^2\right)^{1/2}\sqrt{n}
\end{align*}
\]</span></p>
<p>So, our claim is <span class="math inline">\(\normp{x}{1} \leq \sqrt{n}\normp{x}{2}\)</span>.</p>
<p>Also, consider the vector <span class="math inline">\(\bf{v}=\left(\frac{1}{\sqrt{n}},\ldots,\frac{1}{\sqrt{n}}\right)\)</span>. <span class="math inline">\(\norm{\bf{v}}_1 = \sqrt{n}\norm{\bf{v}}_2\)</span>. So, the bound is tight.</p>
<p>Moreover:</p>
<p><span class="math display">\[
\begin{align*}
\normp{x}{2}^2 &amp;= \sum_{i=1}^n |x_i|^2 \\
&amp;\leq \sum_{i=1}^n |x_i|^2 + \sum_{i \neq j}|x_i||x_j|\\
&amp;= \sum_{i=1}^n |x_i|^2 + \sum_{i &lt; j}2|x_i||x_j|\\
&amp;= \left(\sum_{i=1}^n |x_i|\right)^2
\end{align*}
\]</span></p>
<p>So, <span class="math inline">\(\normp{x}{2} \leq \normp{x}{1}\)</span>. Consider the standard basis vector <span class="math inline">\(\bf{e}_1 = (1,0,0,\ldots,0)\)</span>. <span class="math inline">\(\norm{\bf{e}_1}_2 = \norm{\bf{e}_1}_1\)</span>. Hence, the bound is tight. We conclude that:</p>
<p><span class="math display">\[
\begin{align*}
\normp{x}{1} \leq C_{1,2} \normp{x}{2}\\
\normp{x}{2} \leq C_{2,1} \normp{x}{1}
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(C_{1,2} = \sqrt{n}\)</span> and <span class="math inline">\(C_{2,1} = 1\)</span>.</p>
</section>
<section id="deriving-the-constants-c_2infty-and-c_infty2" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-constants-c_2infty-and-c_infty2">Deriving the constants <span class="math inline">\(C_{2,\infty}\)</span> and <span class="math inline">\(C_{\infty,2}\)</span></h3>
<p>Let <span class="math inline">\(x \in \C^n\)</span>. We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{x}_2^2 &amp; = \sum_{i=0}^{n-1}|\chi_i|^2\\
&amp;\leq\sum_{i=0}^{n-1} (\max_{i=0}^{n-1}|\chi_i|)^2\\
&amp;= n \norm{x}_\infty
\end{align}
\]</span></p>
<p>So, <span class="math inline">\(\norm{x}_2 \leq \sqrt{n} \norm{x}_\infty\)</span>.</p>
<p>Moreover, let <span class="math inline">\(x = (1, 1, \ldots, 1)^T\)</span>. Then, <span class="math inline">\(\norm{x}_2 = \sqrt{n}\)</span> and <span class="math inline">\(\norm{x}_\infty = 1\)</span>, so <span class="math inline">\(\norm{x}_2 = \sqrt{n}\norm{x}_\infty\)</span>. Hence, it is a tight inequality.</p>
<p>Also, we have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{x}_\infty^2 &amp;= \max \{|\chi_0|^2,|\chi_1|^2,\ldots,|\chi_{n-1}^2|\}\\
&amp;\leq \max \{\sum_{i=0}^{n-1}|\chi_i|^2,\sum_{i=0}^{n-1}|\chi_i|^2,\ldots,\sum_{i=0}^{n-1}|\chi_i|^2|\}\\
&amp;= \norm{x}_2^2
\end{align*}
\]</span></p>
<p>Moreover, let <span class="math inline">\(x = (1, 0)\)</span>. Then, <span class="math inline">\(\norm{x}_2 = 1\)</span> and <span class="math inline">\(\norm{x}_\infty = 1\)</span>. So, <span class="math inline">\(\norm{x}_\infty = \norm{x}_2\)</span>. Hence, the inequality is tight.</p>
</section>
</section>
<section id="matrix-norms" class="level2">
<h2 class="anchored" data-anchor-id="matrix-norms">Matrix Norms</h2>
<p>The analysis of matrix algorithms requires the use of matrix norms. For example, the quality of a linear system solution may be poor, if the matrix of coefficients is <em>nearly singular</em>. To quantify the notion of singularity, we need a measure of the distance on the space of matrices. Matrix norms can be used to provide that measure.</p>
<section id="definitions" class="level3">
<h3 class="anchored" data-anchor-id="definitions">Definitions</h3>
<p>Since <span class="math inline">\(\R^{m \times n}\)</span> is isomorphic <span class="math inline">\(\R^{mn}\)</span>, the definition of a matrix norm is equivalent to the definition of a vector norm. In particular, <span class="math inline">\(f:\R^{m \times n} \to \R\)</span> is a matrix norm, if the following three properties holds:</p>
<p><span class="math display">\[
\begin{align*}
f(A) \geq 0, &amp; &amp; A \in \R^{m \times n}\\
f(A + B) \leq f(A) + f(B), &amp; &amp; A,B \in \R^{m \times n}\\
f(\alpha A) = |\alpha|f(A), &amp; &amp; \alpha \in \R, A \in \R^{m \times n}
\end{align*}
\]</span></p>
<p>The most frequently used matrix norms in numerical linear algebra are the Frobenius norm and the <span class="math inline">\(p\)</span>-norms.</p>
<div id="def-the-frobenius-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8 (Frobenius Norm)</strong></span> The Frobenius norm <span class="math inline">\(\norm{\cdot}_F : \C^{m \times n} \to \R\)</span> is defined for <span class="math inline">\(A \in \C^{m \times n}\)</span> by:</p>
<p><span class="math display">\[
\norm{A}_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}
\]</span></p>
</div>
<div id="thm-frobenius-norm-is-well-defined" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10</strong></span> The Frobenius norm is a well-defined norm.</p>
</div>
<p><em>Proof.</em></p>
<p><em>Positive Semi-definite</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_F &amp;= \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}\\
&amp;\geq \left( |a_{ij}|^2\right)^{1/2} = |a_{ij}|\\
&amp;\geq 0
\end{align*}
\]</span></p>
<p><em>Triangle Inequality</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A + B}_F^2 &amp;= \sum_{i=1}^m \sum_{j=1}^n |a_{ij} + b_{ij}|^2 \\
&amp;\leq \sum_{i=1}^m \sum_{j=1}^n \left(|a_{ij}|^2 + |b_{ij}|^2 + 2|a_{ij}||b_{ij}|\right)\\
&amp;= \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 + \sum_{i=1}^m \sum_{j=1}^n |b_{ij}|^2 + 2\sum_{i=1}^m \sum_{j=1}^n|a_{ij}||b_{ij}|\\
&amp;\leq \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 + \sum_{i=1}^m \sum_{j=1}^n |b_{ij}|^2 + 2\left(\sum_{i=1}^m \sum_{j=1}^n|a_{ij}|^2\right)^{1/2}\left(\sum_{i=1}^m \sum_{j=1}^n|b_{ij}|^2\right)^{1/2} &amp; \{\text{ Cauchy-Schwarz }\}\\
&amp;= \norm{A}_F^2 + \norm{B}_F^2 + 2\norm{A}_F \norm{B}_F\\
&amp;= (\norm{A}_F + \norm{B}_F)^2\\\\
\Longrightarrow \norm{A + B}_F &amp;\leq \norm{A}_F + \norm{B}_F
\end{align*}
\]</span></p>
<p><em>Homogeneity</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\alpha A}_F &amp;= \left(\sum_{i=1}^m \sum_{j=1}^n |\alpha a_{ij}|^2\right)^{1/2}\\
&amp;=\left(\sum_{i=1}^m \sum_{j=1}^n |\alpha|^2 |a_{ij}|^2\right)^{1/2}\\
&amp;= |\alpha| \norm{A}_F
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="def-induced-matrix-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9 (Induced matrix norm)</strong></span> Let <span class="math inline">\(\norm{\cdot}_\mu : \C^m \to \R\)</span> and <span class="math inline">\(\norm{\cdot}_\nu : \C^n \to R\)</span> be vector norms. Define <span class="math inline">\(\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to R\)</span> by:</p>
<p><span class="math display">\[
\norm{A}_{\mu,\nu} = \sup_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu}
\]</span></p>
<p>Matrix norms that are defined in this way are called <em>induced</em> matrix norms.</p>
</div>
<p>Let us start by interpreting this. How <em>large</em> <span class="math inline">\(A\)</span> is, as measured by <span class="math inline">\(\norm{A}_{\mu,\nu}\)</span> is defined as the most that <span class="math inline">\(A\)</span> magnifies the length of non-zero vectors, where the length of the <span class="math inline">\(\bf{x}\)</span> is measured with the norm <span class="math inline">\(\norm{\cdot}_\nu\)</span> and the length of the transformed vector <span class="math inline">\(A\bf{x}\)</span> is measured with the norm <span class="math inline">\(\norm{\cdot}_\mu\)</span>.</p>
<p>Two comments are in order. First,</p>
<p><span class="math display">\[
\begin{align*}
\sup_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} = \sup_{\bf{x} \neq \bf{0}} \norm{A\frac{\bf{x}}{\norm{\bf{x}}_\nu}}_\mu = \sup_{\norm{\bf{u}}_\nu = 1} \norm{A\bf{u}}_\mu
\end{align*}
\]</span></p>
<p>Second, it is not immediately obvious, that there is a vector <span class="math inline">\(\bf{x}\)</span> for which a supremum is attained. The fact is there is always such a vector <span class="math inline">\(\bf{x}\)</span>. The <span class="math inline">\(K=\{\bf{u}:\norm{\bf{u}}_\nu = 1\}\)</span> is a compact set, and <span class="math inline">\(\norm{\cdot}_\mu : \C^m \to \R\)</span> is a continuous function. Continuous functions preserve compact sets. So, the supremum exists and further it belongs to <span class="math inline">\(\{A\bf{x}:\norm{\bf{x}}_\nu = 1\}\)</span>.</p>
<div id="thm-the-induced-matrix-norm-is-well-defined" class="theorem">
<p><span class="theorem-title"><strong>Theorem 11</strong></span> The induced matrix norm <span class="math inline">\(\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to \R\)</span> is a well-defined norm.</p>
</div>
<p><em>Proof</em></p>
<p>To prove this, we merely check if the three conditions are met:</p>
<p>Let <span class="math inline">\(A,B \in \C^{m \times n}\)</span> and <span class="math inline">\(\alpha \in \C\)</span> be arbitrarily chosen. Then:</p>
<p><em>Positive definite</em></p>
<p>Let <span class="math inline">\(A \neq 0\)</span>. That means, at least one of the columns of <span class="math inline">\(A\)</span> is not a zero-vector. Partition <span class="math inline">\(A\)</span> by columns:</p>
<p><span class="math display">\[
\left[
    \begin{array}{c|c|c|c}
        a_{1} &amp; a_2 &amp; \ldots &amp; a_{n}
    \end{array}
\right]
\]</span></p>
<p>Let us assume that, it is the <span class="math inline">\(j\)</span>-th column <span class="math inline">\(a_j\)</span>, that is non-zero. Let <span class="math inline">\(\bf{e}_j\)</span> be the column of <span class="math inline">\(I\)</span>(the identity matrix) indexed with <span class="math inline">\(j\)</span>. Then:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_{\mu,\nu} &amp;= \sup \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Definition }\}\\
&amp;\geq \frac{\norm{A\bf{e}_j}_\mu}{\norm{\bf{e}_j}_\nu}\\
&amp;= \frac{\norm{a_j}_\mu}{\norm{\bf{e}_j}_\nu} &amp; \{ A\bf{e}_j = a_j \}\\
&amp;&gt; 0 &amp; \{ \text{ we assumed } a_j \neq \bf{0}\}
\end{align*}
\]</span></p>
<p><em>Homogeneity</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\alpha A}_{\mu,\nu} &amp;= \sup_{\bf{x}\neq \bf{0}} \frac{\norm{\alpha A \bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Definition }\}\\
&amp;= \sup_{\bf{x}\neq \bf{0}} \frac{|\alpha|\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Homogeneity of vector norm }\norm{\cdot}_\mu\}\\
&amp;= |\alpha|\sup_{\bf{x}\neq \bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Algebra }\}\\
&amp;= |\alpha|\norm{A}_{\mu,\nu}
\end{align*}
\]</span></p>
<p><em>Triangle Inequality</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A + B}_{\mu,\nu} &amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A + B) \bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Definition }\}\\
&amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x} + B\bf{x})}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Distribute }\}\\
&amp;\leq \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu + \norm{B\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Triangle inequality for vector norms }\}\\
&amp;= \max_{\bf{x}\neq \bf{0}} \left(\frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} + \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} \right) &amp; \{ \text{ Algebra }\}\\
&amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} + \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} \\
&amp;= \norm{A}_{\mu,\nu} + \norm{B}_{\mu,\nu} &amp; \{ \text{ Definition }\}
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>When <span class="math inline">\(\norm{\cdot}_\mu\)</span> and <span class="math inline">\(\norm{\cdot}_\nu\)</span> are the same norm, the induced norm becomes:</p>
<p><span class="math display">\[
\norm{A}_\mu = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\mu}
\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[
\norm{A}_\mu = \max_{\norm{\bf{u}}_\mu = 1} \norm{A\bf{u}}_\mu
\]</span></p>
<div id="exm-p-norm-is-the-same" class="theorem example">
<p><span class="theorem-title"><strong>Example 1</strong></span> Consider the vector <span class="math inline">\(p\)</span>-norm <span class="math inline">\(\norm{\cdot}_p:\C^n \to \R\)</span> and let us denote the induced matrix norm <span class="math inline">\(|||\cdot|||:\C^{m \times n} \to \R\)</span> by <span class="math inline">\(|||A||| = \max_{\bf{x}\neq\bf{0}}\frac{\norm{A\bf{x}}_p}{\norm{\bf{x}}_p}\)</span>. Prove that <span class="math inline">\(|||\bf{y}||| = \norm{\bf{y}}_p\)</span> for all <span class="math inline">\(\bf{y}\in\C^m\)</span>.</p>
</div>
<p><em>Proof</em>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
|||\bf{y}||| &amp;= \frac{\norm{\bf{y}x}_p}{\norm{x}_p} &amp; \{ \text{ Definition }\}\\
&amp;= \frac{|x_1| \norm{\bf{y}}_p}{|x_1|} &amp; \{ x \text{ has to be } 1 \times 1, \text{ a scalar }\}\\
&amp;= \norm{\bf{y}}_p
\end{align*}
\]</span></p>
<p>The last example is important. One can view a vector <span class="math inline">\(\bf{y}\in \C^m\)</span> as an <span class="math inline">\(m \times 1\)</span> matrix. What this last exercise tells us is that regardless of whether we view <span class="math inline">\(\bf{y}\)</span> as a matrix or a vector, <span class="math inline">\(\norm{y}_p\)</span> is the same.</p>
<p>We already encountered the vector <span class="math inline">\(p\)</span>-norms as an important class of vector norms. The matrix <span class="math inline">\(p\)</span>-norm is induced by the corresponding vector norm.</p>
<div id="def-the-matrix-p-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10 (The matrix <span class="math inline">\(p\)</span>-norm)</strong></span> For any vector <span class="math inline">\(p\)</span>-norm, define the corresponding matrix <span class="math inline">\(p\)</span>-norm <span class="math inline">\(\norm{\cdot}_p : \C^{m \times n} \to \R\)</span> by:</p>
<p><span class="math display">\[
\norm{A}_p = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_p}{\norm{\bf{x}}_p} \quad \text{ or equivalently } \quad \norm{A}_p = \max_{\norm{\bf{x}}_p = 1} \norm{A\bf{x}}_p
\]</span></p>
</div>
<p>In practice, the matrix <span class="math inline">\(2\)</span>-norm is of great theoretical importance, but difficult to evaluate, except for special matrices. The <span class="math inline">\(1\)</span>-norm, the <span class="math inline">\(\infty\)</span>-norm and Frobenius norms are straightforward and relatively cheap to compute.</p>
<p>Let us instantiate the definition of the vector <span class="math inline">\(p\)</span>-norm where <span class="math inline">\(p=2\)</span>, giving us a matrix norm induced by the vector <span class="math inline">\(2\)</span>-norm or the Euclidean norm:</p>
<div id="def-the-matrix-2-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 11 (The matrix <span class="math inline">\(2\)</span>-norm)</strong></span> Define the matrix <span class="math inline">\(2\)</span>-norm <span class="math inline">\(\norm{\cdot}_2:\C^{m \times n} \to \R\)</span> by :</p>
<p><span class="math display">\[
\norm{A}_2 = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_2}{\norm{\bf{x}}_2} = \max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2
\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The problem with the matrix <span class="math inline">\(2\)</span>-norm is that it is hard to compute. In future posts, we shall find out that if <span class="math inline">\(A\)</span> is a Hermitian matrix (<span class="math inline">\(A = A^H\)</span>), then <span class="math inline">\(\norm{A}_2 = |\lambda_1|\)</span> where <span class="math inline">\(\lambda_1\)</span> is the eigenvalue of <span class="math inline">\(A\)</span> that is largest in magnitude.</p>
<p>Recall from basic linear algebra, that computing eigenvalues involves computing the roots of polynomials, and for polynomials of degree three or greater, this is a non-trivial task. We shall see that the matrix <span class="math inline">\(2\)</span>-norm plays an important part in theory, but less so in practical computation.</p>
</div>
</div>
<div id="exm-matrix-2-norm-of-diagonal-matrix" class="theorem example">
<p><span class="theorem-title"><strong>Example 2</strong></span> Show that:</p>
<p><span class="math display">\[
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2 = \max(|d_1|,|d_2|)
\]</span></p>
</div>
<p><em>Solution</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}}_2^2 &amp; \{ \text{ Definition }\}\\
&amp;= \max_{\norm{\bf{x}}_2 = 1}|d_1x_1|^2 + |d_2 x_2|^2\\
&amp;\leq \max_{\norm{\bf{x}}_2 = 1} [\max(|d_1|,|d_2|)^2 |x_1|^2 + \max(|d_1|,|d_2|)^2 |x_2|^2]\\
&amp;= \max(|d_1|,|d_2|)^2 \max_{\norm{\bf{x}}_2 = 1} (|x_1|^2 + |x_2|^2)\\
&amp;= \max(|d_1|,|d_2|)^2
\end{align*}
\]</span></p>
<p>Moreover, if we take <span class="math inline">\(\bf{x} = \bf{e}_1\)</span> and <span class="math inline">\(\bf{x}=\bf{e}_2\)</span>, we get:</p>
<p><span class="math display">\[
\begin{align*}
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2^2  &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}}_2 &amp; \{ \text{ Definition }\}\\
&amp;\geq  \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}1 \\ 0\end{bmatrix}}_2^2 \\
&amp;= |d_1|^2
\end{align*}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align*}
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2^2  &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}}_2 &amp; \{ \text{ Definition }\}\\
&amp;\geq  \norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}\begin{bmatrix}0 \\ 1\end{bmatrix}}_2 \\
&amp;= |d_2|^2
\end{align*}
\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2^2 \geq \max(|d_1|,|d_2|)^2
\]</span></p>
<p>We conclude that</p>
<p><span class="math display">\[
\norm{\begin{bmatrix}
d_1 &amp; 0 \\
0 &amp; d_2
\end{bmatrix}}_2 = \max(|d_1|,|d_2|)
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The proof of the last example builds on a general principle: Showing that <span class="math inline">\(\max_{x \in D} f(x) = \alpha\)</span> for some function <span class="math inline">\(f:D \to \R\)</span> can be broken down into showing that both:</p>
<p><span class="math display">\[
\max_{x \in D} f(x) \leq \alpha
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\max_{x \in D} f(x) \geq \alpha
\]</span></p>
<p>In turn, showing that <span class="math inline">\(\max_{x \in D}f(x) \geq \alpha\)</span> can often be accomplished by showing that there exists a vector <span class="math inline">\(y \in D\)</span> such that <span class="math inline">\(f(y) = \alpha\)</span> since then</p>
<p><span class="math display">\[
\max_{x \in D}f(x) \geq f(y) = \alpha
\]</span></p>
<p>We will use this technique in future proofs involving matrix norms.</p>
</div>
</div>
<div id="exr-2-norm-of-a-diag-matrix" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1</strong></span> Let <span class="math inline">\(D \in C^{m \times m}\)</span> be a diagonal matrix <span class="math inline">\(diag(d_1,d_2,\ldots,d_m)\)</span>. Show that:</p>
<p><span class="math display">\[
\norm{D}_2 = \max_{j=1}^{m} |d_j|
\]</span></p>
</div>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{D}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{
    \begin{bmatrix}
    d_1 \\
    &amp; d_2 \\
    &amp; &amp; \ddots\\
    &amp; &amp; &amp; d_m
    \end{bmatrix}
    \begin{bmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_m
    \end{bmatrix}
}_2^2 \{ \text{ Definition }\}\\
&amp;=\max_{\norm{\bf{x}}_2 = 1} \norm{
    \begin{bmatrix}
    d_1 x_1\\
    d_2 x_2\\
    \vdots\\
    d_m x_m
    \end{bmatrix}
}_2^2\\
&amp;= \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m |d_j x_j|^2\\
&amp;\leq \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m \max(|d_1|,\ldots,|d_m|)^2 |x_j|^2\\
&amp;= \max(|d_1|,\ldots,|d_m|)^2 \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m |x_j|^2 \\
&amp;= \max(|d_1|,\ldots,|d_m|)^2
\end{align*}
\]</span></p>
<p>Moreover, if we take take <span class="math inline">\(\bf{x} = \bf{e}_j\)</span>, the standard basis vector with its <span class="math inline">\(j\)</span>-th coordinate equal to one, we find that</p>
<p><span class="math display">\[
\norm{D}_2^2 \geq |d_j|^2
\]</span></p>
<p>Consequently, <span class="math inline">\(\norm{D}_2^2 \geq \max(|d_1|,\ldots,|d_m|)^2\)</span>.</p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-properties-of-two-norm-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2</strong></span> Let <span class="math inline">\(\bf{y}\in\C^m\)</span> and <span class="math inline">\(\bf{x} \in \C^n\)</span>. Show that:</p>
<p><span class="math display">\[
\norm{\bf{y}\bf{x}^H}_2 = \norm{\bf{y}}_2 \norm{\bf{x}}_2
\]</span></p>
</div>
<p><em>Proof</em>.</p>
<p>From the Cauchy-Schwarz inequality, we know that:</p>
<p><span class="math display">\[
|x^H z| \leq \norm{\bf{x}^H}_2 \norm{\bf{z}}_2
\]</span></p>
<p>Now, <span class="math inline">\(\bf{x}^H \in \C^{1 \times n}\)</span> and <span class="math inline">\(\bf{z} \in \C^{n \times 1}\)</span>. So, <span class="math inline">\(\bf{x}^H \bf{z} \in \C^{1 \times 1}\)</span>, and it is a scalar.</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{y}\bf{x}^H}_2 &amp;= \max_{\norm{\bf{z}}_2 = 1} \norm{\bf{y}\bf{x}^H \bf{z}}_2 \\
&amp;= \max_{\norm{\bf{z}}_2 = 1} |\bf{x}^H \bf{z}| \norm{\bf{y}}_2 \{ \bf{x}^H\bf{z}\text{ is scalar }\}\\
&amp;\leq \max_{\norm{\bf{z}}_2 = 1} \norm{\bf{x}^H}_2 \norm{\bf{z}}_2 \norm{\bf{y}}_2 \\
&amp;= \norm{\bf{x}}_2 \norm{\bf{y}}_2
\end{align*}
\]</span></p>
<p>On the other hand,</p>
<p><span class="math display">\[
\begin{align*}
\norm{\bf{y}\bf{x}^H}_2 &amp;= \max_{\bf{z}\neq \bf{0}} \frac{\norm{\bf{y}\bf{x}^H \bf{z}}_2}{\norm{\bf{z}}_2}\\
&amp;\geq \frac{\norm{\bf{y}\bf{x}^H \bf{x}}_2}{\norm{\bf{x}}_2} &amp; \{ \text{ Specific }\bf{z} \}\\
&amp;= \frac{\norm{\bf{y}\norm{\bf{x}}_2^2}_2}{\norm{\bf{x}}_2} &amp; \{ \bf{x}^H \bf{x} = \norm{\bf{x}}_2^2\}\\
&amp;= \norm{\bf{y}}_2 \norm{\bf{x}}_2
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-properties-of-two-norm-2" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3</strong></span> Let <span class="math inline">\(A \in \C^{m \times n}\)</span> and <span class="math inline">\(a_j\)</span> be its column indexed with <span class="math inline">\(j\)</span>. Prove that:</p>
<p><span class="math display">\[
\norm{a_j}_2 \leq \norm{A}_2
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_2 &amp;= \max_{\norm{\bf{z}}_2 = 1} \norm{A\bf{z}}_2 \\
&amp;\geq  \norm{A\bf{e}_j}_2\\
&amp;= \norm{a_j}_2
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-properties-of-two-norm-3" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4</strong></span> Let <span class="math inline">\(A \in \C^{m \times n}\)</span>. Prove that:</p>
<ol type="i">
<li><p><span class="math display">\[
\norm{A}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|
\]</span></p></li>
<li><p><span class="math display">\[
\norm{A^H}_2 = \norm{A}_2
\]</span></p></li>
<li><p><span class="math display">\[
\norm{A^H A}_2 = \norm{A}_2^2
\]</span></p></li>
</ol>
</div>
<p><em>Claim</em>.</p>
<p><span class="math display">\[
\norm{A}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|
\]</span></p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}| &amp;\leq \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} \norm{\bf{y}^H}_2 \norm{A\bf{x}}_2 &amp; \{ \text{ Cauchy-Schwarz }\} \\
&amp;= \max_{\norm{\bf{x}}_2 } \norm{A\bf{x}}_2\\
&amp;= \norm{A}_2
\end{align*}
\]</span></p>
<p>On the other hand:</p>
<p><span class="math display">\[
\begin{align*}
\max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}| &amp;\geq \max_{\norm{\bf{x}}_2 = 1} |\left(\frac{A\bf{x}}{\norm{A\bf{x}}_2}\right)^H A \bf{x}| &amp; \{\text{ Specific vector }\}\\
&amp;= \max_{\norm{\bf{x}}_2 = 1} \frac{\norm{A\bf{x}}_2^2}{\norm{A\bf{x}}_2}\\
&amp;=\max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2\\
&amp;= \norm{A}_2
\end{align*}
\]</span></p>
<p>We have the desired result.</p>
<p><em>Claim</em>.</p>
<p><span class="math display">\[
\norm{A^H}_2 = \norm{A}_2
\]</span></p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A^H}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{A^H\bf{x}}_2^2 \\
&amp;= \max_{\norm{\bf{x}}_2 = 1} |(A^H \bf{x})^H (A^H \bf{x})|
\end{align*}
\]</span></p>
<p><em>Claim.</em></p>
<p><span class="math display">\[
\norm{A^H}_2 = \norm{A}_2
\]</span></p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A^H}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H \bf{x}| \\
&amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{x}^H A \bf{y}| &amp; \{ |\overline \alpha| = |\alpha| \}\\
&amp;= \max_{\norm{\bf{y}}_2 = 1} \norm{A \bf{y}}_2 \\
&amp;= \norm{A}_2
\end{align*}
\]</span></p>
<p><em>Claim</em></p>
<p><span class="math display">\[
\norm{A^H A}_2 = \norm{A}_2^2
\]</span></p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A^H A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H A \bf{x}|\\
&amp;\leq \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} \norm{\bf{y}^H A^H}_2 \norm{A\bf{x}}_2 &amp; \{ \text{ Cauchy-Schwarz }\}\\
&amp;= \max_{\norm{\bf{y}}_2 = 1} \norm{A\bf{y}}_2 \max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2  &amp; \{ \norm{A^H}_2 = \norm{A}_2 \}\\
&amp;= \norm{A}_2^2
\end{align*}
\]</span></p>
<p>Moreover:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A^H A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H A \bf{x}|\\
&amp;\geq \max_{\norm{\bf{x}}_2 = 1}  |\bf{x}^H A^H A \bf{x}| \{ \text{ Restrict the choices of }\bf{y}\}\\
&amp;=  \max_{\norm{\bf{x}}_2 = 1}  |(A\bf{x})^H (A \bf{x})| \\
&amp;=  \max_{\norm{\bf{x}}_2 = 1}  \norm{A\bf{x}}_2^2\\
&amp;= \norm{A}_2^2
\end{align*}
\]</span></p>
<div id="exr-properties-of-two-norm-4" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5</strong></span> Partition</p>
<p><span class="math display">\[
A = \left[
    \begin{array}{c|c|c}
        A_{1,1} &amp; \ldots &amp; A_{1,N}\\
        \hline
        \vdots &amp; &amp; \vdots\\
        \hline
        A_{M,1} &amp; \ldots &amp; A_{M,N}
    \end{array}
\right]
\]</span></p>
<p>Prove that <span class="math inline">\(\norm{A_{i,j}}_2 \leq \norm{A}_2\)</span>.</p>
</div>
<p><em>Proof.</em></p>
<p>By definition,</p>
<p><span class="math display">\[
\begin{align*}
\norm{A_{i,j}}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A_{i,j} \bf{x}|
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1\)</span> is a compact set, the above maximum exists. There exists <span class="math inline">\(\bf{w}_i\)</span> and <span class="math inline">\(\bf{v}_j\)</span>, satisfying <span class="math inline">\(\norm{\bf{w}_i}_2 = \norm{\bf{v}_j}_2 = 1\)</span> such that:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A_{i,j}}_2 = |\bf{w}_i^H A_{i,j} \bf{v}_j|
\end{align*}
\]</span></p>
<p>Next, we choose</p>
<p><span class="math display">\[
\bf{w} = \left[
    \begin{array}{c}
    0 \\
    \hline
    0 \\
    \hline
    \vdots \\
    \hline
    \bf{w}_i\\
    \hline
    0 \\
    \hline
    \vdots\\
    0
    \end{array}
\right] \quad
\bf{v} = \left[
    \begin{array}{c}
    0 \\
    \hline
    0 \\
    \hline
    \vdots \\
    \hline
    \bf{v}_j\\
    \hline
    0 \\
    \hline
    \vdots\\
    0
    \end{array}
\right]
\]</span></p>
<p>Consider:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|\\
&amp; \geq |\bf{w}^H A \bf{v}|\\
&amp;= |\bf{w}_j^H A_{i,j} \bf{v}_i|\\
&amp;= \norm{A_{i,j}}_2
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="computing-the-matrix-1-norm-and-infty-norm" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-matrix-1-norm-and-infty-norm">Computing the matrix <span class="math inline">\(1\)</span>-norm and <span class="math inline">\(\infty\)</span>-norm</h3>
<p>The matrix <span class="math inline">\(1\)</span>-norm and the matrix <span class="math inline">\(\infty\)</span>-norm are of great importance, because, unlike the matrix <span class="math inline">\(2\)</span>-norm, they are easy and relatively cheap to compute. The following exercises show how to practically compute the matrix <span class="math inline">\(1\)</span>-norm and <span class="math inline">\(\infty\)</span>-norm.</p>
<div id="exr-exercise-on-matrix-1-norm-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6</strong></span> Let <span class="math inline">\(A = \C^{m \times n}\)</span> and partition <span class="math inline">\(A = [a_1 | a_2 | \ldots | a_n]\)</span>. Prove that</p>
<p><span class="math display">\[
\norm{A}_1 = \max_{1 \leq j \leq n}\norm{a_j}_1
\]</span></p>
</div>
<p><em>Proof</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_1 &amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{A\bf{x}}_1 &amp; \{ \text{ Definition }\}\\
&amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{a_1 x_1 + a_2 x_2 + \ldots + a_n x_n}_1 &amp; \{ \text{ Algebra }\}\\
&amp;\leq \max_{\norm{\bf{x}}_1 = 1} \norm{a_1 x_1}_1 + \norm{a_2 x_2}_1 + \ldots + \norm{a_n x_n}_1 &amp; \{ \text{ Triangle Inequality }\}\\
&amp;= \max_{\norm{\bf{x}}_1 = 1} |x_1| \norm{a_1}_1 + |x_2| \norm{a_2}_1 + \ldots + |x_n| \norm{a_n}_1  &amp; \{ \text{ Homogeneity }\}\\
&amp;= \max_{\norm{\bf{x}}_1 = 1}  |x_1| (\max_{1 \leq j \leq n} \norm{a_j}_1) + |x_2| (\max_{1 \leq j \leq n} \norm{a_j}_1)+ \ldots + |x_n| (\max_{1 \leq j \leq n} \norm{a_j}_1)\\
&amp;= \max_{1 \leq j \leq n} \norm{a_j}_1 \max_{\norm{\bf{x}}_1 = 1} \sum_{j=1}^n |x_j|\\
&amp;= \max_{1 \leq j \leq n} \norm{a_j}_1
\end{align*}
\]</span></p>
<p>On the other hand,</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_1  &amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{A\bf{x}}_1 &amp; \{ \text{ Definition }\}\\
&amp;\geq \norm{A\bf{e}_j}_1 &amp; \{ \text{ Specific vector }\}\\
&amp;= \norm{a_j}_1
\end{align*}
\]</span></p>
<p>This concludes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-exercise-on-matrix-infty-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7</strong></span> Let <span class="math inline">\(A = \C^{m \times n}\)</span> and partition</p>
<p><span class="math display">\[
A = \left[
    \begin{array}{c}
        \tilde{a}_0^T\\
        \hline
        \tilde{a}_1^T\\
        \hline
        \vdots\\
        \hline
        \tilde{a}_{m-1}^T\\
    \end{array}
\right]
\]</span></p>
<p>Prove that</p>
<p><span class="math display">\[
\norm{A}_\infty = \max_{0\leq i &lt; m} \norm{\tilde{a}_i}_1 = \max_{0 \leq i &lt; m} (|\alpha_{i,0}| + |\alpha_{i,1}| + \ldots + |\alpha_{i,n-1}|)
\]</span></p>
</div>
<p>*Notice that in this exercise, <span class="math inline">\(\tilde{a}_i\)</span> is really <span class="math inline">\((\tilde{a}_i^T)^T\)</span>, since <span class="math inline">\(\tilde{a}_i^T\)</span> is the label for the <span class="math inline">\(i\)</span>-th row of the matrix.</p>
<p><em>Proof</em>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_\infty &amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{A\bf{x}}_\infty &amp; \{ \text{ Definition }\} \\
&amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{\left[
    \begin{array}{c}
        \tilde{a}_0^T \bf{x}\\
        \hline
        \tilde{a}_1^T \bf{x}\\
        \hline
        \vdots\\
        \hline
        \tilde{a}_{m-1}^T \bf{x}\\
    \end{array}
\right]}_\infty &amp; \{ \text{ Algebra }\}\\
&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} |\tilde{a}_i^T \bf{x}|\\
&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} |\alpha_{i,0}x_0 + \ldots + \alpha_{i,n-1}x_{n-1}|\\
&amp;\leq  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} \left( |\alpha_{i,0}x_0 | + \ldots + |\alpha_{i,n-1}x_{n-1}| \right) &amp; \{ \text{ Triangle Inequality }\}\\
&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} \left( |\alpha_{i,0}||x_0 | + \ldots + |\alpha_{i,n-1}||x_{n-1}| \right) &amp; \{ \text{ Algebra }\}\\
&amp;\leq \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m}\left( |\alpha_{i,0}|\norm{\bf{x}}_\infty + \ldots + |\alpha_{i,n-1}|\norm{\bf{x}}_\infty \right) &amp; \{ |x_i| \leq \norm{\bf{x}}_\infty \}\\
&amp;= \max_{0 \leq i &lt; m} ( |\alpha_{i,0}| + \ldots + |\alpha_{i,n-1}|) \max_{\norm{\bf{x}}_\infty = 1} \norm{\bf{x}}_\infty\\
&amp;= \max_{0 \leq i &lt; m} ( |\alpha_{i,0}| + \ldots + |\alpha_{i,n-1}|)\\
&amp;= \max_{0 \leq i &lt; m}
\norm{\tilde{a}_i}_1
\end{align*}
\]</span></p>
<p>We also want to show that <span class="math inline">\(\norm{A}_\infty \geq \max_{0 \leq i &lt; m} \norm{\tilde{a}_i}_1\)</span>. Let <span class="math inline">\(k\)</span> be such that <span class="math inline">\(\max_{0 \leq i &lt; m}\norm{\tilde{a}_i}_1 = \norm{\tilde{a}_k}_1\)</span> and pick <span class="math inline">\(\bf{y} = \left(\begin{array}{c}\psi_0\\ \vdots\\ \psi_{n-1}\end{array}\right)\)</span> so that <span class="math inline">\(\tilde{a}_k^T \bf{y} = |\alpha_{k,0}| + |\alpha_{k,1}| + \ldots + |\alpha_{k,n-1}|=\norm{\tilde{a}_k}_1\)</span>. This is a matter of picking <span class="math inline">\(\psi_j = |\alpha_{k,j}|/\alpha_{k,j}\)</span>. Then, <span class="math inline">\(|\psi_j| = 1\)</span> and hence, <span class="math inline">\(\norm{\bf{y}}_\infty = 1\)</span> and <span class="math inline">\(\psi_j \alpha_{k,j} = |\alpha_{k,j}|\)</span>. Then:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_\infty &amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{A\bf{x}}_\infty &amp; \{ \text{ Definition }\} \\
&amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{\left[
    \begin{array}{c}
        \tilde{a}_0^T \bf{x}\\
        \hline
        \tilde{a}_1^T \bf{x}\\
        \hline
        \vdots\\
        \hline
        \tilde{a}_{m-1}^T \bf{x}\\
    \end{array}
\right]}_\infty &amp; \{ \text{ Expose rows }\}\\
&amp;\geq  \norm{\left[
    \begin{array}{c}
        \tilde{a}_0^T \bf{y}\\
        \hline
        \tilde{a}_1^T \bf{y}\\
        \hline
        \vdots\\
        \hline
        \tilde{a}_{m-1}^T \bf{y}\\
    \end{array}
\right]}_\infty &amp; \{ \text{ Specific vector }\}\\
&amp;\geq |\tilde{a}_k^T \bf{y}|\\
&amp;= \norm{\tilde{a}_k}_1 \\
&amp;= \max_{0 \leq i &lt; m} \norm{\tilde{a}_i}_1
\end{align*}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-computing-matrix-norms" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 8</strong></span> Fill out the following table:</p>
<p><span class="math display">\[
\begin{array}{|c|c|c|c|}
\hline
A &amp; \norm{A}_1 &amp; \norm{A}_\infty &amp; \norm{A}_F &amp; \norm{A}_2\\
\hline
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}\\
\hline
\begin{bmatrix}
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1
\end{bmatrix}\\
\hline
\begin{bmatrix}
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0
\end{bmatrix}\\
\hline
\end{array}
\]</span></p>
</div>
<p><em>Solution</em>.</p>
<p>Let</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<p>We have, <span class="math inline">\(\norm{A}_1 = 1\)</span>, <span class="math inline">\(\norm{A}_\infty = 1\)</span>, <span class="math inline">\(\norm{A}_F = \sqrt{3}\)</span>. Since this is a diagonal matrix, <span class="math inline">\(\norm{A}_2 = \max_{0 \leq i \leq 2} |d_{i}|\)</span> = 1.</p>
<p>Next, consider:</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1\\
1 &amp; 1 &amp; 1
\end{bmatrix}
\]</span></p>
<p>We have, <span class="math inline">\(\norm{A}_1 = 4\)</span>, <span class="math inline">\(\norm{A}_\infty = 3\)</span>, <span class="math inline">\(\norm{A}_F = \sqrt{12}\)</span>.</p>
<p>Note that, we can write</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 \\
1 \\
1 \\
1 \\
\end{bmatrix} [1, 1, 1, 1] = \bf{x}\bf{y}^H
\]</span></p>
<p>where <span class="math inline">\(\bf{x} = \bf{y} = \begin{bmatrix}
1 \\
1 \\
1 \\
1 \\
\end{bmatrix}\)</span>. Using the property that, <span class="math inline">\(\norm{\bf{x}\bf{y}^H}_2 = \norm{\bf{x}}_2 \norm{\bf{y}}_2\)</span>, we have that, <span class="math inline">\(\norm{A}_2 = 4\)</span>.</p>
<p>Finally, if</p>
<p><span class="math display">\[
A = \begin{bmatrix}
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0
\end{bmatrix}
\]</span></p>
<p>we find that <span class="math inline">\(\norm{A}_1 = 3\)</span>, <span class="math inline">\(\norm{A}_\infty = 1\)</span>, <span class="math inline">\(\norm{A}_F = \sqrt{3}\)</span>. Finally, let <span class="math inline">\(\bf{x} = \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}\)</span> and <span class="math inline">\(\bf{y} = \begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix}\)</span>. Then, <span class="math inline">\(A = \bf{x}\bf{y}^H\)</span>. So, <span class="math inline">\(\norm{A}_2 = \norm{\bf{x}}_2 \norm{\bf{y}}_2 = \sqrt{3}\)</span>.</p>
</section>
<section id="equivalence-of-matrix-norms" class="level3">
<h3 class="anchored" data-anchor-id="equivalence-of-matrix-norms">Equivalence of matrix norms</h3>
<p>We saw that vector norms are equivalent in the sense that if a vector is <em>small</em> in one norm, it is small in all other norms and if it is large in one norm, it is large in all other norms. The same is true for matrix norms.</p>
<div id="thm-equivalence-of-matrix-norms" class="theorem">
<p><span class="theorem-title"><strong>Theorem 12 (Equivalence of matrix norms)</strong></span> Let <span class="math inline">\(\norm{\cdot} : \C^{m \times n} \to \R\)</span> and <span class="math inline">\(|||\cdot|||:\C^{m \times n} \to \R\)</span> both be matrix norms. Then, there exist positive scalars <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tau\)</span> such that for all <span class="math inline">\(A \in \C^{m \times n}\)</span></p>
<p><span class="math display">\[
\sigma \norm{A} \leq |||A||| \leq \tau \norm{A}
\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>The proof again builds on the fact that the supremum over a compact set is achieved and can be replaced by the maximum. We will prove that there exists <span class="math inline">\(\tau\)</span> such that for all <span class="math inline">\(A \in \C^{m \times n}\)</span></p>
<p><span class="math display">\[
|||A||| \leq \tau \norm{A}
\]</span></p>
<p>Let <span class="math inline">\(A \in \C^{m \times n}\)</span> be an arbitrary matrix. Assume that <span class="math inline">\(A \neq 0\)</span> (the zero matrix). Then:</p>
<p><span class="math display">\[
\begin{align*}
|||A||| &amp;= \frac{|||A|||}{\norm{A}} \cdot \norm{A} &amp; \{\text{Algebra}\}\\
&amp;\leq \sup_{Z \neq 0} \left(\frac{|||Z|||}{\norm{Z}}\right) \norm{A} &amp; \{\text{Definition of supremum}\}\\
&amp;= \sup_{Z \neq 0} \left(\Biggl|\Biggl|\Biggl|\frac{Z}{\norm{Z}}\Biggr|\Biggr|\Biggr|\right) \norm{A} &amp; \{\text{Homogeneity}\}\\
&amp;= \left(\sup_{\norm{B} = 1} |||B||| \right) \norm{A} &amp;\{ \text{change of variables }B=Z/\norm{Z}\}\\
&amp;= \left(\max_{\norm{B}=1}|||B|||\right) \norm{A} &amp; \{\text{the set }\norm{B} = 1\text{ is compact}\}
\end{align*}
\]</span></p>
<p>So, we can choose <span class="math inline">\(\tau = \max_{\norm{B}=1} |||B|||\)</span>.</p>
<p>Also, from the above proof, we deduce that, there exists <span class="math inline">\(\sigma\)</span> given by:</p>
<p><span class="math display">\[
\sigma = \frac{1}{\max_{|||B|||=1}||B||}
\]</span></p>
<p>such that:</p>
<p><span class="math display">\[
\sigma \norm{A} \leq |||A|||
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-matrix-2-norm-bounded-by-frobenius-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 9</strong></span> Given <span class="math inline">\(A \in \C^{m \times n}\)</span>, show that <span class="math inline">\(\norm{A}_2 \leq \norm{A}_F\)</span>. For what matrix, is the equality attained?</p>
</div>
<p><em>Solution</em>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_2^2 &amp;= \max_{\norm{x}_2 = 1} \norm{Ax}_2^2  &amp; \{\text{Definition}\}\\
&amp;= \max_{\norm{x}_2 = 1} \norm{\begin{bmatrix}
\sum_{j=0}^{n-1} a_{0,j} x_j \\
\sum_{j=0}^{n-1} a_{1,j} x_j \\
\vdots\\
\sum_{j=0}^{n-1} a_{m-1,j} x_j
\end{bmatrix}
}_2^2\\
&amp;= \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \Biggl|\sum_{j=0}^{n-1} a_{i,j} x_j\Biggr|^2\\
&amp;\leq \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j} x_j|\right)^2 &amp; \{\text{Triangle Inequality}\}\\
&amp;\leq \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left\{\left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right)^{1/2} \left(\sum_{j=0}^{n-1}|x_j|^2\right)^{1/2}\right\}^2 &amp; \{\text{Cauchy-Schwarz}\}\\
&amp;=\max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right) \left(\sum_{j=0}^{n-1}|x_j|^2\right) &amp; \{\text{Simplify}\}\\
&amp;=\sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right) &amp; \{\norm{x}_2 = 1\}\\
&amp;= \norm{A}_F
\end{align*}
\]</span></p>
<p>Also, consider</p>
<p><span class="math display">\[A = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}\]</span></p>
<p>Then, <span class="math inline">\(\norm{A}_2 = \norm{A}_F = 1\)</span>. So, the inequality <span class="math inline">\(\norm{A}_2 \leq \norm{A}_F\)</span> is tight. This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-matrix-norm-equivalences" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 10</strong></span> Let <span class="math inline">\(A \in \C^{m \times n}\)</span>. The following table summarizes the equivalences of various matrix norms:</p>
<p><span class="math display">\[
\begin{array}{c|c|c|c}
&amp; \norm{A}_1 \leq \sqrt{m}\norm{A}_2 &amp; \norm{A}_1 \leq m \norm{A}_\infty &amp; \norm{A}_1 \leq \sqrt{m}\norm{A}_F \\
\hline
\norm{A}_2 \leq \sqrt{n}\norm{A}_1 &amp; &amp; \norm{A}_2 \leq \sqrt{m}\norm{A}_\infty &amp; \norm{A}_2 \leq \norm{A}_F \\
\hline
\norm{A}_\infty \leq n \norm{A}_1 &amp; \norm{A}_\infty \leq \sqrt{n} \norm{A}_2 &amp; &amp; \norm{A}_\infty \leq \sqrt{n}\norm{A}_F\\
\hline
\norm{A}_F \leq \sqrt{n} \norm{A}_1 &amp; \norm{A}_F \leq \tau \norm{A}_2 &amp; \norm{A}_F \leq \sqrt{m}\norm{A}_\infty
\end{array}
\]</span></p>
<p>For each, prove the inequality, including that it is a tight inequality for some nonzero <span class="math inline">\(A\)</span>. (Skip <span class="math inline">\(\norm{A}_F \leq \tau \norm{A}_2\)</span>, we revisit it in a later post)</p>
</div>
<p><em>Solution</em>.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_1 \leq \sqrt{m} \norm{A}_2\)</span>.</p>
<p>Partition <span class="math inline">\(A = [a_0 | a_1 | \ldots | a_{n-1}]\)</span>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_1 &amp;= \max_{0 \leq j &lt; n} \norm{a_j}_1 &amp; \{\text{Definition}\}\\
&amp;= \max_{0 \leq j &lt; n} \norm{
    \begin{bmatrix}
    \alpha_{0,j}\\
    \alpha_{1,j}\\
    \vdots\\
    \alpha_{m-1,j}
    \end{bmatrix}
}_1\\
&amp;= \max_{0 \leq j &lt; n} \sum_{i=0}^{m-1}|\alpha_{i,j}| \cdot |1|\\
&amp;= \max_{0 \leq j &lt; n} \left(\sum_{i=0}^{m-1}|\alpha_{i,j}|^2\right)^{1/2} \left(\sum_{i=0}^{m-1}|1|^2\right)^{1/2} &amp; \{\text{Cauchy-Schwarz}\}\\
&amp;= \max_{0 \leq j &lt; n} \norm{a_j}_2 \sqrt{m}\\
&amp;= \max_{0 \leq j &lt; n} \norm{A}_2 \sqrt{m} &amp; \{\norm{A_{i,j}}_2 \leq \norm{A}_2\}\\
&amp;= \sqrt{m} \norm{A}_2
\end{align*}
\]</span></p>
<p>Moreover, consider the matrix</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 &amp; 0 \\
1 &amp; 0
\end{bmatrix} =
\begin{bmatrix}
1 \\
1
\end{bmatrix}
\begin{bmatrix}
1 &amp; 0
\end{bmatrix}
\]</span></p>
<p>We have, <span class="math inline">\(\norm{A}_2 = \sqrt{2}\)</span> and <span class="math inline">\(\norm{A}_1 = 2\)</span>, so <span class="math inline">\(\norm{A}_1 = \sqrt{2}\norm{A}_2\)</span>. Thus, the inequality is tight. This closes the proof.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_1 \leq m \norm{A}_\infty\)</span>.</p>
<p><em>Solution</em>.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_1 &amp;= \max_{0 \leq j &lt; n} \norm{a_j}_1 &amp; \{\text{Definition}\}\\
&amp;= \max_{0 \leq j &lt; n} \sum_{i=0}^{m-1}|\alpha_{i,j}|\\
&amp;\leq \sum_{j=0}^{n-1} \sum_{i=0}^{m-1} |\alpha_{i,j}| = \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}| \\
&amp;=\sum_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1\\
&amp;= m \max_{0 \leq i &lt; m} \norm{\tilde{a}_i^T}_1\\
&amp;= m \norm{A}_\infty
\end{align}
\]</span></p>
<p>Again, consider the matrix</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 &amp; 0\\
1 &amp; 0
\end{bmatrix}
\]</span></p>
<p><span class="math inline">\(\norm{A}_1 = 2\)</span> and <span class="math inline">\(\norm{A}_\infty = 1\)</span>, so <span class="math inline">\(\norm{A}_1 = 2 \norm{A}_\infty\)</span>. Hence, the inequality is tight. This closes the proof.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_1 \leq \sqrt{m}\norm{A}_F\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have shown that:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_1 &amp;\leq \sqrt{m}\norm{A}_2 \\
\norm{A}_2 &amp;\leq \norm{A}_F
\end{align}
\]</span></p>
<p>So, we deduce that <span class="math inline">\(\norm{A}_1 \leq \sqrt{m}\norm{A}_F\)</span>. Moreover, consider</p>
<p><span class="math display">\[
A = \sqrt{2}I
\]</span></p>
<p>Then, <span class="math inline">\(\norm{A}_1 = \sqrt{2}\)</span> and <span class="math inline">\(\norm{A}_F = 2\)</span>, so <span class="math inline">\(\norm{A}_1 = \sqrt{2}\norm{A}_F\)</span>. Hence, the inequality is tight.</p>
<p><em>Claim.</em> Our claim is that <span class="math inline">\(\norm{A}_2 \leq \sqrt{n}\norm{A}_1\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_2 &amp;= \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} &amp; \{\text{Definition}\}\\
&amp;= \max_{x \neq 0} \frac{\norm{Ax}_1}{\norm{x}_2}  &amp; \{\norm{z}_2 \leq \norm{z}_1\} \\
&amp;= \max_{x \neq 0} \frac{\norm{Ax}_1}{\frac{1}{\sqrt{n}}\norm{x}_1}  &amp; \{\norm{z}_1 \leq \sqrt{n}\norm{z}_2\} \\
&amp;= \sqrt{n}\norm{A}_1
\end{align}
\]</span></p>
<p>Again, consider the matrix <span class="math inline">\(A = [1 | 1| \ldots | 1]\)</span>. Then, <span class="math inline">\(\norm{A}_2 = \sqrt{n}\)</span> and <span class="math inline">\(\norm{A}_1 = 1\)</span>. So, <span class="math inline">\(\norm{A}_2 = \sqrt{n}\norm{A}_1\)</span>.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_2 \leq \sqrt{m} \norm{A}_\infty\)</span>.</p>
<p><em>Solution.</em></p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_2 &amp;= \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} &amp;\{\text{Definition}\}\\
&amp;=\max_{x \neq 0} \frac{\norm{\begin{bmatrix}\tilde{a}_0^T \\ \tilde{a}_1^T \\ \vdots \\ \tilde{a}_{m-1}^T\end{bmatrix}x}_2}{\norm{x}_2} &amp;\{\text{Expose rows}\}\\
&amp;=\max_{x \neq 0} \frac{\norm{\begin{bmatrix}\tilde{a}_0^T x \\ \tilde{a}_1^T x\\ \vdots \\ \tilde{a}_{m-1}^T x\end{bmatrix}}_2}{\norm{x}_2} &amp;\{\text{Algebra}\}\\
&amp;\leq \max_{x \neq 0} \frac{\sqrt{m}\norm{Ax}_\infty}{\norm{x}_2} &amp;\{\norm{z}_2 \leq \sqrt{n}\norm{z}_\infty\}\\
&amp; \leq \max_{x \neq 0} \frac{\sqrt{m}\norm{Ax}_\infty}{\norm{x}_\infty} &amp;\{\norm{z}_\infty \leq \norm{z}_2\}\\
&amp;= \sqrt{m} \norm{A}_\infty
\end{align*}
\]</span></p>
<p>Moreover, consider the matrix</p>
<p><span class="math display">\[
A = \begin{bmatrix}
1 \\
1 \\
\vdots \\
1
\end{bmatrix}
\]</span></p>
<p>We have <span class="math inline">\(\norm{A}_2 = \sqrt{m}\)</span>, <span class="math inline">\(\norm{A}_\infty = 1\)</span>, so <span class="math inline">\(\norm{A}_2 = \sqrt{m}\norm{A}_\infty\)</span>. Hence, the inequality is tight.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_\infty \leq n \norm{A}_1\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_\infty &amp;= \max_{x \neq 0} \frac{\norm{Ax}_\infty}{\norm{x}_\infty} &amp; \{\text{Definition}\}\\
&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_1}{\norm{x}_\infty} &amp; \{\norm{x}_\infty \leq \norm{x}_1\}\\
&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_1}{\frac{1}{n}\norm{x}_1} &amp; \{\norm{x}_1 \leq n \norm{x}_\infty\}\\
&amp;= n \norm{A}_1
\end{align*}
\]</span></p>
<p>Moreover, let <span class="math inline">\(A = [1 | 1 | \ldots | 1]\)</span>. Then, <span class="math inline">\(\norm{A}_\infty = n\)</span> and <span class="math inline">\(\norm{A}_1 = 1\)</span>, so <span class="math inline">\(\norm{A}_\infty = n \norm{A}_1\)</span>. Hence, the inequality is tight.</p>
<p><em>Claim.</em> Our claim is that <span class="math inline">\(\norm{A}_\infty \leq \sqrt{n} \norm{A}_2\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align*}
\norm{A}_\infty &amp;= \max_{x \neq 0} \frac{\norm{Ax}_\infty}{\norm{x}_\infty} &amp; \{\text{Definition}\}\\
&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_\infty} &amp; \{\norm{x}_\infty \leq \norm{x}_2\}\\
&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_2}{\frac{1}{\sqrt{n}}\norm{x}_2} &amp; \{\norm{x}_2 \leq \sqrt{n} \norm{x}_\infty\}\\
&amp;= \sqrt{n} \norm{A}_2
\end{align*}
\]</span></p>
<p>Moreover, let <span class="math inline">\(A = [1|1|\ldots|1]\)</span></p>
<p>Then, <span class="math inline">\(\norm{A}_\infty = n\)</span>, <span class="math inline">\(\norm{A}_2 = \sqrt{n}\)</span> and <span class="math inline">\(\norm{A}_\infty = \sqrt{n} \norm{A}_2\)</span>. So, the bound is tight.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_\infty \leq \sqrt{n}\norm{A}_F\)</span>.</p>
<p><em>Solution.</em> This is true since <span class="math inline">\(\norm{A}_\infty \leq \sqrt{n}\norm{A}_2\)</span> and <span class="math inline">\(\norm{A}_2 \leq \norm{A}_F\)</span>.</p>
<p>Let <span class="math inline">\(A = [1 | 1 | \ldots | 1]\)</span>. Then, <span class="math inline">\(\norm{A}_\infty = n\)</span> and <span class="math inline">\(\norm{A}_F = \sqrt{n}\)</span>. So, <span class="math inline">\(\norm{A}_\infty = \sqrt{n}\norm{A}_F\)</span>. The bound is tight.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_F \leq \sqrt{n} \norm{A}_1\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_F^2 &amp;= \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}|^2 &amp; \{\text{Definition}\}\\
&amp;= \sum_{j=0}^{n-1} \sum_{i=0}^{m-1} |\alpha_{i,j}|^2 \\
&amp;\leq \sum_{j=0}^{n-1} \left(\sum_{i=0}^{m-1} |\alpha_{i,j}| \right)^2 \\
&amp;= \sum_{j=0}^{n-1} \norm{a_j}_1^2 \\
&amp;\leq \sum_{j=0}^{n-1} \max_{j=0}^{n-1} \norm{a_j}_1^2 \\
&amp;= n \norm{A}_1^2
\end{align}
\]</span></p>
<p>Consequently, <span class="math inline">\(\norm{A}_F \leq \sqrt{n}\norm{A}_1\)</span>. Let <span class="math inline">\(A = [1 |1 | \ldots| 1]\)</span>. Then, <span class="math inline">\(\norm{A}_F = \sqrt{n}\)</span> and <span class="math inline">\(\norm{A}_1 = 1\)</span>, so <span class="math inline">\(\norm{A}_F = \sqrt{n}\norm{A}_1\)</span>. Hence, the bound is tight.</p>
<p><em>Claim</em>. Our claim is that <span class="math inline">\(\norm{A}_F \leq \sqrt{m} \norm{A}_\infty\)</span>.</p>
<p><em>Solution.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{A}_F^2 &amp;= \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}|^2 &amp; \{\text{Definition}\}\\
&amp;\leq \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |\alpha_{i,j}| \right)^2 \\
&amp;= \sum_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1^2 \\
&amp;\leq \sum_{j=0}^{n-1} \max_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1^2 \\
&amp;= m \norm{A}_\infty^2
\end{align}
\]</span></p>
<p>Consequently, <span class="math inline">\(\norm{A}_F \leq \sqrt{m}\norm{A}_\infty\)</span>. Let <span class="math inline">\(A = [1, 1, \ldots, 1]^T\)</span>. Then, <span class="math inline">\(\norm{A}_F = \sqrt{m}\)</span> and <span class="math inline">\(\norm{A}_\infty = 1\)</span>, so <span class="math inline">\(\norm{A}_F = \sqrt{m}\norm{A}_1\)</span>. Hence, the bound is tight.</p>
</section>
<section id="sub-multiplicative-norms" class="level3">
<h3 class="anchored" data-anchor-id="sub-multiplicative-norms">Sub-multiplicative norms</h3>
<p>There are a number of properties that we would like a matrix norm to have(but not all matrix norms do). Given a matrix norm <span class="math inline">\(\norm{\cdot} : \C^{m \times n} \to \R\)</span>, we may ask the following question. Do there exist vector norms <span class="math inline">\(\norm{\cdot}_\mu : C^m \to \R\)</span> and <span class="math inline">\(\norm{\cdot}:\C^n \to R\)</span>, such that the matrix norm is an upper bound on how much the non-zero vector <span class="math inline">\(x\)</span> is stretched? That is, the following inequality is satisfied:</p>
<p><span class="math display">\[
\frac{\norm{Ax}_\mu}{\norm{x}_\nu} \leq \norm{A}
\]</span></p>
<p>or equivalently</p>
<p><span class="math display">\[
\norm{Ax}_\mu \leq \norm{A} \norm{x}_\nu
\]</span></p>
<p>where this second formulation has the benefit that it also holds if <span class="math inline">\(x = 0\)</span>.</p>
<div id="def-subordinate-matrix-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 12 (Subordinate matrix norm)</strong></span> A matrix norm <span class="math inline">\(\norm{\cdot}:\C^{m \times n} \to \R\)</span> is said to be subordinate to vector norms <span class="math inline">\(\norm{\cdot}_\mu :\C^m \to \R\)</span> and <span class="math inline">\(\norm{\cdot}_\nu : \C^n \to \R\)</span>, if for all, <span class="math inline">\(x \in \C^n\)</span>,</p>
<p><span class="math display">\[
\norm{Ax}_\mu \leq \norm{A} \norm{x}_\nu
\]</span></p>
<p>If <span class="math inline">\(\norm{\cdot}_\mu\)</span> and <span class="math inline">\(\norm{\cdot}_\nu\)</span> are the same norm (but perhaps for different <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>), then <span class="math inline">\(\norm{\cdot}\)</span> is said to be subordinate to the given vector norm.</p>
</div>
<div id="exr-matrix-2-norm-is-subordinate-to-vector-2-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 11</strong></span> Prove that the matrix <span class="math inline">\(2\)</span>-norm is subordinate to the vector <span class="math inline">\(2\)</span>-norm.</p>
</div>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\(A \in C^{m \times n}\)</span> and let <span class="math inline">\(x \in \C^n\)</span>. Assume that <span class="math inline">\(x \neq 0\)</span>, for if <span class="math inline">\(x = 0\)</span>, then the inequality <span class="math inline">\(\norm{Ax}_2 \leq \norm{A}_2 \norm{x}_2\)</span> is vacuously true.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{Ax}_2&amp;= \left(\frac{\norm{Ax}_2}{\norm{x}_2}\right) \norm{x}_2 &amp; \{x \neq 0\} \\
&amp;\leq \left(\max_{y \neq 0} \frac{\norm{Ay}_2}{\norm{y}_2}\right)\norm{x}_2\\
&amp;= \norm{A}_2 \norm{x}_2
\end{align}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exr-frobenius-norm-is-subordinate-to-vector-2-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 12</strong></span> Prove that the Frobenius norm is subordinate to the vector <span class="math inline">\(2\)</span>-norm.</p>
</div>
<p><em>Proof.</em></p>
<p>We are interested to prove the claim that, <span class="math inline">\((\forall A \in \C^{m\times n})(\forall x \in \C^n)\)</span>:</p>
<p><span class="math display">\[ \norm{Ax}_2 \leq \norm{A}_F \norm{x}_2 \]</span></p>
<p>Again, without loss of generality, we have:</p>
<p><span class="math display">\[
\begin{align}
\norm{Ax}_2^2 &amp;= \norm{
    \begin{bmatrix}
        \sum_{j=0}^{n-1}\alpha_{0,j} x_j \\
        \sum_{j=0}^{n-1}\alpha_{1,j} x_j \\
        \vdots
        \sum_{j=0}^{n-1}\alpha_{m-1,j} x_j
    \end{bmatrix}
}_2^2 &amp; \{\text{Definition}\}\\
&amp;= \sum_{i=0}^{m-1} \Biggl| \sum_{j=0}^{n-1}\alpha_{i,j} x_j \Biggr|^2\\
&amp;= \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1}|\alpha_{i,j} x_j| \right)^2 &amp; \{\text{Triangle Inequality}\}\\
&amp;\leq \sum_{i=0}^{m-1} \left[\left(\sum_{j=0}^{n-1}|\alpha_{i,j}|^2 \right)  \left(\sum_{j=0}^{n-1} |x_j|^2\right)\right] &amp; \{\text{Cauchy-Schwarz}\}\\
&amp;= \left(\sum_{j=0}^{n-1} |x_j|^2\right) \left(\sum_{i=0}^{m-1} \sum_{j=0}^{n-1}|\alpha_{i,j}|^2 \right)   &amp; \{\text{Algebra}\}\\
&amp;= \norm{A}_F^2 \norm{x}_2^2
\end{align}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="thm-induced-matrix-norms-are-subordinate-norms" class="theorem">
<p><span class="theorem-title"><strong>Theorem 13</strong></span> Induced matrix norms, <span class="math inline">\(\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to \R\)</span> are subordinate to the norms, <span class="math inline">\(\norm{\cdot}_\mu\)</span> and <span class="math inline">\(\norm{\cdot}_\nu\)</span> that induce them.</p>
</div>
<p><em>Proof</em>.</p>
<p>Without loss of generality, assume that <span class="math inline">\(x \neq 0\)</span>, otherwise the proposition is vacuously true.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{Ax}_\mu &amp;= \frac{\norm{Ax}_\mu}{\norm{x}_\nu} \norm{x}_\nu \\
&amp;\leq \left(\max_{x \neq 0} \frac{\norm{Ax}_\mu}{\norm{x}_\nu} \right) \norm{x}_\nu \\
&amp;= \left(\max_{y \neq 0} \frac{\norm{Ay}_\mu}{\norm{y}_\nu} \right) \norm{x}_\nu \\
&amp;= \norm{A} \norm{x}_\nu
\end{align}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="cor-matrix-p-norms-are-subordinate" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1</strong></span> Any matrix <span class="math inline">\(p\)</span>-norm is subordinate to the corresponding vector norm.</p>
</div>
<p><em>Proof.</em></p>
<p>Without the loss of generality, assume that <span class="math inline">\(x \neq 0\)</span>. If <span class="math inline">\(x = 0\)</span>, the proposition is vacuously true.</p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{Ax}_p &amp;= \left(\frac{\norm{Ax}_p}{\norm{x}_p} \right) \norm{x}_p &amp; \{ x \neq 0\}\\
&amp;\leq  \left(\max_{x \neq 0} \frac{\norm{Ax}_p}{\norm{x}_p} \right) \norm{x}_p\\
&amp;=  \left(\max_{y \neq 0}\frac{\norm{Ay}_p}{\norm{y}_p} \right) \norm{x}_p\\
&amp;= \norm{A}_p \norm{x}_p
\end{align}
\]</span></p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<p>Another desirable property that not all norms have is that:</p>
<p><span class="math display">\[
\norm{AB} \leq \norm{A} \norm{B}
\]</span></p>
<p>This requires the given norm to be defined for all matrix sizes.</p>
<div id="def-consistent-matrix-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 13 (Consistent matrix norm)</strong></span> A matrix norm <span class="math inline">\(\norm{\cdot} : \C^{m \times n} \to \R\)</span> is said to be consistent matrix norm if it is defined for all <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>, using the same formula for all <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>.</p>
</div>
<div id="def-submulticative-matrix-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 14 (Submultiplicative matrix norm)</strong></span> A consistent matrix norm <span class="math inline">\(\norm{\cdot} : \C^{m \times n} \to \R\)</span> is said to be submultiplicative if it satisfies:</p>
<p><span class="math display">\[
\norm{AB} \leq \norm{A} \norm{B}
\]</span></p>
</div>
<div id="thm-induced-matrix-norms-are-submultiplicative" class="theorem">
<p><span class="theorem-title"><strong>Theorem 14</strong></span> Let <span class="math inline">\(\norm{\cdot} : \C^n \to \R\)</span> be a vector norm defined for all <span class="math inline">\(n\)</span>. Define the corresponding induced matrix norm as:</p>
<p><span class="math display">\[
\norm{A} = \max_{x \neq 0} \frac{\norm{Ax}}{\norm{x}} = \max_{\norm{x} = 1} \norm{Ax}
\]</span></p>
<p>Then, for any <span class="math inline">\(A \in \C^{m \times k}\)</span> and <span class="math inline">\(B^{k \times n}\)</span>, the inequality <span class="math inline">\(\norm{AB} \leq \norm{A} \norm{B}\)</span> holds.</p>
</div>
<p>In other words, induced matrix norms are submultiplicative.</p>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
\norm{AB} &amp;= \max_{\norm{x}=1} \norm{ABx} &amp; \{\text{Definition}\}\\
&amp;= \max_{\norm{x}=1} \norm{A(Bx)} &amp; \{\text{Associativity}\}\\
&amp;\leq \max_{\norm{x}=1} \norm{A} \norm{Bx} &amp; \{\text{Subordinate property}\}\\
&amp;\leq \max_{\norm{x}=1} \norm{A} \norm{B} \norm{x} &amp; \{\text{Subordinate property}\}\\
&amp;= \norm{A} \norm{B} &amp; \{\norm{x}=1\}
\end{align}
\]</span></p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="an">title:</span><span class="co"> "Norms"</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="an">author:</span><span class="co"> "Quasar"</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="an">date:</span><span class="co"> "2024-07-25"</span></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="an">categories:</span><span class="co"> [Numerical Methods]      </span></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="an">image:</span><span class="co"> "image.jpg"</span></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="co">---</span></span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="fu">## Inner product</span></span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a>Consider geometric vectors $\mathbf{x}, \mathbf{y} \in \mathbf{R}^2$. The scalar product(dot-product) of these two vectors is defined by:</span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a>$$</span>
<span id="cb6-16"><a href="#cb6-16"></a>\mathbf{x} \cdot \mathbf{y} = x_1 y_1 + x_2 y_2</span>
<span id="cb6-17"><a href="#cb6-17"></a>$$</span>
<span id="cb6-18"><a href="#cb6-18"></a></span>
<span id="cb6-19"><a href="#cb6-19"></a>An inner-product is a mathematical generalization of the dot-product. </span>
<span id="cb6-20"><a href="#cb6-20"></a></span>
<span id="cb6-21"><a href="#cb6-21"></a>::: {.hidden}</span>
<span id="cb6-22"><a href="#cb6-22"></a>$$</span>
<span id="cb6-23"><a href="#cb6-23"></a> \newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb6-24"><a href="#cb6-24"></a> \newcommand{\normp}<span class="co">[</span><span class="ot">2</span><span class="co">]</span>{\left\lVert\mathbf{#1}\right\rVert_{#2}}</span>
<span id="cb6-25"><a href="#cb6-25"></a> \newcommand\inner<span class="co">[</span><span class="ot">2</span><span class="co">]</span>{\left\langle #1, #2 \right\rangle}</span>
<span id="cb6-26"><a href="#cb6-26"></a> \newcommand{\bf}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathbf{#1}}</span>
<span id="cb6-27"><a href="#cb6-27"></a> \newcommand{\R}{\mathbf{R}}</span>
<span id="cb6-28"><a href="#cb6-28"></a> \newcommand{\RR}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathbf{R}^2}</span>
<span id="cb6-29"><a href="#cb6-29"></a> \newcommand{\RRR}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathbf{R}^3}</span>
<span id="cb6-30"><a href="#cb6-30"></a> \newcommand{\C}{\mathbf{C}}</span>
<span id="cb6-31"><a href="#cb6-31"></a> \newcommand{\CC}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathbf{C}^2}</span>
<span id="cb6-32"><a href="#cb6-32"></a> \newcommand{\CCC}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\mathbf{C}^3}</span>
<span id="cb6-33"><a href="#cb6-33"></a>$$</span>
<span id="cb6-34"><a href="#cb6-34"></a>:::</span>
<span id="cb6-35"><a href="#cb6-35"></a></span>
<span id="cb6-36"><a href="#cb6-36"></a>::: {#def-inner-product}</span>
<span id="cb6-37"><a href="#cb6-37"></a></span>
<span id="cb6-38"><a href="#cb6-38"></a><span class="fu">### Inner product </span></span>
<span id="cb6-39"><a href="#cb6-39"></a></span>
<span id="cb6-40"><a href="#cb6-40"></a>Let $V$ be a vector space and $F$ be a scalar field, which is either $\bf{R}$ or $\bf{C}$. Let $\inner{\cdot}{\cdot}$ be a map from $V\times V \to F$. Then, $\inner{\cdot}{\cdot}$ is an inner product if for all $\bf{u},\bf{v}, \bf{w} \in V$, it satisfies:</span>
<span id="cb6-41"><a href="#cb6-41"></a></span>
<span id="cb6-42"><a href="#cb6-42"></a><span class="fu">#### Positive semi-definite</span></span>
<span id="cb6-43"><a href="#cb6-43"></a></span>
<span id="cb6-44"><a href="#cb6-44"></a>$$</span>
<span id="cb6-45"><a href="#cb6-45"></a>\inner{\bf{v}}{\bf{v}} \geq 0 \quad \text { and } \quad  \inner{\bf{v}}{\bf{v}} = 0 \Longleftrightarrow \bf{v} = \bf{0}</span>
<span id="cb6-46"><a href="#cb6-46"></a>$$</span>
<span id="cb6-47"><a href="#cb6-47"></a></span>
<span id="cb6-48"><a href="#cb6-48"></a><span class="fu">#### Additivity in the first slot</span></span>
<span id="cb6-49"><a href="#cb6-49"></a></span>
<span id="cb6-50"><a href="#cb6-50"></a>$$</span>
<span id="cb6-51"><a href="#cb6-51"></a>\inner{\bf{u} + \bf{v}}{\bf{w}} = \inner{\bf{u}}{\bf{w}} + \inner{\bf{v}}{\bf{w}}</span>
<span id="cb6-52"><a href="#cb6-52"></a>$$</span>
<span id="cb6-53"><a href="#cb6-53"></a></span>
<span id="cb6-54"><a href="#cb6-54"></a><span class="fu">#### Homogeneity</span></span>
<span id="cb6-55"><a href="#cb6-55"></a></span>
<span id="cb6-56"><a href="#cb6-56"></a>$$</span>
<span id="cb6-57"><a href="#cb6-57"></a>\begin{align*}</span>
<span id="cb6-58"><a href="#cb6-58"></a>\inner{\alpha \bf{v}}{\bf{w}} &amp;= \overline{\alpha} \inner{\bf{v}}{\bf{w}}<span class="sc">\\</span></span>
<span id="cb6-59"><a href="#cb6-59"></a>\inner{\bf{v}}{\alpha \bf{w}} &amp;= \alpha \inner{\bf{v}}{\bf{w}}</span>
<span id="cb6-60"><a href="#cb6-60"></a>\end{align*}</span>
<span id="cb6-61"><a href="#cb6-61"></a>$$</span>
<span id="cb6-62"><a href="#cb6-62"></a></span>
<span id="cb6-63"><a href="#cb6-63"></a><span class="fu">#### Conjugate symmetry</span></span>
<span id="cb6-64"><a href="#cb6-64"></a></span>
<span id="cb6-65"><a href="#cb6-65"></a>$$</span>
<span id="cb6-66"><a href="#cb6-66"></a>\inner{\bf{v}}{\bf{w}} = \overline{\inner{\bf{w}}{\bf{v}}}</span>
<span id="cb6-67"><a href="#cb6-67"></a>$$</span>
<span id="cb6-68"><a href="#cb6-68"></a></span>
<span id="cb6-69"><a href="#cb6-69"></a>:::</span>
<span id="cb6-70"><a href="#cb6-70"></a></span>
<span id="cb6-71"><a href="#cb6-71"></a>The most important example of inner-product is the Euclidean inner product on $\C^n$. Let $\bf{w},\bf{z}$ be (column) vectors in $\C^n$. </span>
<span id="cb6-72"><a href="#cb6-72"></a></span>
<span id="cb6-73"><a href="#cb6-73"></a>$$</span>
<span id="cb6-74"><a href="#cb6-74"></a>\inner{\bf{w}}{\bf{z}} = (\bf{w}^H \bf{z}) =  \overline{w_1}z_1 + \overline{w_2}z_2 + \ldots + \overline{w_n} z_n</span>
<span id="cb6-75"><a href="#cb6-75"></a>$$</span>
<span id="cb6-76"><a href="#cb6-76"></a></span>
<span id="cb6-77"><a href="#cb6-77"></a>Firstly,</span>
<span id="cb6-78"><a href="#cb6-78"></a></span>
<span id="cb6-79"><a href="#cb6-79"></a>$$</span>
<span id="cb6-80"><a href="#cb6-80"></a>\begin{align*}</span>
<span id="cb6-81"><a href="#cb6-81"></a>\inner{\bf{v} + \bf{w}}{\bf{z}} &amp;= (\bf{v} + \bf{w})^H \bf{z} &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-82"><a href="#cb6-82"></a>&amp;= (\bf{v}^H + \bf{w}^H)\bf{z} &amp; <span class="sc">\{</span> \overline{z_1 + z_2} = \overline{z_1} + \overline{z_2}; z_1,z_2\in \C <span class="sc">\}\\</span></span>
<span id="cb6-83"><a href="#cb6-83"></a>&amp;= \bf{v}^H \bf{z} + \bf{w}^H \bf{z}<span class="sc">\\</span></span>
<span id="cb6-84"><a href="#cb6-84"></a>&amp;= \inner{\bf{v}}{\bf{z}} + \inner{\bf{w}}{\bf{z}}</span>
<span id="cb6-85"><a href="#cb6-85"></a>\end{align*}</span>
<span id="cb6-86"><a href="#cb6-86"></a>$$</span>
<span id="cb6-87"><a href="#cb6-87"></a></span>
<span id="cb6-88"><a href="#cb6-88"></a>So, it is additive in the first slot.</span>
<span id="cb6-89"><a href="#cb6-89"></a></span>
<span id="cb6-90"><a href="#cb6-90"></a>Next, let $\alpha \in \C$.</span>
<span id="cb6-91"><a href="#cb6-91"></a></span>
<span id="cb6-92"><a href="#cb6-92"></a>$$</span>
<span id="cb6-93"><a href="#cb6-93"></a>\begin{align*}</span>
<span id="cb6-94"><a href="#cb6-94"></a>\inner{\alpha\bf{u}}{\bf{v}} &amp;= (\alpha \bf{u})^H \bf{v}  &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-95"><a href="#cb6-95"></a>&amp;= \overline{\alpha} \bf{u}^H \bf{v} = \overline{\alpha} \inner{\bf{u}}{\bf{v}}</span>
<span id="cb6-96"><a href="#cb6-96"></a>\end{align*}</span>
<span id="cb6-97"><a href="#cb6-97"></a>$$</span>
<span id="cb6-98"><a href="#cb6-98"></a></span>
<span id="cb6-99"><a href="#cb6-99"></a>and</span>
<span id="cb6-100"><a href="#cb6-100"></a></span>
<span id="cb6-101"><a href="#cb6-101"></a>$$</span>
<span id="cb6-102"><a href="#cb6-102"></a>\begin{align*}</span>
<span id="cb6-103"><a href="#cb6-103"></a>\inner{\bf{u}}{\alpha\bf{v}} &amp;= (\bf{u})^H \bf{ \alpha v}  &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-104"><a href="#cb6-104"></a>&amp;= \alpha \bf{u}^H \bf{v} = \alpha \inner{\bf{u}}{\bf{v}}</span>
<span id="cb6-105"><a href="#cb6-105"></a>\end{align*}</span>
<span id="cb6-106"><a href="#cb6-106"></a>$$</span>
<span id="cb6-107"><a href="#cb6-107"></a></span>
<span id="cb6-108"><a href="#cb6-108"></a>It is homogenous.</span>
<span id="cb6-109"><a href="#cb6-109"></a></span>
<span id="cb6-110"><a href="#cb6-110"></a>Finally, </span>
<span id="cb6-111"><a href="#cb6-111"></a></span>
<span id="cb6-112"><a href="#cb6-112"></a>$$</span>
<span id="cb6-113"><a href="#cb6-113"></a>\begin{align*}</span>
<span id="cb6-114"><a href="#cb6-114"></a>\inner{\bf{v}}{\bf{w}} &amp;= \sum_{i=1}^n \overline{v_i}w_i<span class="sc">\\</span></span>
<span id="cb6-115"><a href="#cb6-115"></a>&amp;= \sum_{i=1}^n \overline{v_i \overline{w_i}}<span class="sc">\\</span></span>
<span id="cb6-116"><a href="#cb6-116"></a>&amp;= \overline{\left(\sum_{i=1}^n \overline{w_i} v_i\right)}<span class="sc">\\</span></span>
<span id="cb6-117"><a href="#cb6-117"></a>&amp;= \overline{\inner{\bf{w}}{\bf{v}}}</span>
<span id="cb6-118"><a href="#cb6-118"></a>\end{align*}</span>
<span id="cb6-119"><a href="#cb6-119"></a>$$</span>
<span id="cb6-120"><a href="#cb6-120"></a></span>
<span id="cb6-121"><a href="#cb6-121"></a><span class="fu">## Norms</span></span>
<span id="cb6-122"><a href="#cb6-122"></a></span>
<span id="cb6-123"><a href="#cb6-123"></a>Very often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length. </span>
<span id="cb6-124"><a href="#cb6-124"></a></span>
<span id="cb6-125"><a href="#cb6-125"></a>::: {#def-vector-norm}</span>
<span id="cb6-126"><a href="#cb6-126"></a></span>
<span id="cb6-127"><a href="#cb6-127"></a><span class="fu">### Vector norm </span></span>
<span id="cb6-128"><a href="#cb6-128"></a>Let $\nu:V \to \mathbf{R}$. Then, $\nu$ is a (vector) norm if for all $\mathbf{x},\mathbf{y}\in V$ and for all $\alpha \in \mathbf{C}$, $\nu(\cdot)$ satisfies:</span>
<span id="cb6-129"><a href="#cb6-129"></a></span>
<span id="cb6-130"><a href="#cb6-130"></a><span class="fu">#### Positive Semi-Definiteness</span></span>
<span id="cb6-131"><a href="#cb6-131"></a></span>
<span id="cb6-132"><a href="#cb6-132"></a>$$\nu(\mathbf{x}) \geq 0, \quad \forall \bf{x}\in V$$ </span>
<span id="cb6-133"><a href="#cb6-133"></a></span>
<span id="cb6-134"><a href="#cb6-134"></a>and </span>
<span id="cb6-135"><a href="#cb6-135"></a></span>
<span id="cb6-136"><a href="#cb6-136"></a>$$\nu(\mathbf{x})=0 \Longleftrightarrow \mathbf{x}=\mathbf{0}$$</span>
<span id="cb6-137"><a href="#cb6-137"></a></span>
<span id="cb6-138"><a href="#cb6-138"></a><span class="fu">#### Homogeneity</span></span>
<span id="cb6-139"><a href="#cb6-139"></a></span>
<span id="cb6-140"><a href="#cb6-140"></a>$$\nu(\alpha \mathbf{x}) = |\alpha|\nu(\mathbf{x})$$</span>
<span id="cb6-141"><a href="#cb6-141"></a></span>
<span id="cb6-142"><a href="#cb6-142"></a><span class="fu">#### Triangle inequality</span></span>
<span id="cb6-143"><a href="#cb6-143"></a></span>
<span id="cb6-144"><a href="#cb6-144"></a>$$\nu(\mathbf{x} + \mathbf{y}) \leq \nu(\mathbf{x}) + \nu(\mathbf{y})$$</span>
<span id="cb6-145"><a href="#cb6-145"></a>:::</span>
<span id="cb6-146"><a href="#cb6-146"></a></span>
<span id="cb6-147"><a href="#cb6-147"></a><span class="fu">### The vector $2-$norm</span></span>
<span id="cb6-148"><a href="#cb6-148"></a></span>
<span id="cb6-149"><a href="#cb6-149"></a>The length of a vector is most commonly measured by the *square root of the sum of the squares of the components of the vector*, also known as the *euclidean norm*.</span>
<span id="cb6-150"><a href="#cb6-150"></a></span>
<span id="cb6-151"><a href="#cb6-151"></a>::: {#def-euclidean-norm}</span>
<span id="cb6-152"><a href="#cb6-152"></a></span>
<span id="cb6-153"><a href="#cb6-153"></a><span class="fu">### Vector $2-$norm</span></span>
<span id="cb6-154"><a href="#cb6-154"></a></span>
<span id="cb6-155"><a href="#cb6-155"></a>The vector $2-$ norm, $||\cdot||:\mathbf{C}^n \to \mathbf{R}$ is defined for $\mathbf{x}\in\mathbf{C}^n$ by:</span>
<span id="cb6-156"><a href="#cb6-156"></a></span>
<span id="cb6-157"><a href="#cb6-157"></a>$$</span>
<span id="cb6-158"><a href="#cb6-158"></a>\norm{\bf{x}}_2 = \sqrt{|\chi_1|^2 + |\chi_2|^2 + |\chi_n|^2} = \sqrt{\sum_{i=1}^n |\chi_i^2|}</span>
<span id="cb6-159"><a href="#cb6-159"></a>$$</span>
<span id="cb6-160"><a href="#cb6-160"></a></span>
<span id="cb6-161"><a href="#cb6-161"></a>Equivalently, it can be defined as:</span>
<span id="cb6-162"><a href="#cb6-162"></a></span>
<span id="cb6-163"><a href="#cb6-163"></a>$$</span>
<span id="cb6-164"><a href="#cb6-164"></a>\norm{\bf{x}}_2 = \sqrt{\inner{\bf{x}}{\bf{x}}} =  (\bf{x}^H \bf{x})^{1/2} = \sqrt{\overline{\chi_1}\chi_1 +\overline{\chi_2}\chi_2+\ldots+\overline{\chi_n}\chi_n}</span>
<span id="cb6-165"><a href="#cb6-165"></a>$$</span>
<span id="cb6-166"><a href="#cb6-166"></a></span>
<span id="cb6-167"><a href="#cb6-167"></a>:::</span>
<span id="cb6-168"><a href="#cb6-168"></a></span>
<span id="cb6-169"><a href="#cb6-169"></a>To prove that the vector $2-$norm is indeed a valid norm(just calling it a norm, doesn't mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if $\bf{x},\bf{y} \in \R^n$, then $|\bf{x}^T \bf{y}|\leq \norm{\bf{x}}_2\cdot\norm{\bf{y}}_2$. </span>
<span id="cb6-170"><a href="#cb6-170"></a></span>
<span id="cb6-171"><a href="#cb6-171"></a>Before we rigorously prove this result, let's review the idea of orthogonality.</span>
<span id="cb6-172"><a href="#cb6-172"></a></span>
<span id="cb6-173"><a href="#cb6-173"></a>::: {#def-orthogonal-vectors}</span>
<span id="cb6-174"><a href="#cb6-174"></a></span>
<span id="cb6-175"><a href="#cb6-175"></a><span class="fu">### Orthogonal vectors</span></span>
<span id="cb6-176"><a href="#cb6-176"></a></span>
<span id="cb6-177"><a href="#cb6-177"></a>Two vectors $\bf{u},\bf{v} \in V$ are said to be orthogonal to each other if and only if their inner product equals zero:</span>
<span id="cb6-178"><a href="#cb6-178"></a></span>
<span id="cb6-179"><a href="#cb6-179"></a>$$</span>
<span id="cb6-180"><a href="#cb6-180"></a>\inner{\bf{u}}{\bf{v}} = 0 </span>
<span id="cb6-181"><a href="#cb6-181"></a>$$</span>
<span id="cb6-182"><a href="#cb6-182"></a>:::</span>
<span id="cb6-183"><a href="#cb6-183"></a></span>
<span id="cb6-184"><a href="#cb6-184"></a></span>
<span id="cb6-185"><a href="#cb6-185"></a>::: {#thm-pythagorean-theorem}</span>
<span id="cb6-186"><a href="#cb6-186"></a></span>
<span id="cb6-187"><a href="#cb6-187"></a><span class="fu">### Pythagorean Theorem</span></span>
<span id="cb6-188"><a href="#cb6-188"></a></span>
<span id="cb6-189"><a href="#cb6-189"></a>If $\bf{u}$ and $\bf{v}$ are orthogonal vectors, then</span>
<span id="cb6-190"><a href="#cb6-190"></a></span>
<span id="cb6-191"><a href="#cb6-191"></a>$$</span>
<span id="cb6-192"><a href="#cb6-192"></a>\inner{\bf{u} + \bf{v}}{\bf{u} + \bf{v}} = \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}</span>
<span id="cb6-193"><a href="#cb6-193"></a>$$</span>
<span id="cb6-194"><a href="#cb6-194"></a>:::</span>
<span id="cb6-195"><a href="#cb6-195"></a></span>
<span id="cb6-196"><a href="#cb6-196"></a>*Proof.*</span>
<span id="cb6-197"><a href="#cb6-197"></a></span>
<span id="cb6-198"><a href="#cb6-198"></a>We have:</span>
<span id="cb6-199"><a href="#cb6-199"></a></span>
<span id="cb6-200"><a href="#cb6-200"></a>$$</span>
<span id="cb6-201"><a href="#cb6-201"></a>\begin{align*}</span>
<span id="cb6-202"><a href="#cb6-202"></a>\inner{\bf{u} + \bf{v}}{\bf{u}+\bf{v}} &amp;= \inner{\bf{u}}{\bf{u} + \bf{v}} + \inner{\bf{v}}{\bf{u} + \bf{v}} &amp; <span class="sc">\{</span> \text{ Additivity in the first slot }<span class="sc">\}\\</span></span>
<span id="cb6-203"><a href="#cb6-203"></a>&amp;= \overline{\inner{\bf{u} + \bf{v}}{\bf{u}}} + \overline{\inner{\bf{u} + \bf{v}}{\bf{v}}} &amp; <span class="sc">\{</span> \text{ Conjugate symmetry }<span class="sc">\}\\</span></span>
<span id="cb6-204"><a href="#cb6-204"></a>&amp;= \overline{\inner{\bf{u}}{\bf{u}}} + \overline{\inner{\bf{v}}{\bf{u}}} + \overline{\inner{\bf{u}}{\bf{v}}} + \overline{\inner{\bf{v}}{\bf{v}}} <span class="sc">\\</span></span>
<span id="cb6-205"><a href="#cb6-205"></a>&amp;= \inner{\bf{u}}{\bf{u}} + \inner{\bf{u}}{\bf{v}} + \inner{\bf{v}}{\bf{u}} + \inner{\bf{v}}{\bf{v}} <span class="sc">\\</span></span>
<span id="cb6-206"><a href="#cb6-206"></a>&amp;= \inner{\bf{u}}{\bf{u}} + 0 + 0 + \inner{\bf{v}}{\bf{v}} &amp; <span class="sc">\{</span> \bf{u} \perp \bf{v}<span class="sc">\}\\</span></span>
<span id="cb6-207"><a href="#cb6-207"></a>&amp;= \inner{\bf{u}}{\bf{u}} + \inner{\bf{v}}{\bf{v}}</span>
<span id="cb6-208"><a href="#cb6-208"></a>\end{align*}</span>
<span id="cb6-209"><a href="#cb6-209"></a>$$</span>
<span id="cb6-210"><a href="#cb6-210"></a></span>
<span id="cb6-211"><a href="#cb6-211"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-212"><a href="#cb6-212"></a></span>
<span id="cb6-213"><a href="#cb6-213"></a>In the special case that $V=\C^n$ or $V=\R^n$, the pythagorean theorem reduces to:</span>
<span id="cb6-214"><a href="#cb6-214"></a></span>
<span id="cb6-215"><a href="#cb6-215"></a>$$</span>
<span id="cb6-216"><a href="#cb6-216"></a>\norm{\bf{u} + \bf{v}}_2^2 = \norm{\bf{u}}_2^2 + \norm{\bf{v}}_2^2 </span>
<span id="cb6-217"><a href="#cb6-217"></a>$$</span>
<span id="cb6-218"><a href="#cb6-218"></a></span>
<span id="cb6-219"><a href="#cb6-219"></a><span class="fu">## Cauchy-Schwarz Inequality</span></span>
<span id="cb6-220"><a href="#cb6-220"></a></span>
<span id="cb6-221"><a href="#cb6-221"></a>Suppose $\bf{u},\bf{v}\in V$. We would like to write $\bf{u}$ as a scalar multiple of $\bf{v}$ plus a vector $\bf{w}$ orthogonal to $\bf{v}$, as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of $\bf{u}$. </span>
<span id="cb6-222"><a href="#cb6-222"></a></span>
<span id="cb6-225"><a href="#cb6-225"></a><span class="in">```{python}</span></span>
<span id="cb6-226"><a href="#cb6-226"></a><span class="op">%</span>load_ext itikz</span>
<span id="cb6-227"><a href="#cb6-227"></a><span class="in">```</span></span>
<span id="cb6-228"><a href="#cb6-228"></a></span>
<span id="cb6-231"><a href="#cb6-231"></a><span class="in">```{python}</span></span>
<span id="cb6-232"><a href="#cb6-232"></a><span class="co"># | code-fold: true</span></span>
<span id="cb6-233"><a href="#cb6-233"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb6-234"><a href="#cb6-234"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows,arrows.meta <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb6-235"><a href="#cb6-235"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">2.0</span>]</span>
<span id="cb6-236"><a href="#cb6-236"></a>    \draw [<span class="op">-</span>{Stealth[length<span class="op">=</span><span class="dv">5</span><span class="er">mm</span>]}](<span class="fl">0.0</span>,<span class="fl">0.0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb6-237"><a href="#cb6-237"></a>    \draw [<span class="op">-</span>{Stealth[length<span class="op">=</span><span class="dv">5</span><span class="er">mm</span>]}] (<span class="fl">0.0</span>,<span class="fl">0.0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb6-238"><a href="#cb6-238"></a>    \node []  at (<span class="fl">3.5</span>,<span class="fl">2.25</span>) {\large $\mathbf{u}$}<span class="op">;</span></span>
<span id="cb6-239"><a href="#cb6-239"></a>    \draw [dashed] (<span class="dv">7</span>,<span class="dv">0</span>) <span class="op">--</span> (<span class="dv">7</span>,<span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb6-240"><a href="#cb6-240"></a>    \node [circle,fill,minimum size <span class="op">=</span> <span class="fl">0.5</span><span class="er">mm</span>] at (<span class="dv">5</span>,<span class="dv">0</span>) {}<span class="op">;</span></span>
<span id="cb6-241"><a href="#cb6-241"></a>    \node []  at (<span class="dv">5</span>,<span class="op">-</span><span class="fl">0.40</span>) {\large $\mathbf{v}$}<span class="op">;</span></span>
<span id="cb6-242"><a href="#cb6-242"></a>    \node []  at (<span class="dv">7</span>,<span class="op">-</span><span class="fl">0.40</span>) {\large $\alpha\mathbf{v}$}<span class="op">;</span></span>
<span id="cb6-243"><a href="#cb6-243"></a>    \node []  at (<span class="fl">7.4</span>,<span class="fl">2.0</span>) {\large $\mathbf{w}$}<span class="op">;</span></span>
<span id="cb6-244"><a href="#cb6-244"></a>\end{tikzpicture}</span>
<span id="cb6-245"><a href="#cb6-245"></a><span class="in">```</span></span>
<span id="cb6-246"><a href="#cb6-246"></a></span>
<span id="cb6-247"><a href="#cb6-247"></a>To discover how to write $\bf{u}$ as a scalar multiple of $\bf{v}$ plus a vector orthogonal to $\bf{v}$, let $\alpha$ denote a scalar. Then,</span>
<span id="cb6-248"><a href="#cb6-248"></a></span>
<span id="cb6-249"><a href="#cb6-249"></a>$$</span>
<span id="cb6-250"><a href="#cb6-250"></a>\bf{u} = \alpha \bf{v} + (\bf{u} - \alpha \bf{v})</span>
<span id="cb6-251"><a href="#cb6-251"></a>$$</span>
<span id="cb6-252"><a href="#cb6-252"></a></span>
<span id="cb6-253"><a href="#cb6-253"></a>Thus, we need to choose $\alpha$ so that $\bf{v}$ and $\bf{w} = \bf{u} - \alpha{v}$ are mutually orthogonal. Thus, we must set:</span>
<span id="cb6-254"><a href="#cb6-254"></a></span>
<span id="cb6-255"><a href="#cb6-255"></a>$$</span>
<span id="cb6-256"><a href="#cb6-256"></a>\inner{\bf{u} - \alpha\bf{v}}{\bf{v}} = \inner{\bf{u}}{\bf{v}} - \alpha \inner{\bf{v}}{\bf{v}} = 0</span>
<span id="cb6-257"><a href="#cb6-257"></a>$$</span>
<span id="cb6-258"><a href="#cb6-258"></a></span>
<span id="cb6-259"><a href="#cb6-259"></a>The equation above shows that we choose $\alpha$ to be $\inner{\bf{u}}{\bf{v}}/\inner{\bf{v}}{\bf{v}}$ (assume that $\bf{v} \neq \bf{0}$ to avoid division by 0). Making this choice of $\alpha$, we can write:</span>
<span id="cb6-260"><a href="#cb6-260"></a></span>
<span id="cb6-261"><a href="#cb6-261"></a>$$</span>
<span id="cb6-262"><a href="#cb6-262"></a>\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v} + \left(\bf{u} - \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\bf{v}\right)</span>
<span id="cb6-263"><a href="#cb6-263"></a>$$ {#eq-orthogonal-decomposition}</span>
<span id="cb6-264"><a href="#cb6-264"></a></span>
<span id="cb6-265"><a href="#cb6-265"></a>The equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics</span>
<span id="cb6-266"><a href="#cb6-266"></a></span>
<span id="cb6-267"><a href="#cb6-267"></a>::: {#thm-cauchy-schwarz-inequality}</span>
<span id="cb6-268"><a href="#cb6-268"></a></span>
<span id="cb6-269"><a href="#cb6-269"></a><span class="fu">### Cauchy-Schwarz Inequality</span></span>
<span id="cb6-270"><a href="#cb6-270"></a></span>
<span id="cb6-271"><a href="#cb6-271"></a>Let $\bf{x},\bf{y}\in V$. Then</span>
<span id="cb6-272"><a href="#cb6-272"></a></span>
<span id="cb6-273"><a href="#cb6-273"></a>$$</span>
<span id="cb6-274"><a href="#cb6-274"></a>|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}</span>
<span id="cb6-275"><a href="#cb6-275"></a>$$ {#eq-cauchy-schwarz-inequality}</span>
<span id="cb6-276"><a href="#cb6-276"></a>:::</span>
<span id="cb6-277"><a href="#cb6-277"></a></span>
<span id="cb6-278"><a href="#cb6-278"></a>*Proof.*</span>
<span id="cb6-279"><a href="#cb6-279"></a></span>
<span id="cb6-280"><a href="#cb6-280"></a>Let $\bf{u},\bf{v} \in V$. If $\bf{v} = \bf{0}$, then both sides of @eq-cauchy-schwarz-inequality equal $0$ and the inequality holds. Thus, we assume that $\bf{v}\neq \bf{0}$. Consider the orthogonal decomposition:</span>
<span id="cb6-281"><a href="#cb6-281"></a></span>
<span id="cb6-282"><a href="#cb6-282"></a>$$</span>
<span id="cb6-283"><a href="#cb6-283"></a>\bf{u} = \frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v} + \bf{w}</span>
<span id="cb6-284"><a href="#cb6-284"></a>$$</span>
<span id="cb6-285"><a href="#cb6-285"></a></span>
<span id="cb6-286"><a href="#cb6-286"></a>where $\bf{w}$ is orthogonal to $\bf{v}$ ($\bf{w}$ is taken to be the second term on the right hand side of @eq-orthogonal-decomposition). By the Pythagorean theorem:</span>
<span id="cb6-287"><a href="#cb6-287"></a></span>
<span id="cb6-288"><a href="#cb6-288"></a>$$</span>
<span id="cb6-289"><a href="#cb6-289"></a>\begin{align*}</span>
<span id="cb6-290"><a href="#cb6-290"></a>\inner{\bf{u}}{\bf{u}} &amp;= \inner{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}{\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}} \bf{v}}+\inner{\bf{w}}{\bf{w}}<span class="sc">\\</span></span>
<span id="cb6-291"><a href="#cb6-291"></a>&amp;= \overline{\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)}\left(\frac{\inner{\bf{u}}{\bf{v}}}{\inner{\bf{v}}{\bf{v}}}\right)\inner{\bf{v}}{\bf{v}} + \inner{\bf{w}}{\bf{w}}<span class="sc">\\</span></span>
<span id="cb6-292"><a href="#cb6-292"></a>&amp;= \frac{\overline{\inner{\bf{u}}{\bf{v}}}\inner{\bf{u}}{\bf{v}}}{\overline{\inner{\bf{v}}{\bf{v}}}} + \inner{\bf{w}}{\bf{w}}<span class="sc">\\</span></span>
<span id="cb6-293"><a href="#cb6-293"></a>&amp;= \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}} + \inner{\bf{w}}{\bf{w}}</span>
<span id="cb6-294"><a href="#cb6-294"></a>\end{align*}</span>
<span id="cb6-295"><a href="#cb6-295"></a>$$</span>
<span id="cb6-296"><a href="#cb6-296"></a></span>
<span id="cb6-297"><a href="#cb6-297"></a>Since $\inner{\bf{w}}{\bf{w}} \geq 0$, it follows that:</span>
<span id="cb6-298"><a href="#cb6-298"></a></span>
<span id="cb6-299"><a href="#cb6-299"></a>$$</span>
<span id="cb6-300"><a href="#cb6-300"></a>\inner{\bf{u}}{\bf{u}} \geq \frac{|\inner{\bf{u}}{\bf{v}}|^2}{\inner{\bf{v}}{\bf{v}}}</span>
<span id="cb6-301"><a href="#cb6-301"></a>$$</span>
<span id="cb6-302"><a href="#cb6-302"></a></span>
<span id="cb6-303"><a href="#cb6-303"></a>Consequently, we have:</span>
<span id="cb6-304"><a href="#cb6-304"></a></span>
<span id="cb6-305"><a href="#cb6-305"></a>$$</span>
<span id="cb6-306"><a href="#cb6-306"></a>|\inner{\bf{u}}{\bf{v}}|^2 \leq \inner{\bf{u}}{\bf{u}}\inner{\bf{v}}{\bf{v}}</span>
<span id="cb6-307"><a href="#cb6-307"></a>$$</span>
<span id="cb6-308"><a href="#cb6-308"></a></span>
<span id="cb6-309"><a href="#cb6-309"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-310"><a href="#cb6-310"></a></span>
<span id="cb6-311"><a href="#cb6-311"></a>In the special case, that $V=\R^n$ or $V=\C^n$, we have:</span>
<span id="cb6-312"><a href="#cb6-312"></a></span>
<span id="cb6-313"><a href="#cb6-313"></a>$$</span>
<span id="cb6-314"><a href="#cb6-314"></a>|\inner{\bf{u}}{\bf{v}}| \leq \norm{\bf{u}}_2 \norm{\bf{v}}_2</span>
<span id="cb6-315"><a href="#cb6-315"></a>$$</span>
<span id="cb6-316"><a href="#cb6-316"></a></span>
<span id="cb6-317"><a href="#cb6-317"></a><span class="fu">## Euclidean Norm</span></span>
<span id="cb6-318"><a href="#cb6-318"></a></span>
<span id="cb6-319"><a href="#cb6-319"></a>::: {#prp-euclidean-norm-is-well-defined}</span>
<span id="cb6-320"><a href="#cb6-320"></a></span>
<span id="cb6-321"><a href="#cb6-321"></a><span class="fu">### Well-definedness of the Euclidean norm</span></span>
<span id="cb6-322"><a href="#cb6-322"></a></span>
<span id="cb6-323"><a href="#cb6-323"></a>Let $\norm{\cdot}:\mathbf{C}^n \to \mathbf{C}$ be the euclidean norm. Our claim is, it is well-defined.</span>
<span id="cb6-324"><a href="#cb6-324"></a>:::</span>
<span id="cb6-325"><a href="#cb6-325"></a></span>
<span id="cb6-326"><a href="#cb6-326"></a>*Proof.*</span>
<span id="cb6-327"><a href="#cb6-327"></a></span>
<span id="cb6-328"><a href="#cb6-328"></a>Let $\bf{z} = (z_1,z_2,\ldots,z_n) \in \C^n$. Clearly, it is positive semi-definite.</span>
<span id="cb6-329"><a href="#cb6-329"></a></span>
<span id="cb6-330"><a href="#cb6-330"></a>$$</span>
<span id="cb6-331"><a href="#cb6-331"></a>\begin{align*}</span>
<span id="cb6-332"><a href="#cb6-332"></a>\norm{\bf{z}}_2 = \bf{z}^H \bf{z} &amp;= \overline{z_1} z_1 +\overline{z_2}z_2 + \ldots + \overline{z_n} z_n<span class="sc">\\</span></span>
<span id="cb6-333"><a href="#cb6-333"></a>&amp;= \sum_{i=1}^n |z_i|^2 \geq 0</span>
<span id="cb6-334"><a href="#cb6-334"></a>\end{align*}</span>
<span id="cb6-335"><a href="#cb6-335"></a>$$</span>
<span id="cb6-336"><a href="#cb6-336"></a></span>
<span id="cb6-337"><a href="#cb6-337"></a>It is also homogenous. Let $\alpha \in \C$.</span>
<span id="cb6-338"><a href="#cb6-338"></a></span>
<span id="cb6-339"><a href="#cb6-339"></a>$$</span>
<span id="cb6-340"><a href="#cb6-340"></a>\begin{align*}</span>
<span id="cb6-341"><a href="#cb6-341"></a>\norm{\alpha \bf{z}}_2 &amp;= \norm{(\alpha z_1, \alpha z_2,\ldots,\alpha z_n)}_2<span class="sc">\\</span></span>
<span id="cb6-342"><a href="#cb6-342"></a>&amp;=\sqrt{\sum_{i=1}^n |\alpha z_i|^2}<span class="sc">\\</span></span>
<span id="cb6-343"><a href="#cb6-343"></a>&amp;=|\alpha|\sqrt{\sum_{i=1}^n |z_i|^2} <span class="sc">\\</span></span>
<span id="cb6-344"><a href="#cb6-344"></a>&amp;= |\alpha|\norm{\bf{z}}_2</span>
<span id="cb6-345"><a href="#cb6-345"></a>\end{align*}</span>
<span id="cb6-346"><a href="#cb6-346"></a>$$</span>
<span id="cb6-347"><a href="#cb6-347"></a></span>
<span id="cb6-348"><a href="#cb6-348"></a>Let's verify, if the triangle inequality is satisfied. Let $\bf{x}, \bf{y}\in\C^n$ be arbitrary vectors.</span>
<span id="cb6-349"><a href="#cb6-349"></a></span>
<span id="cb6-350"><a href="#cb6-350"></a>$$</span>
<span id="cb6-351"><a href="#cb6-351"></a>\begin{align*}</span>
<span id="cb6-352"><a href="#cb6-352"></a>\norm{\bf{x} + \bf{y}}_2^2 &amp;= |(\bf{x} + \bf{y})^H(\bf{x} + \bf{y})|<span class="sc">\\</span></span>
<span id="cb6-353"><a href="#cb6-353"></a>&amp;= |(\bf{x}^H + \bf{y}^H)(\bf{x} + \bf{y})|<span class="sc">\\</span></span>
<span id="cb6-354"><a href="#cb6-354"></a>&amp;= |\bf{x}^H \bf{x} + \bf{y}^H \bf{y} + \bf{y}^H \bf{x} + \bf{x}^H \bf{y}|<span class="sc">\\</span></span>
<span id="cb6-355"><a href="#cb6-355"></a>&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + |\inner{\bf{y}}{\bf{x}}| + |\inner{\bf{x}}{\bf{y}}|<span class="sc">\\</span></span>
<span id="cb6-356"><a href="#cb6-356"></a>&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 + \norm{\bf{y}}_2 \norm{\bf{x}}_2  + \norm{\bf{x}}_2 \norm{\bf{y}}_2 &amp; <span class="sc">\{</span> \text{ Cauchy-Schwarz } <span class="sc">\}\\</span></span>
<span id="cb6-357"><a href="#cb6-357"></a>&amp;\leq \norm{\bf{x}}_2^2 + \norm{\bf{y}}_2^2 +  2\norm{\bf{x}}_2 \norm{\bf{y}}_2<span class="sc">\\</span></span>
<span id="cb6-358"><a href="#cb6-358"></a>&amp;= (\norm{\bf{x}}_2 + \norm{\bf{y}}_2)^2</span>
<span id="cb6-359"><a href="#cb6-359"></a>\end{align*}</span>
<span id="cb6-360"><a href="#cb6-360"></a>$$</span>
<span id="cb6-361"><a href="#cb6-361"></a></span>
<span id="cb6-362"><a href="#cb6-362"></a>Consequently, $\norm{\bf{x} + \bf{y}}_2 \leq \norm{\bf{x}}_2 + \norm{\bf{y}}_2$.</span>
<span id="cb6-363"><a href="#cb6-363"></a></span>
<span id="cb6-364"><a href="#cb6-364"></a><span class="fu">## The vector $1-$norm</span></span>
<span id="cb6-365"><a href="#cb6-365"></a></span>
<span id="cb6-366"><a href="#cb6-366"></a>::: {#def-the-vector-1-norm}</span>
<span id="cb6-367"><a href="#cb6-367"></a></span>
<span id="cb6-368"><a href="#cb6-368"></a><span class="fu">### The vector $1-$norm</span></span>
<span id="cb6-369"><a href="#cb6-369"></a></span>
<span id="cb6-370"><a href="#cb6-370"></a>The vector $1$-norm, $\norm{\cdot}_1 : \C^n \to \R$ is defined for all $\bf{x}\in\C^n$ by:</span>
<span id="cb6-371"><a href="#cb6-371"></a></span>
<span id="cb6-372"><a href="#cb6-372"></a>$$</span>
<span id="cb6-373"><a href="#cb6-373"></a>\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| =\sum_{i=1}^n |\chi_i|</span>
<span id="cb6-374"><a href="#cb6-374"></a>$$</span>
<span id="cb6-375"><a href="#cb6-375"></a>:::</span>
<span id="cb6-376"><a href="#cb6-376"></a></span>
<span id="cb6-377"><a href="#cb6-377"></a>::: {#thm-1-norm-is-a-norm}</span>
<span id="cb6-378"><a href="#cb6-378"></a></span>
<span id="cb6-379"><a href="#cb6-379"></a>The vector $1$-norm is well-defined.</span>
<span id="cb6-380"><a href="#cb6-380"></a>:::</span>
<span id="cb6-381"><a href="#cb6-381"></a></span>
<span id="cb6-382"><a href="#cb6-382"></a>*Proof.*</span>
<span id="cb6-383"><a href="#cb6-383"></a></span>
<span id="cb6-384"><a href="#cb6-384"></a>*Positive semi-definitess.*</span>
<span id="cb6-385"><a href="#cb6-385"></a></span>
<span id="cb6-386"><a href="#cb6-386"></a>The absolute value of complex numbers is non-negative. </span>
<span id="cb6-387"><a href="#cb6-387"></a></span>
<span id="cb6-388"><a href="#cb6-388"></a>$$</span>
<span id="cb6-389"><a href="#cb6-389"></a>\norm{\bf{x}}_1 = |\chi_1| + |\chi_2| + \ldots + |\chi_n| \geq |\chi_i| \geq 0</span>
<span id="cb6-390"><a href="#cb6-390"></a>$$</span>
<span id="cb6-391"><a href="#cb6-391"></a></span>
<span id="cb6-392"><a href="#cb6-392"></a>*Homogeneity.*</span>
<span id="cb6-393"><a href="#cb6-393"></a></span>
<span id="cb6-394"><a href="#cb6-394"></a>$$</span>
<span id="cb6-395"><a href="#cb6-395"></a>\norm{\alpha\bf{x}}_1 = \sum_{i=1}^{n}|\alpha \chi_i| = |\alpha| \sum_{i=1}^{n}|\chi_i| = |\alpha| \norm{\bf{x}}_1</span>
<span id="cb6-396"><a href="#cb6-396"></a>$$</span>
<span id="cb6-397"><a href="#cb6-397"></a></span>
<span id="cb6-398"><a href="#cb6-398"></a>*Triangle Inequality.*</span>
<span id="cb6-399"><a href="#cb6-399"></a></span>
<span id="cb6-400"><a href="#cb6-400"></a>$$</span>
<span id="cb6-401"><a href="#cb6-401"></a>\begin{align*}</span>
<span id="cb6-402"><a href="#cb6-402"></a>\norm{\bf{x} + \bf{y}} &amp;= \norm{(\chi_1 + \psi_1, \ldots,\chi_n + \psi_n)}_1<span class="sc">\\</span></span>
<span id="cb6-403"><a href="#cb6-403"></a>&amp;= \sum_{i=1}^n |\chi_i + \psi_i|<span class="sc">\\</span></span>
<span id="cb6-404"><a href="#cb6-404"></a>&amp;\leq \sum_{i=1}^n |\chi_i| + |\psi_i| &amp; <span class="sc">\{</span> \text{ Triangle inequality for complex numbers }<span class="sc">\}\\</span></span>
<span id="cb6-405"><a href="#cb6-405"></a>&amp;= \sum_{i=1}^n |\chi_i| + \sum_{i=1}^{n} |\psi_i| &amp; <span class="sc">\{</span> \text{ Commutativity }<span class="sc">\}\\</span></span>
<span id="cb6-406"><a href="#cb6-406"></a>&amp;= \norm{\bf{x}}_1 + \norm{\bf{y}}_1</span>
<span id="cb6-407"><a href="#cb6-407"></a>\end{align*}</span>
<span id="cb6-408"><a href="#cb6-408"></a>$$</span>
<span id="cb6-409"><a href="#cb6-409"></a></span>
<span id="cb6-410"><a href="#cb6-410"></a>Hence, the three axioms are satisfied. $\blacksquare$</span>
<span id="cb6-411"><a href="#cb6-411"></a></span>
<span id="cb6-412"><a href="#cb6-412"></a><span class="fu">## Jensen's inequality</span></span>
<span id="cb6-413"><a href="#cb6-413"></a></span>
<span id="cb6-414"><a href="#cb6-414"></a><span class="fu">### Convex functions and combinations</span></span>
<span id="cb6-415"><a href="#cb6-415"></a></span>
<span id="cb6-416"><a href="#cb6-416"></a>A function $f$ is said to be *convex* on over an interval $I$, if for all $x_1,x_2 \in I$, and every $p \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$, we have:</span>
<span id="cb6-417"><a href="#cb6-417"></a></span>
<span id="cb6-418"><a href="#cb6-418"></a>$$</span>
<span id="cb6-419"><a href="#cb6-419"></a>f(px_1 + (1-p)x_2) \leq pf(x_1) + (1-p)f(x_2)</span>
<span id="cb6-420"><a href="#cb6-420"></a>$$</span>
<span id="cb6-421"><a href="#cb6-421"></a></span>
<span id="cb6-422"><a href="#cb6-422"></a>In other words, all chords(secants) joining any two points on $f$, lie above the graph of $f$. Note that, if $0 \leq p \leq 1$, then $\min(x_1,x_2) \leq px_1 + (1-p)x_2 \leq \max(x_1,x_2)$. More generally, for non-negative real numbers $p_1, p_2, \ldots, p_n$ summing to one, that is, satisfying $\sum_{i=1}^n p_i = 1$, and for any points $x_1,\ldots,x_n \in I$, the point $\sum_{i=1}^n \lambda_i x_i$ is called a *convex combination* of $x_1,\ldots,x_n$. Since:</span>
<span id="cb6-423"><a href="#cb6-423"></a></span>
<span id="cb6-424"><a href="#cb6-424"></a>$$ \min(x_1,\ldots,x_n) \leq \sum_{i=1}^n p_i x_i \leq \max(x_1,\ldots,x_n)$$</span>
<span id="cb6-425"><a href="#cb6-425"></a></span>
<span id="cb6-426"><a href="#cb6-426"></a>every convex combination of any finite number of points in $I$ is again a point of $I$.</span>
<span id="cb6-427"><a href="#cb6-427"></a></span>
<span id="cb6-428"><a href="#cb6-428"></a>Intuitively, $\sum_{i=1}^{n}p_i x_i$ simply represents the center of mass of the points $x_1,\ldots,x_n$ with weights $p_1,\ldots,p_n$. </span>
<span id="cb6-429"><a href="#cb6-429"></a></span>
<span id="cb6-430"><a href="#cb6-430"></a><span class="fu">### Proving Jensen's inequality</span></span>
<span id="cb6-431"><a href="#cb6-431"></a></span>
<span id="cb6-432"><a href="#cb6-432"></a>Jensen's inequality named after the Danish engineer <span class="co">[</span><span class="ot">Johan Jensen</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)) (1859-1925) can be stated as follows:</span>
<span id="cb6-433"><a href="#cb6-433"></a></span>
<span id="cb6-434"><a href="#cb6-434"></a>::: {#thm-jensens-inequality}</span>
<span id="cb6-435"><a href="#cb6-435"></a></span>
<span id="cb6-436"><a href="#cb6-436"></a>Let $n \in \bf{Z}_+$ be a positive integer and let $f:I \to \R$ be a convex function over the interval $I \subseteq \R$. For any (not necessarily distinct) points $x_1,\ldots,x_n \in I$, and non-negative real numbers $p_1,\ldots,p_n \in \R$ summing to one,</span>
<span id="cb6-437"><a href="#cb6-437"></a></span>
<span id="cb6-438"><a href="#cb6-438"></a>$$</span>
<span id="cb6-439"><a href="#cb6-439"></a>f(\sum_{i=1}^n p_i x_i) \leq \sum_{i=1}^n p_i f(x_i)</span>
<span id="cb6-440"><a href="#cb6-440"></a>$$</span>
<span id="cb6-441"><a href="#cb6-441"></a>:::</span>
<span id="cb6-442"><a href="#cb6-442"></a></span>
<span id="cb6-443"><a href="#cb6-443"></a>*Proof.*</span>
<span id="cb6-444"><a href="#cb6-444"></a></span>
<span id="cb6-445"><a href="#cb6-445"></a>We proceed by induction. Since $f$ is convex, by definition, $\forall x_1,x_2 \in I$, and any $p_1,p_2\in \R$, such that $p_1 + p_2 = 1$, we have $f(p_1 x_1 + p_2 x_2) \leq p_1 f(x_1) + p_2 f(x_2)$. So, the claim is true for $n=2$.</span>
<span id="cb6-446"><a href="#cb6-446"></a></span>
<span id="cb6-447"><a href="#cb6-447"></a>*Inductive hypothesis*. Assume that $\forall x_1,\ldots,x_{k} \in I$ and any $p_1,\ldots,p_k \in \R$, such that $\sum_{i=1}^k p_i = 1$, we have $f(\sum_{i=1}^k p_i x_i) \leq \sum_{i=1}^k p_i f(x_i)$.</span>
<span id="cb6-448"><a href="#cb6-448"></a></span>
<span id="cb6-449"><a href="#cb6-449"></a>*Claim*. The Jensen's inequality holds for $k+1$ points in $I$.</span>
<span id="cb6-450"><a href="#cb6-450"></a></span>
<span id="cb6-451"><a href="#cb6-451"></a>*Proof*. </span>
<span id="cb6-452"><a href="#cb6-452"></a></span>
<span id="cb6-453"><a href="#cb6-453"></a>Let $x_1,\ldots,x_k, x_{k+1}$ be arbitrary points in $I$ and consider any convex combination of these points $\sum_{i=1}^{k+1}p_i x_i$, $p_i \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>, i \in <span class="sc">\{</span>1,2,3,\ldots,k+1<span class="sc">\}</span>, \sum_{i=1}^{k+1}p_i = 1$. </span>
<span id="cb6-454"><a href="#cb6-454"></a></span>
<span id="cb6-455"><a href="#cb6-455"></a>Define:</span>
<span id="cb6-456"><a href="#cb6-456"></a></span>
<span id="cb6-457"><a href="#cb6-457"></a>$$</span>
<span id="cb6-458"><a href="#cb6-458"></a>z := \frac{p_1 x_1 + p_2 x_2 + \ldots + p_k x_k}{\sum_{i=1}^k p_i} </span>
<span id="cb6-459"><a href="#cb6-459"></a>$$</span>
<span id="cb6-460"><a href="#cb6-460"></a></span>
<span id="cb6-461"><a href="#cb6-461"></a>Since, $z$ is a convex combination of $<span class="sc">\{</span>x_1,\ldots,x_k<span class="sc">\}</span>$, $z \in I$. Moreover, by the inductive hypothesis, since $f$ is convex,</span>
<span id="cb6-462"><a href="#cb6-462"></a></span>
<span id="cb6-463"><a href="#cb6-463"></a>$$</span>
<span id="cb6-464"><a href="#cb6-464"></a>\begin{align*}</span>
<span id="cb6-465"><a href="#cb6-465"></a>f(z) &amp;= f\left(\frac{p_1 x_1 + p_2 x_2 + \ldots + p_k x_k}{\sum_{i=1}^k p_i}\right)<span class="sc">\\</span></span>
<span id="cb6-466"><a href="#cb6-466"></a>&amp;\leq \frac{p_1}{\sum_{i=1}^k p_i}f(x_1) + \frac{p_2}{\sum_{i=1}^k p_i}f(x_2) + \ldots + \frac{p_k}{\sum_{i=1}^k p_i}f(x_k) <span class="sc">\\</span></span>
<span id="cb6-467"><a href="#cb6-467"></a>&amp;= \frac{p_1}{1-p_{k+1}}f(x_1) + \frac{p_2}{1-p_{k+1}}f(x_2) + \ldots + \frac{p_k}{1-p_{k+1}}f(x_k) <span class="sc">\\</span></span>
<span id="cb6-468"><a href="#cb6-468"></a>\end{align*}</span>
<span id="cb6-469"><a href="#cb6-469"></a>$$ </span>
<span id="cb6-470"><a href="#cb6-470"></a></span>
<span id="cb6-471"><a href="#cb6-471"></a>Since $0 \leq 1 - p_{k+1} \leq 1$, we deduce that:</span>
<span id="cb6-472"><a href="#cb6-472"></a></span>
<span id="cb6-473"><a href="#cb6-473"></a>$$</span>
<span id="cb6-474"><a href="#cb6-474"></a>(1 - p_{k+1})f(z) \leq p_1 f(x_1) + \ldots + p_k f(x_k)</span>
<span id="cb6-475"><a href="#cb6-475"></a>$$</span>
<span id="cb6-476"><a href="#cb6-476"></a></span>
<span id="cb6-477"><a href="#cb6-477"></a>We have:</span>
<span id="cb6-478"><a href="#cb6-478"></a>$$</span>
<span id="cb6-479"><a href="#cb6-479"></a>\begin{align*}</span>
<span id="cb6-480"><a href="#cb6-480"></a>f(p_1 x_1 + \ldots + p_k x_k + p_{k+1} x_{k+1}) &amp;= f((1-p_{k+1})z + p_{k+1}x_{k+1})<span class="sc">\\</span></span>
<span id="cb6-481"><a href="#cb6-481"></a>&amp;\leq (1-p_{k+1})f(z) + p_{k+1}f(x_{k+1}) &amp; <span class="sc">\{</span> \text{ Jensen's inequality for }n=2<span class="sc">\}\\</span></span>
<span id="cb6-482"><a href="#cb6-482"></a>&amp;\leq p_1 f(x_1) + \ldots + p_k f(x_k) + p_{k+1}f(x_{k+1}) &amp; <span class="sc">\{</span> \text{ Deduction from the inductive hypothesis }<span class="sc">\}</span></span>
<span id="cb6-483"><a href="#cb6-483"></a>\end{align*}</span>
<span id="cb6-484"><a href="#cb6-484"></a>$$</span>
<span id="cb6-485"><a href="#cb6-485"></a></span>
<span id="cb6-486"><a href="#cb6-486"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-487"><a href="#cb6-487"></a></span>
<span id="cb6-488"><a href="#cb6-488"></a><span class="fu">## Young's Inequality</span></span>
<span id="cb6-489"><a href="#cb6-489"></a></span>
<span id="cb6-490"><a href="#cb6-490"></a>Young's inequality is named after the English mathematician <span class="co">[</span><span class="ot">William Henry Young</span><span class="co">](https://en.wikipedia.org/wiki/William_Henry_Young)</span> and can be stated as follows:</span>
<span id="cb6-491"><a href="#cb6-491"></a></span>
<span id="cb6-492"><a href="#cb6-492"></a>::: {#thm-youngs-inequality}</span>
<span id="cb6-493"><a href="#cb6-493"></a></span>
<span id="cb6-494"><a href="#cb6-494"></a><span class="fu">### Young's inequality</span></span>
<span id="cb6-495"><a href="#cb6-495"></a></span>
<span id="cb6-496"><a href="#cb6-496"></a>For any non-negative real numbers $a$ and $b$ and any positive real numbers $p,q$ satisfying $\frac{1}{p} + \frac{1}{q}=1$, we have:</span>
<span id="cb6-497"><a href="#cb6-497"></a></span>
<span id="cb6-498"><a href="#cb6-498"></a>$$</span>
<span id="cb6-499"><a href="#cb6-499"></a>ab \leq \frac{a^p}{p} + \frac{b^q}{q}</span>
<span id="cb6-500"><a href="#cb6-500"></a>$$</span>
<span id="cb6-501"><a href="#cb6-501"></a>:::</span>
<span id="cb6-502"><a href="#cb6-502"></a></span>
<span id="cb6-503"><a href="#cb6-503"></a>*Proof.*</span>
<span id="cb6-504"><a href="#cb6-504"></a></span>
<span id="cb6-505"><a href="#cb6-505"></a>Let $f(x) = \log x$. Since $f$ is concave, we can reverse the Jensen's inequality. Consequently:</span>
<span id="cb6-506"><a href="#cb6-506"></a></span>
<span id="cb6-507"><a href="#cb6-507"></a>$$</span>
<span id="cb6-508"><a href="#cb6-508"></a>\begin{align*}</span>
<span id="cb6-509"><a href="#cb6-509"></a>\log(\frac{a^p}{p} + \frac{b^q}{q}) &amp;\geq \frac{1}{p}\log a^p + \frac{1}{q}\log b^q<span class="sc">\\</span></span>
<span id="cb6-510"><a href="#cb6-510"></a>&amp;= \frac{1}{p}\cdot p \log a + \frac{1}{q}\cdot q \log b<span class="sc">\\</span></span>
<span id="cb6-511"><a href="#cb6-511"></a>&amp;= \log (ab)</span>
<span id="cb6-512"><a href="#cb6-512"></a>\end{align*}</span>
<span id="cb6-513"><a href="#cb6-513"></a>$$</span>
<span id="cb6-514"><a href="#cb6-514"></a></span>
<span id="cb6-515"><a href="#cb6-515"></a>Since $\log x$ is monotonic increasing,</span>
<span id="cb6-516"><a href="#cb6-516"></a></span>
<span id="cb6-517"><a href="#cb6-517"></a>$$</span>
<span id="cb6-518"><a href="#cb6-518"></a>\frac{a^p}{p} + \frac{b^q}{q} \geq ab</span>
<span id="cb6-519"><a href="#cb6-519"></a>$$</span>
<span id="cb6-520"><a href="#cb6-520"></a></span>
<span id="cb6-521"><a href="#cb6-521"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-522"><a href="#cb6-522"></a></span>
<span id="cb6-523"><a href="#cb6-523"></a><span class="fu">## Holder's inequality</span></span>
<span id="cb6-524"><a href="#cb6-524"></a></span>
<span id="cb6-525"><a href="#cb6-525"></a>We can use Young's inequality to prove the Holder's inequality, named after the German mathematician <span class="co">[</span><span class="ot">Otto Ludwig Holder</span><span class="co">](https://en.wikipedia.org/wiki/Otto_H%C3%B6lder)</span> (1859-1937).</span>
<span id="cb6-526"><a href="#cb6-526"></a></span>
<span id="cb6-527"><a href="#cb6-527"></a>::: {#thm-holders-inequality}</span>
<span id="cb6-528"><a href="#cb6-528"></a></span>
<span id="cb6-529"><a href="#cb6-529"></a><span class="fu">### Holder's inequality</span></span>
<span id="cb6-530"><a href="#cb6-530"></a></span>
<span id="cb6-531"><a href="#cb6-531"></a>For any pair of vectors $\bf{x},\bf{y}\in \C^n$, and for any positive real numbers satisfying $p$ and $q$, we have $\frac{1}{p} + \frac{1}{q} = 1$ we have:</span>
<span id="cb6-532"><a href="#cb6-532"></a></span>
<span id="cb6-533"><a href="#cb6-533"></a>$$</span>
<span id="cb6-534"><a href="#cb6-534"></a>\sum_{i=1}^{n}|x_i y_i| \leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p} \left(\sum_{i=1}^n |y_i|^q\right)^{1/q}</span>
<span id="cb6-535"><a href="#cb6-535"></a>$$</span>
<span id="cb6-536"><a href="#cb6-536"></a>:::</span>
<span id="cb6-537"><a href="#cb6-537"></a></span>
<span id="cb6-538"><a href="#cb6-538"></a>*Proof.*</span>
<span id="cb6-539"><a href="#cb6-539"></a></span>
<span id="cb6-540"><a href="#cb6-540"></a>Apply Young's inequality to $a = \frac{|x_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}}$ and $b = \frac{|y_i|}{\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}}$. We get:</span>
<span id="cb6-541"><a href="#cb6-541"></a></span>
<span id="cb6-542"><a href="#cb6-542"></a>$$</span>
<span id="cb6-543"><a href="#cb6-543"></a>\begin{align*}</span>
<span id="cb6-544"><a href="#cb6-544"></a>\frac{|x_i||y_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}} &amp;\leq \frac{1}{p} \frac{|x_i|^p}{\sum_{i=1}^n |x_i|^p} + \frac{1}{q}\frac{|y_i|^q}{\sum_{i=1}^n |y_i|^q}</span>
<span id="cb6-545"><a href="#cb6-545"></a>\end{align*}</span>
<span id="cb6-546"><a href="#cb6-546"></a>$$</span>
<span id="cb6-547"><a href="#cb6-547"></a></span>
<span id="cb6-548"><a href="#cb6-548"></a>Summing on both sides, we get:</span>
<span id="cb6-549"><a href="#cb6-549"></a></span>
<span id="cb6-550"><a href="#cb6-550"></a>$$</span>
<span id="cb6-551"><a href="#cb6-551"></a>\begin{align*}</span>
<span id="cb6-552"><a href="#cb6-552"></a>\frac{\sum_{i=1}^n|x_i y_i|}{\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}} &amp;\leq \frac{1}{p} \frac{\sum_{i=1}^n |x_i|^p}{\sum_{i=1}^n |x_i|^p} + \frac{1}{q}\frac{\sum_{i=1}^n|y_i|^q}{\sum_{i=1}^n |y_i|^q}<span class="sc">\\</span></span>
<span id="cb6-553"><a href="#cb6-553"></a>&amp;= \frac{1}{p} + \frac{1}{q}<span class="sc">\\</span></span>
<span id="cb6-554"><a href="#cb6-554"></a>&amp;= 1<span class="sc">\\</span></span>
<span id="cb6-555"><a href="#cb6-555"></a>\sum_{i=1}^n |x_i y_i| &amp;\leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\left(\sum_{i=1}^n |y_i|^q\right)^{1/q}</span>
<span id="cb6-556"><a href="#cb6-556"></a>\end{align*}</span>
<span id="cb6-557"><a href="#cb6-557"></a>$$</span>
<span id="cb6-558"><a href="#cb6-558"></a></span>
<span id="cb6-559"><a href="#cb6-559"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-560"><a href="#cb6-560"></a></span>
<span id="cb6-561"><a href="#cb6-561"></a><span class="fu">## The vector $p$-norm</span></span>
<span id="cb6-562"><a href="#cb6-562"></a></span>
<span id="cb6-563"><a href="#cb6-563"></a>The vector $1$-norm and $2$-norm are special cases of the $p$-norm.</span>
<span id="cb6-564"><a href="#cb6-564"></a></span>
<span id="cb6-565"><a href="#cb6-565"></a>::: {#def-vector-p-norm} </span>
<span id="cb6-566"><a href="#cb6-566"></a></span>
<span id="cb6-567"><a href="#cb6-567"></a><span class="fu">### $p$-norm</span></span>
<span id="cb6-568"><a href="#cb6-568"></a></span>
<span id="cb6-569"><a href="#cb6-569"></a>Given $p \geq 1$, the vector $p$-norm $\norm{\cdot}_p : \C^n \to \R$ is defined by :</span>
<span id="cb6-570"><a href="#cb6-570"></a></span>
<span id="cb6-571"><a href="#cb6-571"></a>$$</span>
<span id="cb6-572"><a href="#cb6-572"></a>\norm{\bf{x}}_p = \left(\sum_{i=1}^n |\chi_i|^p\right)^{1/p}</span>
<span id="cb6-573"><a href="#cb6-573"></a>$$</span>
<span id="cb6-574"><a href="#cb6-574"></a>:::</span>
<span id="cb6-575"><a href="#cb6-575"></a></span>
<span id="cb6-576"><a href="#cb6-576"></a>::: {#thm-p-norm-is-a-norm}</span>
<span id="cb6-577"><a href="#cb6-577"></a></span>
<span id="cb6-578"><a href="#cb6-578"></a>The vector $p$-norm is a well-defined norm.</span>
<span id="cb6-579"><a href="#cb6-579"></a>:::</span>
<span id="cb6-580"><a href="#cb6-580"></a></span>
<span id="cb6-581"><a href="#cb6-581"></a>*Proof.*</span>
<span id="cb6-582"><a href="#cb6-582"></a></span>
<span id="cb6-583"><a href="#cb6-583"></a>*Positive semi-definite*</span>
<span id="cb6-584"><a href="#cb6-584"></a></span>
<span id="cb6-585"><a href="#cb6-585"></a>We have:</span>
<span id="cb6-586"><a href="#cb6-586"></a></span>
<span id="cb6-587"><a href="#cb6-587"></a>$$</span>
<span id="cb6-588"><a href="#cb6-588"></a>\begin{align*}</span>
<span id="cb6-589"><a href="#cb6-589"></a>\norm{\bf{x}}_p &amp;= \left(\sum_{i=1}^n |\chi_i|^p \right)^{1/p}<span class="sc">\\</span></span>
<span id="cb6-590"><a href="#cb6-590"></a>&amp;\geq \left(|\chi_i|^p \right)^{1/p}<span class="sc">\\</span></span>
<span id="cb6-591"><a href="#cb6-591"></a>&amp;= |\chi_i| \geq 0</span>
<span id="cb6-592"><a href="#cb6-592"></a>\end{align*}</span>
<span id="cb6-593"><a href="#cb6-593"></a>$$</span>
<span id="cb6-594"><a href="#cb6-594"></a></span>
<span id="cb6-595"><a href="#cb6-595"></a>*Homogeneity*</span>
<span id="cb6-596"><a href="#cb6-596"></a></span>
<span id="cb6-597"><a href="#cb6-597"></a>We have:</span>
<span id="cb6-598"><a href="#cb6-598"></a></span>
<span id="cb6-599"><a href="#cb6-599"></a>$$</span>
<span id="cb6-600"><a href="#cb6-600"></a>\begin{align*}</span>
<span id="cb6-601"><a href="#cb6-601"></a>\norm{\alpha \bf{x}}_p &amp;= \left(\sum_{i=1}^n |\alpha \chi_i|^p \right)^{1/p}<span class="sc">\\</span></span>
<span id="cb6-602"><a href="#cb6-602"></a>&amp;= \left(\sum_{i=1}^n |\alpha|^p |\chi_i|^p \right)^{1/p}<span class="sc">\\</span></span>
<span id="cb6-603"><a href="#cb6-603"></a>&amp;= |\alpha|\left(\sum_{i=1}^n |\chi_i|^p \right)^{1/p} &amp;= |\alpha|\norm{\bf{x}}_p</span>
<span id="cb6-604"><a href="#cb6-604"></a>\end{align*}</span>
<span id="cb6-605"><a href="#cb6-605"></a>$$</span>
<span id="cb6-606"><a href="#cb6-606"></a></span>
<span id="cb6-607"><a href="#cb6-607"></a>*Triangle Inequality*</span>
<span id="cb6-608"><a href="#cb6-608"></a></span>
<span id="cb6-609"><a href="#cb6-609"></a>Define $\frac{1}{q} := 1 - \frac{1}{p}$. $\Longrightarrow (p-1)q = p$.</span>
<span id="cb6-610"><a href="#cb6-610"></a></span>
<span id="cb6-611"><a href="#cb6-611"></a>By the Holder's inequality:</span>
<span id="cb6-612"><a href="#cb6-612"></a>$$</span>
<span id="cb6-613"><a href="#cb6-613"></a>\begin{align*}</span>
<span id="cb6-614"><a href="#cb6-614"></a>\sum_{i=1}^n |x_i||x_i + y_i|^{p-1} &amp;\leq \left(\sum_{i=1}^n |x_i|^p\right)^{1/p} \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}<span class="sc">\\</span></span>
<span id="cb6-615"><a href="#cb6-615"></a>\sum_{i=1}^n |y_i||x_i + y_i|^{p-1} &amp;\leq \left(\sum_{i=1}^n |y_i|^p\right)^{1/p} \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}</span>
<span id="cb6-616"><a href="#cb6-616"></a>\end{align*}</span>
<span id="cb6-617"><a href="#cb6-617"></a>$$</span>
<span id="cb6-618"><a href="#cb6-618"></a></span>
<span id="cb6-619"><a href="#cb6-619"></a>Summing, we get:</span>
<span id="cb6-620"><a href="#cb6-620"></a></span>
<span id="cb6-621"><a href="#cb6-621"></a>$$</span>
<span id="cb6-622"><a href="#cb6-622"></a>\begin{align*}</span>
<span id="cb6-623"><a href="#cb6-623"></a>\sum_{i=1}^n |x_i + y_i|^{p} &amp;\leq \left<span class="sc">\{</span>\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right<span class="sc">\}</span> \left(\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\right)^{1/q}<span class="sc">\\</span></span>
<span id="cb6-624"><a href="#cb6-624"></a>&amp;= \left<span class="sc">\{</span>\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right<span class="sc">\}</span>\left(\sum_{i=1}^n |x_i + y_i|^{p}\right)^{1-\frac{1}{p}}<span class="sc">\\</span></span>
<span id="cb6-625"><a href="#cb6-625"></a>\Longrightarrow \left(\sum_{i=1}^n |x_i + y_i|^{p}\right)^{1/p} &amp;\leq \left<span class="sc">\{</span>\left(\sum_{i=1}^n |x_i|^p\right)^{1/p}+ \left(\sum_{i=1}^n |y_i|^p\right)^{1/p}\right<span class="sc">\}</span></span>
<span id="cb6-626"><a href="#cb6-626"></a>\end{align*}</span>
<span id="cb6-627"><a href="#cb6-627"></a>$$</span>
<span id="cb6-628"><a href="#cb6-628"></a></span>
<span id="cb6-629"><a href="#cb6-629"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-630"><a href="#cb6-630"></a></span>
<span id="cb6-631"><a href="#cb6-631"></a><span class="fu">## The vector $\infty$-norm</span></span>
<span id="cb6-632"><a href="#cb6-632"></a></span>
<span id="cb6-633"><a href="#cb6-633"></a></span>
<span id="cb6-634"><a href="#cb6-634"></a>::: {#def-infinity-norm}</span>
<span id="cb6-635"><a href="#cb6-635"></a></span>
<span id="cb6-636"><a href="#cb6-636"></a><span class="fu">### $\infty$-norm</span></span>
<span id="cb6-637"><a href="#cb6-637"></a></span>
<span id="cb6-638"><a href="#cb6-638"></a>The vector $\infty$-norm, $\norm{\cdot}:\C^n \to \R$ is defined for $\bf{x} \in \C^n$ by:</span>
<span id="cb6-639"><a href="#cb6-639"></a></span>
<span id="cb6-640"><a href="#cb6-640"></a>$$</span>
<span id="cb6-641"><a href="#cb6-641"></a>\norm{\bf{x}}_\infty = \max<span class="sc">\{</span>|\chi_1|,|\chi_2|,\ldots,|\chi_n|<span class="sc">\}</span></span>
<span id="cb6-642"><a href="#cb6-642"></a>$$</span>
<span id="cb6-643"><a href="#cb6-643"></a></span>
<span id="cb6-644"><a href="#cb6-644"></a>The $\infty$-norm simply measures how long the vector is by the magnitude of its largest entry.</span>
<span id="cb6-645"><a href="#cb6-645"></a>:::</span>
<span id="cb6-646"><a href="#cb6-646"></a></span>
<span id="cb6-647"><a href="#cb6-647"></a>::: {#thm-infty-norm-is-a-norm}</span>
<span id="cb6-648"><a href="#cb6-648"></a></span>
<span id="cb6-649"><a href="#cb6-649"></a>The vector $\infty$-norm is well-defined.</span>
<span id="cb6-650"><a href="#cb6-650"></a>:::</span>
<span id="cb6-651"><a href="#cb6-651"></a></span>
<span id="cb6-652"><a href="#cb6-652"></a>*Proof.*</span>
<span id="cb6-653"><a href="#cb6-653"></a></span>
<span id="cb6-654"><a href="#cb6-654"></a>*Positive semi-definiteness*</span>
<span id="cb6-655"><a href="#cb6-655"></a></span>
<span id="cb6-656"><a href="#cb6-656"></a>We have:</span>
<span id="cb6-657"><a href="#cb6-657"></a></span>
<span id="cb6-658"><a href="#cb6-658"></a>$$</span>
<span id="cb6-659"><a href="#cb6-659"></a>\norm{\bf{x}}_{\infty} = \max_{1\leq i \leq n} |\chi_i| \geq |\xi_i| \geq 0</span>
<span id="cb6-660"><a href="#cb6-660"></a>$$</span>
<span id="cb6-661"><a href="#cb6-661"></a></span>
<span id="cb6-662"><a href="#cb6-662"></a>*Homogeneity*</span>
<span id="cb6-663"><a href="#cb6-663"></a></span>
<span id="cb6-664"><a href="#cb6-664"></a>We have:</span>
<span id="cb6-665"><a href="#cb6-665"></a></span>
<span id="cb6-666"><a href="#cb6-666"></a>$$</span>
<span id="cb6-667"><a href="#cb6-667"></a>\norm{\alpha \bf{x}}_{\infty} = \max_{1\leq i \leq n}|\alpha \chi_i| =\max_{1\leq i \leq n}|\alpha|| \chi_i| = |\alpha| \max_{1\leq i \leq n}|\chi_i| = |\alpha|\norm{\bf{x}}_{\infty}</span>
<span id="cb6-668"><a href="#cb6-668"></a>$$</span>
<span id="cb6-669"><a href="#cb6-669"></a></span>
<span id="cb6-670"><a href="#cb6-670"></a>*Triangle Inequality*</span>
<span id="cb6-671"><a href="#cb6-671"></a></span>
<span id="cb6-672"><a href="#cb6-672"></a>$$</span>
<span id="cb6-673"><a href="#cb6-673"></a>\begin{align*}</span>
<span id="cb6-674"><a href="#cb6-674"></a>\norm{\bf{x} + \bf{y}}_\infty &amp;= \max_{i=1}^m |\chi_i + \xi_i|<span class="sc">\\</span></span>
<span id="cb6-675"><a href="#cb6-675"></a>&amp;\leq \max_{i=1}^m (|\chi_i| + |\xi_i|)<span class="sc">\\</span></span>
<span id="cb6-676"><a href="#cb6-676"></a>&amp;\leq \max_{i=1}^m |\chi_i| + \max_{i=1}^m |\xi_i|<span class="sc">\\</span></span>
<span id="cb6-677"><a href="#cb6-677"></a>&amp;= \norm{\bf{x}}_\infty + \norm{\bf{y}}_\infty</span>
<span id="cb6-678"><a href="#cb6-678"></a>\end{align*}</span>
<span id="cb6-679"><a href="#cb6-679"></a>$$</span>
<span id="cb6-680"><a href="#cb6-680"></a></span>
<span id="cb6-681"><a href="#cb6-681"></a><span class="fu">## Equivalence of vector norms</span></span>
<span id="cb6-682"><a href="#cb6-682"></a></span>
<span id="cb6-683"><a href="#cb6-683"></a>As I was saying earlier, we often measure if a vector is *small* or *large* or the distance between two vectors by computing norms. It would be unfortunate, if a vector were *small* in one norm, yet *large* in another. Fortunately, the next theorem excludes this possibility.</span>
<span id="cb6-684"><a href="#cb6-684"></a></span>
<span id="cb6-685"><a href="#cb6-685"></a>::: {#thm-equivalence-of-vector-norms}</span>
<span id="cb6-686"><a href="#cb6-686"></a></span>
<span id="cb6-687"><a href="#cb6-687"></a><span class="fu">### Equivalence of vector norms</span></span>
<span id="cb6-688"><a href="#cb6-688"></a></span>
<span id="cb6-689"><a href="#cb6-689"></a>Let $\norm{\cdot}_a:\C^n \to \R$ and $\norm{\cdot}_b:\C^n\to \R$ both be vector norms. Then there exist positive scalars $C_1$ and $C_2$ such that for $\bf{x}\in \C^n$, </span>
<span id="cb6-690"><a href="#cb6-690"></a></span>
<span id="cb6-691"><a href="#cb6-691"></a>$$</span>
<span id="cb6-692"><a href="#cb6-692"></a>C_1 \norm{\bf{x}}_b \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_b</span>
<span id="cb6-693"><a href="#cb6-693"></a>$$</span>
<span id="cb6-694"><a href="#cb6-694"></a>:::</span>
<span id="cb6-695"><a href="#cb6-695"></a></span>
<span id="cb6-696"><a href="#cb6-696"></a>*Proof.*</span>
<span id="cb6-697"><a href="#cb6-697"></a></span>
<span id="cb6-698"><a href="#cb6-698"></a>We can prove equivalence of norms in four steps, the last which uses the extreme value theorem from Real Analysis. </span>
<span id="cb6-699"><a href="#cb6-699"></a></span>
<span id="cb6-700"><a href="#cb6-700"></a><span class="fu">#### Step 1: It is sufficient to consider $\norm{\cdot}_b = \norm{\cdot}_1$ (transitivity).</span></span>
<span id="cb6-701"><a href="#cb6-701"></a></span>
<span id="cb6-702"><a href="#cb6-702"></a>We will show that it is sufficient to prove that $\norm{\cdot}_a$ is equivalent to $\norm{\cdot}_1$ because norm equivalence is *transitive*: if two norms are equivalent to $\norm{\cdot}_1$, then they are equivalent to each other. In particular, suppose both $\norm{\cdot}_a$ and $\norm{\cdot}_{a'}$ are equivalent to $\norm{\cdot}_1$ for constants $0 \leq C_1 \leq C_2$ and $0 \leq C_1' \leq C_2'$ respectively:</span>
<span id="cb6-703"><a href="#cb6-703"></a></span>
<span id="cb6-704"><a href="#cb6-704"></a>$$</span>
<span id="cb6-705"><a href="#cb6-705"></a>C_1 \norm{\bf{x}}_1 \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_1</span>
<span id="cb6-706"><a href="#cb6-706"></a>$$</span>
<span id="cb6-707"><a href="#cb6-707"></a></span>
<span id="cb6-708"><a href="#cb6-708"></a>and</span>
<span id="cb6-709"><a href="#cb6-709"></a></span>
<span id="cb6-710"><a href="#cb6-710"></a>$$</span>
<span id="cb6-711"><a href="#cb6-711"></a>C_1' \norm{\bf{x}}_1 \leq \norm{\bf{x}}_{a'} \leq C_2' \norm{\bf{x}}_1</span>
<span id="cb6-712"><a href="#cb6-712"></a>$$</span>
<span id="cb6-713"><a href="#cb6-713"></a></span>
<span id="cb6-714"><a href="#cb6-714"></a>Then, it immediately follows that:</span>
<span id="cb6-715"><a href="#cb6-715"></a></span>
<span id="cb6-716"><a href="#cb6-716"></a>$$</span>
<span id="cb6-717"><a href="#cb6-717"></a>\norm{\bf{x}}_{a'} \leq C_2' \norm{\bf{x}}_1 \leq \frac{C_2'}{C_1} \norm{\bf{x}}_a</span>
<span id="cb6-718"><a href="#cb6-718"></a>$$</span>
<span id="cb6-719"><a href="#cb6-719"></a></span>
<span id="cb6-720"><a href="#cb6-720"></a>and </span>
<span id="cb6-721"><a href="#cb6-721"></a></span>
<span id="cb6-722"><a href="#cb6-722"></a>$$</span>
<span id="cb6-723"><a href="#cb6-723"></a>\norm{\bf{x}}_{a'} \geq C_1' \norm{\bf{x}}_1 \geq \frac{C_1'}{C_2} \norm{\bf{x}}_a</span>
<span id="cb6-724"><a href="#cb6-724"></a>$$</span>
<span id="cb6-725"><a href="#cb6-725"></a></span>
<span id="cb6-726"><a href="#cb6-726"></a>and hence $\norm{\cdot}_a$ and $\norm{\cdot}_{a'}$ are equivalent. $\blacksquare$</span>
<span id="cb6-727"><a href="#cb6-727"></a></span>
<span id="cb6-728"><a href="#cb6-728"></a><span class="fu">#### Step 2: It is sufficient to consider only $\bf{x}$ with $\norm{\bf{x}}_1 = 1$.</span></span>
<span id="cb6-729"><a href="#cb6-729"></a></span>
<span id="cb6-730"><a href="#cb6-730"></a>We wish to show that </span>
<span id="cb6-731"><a href="#cb6-731"></a></span>
<span id="cb6-732"><a href="#cb6-732"></a>$$</span>
<span id="cb6-733"><a href="#cb6-733"></a>C_1 \norm{\bf{x}}_1 \leq \norm{\bf{x}}_a \leq C_2 \norm{\bf{x}}_1</span>
<span id="cb6-734"><a href="#cb6-734"></a>$$</span>
<span id="cb6-735"><a href="#cb6-735"></a></span>
<span id="cb6-736"><a href="#cb6-736"></a>is true for all $\bf{x} \in V$ for some $C_1$, $C_2$. It is trivially true for $\bf{x}=\bf{0}$, so we only need to consider $\bf{x}\neq\bf{0}$, in which case, we can divide by $\norm{\bf{x}}_1$, to obtain the condition:</span>
<span id="cb6-737"><a href="#cb6-737"></a></span>
<span id="cb6-738"><a href="#cb6-738"></a>$$</span>
<span id="cb6-739"><a href="#cb6-739"></a>C_1 \leq \norm{\frac{\bf{x}}{\norm{\bf{x}}_1 }}_a \leq C_2 </span>
<span id="cb6-740"><a href="#cb6-740"></a>$$</span>
<span id="cb6-741"><a href="#cb6-741"></a></span>
<span id="cb6-742"><a href="#cb6-742"></a>The vector $\bf{u} = \frac{\bf{x}}{\norm{\bf{x}}_1}$ is a unit vector in the $1$-norm, $\norm{\bf{u}}_1 = 1$. So, we can write:</span>
<span id="cb6-743"><a href="#cb6-743"></a></span>
<span id="cb6-744"><a href="#cb6-744"></a>$$</span>
<span id="cb6-745"><a href="#cb6-745"></a>C_1 \leq \norm{\bf{u}}_a \leq C_2 </span>
<span id="cb6-746"><a href="#cb6-746"></a>$$</span>
<span id="cb6-747"><a href="#cb6-747"></a></span>
<span id="cb6-748"><a href="#cb6-748"></a>We have the desired result. $\blacksquare$</span>
<span id="cb6-749"><a href="#cb6-749"></a></span>
<span id="cb6-750"><a href="#cb6-750"></a><span class="fu">#### Step 3: Any norm $\norm{\cdot}_a$ is continuous under $\norm{\cdot}_1$.</span></span>
<span id="cb6-751"><a href="#cb6-751"></a></span>
<span id="cb6-752"><a href="#cb6-752"></a>We wish to show that any norm $\norm{\cdot}_a$ is a continuous function on $V$ under the topology induced by $\norm{\cdot}_1$. That is, we wish to show that for any $\epsilon &gt; 0$, there exists $\delta &gt; 0$, such that for all $\norm{\bf{x} - \bf{c}}_1 &lt; \delta$, we have $\norm{\norm{\bf{x}}_a - \norm{\bf{c}}_a}_1 &lt; \epsilon$. </span>
<span id="cb6-753"><a href="#cb6-753"></a></span>
<span id="cb6-754"><a href="#cb6-754"></a>We prove this into two steps. First, by the triangle inequality on $\norm{\cdot}_a$, it follows that:</span>
<span id="cb6-755"><a href="#cb6-755"></a></span>
<span id="cb6-756"><a href="#cb6-756"></a>$$</span>
<span id="cb6-757"><a href="#cb6-757"></a>\begin{align*}</span>
<span id="cb6-758"><a href="#cb6-758"></a>\norm{\bf{x}}_a - \norm{\bf{c}}_a &amp;= \norm{\bf{c} + (\bf{x} - \bf{c})}_a - \norm{\bf{c}}_a <span class="sc">\\</span></span>
<span id="cb6-759"><a href="#cb6-759"></a>&amp;\leq \norm{\bf{c}}_a + \norm{(\bf{x} - \bf{c})}_a - \norm{\bf{c}}_a<span class="sc">\\</span></span>
<span id="cb6-760"><a href="#cb6-760"></a>&amp;= \norm{(\bf{x} - \bf{c})}_a</span>
<span id="cb6-761"><a href="#cb6-761"></a>\end{align*}</span>
<span id="cb6-762"><a href="#cb6-762"></a>$$</span>
<span id="cb6-763"><a href="#cb6-763"></a></span>
<span id="cb6-764"><a href="#cb6-764"></a>And</span>
<span id="cb6-765"><a href="#cb6-765"></a></span>
<span id="cb6-766"><a href="#cb6-766"></a>$$</span>
<span id="cb6-767"><a href="#cb6-767"></a>\begin{align*}</span>
<span id="cb6-768"><a href="#cb6-768"></a>\norm{\bf{c}}_a - \norm{\bf{x}}_a &amp;\leq \norm{(\bf{x} - \bf{c})}_a</span>
<span id="cb6-769"><a href="#cb6-769"></a>\end{align*}</span>
<span id="cb6-770"><a href="#cb6-770"></a>$$</span>
<span id="cb6-771"><a href="#cb6-771"></a></span>
<span id="cb6-772"><a href="#cb6-772"></a>and hence:</span>
<span id="cb6-773"><a href="#cb6-773"></a></span>
<span id="cb6-774"><a href="#cb6-774"></a>$$</span>
<span id="cb6-775"><a href="#cb6-775"></a>|\norm{\bf{x}}_a - \norm{\bf{c}}_a| \leq \norm{(\bf{x} - \bf{c})}_a</span>
<span id="cb6-776"><a href="#cb6-776"></a>$$</span>
<span id="cb6-777"><a href="#cb6-777"></a></span>
<span id="cb6-778"><a href="#cb6-778"></a>Second applying the triangle inequality again, and writing $\bf{x} = \sum_{i=1}^n \alpha_i \bf{e}_i$ and $\bf{c} = \sum_{i=1}^n \alpha_i' \bf{e}_i$ in our basis, we obtain:</span>
<span id="cb6-779"><a href="#cb6-779"></a></span>
<span id="cb6-780"><a href="#cb6-780"></a>$$</span>
<span id="cb6-781"><a href="#cb6-781"></a>\begin{align*}</span>
<span id="cb6-782"><a href="#cb6-782"></a>\norm{\bf{x}-\bf{c}}_a &amp;= \norm{\sum_{i=1}^n (\alpha_i - \alpha_i')\bf{e}_i}_a<span class="sc">\\</span></span>
<span id="cb6-783"><a href="#cb6-783"></a>&amp;\leq \sum_{i=1}^n \norm{(\alpha_i - \alpha_i')\bf{e}_i}_a &amp; <span class="sc">\{</span> \text{ Triangle Inequality }<span class="sc">\}\\</span></span>
<span id="cb6-784"><a href="#cb6-784"></a>&amp;= \sum_{i=1}^n |(\alpha_i - \alpha_i')|\norm{\bf{e}_i}_a <span class="sc">\\</span></span>
<span id="cb6-785"><a href="#cb6-785"></a>&amp;= \norm{\bf{x} - \bf{c}}_1 \left(\max_i \norm{\bf{e}_i}_a \right)</span>
<span id="cb6-786"><a href="#cb6-786"></a>\end{align*}</span>
<span id="cb6-787"><a href="#cb6-787"></a>$$</span>
<span id="cb6-788"><a href="#cb6-788"></a></span>
<span id="cb6-789"><a href="#cb6-789"></a>Therefore, if we choose:</span>
<span id="cb6-790"><a href="#cb6-790"></a></span>
<span id="cb6-791"><a href="#cb6-791"></a>$$</span>
<span id="cb6-792"><a href="#cb6-792"></a>\delta = \frac{\epsilon}{\left(\max_i \norm{\bf{e}_i}_a \right)}</span>
<span id="cb6-793"><a href="#cb6-793"></a>$$</span>
<span id="cb6-794"><a href="#cb6-794"></a></span>
<span id="cb6-795"><a href="#cb6-795"></a>it immediate follows that:</span>
<span id="cb6-796"><a href="#cb6-796"></a></span>
<span id="cb6-797"><a href="#cb6-797"></a>$$\begin{align*}</span>
<span id="cb6-798"><a href="#cb6-798"></a>\norm{\bf{x} - \bf{c}}_1 &amp;&lt; \delta <span class="sc">\\</span></span>
<span id="cb6-799"><a href="#cb6-799"></a>\Longrightarrow |\norm{\bf{x}}_a - \norm{\bf{c}}_a| &amp;\leq \norm{\bf{x} - \bf{c}}_a <span class="sc">\\</span> &amp;\leq \norm{\bf{x} - \bf{c}}_1 \left(\max_i \norm{\bf{e}_i}_a \right) <span class="sc">\\</span></span>
<span id="cb6-800"><a href="#cb6-800"></a>&amp; \leq \frac{\epsilon}{\left(\max_i \norm{\bf{e}_i}_a \right)} \left(\max_i \norm{\bf{e}_i}_a \right) = \epsilon</span>
<span id="cb6-801"><a href="#cb6-801"></a>\end{align*}</span>
<span id="cb6-802"><a href="#cb6-802"></a>$$</span>
<span id="cb6-803"><a href="#cb6-803"></a></span>
<span id="cb6-804"><a href="#cb6-804"></a>This proves (uniform) continuity. $\blacksquare$</span>
<span id="cb6-805"><a href="#cb6-805"></a></span>
<span id="cb6-806"><a href="#cb6-806"></a><span class="fu">#### Step 4: The maximum and minimum of $\norm{\cdot}_a$ on the unit ball</span></span>
<span id="cb6-807"><a href="#cb6-807"></a></span>
<span id="cb6-808"><a href="#cb6-808"></a>Let $K:=<span class="sc">\{</span>\bf{u}:\norm{\bf{u}}_1 = 1<span class="sc">\}</span>$. Then, $K$ is a compact set. Since $\norm{\cdot}_a$ is continuous on $K$, by the extreme value theorem, $\norm{\cdot}_a$ must achieve a supremum and infimum on the set. So, for all $\bf{u}$ with $\norm{\bf{u}}_1 = 1$, there exists $C_1,C_2 &gt; 0$, such that:</span>
<span id="cb6-809"><a href="#cb6-809"></a></span>
<span id="cb6-810"><a href="#cb6-810"></a>$$ C_1 \leq \norm{\bf{u}}_a \leq C_2$$</span>
<span id="cb6-811"><a href="#cb6-811"></a></span>
<span id="cb6-812"><a href="#cb6-812"></a>as required by step 2. And we are done! $\blacksquare$</span>
<span id="cb6-813"><a href="#cb6-813"></a></span>
<span id="cb6-814"><a href="#cb6-814"></a><span class="fu">### Deriving the constants $C_{1,\infty}$, $C_{\infty,1}$</span></span>
<span id="cb6-815"><a href="#cb6-815"></a></span>
<span id="cb6-816"><a href="#cb6-816"></a>Let's write a python implementation of the various norms.</span>
<span id="cb6-817"><a href="#cb6-817"></a></span>
<span id="cb6-820"><a href="#cb6-820"></a><span class="in">```{python}</span></span>
<span id="cb6-821"><a href="#cb6-821"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-822"><a href="#cb6-822"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-823"><a href="#cb6-823"></a><span class="im">import</span> itertools</span>
<span id="cb6-824"><a href="#cb6-824"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb6-825"><a href="#cb6-825"></a></span>
<span id="cb6-826"><a href="#cb6-826"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb6-827"><a href="#cb6-827"></a></span>
<span id="cb6-828"><a href="#cb6-828"></a><span class="kw">def</span> one_norm(x):</span>
<span id="cb6-829"><a href="#cb6-829"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(x))</span>
<span id="cb6-830"><a href="#cb6-830"></a></span>
<span id="cb6-831"><a href="#cb6-831"></a><span class="kw">def</span> two_norm(x):</span>
<span id="cb6-832"><a href="#cb6-832"></a>    <span class="cf">return</span> np.sqrt(np.<span class="bu">sum</span>(x<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb6-833"><a href="#cb6-833"></a></span>
<span id="cb6-834"><a href="#cb6-834"></a><span class="kw">def</span> p_norm(x,p):</span>
<span id="cb6-835"><a href="#cb6-835"></a>    <span class="cf">return</span> np.<span class="bu">pow</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(x)<span class="op">**</span>p),<span class="fl">1.0</span><span class="op">/</span>p)</span>
<span id="cb6-836"><a href="#cb6-836"></a></span>
<span id="cb6-837"><a href="#cb6-837"></a><span class="kw">def</span> infty_norm(x):</span>
<span id="cb6-838"><a href="#cb6-838"></a>    <span class="cf">return</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(x))</span>
<span id="cb6-839"><a href="#cb6-839"></a></span>
<span id="cb6-840"><a href="#cb6-840"></a><span class="kw">def</span> get_vectors_eq_norm_val(func, val, lower_bound, upper_bound):</span>
<span id="cb6-841"><a href="#cb6-841"></a>    x_1 <span class="op">=</span> np.linspace(lower_bound, upper_bound, </span>
<span id="cb6-842"><a href="#cb6-842"></a>    <span class="bu">int</span>((upper_bound <span class="op">-</span> lower_bound)<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb6-843"><a href="#cb6-843"></a>    x_2 <span class="op">=</span> np.linspace(lower_bound, upper_bound, </span>
<span id="cb6-844"><a href="#cb6-844"></a>    <span class="bu">int</span>((upper_bound <span class="op">-</span> lower_bound)<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb6-845"><a href="#cb6-845"></a></span>
<span id="cb6-846"><a href="#cb6-846"></a>    pts <span class="op">=</span> np.array(<span class="bu">list</span>(itertools.product(x_1, x_2)))</span>
<span id="cb6-847"><a href="#cb6-847"></a>    norm_arr <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(func, pts)))</span>
<span id="cb6-848"><a href="#cb6-848"></a></span>
<span id="cb6-849"><a href="#cb6-849"></a>    pts_norm_list <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(pts,norm_arr))</span>
<span id="cb6-850"><a href="#cb6-850"></a></span>
<span id="cb6-851"><a href="#cb6-851"></a>    pts_with_norm_eq_val <span class="op">=</span> []</span>
<span id="cb6-852"><a href="#cb6-852"></a>    <span class="cf">for</span> pt <span class="kw">in</span> pts_norm_list:</span>
<span id="cb6-853"><a href="#cb6-853"></a>        <span class="cf">if</span> pt[<span class="dv">1</span>] <span class="op">==</span> val:</span>
<span id="cb6-854"><a href="#cb6-854"></a>            pts_with_norm_eq_val.append(pt[<span class="dv">0</span>])</span>
<span id="cb6-855"><a href="#cb6-855"></a></span>
<span id="cb6-856"><a href="#cb6-856"></a>    <span class="cf">return</span> np.array(pts_with_norm_eq_val)</span>
<span id="cb6-857"><a href="#cb6-857"></a><span class="in">```</span></span>
<span id="cb6-858"><a href="#cb6-858"></a></span>
<span id="cb6-859"><a href="#cb6-859"></a>Now, we can glean useful information by visualizing the set of points(vectors) with a given norm.</span>
<span id="cb6-860"><a href="#cb6-860"></a></span>
<span id="cb6-863"><a href="#cb6-863"></a><span class="in">```{python}</span></span>
<span id="cb6-864"><a href="#cb6-864"></a><span class="co"># | code-fold: true</span></span>
<span id="cb6-865"><a href="#cb6-865"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb6-866"><a href="#cb6-866"></a>pts1 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb6-867"><a href="#cb6-867"></a>    func<span class="op">=</span>infty_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">1.0</span>, upper_bound<span class="op">=</span><span class="fl">1.0</span></span>
<span id="cb6-868"><a href="#cb6-868"></a>)</span>
<span id="cb6-869"><a href="#cb6-869"></a></span>
<span id="cb6-870"><a href="#cb6-870"></a>pts2 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb6-871"><a href="#cb6-871"></a>    func<span class="op">=</span>one_norm, val<span class="op">=</span><span class="fl">2.0</span>, lower_bound<span class="op">=-</span><span class="fl">2.0</span>, upper_bound<span class="op">=</span><span class="fl">2.0</span></span>
<span id="cb6-872"><a href="#cb6-872"></a>)</span>
<span id="cb6-873"><a href="#cb6-873"></a></span>
<span id="cb6-874"><a href="#cb6-874"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb6-875"><a href="#cb6-875"></a>plt.xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb6-876"><a href="#cb6-876"></a>plt.ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb6-877"><a href="#cb6-877"></a>a <span class="op">=</span> plt.scatter(pts1[:, <span class="dv">0</span>], pts1[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-878"><a href="#cb6-878"></a>b <span class="op">=</span> plt.scatter(pts2[:, <span class="dv">0</span>], pts2[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-879"><a href="#cb6-879"></a><span class="co"># c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)</span></span>
<span id="cb6-880"><a href="#cb6-880"></a></span>
<span id="cb6-881"><a href="#cb6-881"></a>plt.legend(</span>
<span id="cb6-882"><a href="#cb6-882"></a>    (a, b), (<span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_\infty = 1$"</span>, <span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_1=2$"</span>), loc<span class="op">=</span><span class="st">"lower left"</span></span>
<span id="cb6-883"><a href="#cb6-883"></a>)</span>
<span id="cb6-884"><a href="#cb6-884"></a>plt.show()</span>
<span id="cb6-885"><a href="#cb6-885"></a><span class="in">```</span></span>
<span id="cb6-886"><a href="#cb6-886"></a></span>
<span id="cb6-887"><a href="#cb6-887"></a>The blue rectangle represents all vectors $\bf{x}\in\R^2$ with unit $\infty$-norm, $\norm{\bf{x}}_\infty = 1$. The orange rhombus represents all vectors $\bf{x}$ with $\norm{\bf{x}}_1 = 2$. All points on or outside the blue square represent vectors $\bf{y}$, such that $\norm{\bf{y}}_\infty \geq 1$. Hence, if $\norm{\bf{y}}_1 = 2$, $\norm{\bf{y}}_\infty \geq 1$. </span>
<span id="cb6-888"><a href="#cb6-888"></a></span>
<span id="cb6-889"><a href="#cb6-889"></a>Now, pick any $\bf{z}\neq \bf{0}$. Then, $2\norm{\frac{\bf{z}}{\norm{\bf{z}}_1}}_1 =2$. Thus, $\norm{\frac{2\bf{z}}{\norm{\bf{z}}_1}}_\infty \geq 1$. So, it follows that if $\bf{z}\in\R^2$ is any arbitrary vector, $\norm{\bf{z}}_1 \leq 2 \norm{\bf{z}}_\infty$.</span>
<span id="cb6-890"><a href="#cb6-890"></a></span>
<span id="cb6-891"><a href="#cb6-891"></a>In general, if $\bf{x}\in\C^n$, then:</span>
<span id="cb6-892"><a href="#cb6-892"></a></span>
<span id="cb6-893"><a href="#cb6-893"></a>$$</span>
<span id="cb6-894"><a href="#cb6-894"></a>\begin{align*}</span>
<span id="cb6-895"><a href="#cb6-895"></a>\norm{\bf{x}}_1 &amp;= \sum_{i=1}^n |x_i|<span class="sc">\\</span></span>
<span id="cb6-896"><a href="#cb6-896"></a>&amp;\leq \sum_{i=1}^n \max<span class="sc">\{</span>|x_i|:i=1,2,\ldots,n<span class="sc">\}\\</span></span>
<span id="cb6-897"><a href="#cb6-897"></a>&amp;= n \norm{\bf{x}}_\infty</span>
<span id="cb6-898"><a href="#cb6-898"></a>\end{align*}</span>
<span id="cb6-899"><a href="#cb6-899"></a>$$</span>
<span id="cb6-900"><a href="#cb6-900"></a></span>
<span id="cb6-901"><a href="#cb6-901"></a>Next, in the below plot, the orange rhombus represents vectors $\bf{x}\in\R^2$, such that $\normp{x}{1} = 1$ and all points on or outside the orange rhombus are such that $\normp{y}{1} \geq 1$. The blue square represents vectors $\normp{y}{\infty} = 1$. Consequently, if $\normp{y}{1} = 1$, then $\normp{y}{\infty} \leq \normp{y}{1}$. In general, if $\bf{x}\in C^n$, we have:</span>
<span id="cb6-902"><a href="#cb6-902"></a></span>
<span id="cb6-903"><a href="#cb6-903"></a>$$</span>
<span id="cb6-904"><a href="#cb6-904"></a>\begin{align*}</span>
<span id="cb6-905"><a href="#cb6-905"></a>\normp{x}{\infty} &amp;= \max<span class="sc">\{</span>|x_1|,\ldots,|x_n|<span class="sc">\}\\</span></span>
<span id="cb6-906"><a href="#cb6-906"></a>&amp;\leq \sum_{i=1}^n |x_i|=\normp{x}{1}</span>
<span id="cb6-907"><a href="#cb6-907"></a>\end{align*}</span>
<span id="cb6-908"><a href="#cb6-908"></a>$$</span>
<span id="cb6-909"><a href="#cb6-909"></a></span>
<span id="cb6-910"><a href="#cb6-910"></a>Putting together, we have:</span>
<span id="cb6-911"><a href="#cb6-911"></a></span>
<span id="cb6-912"><a href="#cb6-912"></a>$$</span>
<span id="cb6-913"><a href="#cb6-913"></a>\begin{align*}</span>
<span id="cb6-914"><a href="#cb6-914"></a>\normp{x}{\infty} \leq C_{\infty,1} \normp{x}{1} <span class="sc">\\</span></span>
<span id="cb6-915"><a href="#cb6-915"></a>\normp{x}{1} \leq C_{1,\infty} \normp{x}{\infty}</span>
<span id="cb6-916"><a href="#cb6-916"></a>\end{align*}</span>
<span id="cb6-917"><a href="#cb6-917"></a>$$</span>
<span id="cb6-918"><a href="#cb6-918"></a></span>
<span id="cb6-919"><a href="#cb6-919"></a>where $C_{\infty,1} = 1$ and $C_{1,\infty}=n$.</span>
<span id="cb6-920"><a href="#cb6-920"></a></span>
<span id="cb6-923"><a href="#cb6-923"></a><span class="in">```{python}</span></span>
<span id="cb6-924"><a href="#cb6-924"></a><span class="co"># | code-fold: true</span></span>
<span id="cb6-925"><a href="#cb6-925"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb6-926"><a href="#cb6-926"></a>pts1 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb6-927"><a href="#cb6-927"></a>    func<span class="op">=</span>infty_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">1.0</span>, upper_bound<span class="op">=</span><span class="fl">1.0</span></span>
<span id="cb6-928"><a href="#cb6-928"></a>)</span>
<span id="cb6-929"><a href="#cb6-929"></a></span>
<span id="cb6-930"><a href="#cb6-930"></a>pts2 <span class="op">=</span> get_vectors_eq_norm_val(</span>
<span id="cb6-931"><a href="#cb6-931"></a>    func<span class="op">=</span>one_norm, val<span class="op">=</span><span class="fl">1.0</span>, lower_bound<span class="op">=-</span><span class="fl">2.0</span>, upper_bound<span class="op">=</span><span class="fl">2.0</span></span>
<span id="cb6-932"><a href="#cb6-932"></a>)</span>
<span id="cb6-933"><a href="#cb6-933"></a></span>
<span id="cb6-934"><a href="#cb6-934"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb6-935"><a href="#cb6-935"></a>plt.xlabel(<span class="vs">r"$x_1$"</span>)</span>
<span id="cb6-936"><a href="#cb6-936"></a>plt.ylabel(<span class="vs">r"$x_2$"</span>)</span>
<span id="cb6-937"><a href="#cb6-937"></a>a <span class="op">=</span> plt.scatter(pts1[:, <span class="dv">0</span>], pts1[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-938"><a href="#cb6-938"></a>b <span class="op">=</span> plt.scatter(pts2[:, <span class="dv">0</span>], pts2[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-939"><a href="#cb6-939"></a><span class="co"># c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)</span></span>
<span id="cb6-940"><a href="#cb6-940"></a></span>
<span id="cb6-941"><a href="#cb6-941"></a>plt.legend(</span>
<span id="cb6-942"><a href="#cb6-942"></a>    (a, b), (<span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_\infty = 1$"</span>, <span class="vs">r"$||\mathbf</span><span class="sc">{x}</span><span class="vs">||_1=1$"</span>), loc<span class="op">=</span><span class="st">"lower left"</span></span>
<span id="cb6-943"><a href="#cb6-943"></a>)</span>
<span id="cb6-944"><a href="#cb6-944"></a>plt.show()</span>
<span id="cb6-945"><a href="#cb6-945"></a><span class="in">```</span></span>
<span id="cb6-946"><a href="#cb6-946"></a></span>
<span id="cb6-947"><a href="#cb6-947"></a><span class="fu">### Deriving the constants $C_{1,2}$, $C_{2,1}$</span></span>
<span id="cb6-948"><a href="#cb6-948"></a></span>
<span id="cb6-949"><a href="#cb6-949"></a>We can also derive the constants $C_{1,2}$ and $C_{2,1}$. We have:</span>
<span id="cb6-950"><a href="#cb6-950"></a></span>
<span id="cb6-951"><a href="#cb6-951"></a>Let $\bf{x}\in\C^n$ be an arbitrary vector. And let $\bf{y}=(1+0i,\ldots,1+0i)$. By the Cauchy-Schwarz inequality,</span>
<span id="cb6-952"><a href="#cb6-952"></a></span>
<span id="cb6-953"><a href="#cb6-953"></a>$$</span>
<span id="cb6-954"><a href="#cb6-954"></a>\begin{align*}</span>
<span id="cb6-955"><a href="#cb6-955"></a>\sum_{i=1}^n |x_i| \leq \left(\sum_{i=1}^n |x_i|^2\right)^{1/2}\sqrt{n}</span>
<span id="cb6-956"><a href="#cb6-956"></a>\end{align*}</span>
<span id="cb6-957"><a href="#cb6-957"></a>$$</span>
<span id="cb6-958"><a href="#cb6-958"></a></span>
<span id="cb6-959"><a href="#cb6-959"></a>So, our claim is $\normp{x}{1} \leq \sqrt{n}\normp{x}{2}$. </span>
<span id="cb6-960"><a href="#cb6-960"></a></span>
<span id="cb6-961"><a href="#cb6-961"></a>Also, consider the vector $\bf{v}=\left(\frac{1}{\sqrt{n}},\ldots,\frac{1}{\sqrt{n}}\right)$. $\norm{\bf{v}}_1 = \sqrt{n}\norm{\bf{v}}_2$. So, the bound is tight.</span>
<span id="cb6-962"><a href="#cb6-962"></a></span>
<span id="cb6-963"><a href="#cb6-963"></a>Moreover:</span>
<span id="cb6-964"><a href="#cb6-964"></a></span>
<span id="cb6-965"><a href="#cb6-965"></a>$$</span>
<span id="cb6-966"><a href="#cb6-966"></a>\begin{align*}</span>
<span id="cb6-967"><a href="#cb6-967"></a>\normp{x}{2}^2 &amp;= \sum_{i=1}^n |x_i|^2 <span class="sc">\\</span></span>
<span id="cb6-968"><a href="#cb6-968"></a>&amp;\leq \sum_{i=1}^n |x_i|^2 + \sum_{i \neq j}|x_i||x_j|<span class="sc">\\</span></span>
<span id="cb6-969"><a href="#cb6-969"></a>&amp;= \sum_{i=1}^n |x_i|^2 + \sum_{i &lt; j}2|x_i||x_j|<span class="sc">\\</span></span>
<span id="cb6-970"><a href="#cb6-970"></a>&amp;= \left(\sum_{i=1}^n |x_i|\right)^2</span>
<span id="cb6-971"><a href="#cb6-971"></a>\end{align*}</span>
<span id="cb6-972"><a href="#cb6-972"></a>$$</span>
<span id="cb6-973"><a href="#cb6-973"></a></span>
<span id="cb6-974"><a href="#cb6-974"></a>So, $\normp{x}{2} \leq \normp{x}{1}$. Consider the standard basis vector $\bf{e}_1 = (1,0,0,\ldots,0)$. $\norm{\bf{e}_1}_2 = \norm{\bf{e}_1}_1$. Hence, the bound is tight. We conclude that:</span>
<span id="cb6-975"><a href="#cb6-975"></a></span>
<span id="cb6-976"><a href="#cb6-976"></a>$$</span>
<span id="cb6-977"><a href="#cb6-977"></a>\begin{align*}</span>
<span id="cb6-978"><a href="#cb6-978"></a>\normp{x}{1} \leq C_{1,2} \normp{x}{2}<span class="sc">\\</span></span>
<span id="cb6-979"><a href="#cb6-979"></a>\normp{x}{2} \leq C_{2,1} \normp{x}{1}</span>
<span id="cb6-980"><a href="#cb6-980"></a>\end{align*}</span>
<span id="cb6-981"><a href="#cb6-981"></a>$$</span>
<span id="cb6-982"><a href="#cb6-982"></a></span>
<span id="cb6-983"><a href="#cb6-983"></a>where $C_{1,2} = \sqrt{n}$ and $C_{2,1} = 1$.</span>
<span id="cb6-984"><a href="#cb6-984"></a></span>
<span id="cb6-985"><a href="#cb6-985"></a><span class="fu">### Deriving the constants $C_{2,\infty}$ and $C_{\infty,2}$</span></span>
<span id="cb6-986"><a href="#cb6-986"></a></span>
<span id="cb6-987"><a href="#cb6-987"></a>Let $x \in \C^n$. We have:</span>
<span id="cb6-988"><a href="#cb6-988"></a></span>
<span id="cb6-989"><a href="#cb6-989"></a>$$</span>
<span id="cb6-990"><a href="#cb6-990"></a>\begin{align}</span>
<span id="cb6-991"><a href="#cb6-991"></a>\norm{x}_2^2 &amp; = \sum_{i=0}^{n-1}|\chi_i|^2<span class="sc">\\</span></span>
<span id="cb6-992"><a href="#cb6-992"></a>&amp;\leq\sum_{i=0}^{n-1} (\max_{i=0}^{n-1}|\chi_i|)^2<span class="sc">\\</span></span>
<span id="cb6-993"><a href="#cb6-993"></a>&amp;= n \norm{x}_\infty</span>
<span id="cb6-994"><a href="#cb6-994"></a>\end{align}</span>
<span id="cb6-995"><a href="#cb6-995"></a>$$</span>
<span id="cb6-996"><a href="#cb6-996"></a></span>
<span id="cb6-997"><a href="#cb6-997"></a>So, $\norm{x}_2 \leq \sqrt{n} \norm{x}_\infty$.</span>
<span id="cb6-998"><a href="#cb6-998"></a></span>
<span id="cb6-999"><a href="#cb6-999"></a>Moreover, let $x = (1, 1, \ldots, 1)^T$. Then, $\norm{x}_2 = \sqrt{n}$ and $\norm{x}_\infty = 1$, so $\norm{x}_2 = \sqrt{n}\norm{x}_\infty$. Hence, it is a tight inequality.</span>
<span id="cb6-1000"><a href="#cb6-1000"></a></span>
<span id="cb6-1001"><a href="#cb6-1001"></a>Also, we have:</span>
<span id="cb6-1002"><a href="#cb6-1002"></a></span>
<span id="cb6-1003"><a href="#cb6-1003"></a>$$</span>
<span id="cb6-1004"><a href="#cb6-1004"></a>\begin{align*}</span>
<span id="cb6-1005"><a href="#cb6-1005"></a>\norm{x}_\infty^2 &amp;= \max \{|\chi_0|^2,|\chi_1|^2,\ldots,|\chi_{n-1}^2|<span class="sc">\}\\</span></span>
<span id="cb6-1006"><a href="#cb6-1006"></a>&amp;\leq \max <span class="sc">\{</span>\sum_{i=0}^{n-1}|\chi_i|^2,\sum_{i=0}^{n-1}|\chi_i|^2,\ldots,\sum_{i=0}^{n-1}|\chi_i|^2|<span class="sc">\}\\</span></span>
<span id="cb6-1007"><a href="#cb6-1007"></a>&amp;= \norm{x}_2^2</span>
<span id="cb6-1008"><a href="#cb6-1008"></a>\end{align*}</span>
<span id="cb6-1009"><a href="#cb6-1009"></a>$$</span>
<span id="cb6-1010"><a href="#cb6-1010"></a></span>
<span id="cb6-1011"><a href="#cb6-1011"></a>Moreover, let $x = (1, 0)$. Then, $\norm{x}_2 = 1$ and $\norm{x}_\infty = 1$. So, $\norm{x}_\infty = \norm{x}_2$. Hence, the inequality is tight.</span>
<span id="cb6-1012"><a href="#cb6-1012"></a></span>
<span id="cb6-1013"><a href="#cb6-1013"></a><span class="fu">## Matrix Norms</span></span>
<span id="cb6-1014"><a href="#cb6-1014"></a></span>
<span id="cb6-1015"><a href="#cb6-1015"></a>The analysis of matrix algorithms requires the use of matrix norms. For example, the quality of a linear system solution may be poor, if the matrix of coefficients is *nearly singular*. To quantify the notion of singularity, we need a measure of the distance on the space of matrices. Matrix norms can be used to  provide that measure.</span>
<span id="cb6-1016"><a href="#cb6-1016"></a></span>
<span id="cb6-1017"><a href="#cb6-1017"></a><span class="fu">### Definitions</span></span>
<span id="cb6-1018"><a href="#cb6-1018"></a></span>
<span id="cb6-1019"><a href="#cb6-1019"></a>Since $\R^{m \times n}$ is isomorphic $\R^{mn}$, the definition of a matrix norm is equivalent to the definition of a vector norm. In particular, $f:\R^{m \times n} \to \R$ is a matrix norm, if the following three properties holds:</span>
<span id="cb6-1020"><a href="#cb6-1020"></a></span>
<span id="cb6-1021"><a href="#cb6-1021"></a>$$</span>
<span id="cb6-1022"><a href="#cb6-1022"></a>\begin{align*}</span>
<span id="cb6-1023"><a href="#cb6-1023"></a>f(A) \geq 0, &amp; &amp; A \in \R^{m \times n}<span class="sc">\\</span></span>
<span id="cb6-1024"><a href="#cb6-1024"></a>f(A + B) \leq f(A) + f(B), &amp; &amp; A,B \in \R^{m \times n}<span class="sc">\\</span></span>
<span id="cb6-1025"><a href="#cb6-1025"></a>f(\alpha A) = |\alpha|f(A), &amp; &amp; \alpha \in \R, A \in \R^{m \times n}</span>
<span id="cb6-1026"><a href="#cb6-1026"></a>\end{align*}</span>
<span id="cb6-1027"><a href="#cb6-1027"></a>$$</span>
<span id="cb6-1028"><a href="#cb6-1028"></a></span>
<span id="cb6-1029"><a href="#cb6-1029"></a>The most frequently used matrix norms in numerical linear algebra are the Frobenius norm and the $p$-norms.</span>
<span id="cb6-1030"><a href="#cb6-1030"></a></span>
<span id="cb6-1031"><a href="#cb6-1031"></a>::: {#def-the-frobenius-norm}</span>
<span id="cb6-1032"><a href="#cb6-1032"></a></span>
<span id="cb6-1033"><a href="#cb6-1033"></a><span class="fu">### Frobenius Norm</span></span>
<span id="cb6-1034"><a href="#cb6-1034"></a></span>
<span id="cb6-1035"><a href="#cb6-1035"></a>The Frobenius norm $\norm{\cdot}_F : \C^{m \times n} \to \R$ is defined for $A \in \C^{m \times n}$ by:</span>
<span id="cb6-1036"><a href="#cb6-1036"></a></span>
<span id="cb6-1037"><a href="#cb6-1037"></a>$$</span>
<span id="cb6-1038"><a href="#cb6-1038"></a>\norm{A}_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}</span>
<span id="cb6-1039"><a href="#cb6-1039"></a>$$</span>
<span id="cb6-1040"><a href="#cb6-1040"></a>:::</span>
<span id="cb6-1041"><a href="#cb6-1041"></a></span>
<span id="cb6-1042"><a href="#cb6-1042"></a>:::{#thm-frobenius-norm-is-well-defined}</span>
<span id="cb6-1043"><a href="#cb6-1043"></a></span>
<span id="cb6-1044"><a href="#cb6-1044"></a>The Frobenius norm is a well-defined norm.</span>
<span id="cb6-1045"><a href="#cb6-1045"></a>:::</span>
<span id="cb6-1046"><a href="#cb6-1046"></a></span>
<span id="cb6-1047"><a href="#cb6-1047"></a>*Proof.*</span>
<span id="cb6-1048"><a href="#cb6-1048"></a></span>
<span id="cb6-1049"><a href="#cb6-1049"></a>*Positive Semi-definite*</span>
<span id="cb6-1050"><a href="#cb6-1050"></a></span>
<span id="cb6-1051"><a href="#cb6-1051"></a>We have:</span>
<span id="cb6-1052"><a href="#cb6-1052"></a></span>
<span id="cb6-1053"><a href="#cb6-1053"></a>$$</span>
<span id="cb6-1054"><a href="#cb6-1054"></a>\begin{align*}</span>
<span id="cb6-1055"><a href="#cb6-1055"></a>\norm{A}_F &amp;= \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}<span class="sc">\\</span></span>
<span id="cb6-1056"><a href="#cb6-1056"></a>&amp;\geq \left( |a_{ij}|^2\right)^{1/2} = |a_{ij}|<span class="sc">\\</span></span>
<span id="cb6-1057"><a href="#cb6-1057"></a>&amp;\geq 0</span>
<span id="cb6-1058"><a href="#cb6-1058"></a>\end{align*}</span>
<span id="cb6-1059"><a href="#cb6-1059"></a>$$</span>
<span id="cb6-1060"><a href="#cb6-1060"></a></span>
<span id="cb6-1061"><a href="#cb6-1061"></a>*Triangle Inequality*</span>
<span id="cb6-1062"><a href="#cb6-1062"></a></span>
<span id="cb6-1063"><a href="#cb6-1063"></a>We have:</span>
<span id="cb6-1064"><a href="#cb6-1064"></a></span>
<span id="cb6-1065"><a href="#cb6-1065"></a>$$</span>
<span id="cb6-1066"><a href="#cb6-1066"></a>\begin{align*}</span>
<span id="cb6-1067"><a href="#cb6-1067"></a>\norm{A + B}_F^2 &amp;= \sum_{i=1}^m \sum_{j=1}^n |a_{ij} + b_{ij}|^2 <span class="sc">\\</span></span>
<span id="cb6-1068"><a href="#cb6-1068"></a>&amp;\leq \sum_{i=1}^m \sum_{j=1}^n \left(|a_{ij}|^2 + |b_{ij}|^2 + 2|a_{ij}||b_{ij}|\right)<span class="sc">\\</span></span>
<span id="cb6-1069"><a href="#cb6-1069"></a>&amp;= \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 + \sum_{i=1}^m \sum_{j=1}^n |b_{ij}|^2 + 2\sum_{i=1}^m \sum_{j=1}^n|a_{ij}||b_{ij}|<span class="sc">\\</span></span>
<span id="cb6-1070"><a href="#cb6-1070"></a>&amp;\leq \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 + \sum_{i=1}^m \sum_{j=1}^n |b_{ij}|^2 + 2\left(\sum_{i=1}^m \sum_{j=1}^n|a_{ij}|^2\right)^{1/2}\left(\sum_{i=1}^m \sum_{j=1}^n|b_{ij}|^2\right)^{1/2} &amp; <span class="sc">\{</span>\text{ Cauchy-Schwarz }<span class="sc">\}\\</span></span>
<span id="cb6-1071"><a href="#cb6-1071"></a>&amp;= \norm{A}_F^2 + \norm{B}_F^2 + 2\norm{A}_F \norm{B}_F<span class="sc">\\</span></span>
<span id="cb6-1072"><a href="#cb6-1072"></a>&amp;= (\norm{A}_F + \norm{B}_F)^2<span class="sc">\\\\</span></span>
<span id="cb6-1073"><a href="#cb6-1073"></a>\Longrightarrow \norm{A + B}_F &amp;\leq \norm{A}_F + \norm{B}_F</span>
<span id="cb6-1074"><a href="#cb6-1074"></a>\end{align*}</span>
<span id="cb6-1075"><a href="#cb6-1075"></a>$$</span>
<span id="cb6-1076"><a href="#cb6-1076"></a></span>
<span id="cb6-1077"><a href="#cb6-1077"></a>*Homogeneity*</span>
<span id="cb6-1078"><a href="#cb6-1078"></a></span>
<span id="cb6-1079"><a href="#cb6-1079"></a>We have:</span>
<span id="cb6-1080"><a href="#cb6-1080"></a></span>
<span id="cb6-1081"><a href="#cb6-1081"></a>$$</span>
<span id="cb6-1082"><a href="#cb6-1082"></a>\begin{align*}</span>
<span id="cb6-1083"><a href="#cb6-1083"></a>\norm{\alpha A}_F &amp;= \left(\sum_{i=1}^m \sum_{j=1}^n |\alpha a_{ij}|^2\right)^{1/2}<span class="sc">\\</span></span>
<span id="cb6-1084"><a href="#cb6-1084"></a>&amp;=\left(\sum_{i=1}^m \sum_{j=1}^n |\alpha|^2 |a_{ij}|^2\right)^{1/2}<span class="sc">\\</span></span>
<span id="cb6-1085"><a href="#cb6-1085"></a>&amp;= |\alpha| \norm{A}_F</span>
<span id="cb6-1086"><a href="#cb6-1086"></a>\end{align*}</span>
<span id="cb6-1087"><a href="#cb6-1087"></a>$$</span>
<span id="cb6-1088"><a href="#cb6-1088"></a></span>
<span id="cb6-1089"><a href="#cb6-1089"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-1090"><a href="#cb6-1090"></a></span>
<span id="cb6-1091"><a href="#cb6-1091"></a>::: {#def-induced-matrix-norm}</span>
<span id="cb6-1092"><a href="#cb6-1092"></a></span>
<span id="cb6-1093"><a href="#cb6-1093"></a><span class="fu">### Induced matrix norm</span></span>
<span id="cb6-1094"><a href="#cb6-1094"></a></span>
<span id="cb6-1095"><a href="#cb6-1095"></a>Let $\norm{\cdot}_\mu : \C^m \to \R$ and $\norm{\cdot}_\nu : \C^n \to R$ be vector norms. Define $\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to R$ by:</span>
<span id="cb6-1096"><a href="#cb6-1096"></a></span>
<span id="cb6-1097"><a href="#cb6-1097"></a>$$</span>
<span id="cb6-1098"><a href="#cb6-1098"></a>\norm{A}_{\mu,\nu} = \sup_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu}</span>
<span id="cb6-1099"><a href="#cb6-1099"></a>$$</span>
<span id="cb6-1100"><a href="#cb6-1100"></a></span>
<span id="cb6-1101"><a href="#cb6-1101"></a>Matrix norms that are defined in this way are called *induced* matrix norms. </span>
<span id="cb6-1102"><a href="#cb6-1102"></a>:::</span>
<span id="cb6-1103"><a href="#cb6-1103"></a></span>
<span id="cb6-1104"><a href="#cb6-1104"></a>Let us start by interpreting this. How *large* $A$ is, as measured by $\norm{A}_{\mu,\nu}$ is defined as the most that $A$ magnifies the length of non-zero vectors, where the length of the $\bf{x}$ is measured with the norm $\norm{\cdot}_\nu$ and the length of the transformed vector $A\bf{x}$ is measured with the norm $\norm{\cdot}_\mu$.</span>
<span id="cb6-1105"><a href="#cb6-1105"></a></span>
<span id="cb6-1106"><a href="#cb6-1106"></a>Two comments are in order. First, </span>
<span id="cb6-1107"><a href="#cb6-1107"></a></span>
<span id="cb6-1108"><a href="#cb6-1108"></a>$$</span>
<span id="cb6-1109"><a href="#cb6-1109"></a>\begin{align*}</span>
<span id="cb6-1110"><a href="#cb6-1110"></a>\sup_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} = \sup_{\bf{x} \neq \bf{0}} \norm{A\frac{\bf{x}}{\norm{\bf{x}}_\nu}}_\mu = \sup_{\norm{\bf{u}}_\nu = 1} \norm{A\bf{u}}_\mu </span>
<span id="cb6-1111"><a href="#cb6-1111"></a>\end{align*}</span>
<span id="cb6-1112"><a href="#cb6-1112"></a>$$</span>
<span id="cb6-1113"><a href="#cb6-1113"></a></span>
<span id="cb6-1114"><a href="#cb6-1114"></a>Second, it is not immediately obvious, that there is a vector $\bf{x}$ for which a supremum is attained. The fact is there is always such a vector $\bf{x}$. The $K=<span class="sc">\{</span>\bf{u}:\norm{\bf{u}}_\nu = 1\}$ is a compact set, and $\norm{\cdot}_\mu : \C^m \to \R$ is a continuous function. Continuous functions preserve compact sets. So, the supremum exists and further it belongs to $\{A\bf{x}:\norm{\bf{x}}_\nu = 1<span class="sc">\}</span>$.</span>
<span id="cb6-1115"><a href="#cb6-1115"></a></span>
<span id="cb6-1116"><a href="#cb6-1116"></a>:::{#thm-the-induced-matrix-norm-is-well-defined}</span>
<span id="cb6-1117"><a href="#cb6-1117"></a></span>
<span id="cb6-1118"><a href="#cb6-1118"></a>The induced matrix norm $\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to \R$ is a well-defined norm. </span>
<span id="cb6-1119"><a href="#cb6-1119"></a>:::</span>
<span id="cb6-1120"><a href="#cb6-1120"></a></span>
<span id="cb6-1121"><a href="#cb6-1121"></a>*Proof*</span>
<span id="cb6-1122"><a href="#cb6-1122"></a></span>
<span id="cb6-1123"><a href="#cb6-1123"></a>To prove this, we merely check if the three conditions are met:</span>
<span id="cb6-1124"><a href="#cb6-1124"></a></span>
<span id="cb6-1125"><a href="#cb6-1125"></a>Let $A,B \in \C^{m \times n}$ and $\alpha \in \C$ be arbitrarily chosen. Then:</span>
<span id="cb6-1126"><a href="#cb6-1126"></a></span>
<span id="cb6-1127"><a href="#cb6-1127"></a>*Positive definite*</span>
<span id="cb6-1128"><a href="#cb6-1128"></a></span>
<span id="cb6-1129"><a href="#cb6-1129"></a>Let $A \neq 0$. That means, at least one of the columns of $A$ is not a zero-vector. Partition $A$ by columns:</span>
<span id="cb6-1130"><a href="#cb6-1130"></a></span>
<span id="cb6-1131"><a href="#cb6-1131"></a>$$</span>
<span id="cb6-1132"><a href="#cb6-1132"></a>\left[</span>
<span id="cb6-1133"><a href="#cb6-1133"></a>    \begin{array}{c|c|c|c}</span>
<span id="cb6-1134"><a href="#cb6-1134"></a>        a_{1} &amp; a_2 &amp; \ldots &amp; a_{n}</span>
<span id="cb6-1135"><a href="#cb6-1135"></a>    \end{array}</span>
<span id="cb6-1136"><a href="#cb6-1136"></a>\right]</span>
<span id="cb6-1137"><a href="#cb6-1137"></a>$$</span>
<span id="cb6-1138"><a href="#cb6-1138"></a></span>
<span id="cb6-1139"><a href="#cb6-1139"></a>Let us assume that, it is the $j$-th column $a_j$, that is non-zero. Let $\bf{e}_j$ be the column of $I$(the identity matrix) indexed with $j$. Then:</span>
<span id="cb6-1140"><a href="#cb6-1140"></a></span>
<span id="cb6-1141"><a href="#cb6-1141"></a>$$</span>
<span id="cb6-1142"><a href="#cb6-1142"></a>\begin{align*}</span>
<span id="cb6-1143"><a href="#cb6-1143"></a>\norm{A}_{\mu,\nu} &amp;= \sup \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1144"><a href="#cb6-1144"></a>&amp;\geq \frac{\norm{A\bf{e}_j}_\mu}{\norm{\bf{e}_j}_\nu}<span class="sc">\\</span></span>
<span id="cb6-1145"><a href="#cb6-1145"></a>&amp;= \frac{\norm{a_j}_\mu}{\norm{\bf{e}_j}_\nu} &amp; <span class="sc">\{</span> A\bf{e}_j = a_j <span class="sc">\}\\</span></span>
<span id="cb6-1146"><a href="#cb6-1146"></a>&amp;&gt; 0 &amp; <span class="sc">\{</span> \text{ we assumed } a_j \neq \bf{0}<span class="sc">\}</span> </span>
<span id="cb6-1147"><a href="#cb6-1147"></a>\end{align*}</span>
<span id="cb6-1148"><a href="#cb6-1148"></a>$$</span>
<span id="cb6-1149"><a href="#cb6-1149"></a></span>
<span id="cb6-1150"><a href="#cb6-1150"></a>*Homogeneity*</span>
<span id="cb6-1151"><a href="#cb6-1151"></a></span>
<span id="cb6-1152"><a href="#cb6-1152"></a>We have:</span>
<span id="cb6-1153"><a href="#cb6-1153"></a></span>
<span id="cb6-1154"><a href="#cb6-1154"></a>$$</span>
<span id="cb6-1155"><a href="#cb6-1155"></a>\begin{align*}</span>
<span id="cb6-1156"><a href="#cb6-1156"></a>\norm{\alpha A}_{\mu,\nu} &amp;= \sup_{\bf{x}\neq \bf{0}} \frac{\norm{\alpha A \bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1157"><a href="#cb6-1157"></a>&amp;= \sup_{\bf{x}\neq \bf{0}} \frac{|\alpha|\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; \{ \text{ Homogeneity of vector norm }\norm{\cdot}_\mu<span class="sc">\}\\</span></span>
<span id="cb6-1158"><a href="#cb6-1158"></a>&amp;= |\alpha|\sup_{\bf{x}\neq \bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; <span class="sc">\{</span> \text{ Algebra }<span class="sc">\}\\</span></span>
<span id="cb6-1159"><a href="#cb6-1159"></a>&amp;= |\alpha|\norm{A}_{\mu,\nu}</span>
<span id="cb6-1160"><a href="#cb6-1160"></a>\end{align*}</span>
<span id="cb6-1161"><a href="#cb6-1161"></a>$$</span>
<span id="cb6-1162"><a href="#cb6-1162"></a></span>
<span id="cb6-1163"><a href="#cb6-1163"></a>*Triangle Inequality*</span>
<span id="cb6-1164"><a href="#cb6-1164"></a></span>
<span id="cb6-1165"><a href="#cb6-1165"></a>We have:</span>
<span id="cb6-1166"><a href="#cb6-1166"></a></span>
<span id="cb6-1167"><a href="#cb6-1167"></a>$$</span>
<span id="cb6-1168"><a href="#cb6-1168"></a>\begin{align*}</span>
<span id="cb6-1169"><a href="#cb6-1169"></a>\norm{A + B}_{\mu,\nu} &amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A + B) \bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1170"><a href="#cb6-1170"></a> &amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x} + B\bf{x})}_\mu}{\norm{\bf{x}}_\nu} &amp; <span class="sc">\{</span> \text{ Distribute }<span class="sc">\}\\</span></span>
<span id="cb6-1171"><a href="#cb6-1171"></a> &amp;\leq \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu + \norm{B\bf{x}}_\mu}{\norm{\bf{x}}_\nu} &amp; <span class="sc">\{</span> \text{ Triangle inequality for vector norms }<span class="sc">\}\\</span></span>
<span id="cb6-1172"><a href="#cb6-1172"></a> &amp;= \max_{\bf{x}\neq \bf{0}} \left(\frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} + \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} \right) &amp; <span class="sc">\{</span> \text{ Algebra }<span class="sc">\}\\</span></span>
<span id="cb6-1173"><a href="#cb6-1173"></a>&amp;= \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} + \max_{\bf{x}\neq \bf{0}} \frac{\norm{(A\bf{x}}_\mu}{\norm{\bf{x}}_\nu} <span class="sc">\\</span></span>
<span id="cb6-1174"><a href="#cb6-1174"></a>&amp;= \norm{A}_{\mu,\nu} + \norm{B}_{\mu,\nu} &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}</span> </span>
<span id="cb6-1175"><a href="#cb6-1175"></a>\end{align*}</span>
<span id="cb6-1176"><a href="#cb6-1176"></a>$$</span>
<span id="cb6-1177"><a href="#cb6-1177"></a></span>
<span id="cb6-1178"><a href="#cb6-1178"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-1179"><a href="#cb6-1179"></a></span>
<span id="cb6-1180"><a href="#cb6-1180"></a>When $\norm{\cdot}_\mu$ and $\norm{\cdot}_\nu$ are the same norm, the induced norm becomes:</span>
<span id="cb6-1181"><a href="#cb6-1181"></a></span>
<span id="cb6-1182"><a href="#cb6-1182"></a>$$</span>
<span id="cb6-1183"><a href="#cb6-1183"></a>\norm{A}_\mu = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_\mu}{\norm{\bf{x}}_\mu}</span>
<span id="cb6-1184"><a href="#cb6-1184"></a>$$</span>
<span id="cb6-1185"><a href="#cb6-1185"></a></span>
<span id="cb6-1186"><a href="#cb6-1186"></a>or equivalently:</span>
<span id="cb6-1187"><a href="#cb6-1187"></a></span>
<span id="cb6-1188"><a href="#cb6-1188"></a>$$</span>
<span id="cb6-1189"><a href="#cb6-1189"></a>\norm{A}_\mu = \max_{\norm{\bf{u}}_\mu = 1} \norm{A\bf{u}}_\mu</span>
<span id="cb6-1190"><a href="#cb6-1190"></a>$$</span>
<span id="cb6-1191"><a href="#cb6-1191"></a></span>
<span id="cb6-1192"><a href="#cb6-1192"></a>::: {#exm-p-norm-is-the-same}</span>
<span id="cb6-1193"><a href="#cb6-1193"></a></span>
<span id="cb6-1194"><a href="#cb6-1194"></a>Consider the vector $p$-norm $\norm{\cdot}_p:\C^n \to \R$ and let us denote the induced matrix norm $|||\cdot|||:\C^{m \times n} \to \R$ by $|||A||| = \max_{\bf{x}\neq\bf{0}}\frac{\norm{A\bf{x}}_p}{\norm{\bf{x}}_p}$. Prove that $|||\bf{y}||| = \norm{\bf{y}}_p$ for all $\bf{y}\in\C^m$.</span>
<span id="cb6-1195"><a href="#cb6-1195"></a>:::</span>
<span id="cb6-1196"><a href="#cb6-1196"></a></span>
<span id="cb6-1197"><a href="#cb6-1197"></a>*Proof*.</span>
<span id="cb6-1198"><a href="#cb6-1198"></a></span>
<span id="cb6-1199"><a href="#cb6-1199"></a>We have:</span>
<span id="cb6-1200"><a href="#cb6-1200"></a></span>
<span id="cb6-1201"><a href="#cb6-1201"></a>$$</span>
<span id="cb6-1202"><a href="#cb6-1202"></a>\begin{align*}</span>
<span id="cb6-1203"><a href="#cb6-1203"></a>|||\bf{y}||| &amp;= \frac{\norm{\bf{y}x}_p}{\norm{x}_p} &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1204"><a href="#cb6-1204"></a>&amp;= \frac{|x_1| \norm{\bf{y}}_p}{|x_1|} &amp; <span class="sc">\{</span> x \text{ has to be } 1 \times 1, \text{ a scalar }<span class="sc">\}\\</span></span>
<span id="cb6-1205"><a href="#cb6-1205"></a>&amp;= \norm{\bf{y}}_p</span>
<span id="cb6-1206"><a href="#cb6-1206"></a>\end{align*}</span>
<span id="cb6-1207"><a href="#cb6-1207"></a>$$</span>
<span id="cb6-1208"><a href="#cb6-1208"></a></span>
<span id="cb6-1209"><a href="#cb6-1209"></a>The last example is important. One can view a vector $\bf{y}\in \C^m$ as an $m \times 1$ matrix. What this last exercise tells us is that regardless of whether we view $\bf{y}$ as a matrix or a vector, $\norm{y}_p$ is the same. </span>
<span id="cb6-1210"><a href="#cb6-1210"></a></span>
<span id="cb6-1211"><a href="#cb6-1211"></a>We already encountered the vector $p$-norms as an important class of vector norms. The matrix $p$-norm is induced by the corresponding vector norm.</span>
<span id="cb6-1212"><a href="#cb6-1212"></a></span>
<span id="cb6-1213"><a href="#cb6-1213"></a>::: {#def-the-matrix-p-norm}</span>
<span id="cb6-1214"><a href="#cb6-1214"></a></span>
<span id="cb6-1215"><a href="#cb6-1215"></a><span class="fu">### The matrix $p$-norm</span></span>
<span id="cb6-1216"><a href="#cb6-1216"></a></span>
<span id="cb6-1217"><a href="#cb6-1217"></a>For any vector $p$-norm, define the corresponding matrix $p$-norm $\norm{\cdot}_p : \C^{m \times n} \to \R$ by:</span>
<span id="cb6-1218"><a href="#cb6-1218"></a></span>
<span id="cb6-1219"><a href="#cb6-1219"></a>$$</span>
<span id="cb6-1220"><a href="#cb6-1220"></a>\norm{A}_p = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_p}{\norm{\bf{x}}_p} \quad \text{ or equivalently } \quad \norm{A}_p = \max_{\norm{\bf{x}}_p = 1} \norm{A\bf{x}}_p</span>
<span id="cb6-1221"><a href="#cb6-1221"></a>$$</span>
<span id="cb6-1222"><a href="#cb6-1222"></a>:::</span>
<span id="cb6-1223"><a href="#cb6-1223"></a></span>
<span id="cb6-1224"><a href="#cb6-1224"></a>In practice, the matrix $2$-norm is of great theoretical importance, but difficult to evaluate, except for special matrices. The $1$-norm, the $\infty$-norm and Frobenius norms are straightforward and relatively cheap to compute. </span>
<span id="cb6-1225"><a href="#cb6-1225"></a></span>
<span id="cb6-1226"><a href="#cb6-1226"></a>Let us instantiate the definition of the vector $p$-norm where $p=2$, giving us a matrix norm induced by the vector $2$-norm or the Euclidean norm:</span>
<span id="cb6-1227"><a href="#cb6-1227"></a></span>
<span id="cb6-1228"><a href="#cb6-1228"></a>::: {#def-the-matrix-2-norm}</span>
<span id="cb6-1229"><a href="#cb6-1229"></a></span>
<span id="cb6-1230"><a href="#cb6-1230"></a><span class="fu">### The matrix $2$-norm</span></span>
<span id="cb6-1231"><a href="#cb6-1231"></a></span>
<span id="cb6-1232"><a href="#cb6-1232"></a>Define the matrix $2$-norm $\norm{\cdot}_2:\C^{m \times n} \to \R$ by :</span>
<span id="cb6-1233"><a href="#cb6-1233"></a></span>
<span id="cb6-1234"><a href="#cb6-1234"></a>$$</span>
<span id="cb6-1235"><a href="#cb6-1235"></a>\norm{A}_2 = \max_{\bf{x}\neq\bf{0}} \frac{\norm{A\bf{x}}_2}{\norm{\bf{x}}_2} = \max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2</span>
<span id="cb6-1236"><a href="#cb6-1236"></a>$$</span>
<span id="cb6-1237"><a href="#cb6-1237"></a>:::</span>
<span id="cb6-1238"><a href="#cb6-1238"></a></span>
<span id="cb6-1239"><a href="#cb6-1239"></a>::: {.callout-note}</span>
<span id="cb6-1240"><a href="#cb6-1240"></a></span>
<span id="cb6-1241"><a href="#cb6-1241"></a>The problem with the matrix $2$-norm is that it is hard to compute. In future posts, we shall find out that if $A$ is a Hermitian matrix ($A = A^H$), then $\norm{A}_2 = |\lambda_1|$ where $\lambda_1$ is the eigenvalue of $A$ that is largest in magnitude. </span>
<span id="cb6-1242"><a href="#cb6-1242"></a></span>
<span id="cb6-1243"><a href="#cb6-1243"></a>Recall from basic linear algebra, that computing eigenvalues involves computing the roots of polynomials, and for polynomials of degree three or greater, this is a non-trivial task. We shall see that the matrix $2$-norm plays an important part in theory, but less so in practical computation. </span>
<span id="cb6-1244"><a href="#cb6-1244"></a>:::</span>
<span id="cb6-1245"><a href="#cb6-1245"></a></span>
<span id="cb6-1246"><a href="#cb6-1246"></a>::: {#exm-matrix-2-norm-of-diagonal-matrix}</span>
<span id="cb6-1247"><a href="#cb6-1247"></a></span>
<span id="cb6-1248"><a href="#cb6-1248"></a>Show that:</span>
<span id="cb6-1249"><a href="#cb6-1249"></a></span>
<span id="cb6-1250"><a href="#cb6-1250"></a>$$</span>
<span id="cb6-1251"><a href="#cb6-1251"></a>\norm{\begin{bmatrix}</span>
<span id="cb6-1252"><a href="#cb6-1252"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1253"><a href="#cb6-1253"></a>0 &amp; d_2 </span>
<span id="cb6-1254"><a href="#cb6-1254"></a>\end{bmatrix}}_2 = \max(|d_1|,|d_2|)</span>
<span id="cb6-1255"><a href="#cb6-1255"></a>$$</span>
<span id="cb6-1256"><a href="#cb6-1256"></a>:::</span>
<span id="cb6-1257"><a href="#cb6-1257"></a></span>
<span id="cb6-1258"><a href="#cb6-1258"></a>*Solution*</span>
<span id="cb6-1259"><a href="#cb6-1259"></a></span>
<span id="cb6-1260"><a href="#cb6-1260"></a>We have:</span>
<span id="cb6-1261"><a href="#cb6-1261"></a></span>
<span id="cb6-1262"><a href="#cb6-1262"></a>$$</span>
<span id="cb6-1263"><a href="#cb6-1263"></a>\begin{align*}</span>
<span id="cb6-1264"><a href="#cb6-1264"></a>\norm{\begin{bmatrix}</span>
<span id="cb6-1265"><a href="#cb6-1265"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1266"><a href="#cb6-1266"></a>0 &amp; d_2 </span>
<span id="cb6-1267"><a href="#cb6-1267"></a>\end{bmatrix}}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}</span>
<span id="cb6-1268"><a href="#cb6-1268"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1269"><a href="#cb6-1269"></a>0 &amp; d_2 </span>
<span id="cb6-1270"><a href="#cb6-1270"></a>\end{bmatrix}\begin{bmatrix}x_1 <span class="sc">\\</span> x_2\end{bmatrix}}_2^2 &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1271"><a href="#cb6-1271"></a>&amp;= \max_{\norm{\bf{x}}_2 = 1}|d_1x_1|^2 + |d_2 x_2|^2<span class="sc">\\</span></span>
<span id="cb6-1272"><a href="#cb6-1272"></a>&amp;\leq \max_{\norm{\bf{x}}_2 = 1} <span class="co">[</span><span class="ot">\max(|d_1|,|d_2|)^2 |x_1|^2 + \max(|d_1|,|d_2|)^2 |x_2|^2</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb6-1273"><a href="#cb6-1273"></a>&amp;= \max(|d_1|,|d_2|)^2 \max_{\norm{\bf{x}}_2 = 1} (|x_1|^2 + |x_2|^2)<span class="sc">\\</span></span>
<span id="cb6-1274"><a href="#cb6-1274"></a>&amp;= \max(|d_1|,|d_2|)^2</span>
<span id="cb6-1275"><a href="#cb6-1275"></a>\end{align*}</span>
<span id="cb6-1276"><a href="#cb6-1276"></a>$$</span>
<span id="cb6-1277"><a href="#cb6-1277"></a></span>
<span id="cb6-1278"><a href="#cb6-1278"></a>Moreover, if we take $\bf{x} = \bf{e}_1$ and $\bf{x}=\bf{e}_2$, we get:</span>
<span id="cb6-1279"><a href="#cb6-1279"></a></span>
<span id="cb6-1280"><a href="#cb6-1280"></a>$$</span>
<span id="cb6-1281"><a href="#cb6-1281"></a>\begin{align*}</span>
<span id="cb6-1282"><a href="#cb6-1282"></a>\norm{\begin{bmatrix}</span>
<span id="cb6-1283"><a href="#cb6-1283"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1284"><a href="#cb6-1284"></a>0 &amp; d_2 </span>
<span id="cb6-1285"><a href="#cb6-1285"></a>\end{bmatrix}}_2^2  &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}</span>
<span id="cb6-1286"><a href="#cb6-1286"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1287"><a href="#cb6-1287"></a>0 &amp; d_2 </span>
<span id="cb6-1288"><a href="#cb6-1288"></a>\end{bmatrix}\begin{bmatrix}x_1 <span class="sc">\\</span> x_2\end{bmatrix}}_2 &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1289"><a href="#cb6-1289"></a>&amp;\geq  \norm{\begin{bmatrix}</span>
<span id="cb6-1290"><a href="#cb6-1290"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1291"><a href="#cb6-1291"></a>0 &amp; d_2 </span>
<span id="cb6-1292"><a href="#cb6-1292"></a>\end{bmatrix}\begin{bmatrix}1 <span class="sc">\\</span> 0\end{bmatrix}}_2^2 <span class="sc">\\</span></span>
<span id="cb6-1293"><a href="#cb6-1293"></a>&amp;= |d_1|^2</span>
<span id="cb6-1294"><a href="#cb6-1294"></a>\end{align*}</span>
<span id="cb6-1295"><a href="#cb6-1295"></a>$$</span>
<span id="cb6-1296"><a href="#cb6-1296"></a></span>
<span id="cb6-1297"><a href="#cb6-1297"></a>and</span>
<span id="cb6-1298"><a href="#cb6-1298"></a></span>
<span id="cb6-1299"><a href="#cb6-1299"></a>$$</span>
<span id="cb6-1300"><a href="#cb6-1300"></a>\begin{align*}</span>
<span id="cb6-1301"><a href="#cb6-1301"></a>\norm{\begin{bmatrix}</span>
<span id="cb6-1302"><a href="#cb6-1302"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1303"><a href="#cb6-1303"></a>0 &amp; d_2 </span>
<span id="cb6-1304"><a href="#cb6-1304"></a>\end{bmatrix}}_2^2  &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{\begin{bmatrix}</span>
<span id="cb6-1305"><a href="#cb6-1305"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1306"><a href="#cb6-1306"></a>0 &amp; d_2 </span>
<span id="cb6-1307"><a href="#cb6-1307"></a>\end{bmatrix}\begin{bmatrix}x_1 <span class="sc">\\</span> x_2\end{bmatrix}}_2 &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1308"><a href="#cb6-1308"></a>&amp;\geq  \norm{\begin{bmatrix}</span>
<span id="cb6-1309"><a href="#cb6-1309"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1310"><a href="#cb6-1310"></a>0 &amp; d_2 </span>
<span id="cb6-1311"><a href="#cb6-1311"></a>\end{bmatrix}\begin{bmatrix}0 <span class="sc">\\</span> 1\end{bmatrix}}_2 <span class="sc">\\</span></span>
<span id="cb6-1312"><a href="#cb6-1312"></a>&amp;= |d_2|^2</span>
<span id="cb6-1313"><a href="#cb6-1313"></a>\end{align*}</span>
<span id="cb6-1314"><a href="#cb6-1314"></a>$$</span>
<span id="cb6-1315"><a href="#cb6-1315"></a></span>
<span id="cb6-1316"><a href="#cb6-1316"></a>Consequently, </span>
<span id="cb6-1317"><a href="#cb6-1317"></a></span>
<span id="cb6-1318"><a href="#cb6-1318"></a>$$</span>
<span id="cb6-1319"><a href="#cb6-1319"></a>\norm{\begin{bmatrix}</span>
<span id="cb6-1320"><a href="#cb6-1320"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1321"><a href="#cb6-1321"></a>0 &amp; d_2 </span>
<span id="cb6-1322"><a href="#cb6-1322"></a>\end{bmatrix}}_2^2 \geq \max(|d_1|,|d_2|)^2</span>
<span id="cb6-1323"><a href="#cb6-1323"></a>$$</span>
<span id="cb6-1324"><a href="#cb6-1324"></a></span>
<span id="cb6-1325"><a href="#cb6-1325"></a>We conclude that </span>
<span id="cb6-1326"><a href="#cb6-1326"></a></span>
<span id="cb6-1327"><a href="#cb6-1327"></a>$$</span>
<span id="cb6-1328"><a href="#cb6-1328"></a>\norm{\begin{bmatrix}</span>
<span id="cb6-1329"><a href="#cb6-1329"></a>d_1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1330"><a href="#cb6-1330"></a>0 &amp; d_2 </span>
<span id="cb6-1331"><a href="#cb6-1331"></a>\end{bmatrix}}_2 = \max(|d_1|,|d_2|)</span>
<span id="cb6-1332"><a href="#cb6-1332"></a>$$</span>
<span id="cb6-1333"><a href="#cb6-1333"></a></span>
<span id="cb6-1334"><a href="#cb6-1334"></a>::: {.callout-tip}</span>
<span id="cb6-1335"><a href="#cb6-1335"></a></span>
<span id="cb6-1336"><a href="#cb6-1336"></a>The proof of the last example builds on a general principle: Showing that $\max_{x \in D} f(x) = \alpha$ for some function $f:D \to \R$ can be broken down into showing that both:</span>
<span id="cb6-1337"><a href="#cb6-1337"></a></span>
<span id="cb6-1338"><a href="#cb6-1338"></a>$$</span>
<span id="cb6-1339"><a href="#cb6-1339"></a>\max_{x \in D} f(x) \leq \alpha</span>
<span id="cb6-1340"><a href="#cb6-1340"></a>$$</span>
<span id="cb6-1341"><a href="#cb6-1341"></a></span>
<span id="cb6-1342"><a href="#cb6-1342"></a>and </span>
<span id="cb6-1343"><a href="#cb6-1343"></a></span>
<span id="cb6-1344"><a href="#cb6-1344"></a>$$</span>
<span id="cb6-1345"><a href="#cb6-1345"></a>\max_{x \in D} f(x) \geq \alpha</span>
<span id="cb6-1346"><a href="#cb6-1346"></a>$$</span>
<span id="cb6-1347"><a href="#cb6-1347"></a></span>
<span id="cb6-1348"><a href="#cb6-1348"></a>In turn, showing that $\max_{x \in D}f(x) \geq \alpha$ can often be accomplished by showing that there exists a vector $y \in D$ such that $f(y) = \alpha$ since then </span>
<span id="cb6-1349"><a href="#cb6-1349"></a></span>
<span id="cb6-1350"><a href="#cb6-1350"></a>$$</span>
<span id="cb6-1351"><a href="#cb6-1351"></a>\max_{x \in D}f(x) \geq f(y) = \alpha</span>
<span id="cb6-1352"><a href="#cb6-1352"></a>$$</span>
<span id="cb6-1353"><a href="#cb6-1353"></a></span>
<span id="cb6-1354"><a href="#cb6-1354"></a>We will use this technique in future proofs involving matrix norms.</span>
<span id="cb6-1355"><a href="#cb6-1355"></a>:::</span>
<span id="cb6-1356"><a href="#cb6-1356"></a></span>
<span id="cb6-1357"><a href="#cb6-1357"></a>::: {#exr-2-norm-of-a-diag-matrix}</span>
<span id="cb6-1358"><a href="#cb6-1358"></a>Let $D \in C^{m \times m}$ be a diagonal matrix $diag(d_1,d_2,\ldots,d_m)$. Show that:</span>
<span id="cb6-1359"><a href="#cb6-1359"></a></span>
<span id="cb6-1360"><a href="#cb6-1360"></a>$$</span>
<span id="cb6-1361"><a href="#cb6-1361"></a>\norm{D}_2 = \max_{j=1}^{m} |d_j|</span>
<span id="cb6-1362"><a href="#cb6-1362"></a>$$</span>
<span id="cb6-1363"><a href="#cb6-1363"></a>:::</span>
<span id="cb6-1364"><a href="#cb6-1364"></a></span>
<span id="cb6-1365"><a href="#cb6-1365"></a>*Solution.*</span>
<span id="cb6-1366"><a href="#cb6-1366"></a></span>
<span id="cb6-1367"><a href="#cb6-1367"></a>We have:</span>
<span id="cb6-1368"><a href="#cb6-1368"></a></span>
<span id="cb6-1369"><a href="#cb6-1369"></a>$$</span>
<span id="cb6-1370"><a href="#cb6-1370"></a>\begin{align*}</span>
<span id="cb6-1371"><a href="#cb6-1371"></a>\norm{D}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{</span>
<span id="cb6-1372"><a href="#cb6-1372"></a>    \begin{bmatrix}</span>
<span id="cb6-1373"><a href="#cb6-1373"></a>    d_1 <span class="sc">\\</span></span>
<span id="cb6-1374"><a href="#cb6-1374"></a>    &amp; d_2 <span class="sc">\\</span></span>
<span id="cb6-1375"><a href="#cb6-1375"></a>    &amp; &amp; \ddots<span class="sc">\\</span></span>
<span id="cb6-1376"><a href="#cb6-1376"></a>    &amp; &amp; &amp; d_m</span>
<span id="cb6-1377"><a href="#cb6-1377"></a>    \end{bmatrix}</span>
<span id="cb6-1378"><a href="#cb6-1378"></a>    \begin{bmatrix}</span>
<span id="cb6-1379"><a href="#cb6-1379"></a>    x_1<span class="sc">\\</span></span>
<span id="cb6-1380"><a href="#cb6-1380"></a>    x_2<span class="sc">\\</span></span>
<span id="cb6-1381"><a href="#cb6-1381"></a>    \vdots<span class="sc">\\</span></span>
<span id="cb6-1382"><a href="#cb6-1382"></a>    x_m</span>
<span id="cb6-1383"><a href="#cb6-1383"></a>    \end{bmatrix}</span>
<span id="cb6-1384"><a href="#cb6-1384"></a>}_2^2 <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1385"><a href="#cb6-1385"></a>&amp;=\max_{\norm{\bf{x}}_2 = 1} \norm{</span>
<span id="cb6-1386"><a href="#cb6-1386"></a>    \begin{bmatrix}</span>
<span id="cb6-1387"><a href="#cb6-1387"></a>    d_1 x_1<span class="sc">\\</span></span>
<span id="cb6-1388"><a href="#cb6-1388"></a>    d_2 x_2<span class="sc">\\</span></span>
<span id="cb6-1389"><a href="#cb6-1389"></a>    \vdots<span class="sc">\\</span></span>
<span id="cb6-1390"><a href="#cb6-1390"></a>    d_m x_m</span>
<span id="cb6-1391"><a href="#cb6-1391"></a>    \end{bmatrix}</span>
<span id="cb6-1392"><a href="#cb6-1392"></a>}_2^2<span class="sc">\\</span></span>
<span id="cb6-1393"><a href="#cb6-1393"></a>&amp;= \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m |d_j x_j|^2<span class="sc">\\</span></span>
<span id="cb6-1394"><a href="#cb6-1394"></a>&amp;\leq \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m \max(|d_1|,\ldots,|d_m|)^2 |x_j|^2<span class="sc">\\</span></span>
<span id="cb6-1395"><a href="#cb6-1395"></a>&amp;= \max(|d_1|,\ldots,|d_m|)^2 \max_{\norm{\bf{x}}_2 = 1} \sum_{j=1}^m |x_j|^2 <span class="sc">\\</span></span>
<span id="cb6-1396"><a href="#cb6-1396"></a>&amp;= \max(|d_1|,\ldots,|d_m|)^2</span>
<span id="cb6-1397"><a href="#cb6-1397"></a>\end{align*}</span>
<span id="cb6-1398"><a href="#cb6-1398"></a>$$</span>
<span id="cb6-1399"><a href="#cb6-1399"></a></span>
<span id="cb6-1400"><a href="#cb6-1400"></a>Moreover, if we take take $\bf{x} = \bf{e}_j$, the standard basis vector with its $j$-th coordinate equal to one, we find that </span>
<span id="cb6-1401"><a href="#cb6-1401"></a></span>
<span id="cb6-1402"><a href="#cb6-1402"></a>$$</span>
<span id="cb6-1403"><a href="#cb6-1403"></a>\norm{D}_2^2 \geq |d_j|^2</span>
<span id="cb6-1404"><a href="#cb6-1404"></a>$$</span>
<span id="cb6-1405"><a href="#cb6-1405"></a></span>
<span id="cb6-1406"><a href="#cb6-1406"></a>Consequently, $\norm{D}_2^2 \geq \max(|d_1|,\ldots,|d_m|)^2$.</span>
<span id="cb6-1407"><a href="#cb6-1407"></a></span>
<span id="cb6-1408"><a href="#cb6-1408"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-1409"><a href="#cb6-1409"></a></span>
<span id="cb6-1410"><a href="#cb6-1410"></a>::: {#exr-properties-of-two-norm-1}</span>
<span id="cb6-1411"><a href="#cb6-1411"></a>Let $\bf{y}\in\C^m$ and $\bf{x} \in \C^n$. Show that:</span>
<span id="cb6-1412"><a href="#cb6-1412"></a></span>
<span id="cb6-1413"><a href="#cb6-1413"></a>$$</span>
<span id="cb6-1414"><a href="#cb6-1414"></a>\norm{\bf{y}\bf{x}^H}_2 = \norm{\bf{y}}_2 \norm{\bf{x}}_2</span>
<span id="cb6-1415"><a href="#cb6-1415"></a>$$</span>
<span id="cb6-1416"><a href="#cb6-1416"></a>:::</span>
<span id="cb6-1417"><a href="#cb6-1417"></a></span>
<span id="cb6-1418"><a href="#cb6-1418"></a>*Proof*.</span>
<span id="cb6-1419"><a href="#cb6-1419"></a></span>
<span id="cb6-1420"><a href="#cb6-1420"></a>From the Cauchy-Schwarz inequality, we know that:</span>
<span id="cb6-1421"><a href="#cb6-1421"></a></span>
<span id="cb6-1422"><a href="#cb6-1422"></a>$$</span>
<span id="cb6-1423"><a href="#cb6-1423"></a>|x^H z| \leq \norm{\bf{x}^H}_2 \norm{\bf{z}}_2</span>
<span id="cb6-1424"><a href="#cb6-1424"></a>$$</span>
<span id="cb6-1425"><a href="#cb6-1425"></a></span>
<span id="cb6-1426"><a href="#cb6-1426"></a>Now, $\bf{x}^H \in \C^{1 \times n}$ and $\bf{z} \in \C^{n \times 1}$. So, $\bf{x}^H \bf{z} \in \C^{1 \times 1}$, and it is a scalar.</span>
<span id="cb6-1427"><a href="#cb6-1427"></a></span>
<span id="cb6-1428"><a href="#cb6-1428"></a>$$</span>
<span id="cb6-1429"><a href="#cb6-1429"></a>\begin{align*}</span>
<span id="cb6-1430"><a href="#cb6-1430"></a>\norm{\bf{y}\bf{x}^H}_2 &amp;= \max_{\norm{\bf{z}}_2 = 1} \norm{\bf{y}\bf{x}^H \bf{z}}_2 <span class="sc">\\</span></span>
<span id="cb6-1431"><a href="#cb6-1431"></a>&amp;= \max_{\norm{\bf{z}}_2 = 1} |\bf{x}^H \bf{z}| \norm{\bf{y}}_2 <span class="sc">\{</span> \bf{x}^H\bf{z}\text{ is scalar }<span class="sc">\}\\</span></span>
<span id="cb6-1432"><a href="#cb6-1432"></a>&amp;\leq \max_{\norm{\bf{z}}_2 = 1} \norm{\bf{x}^H}_2 \norm{\bf{z}}_2 \norm{\bf{y}}_2 <span class="sc">\\</span></span>
<span id="cb6-1433"><a href="#cb6-1433"></a>&amp;= \norm{\bf{x}}_2 \norm{\bf{y}}_2 </span>
<span id="cb6-1434"><a href="#cb6-1434"></a>\end{align*}</span>
<span id="cb6-1435"><a href="#cb6-1435"></a>$$</span>
<span id="cb6-1436"><a href="#cb6-1436"></a></span>
<span id="cb6-1437"><a href="#cb6-1437"></a>On the other hand, </span>
<span id="cb6-1438"><a href="#cb6-1438"></a></span>
<span id="cb6-1439"><a href="#cb6-1439"></a>$$</span>
<span id="cb6-1440"><a href="#cb6-1440"></a>\begin{align*}</span>
<span id="cb6-1441"><a href="#cb6-1441"></a>\norm{\bf{y}\bf{x}^H}_2 &amp;= \max_{\bf{z}\neq \bf{0}} \frac{\norm{\bf{y}\bf{x}^H \bf{z}}_2}{\norm{\bf{z}}_2}<span class="sc">\\</span></span>
<span id="cb6-1442"><a href="#cb6-1442"></a>&amp;\geq \frac{\norm{\bf{y}\bf{x}^H \bf{x}}_2}{\norm{\bf{x}}_2} &amp; <span class="sc">\{</span> \text{ Specific }\bf{z} <span class="sc">\}\\</span></span>
<span id="cb6-1443"><a href="#cb6-1443"></a>&amp;= \frac{\norm{\bf{y}\norm{\bf{x}}_2^2}_2}{\norm{\bf{x}}_2} &amp; <span class="sc">\{</span> \bf{x}^H \bf{x} = \norm{\bf{x}}_2^2<span class="sc">\}\\</span></span>
<span id="cb6-1444"><a href="#cb6-1444"></a>&amp;= \norm{\bf{y}}_2 \norm{\bf{x}}_2</span>
<span id="cb6-1445"><a href="#cb6-1445"></a>\end{align*}</span>
<span id="cb6-1446"><a href="#cb6-1446"></a>$$</span>
<span id="cb6-1447"><a href="#cb6-1447"></a></span>
<span id="cb6-1448"><a href="#cb6-1448"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-1449"><a href="#cb6-1449"></a></span>
<span id="cb6-1450"><a href="#cb6-1450"></a>::: {#exr-properties-of-two-norm-2}</span>
<span id="cb6-1451"><a href="#cb6-1451"></a>Let $A \in \C^{m \times n}$ and $a_j$ be its column indexed with $j$. Prove that:</span>
<span id="cb6-1452"><a href="#cb6-1452"></a></span>
<span id="cb6-1453"><a href="#cb6-1453"></a>$$</span>
<span id="cb6-1454"><a href="#cb6-1454"></a>\norm{a_j}_2 \leq \norm{A}_2</span>
<span id="cb6-1455"><a href="#cb6-1455"></a>$$</span>
<span id="cb6-1456"><a href="#cb6-1456"></a>:::</span>
<span id="cb6-1457"><a href="#cb6-1457"></a></span>
<span id="cb6-1458"><a href="#cb6-1458"></a>*Proof.*</span>
<span id="cb6-1459"><a href="#cb6-1459"></a></span>
<span id="cb6-1460"><a href="#cb6-1460"></a>We have:</span>
<span id="cb6-1461"><a href="#cb6-1461"></a></span>
<span id="cb6-1462"><a href="#cb6-1462"></a>$$</span>
<span id="cb6-1463"><a href="#cb6-1463"></a>\begin{align*}</span>
<span id="cb6-1464"><a href="#cb6-1464"></a>\norm{A}_2 &amp;= \max_{\norm{\bf{z}}_2 = 1} \norm{A\bf{z}}_2 <span class="sc">\\</span></span>
<span id="cb6-1465"><a href="#cb6-1465"></a>&amp;\geq  \norm{A\bf{e}_j}_2<span class="sc">\\</span></span>
<span id="cb6-1466"><a href="#cb6-1466"></a>&amp;= \norm{a_j}_2</span>
<span id="cb6-1467"><a href="#cb6-1467"></a>\end{align*}</span>
<span id="cb6-1468"><a href="#cb6-1468"></a>$$</span>
<span id="cb6-1469"><a href="#cb6-1469"></a></span>
<span id="cb6-1470"><a href="#cb6-1470"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-1471"><a href="#cb6-1471"></a></span>
<span id="cb6-1472"><a href="#cb6-1472"></a>::: {#exr-properties-of-two-norm-3}</span>
<span id="cb6-1473"><a href="#cb6-1473"></a>Let $A \in \C^{m \times n}$. Prove that: </span>
<span id="cb6-1474"><a href="#cb6-1474"></a></span>
<span id="cb6-1475"><a href="#cb6-1475"></a>(i)</span>
<span id="cb6-1476"><a href="#cb6-1476"></a>$$</span>
<span id="cb6-1477"><a href="#cb6-1477"></a>\norm{A}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|</span>
<span id="cb6-1478"><a href="#cb6-1478"></a>$$</span>
<span id="cb6-1479"><a href="#cb6-1479"></a></span>
<span id="cb6-1480"><a href="#cb6-1480"></a>(ii) </span>
<span id="cb6-1481"><a href="#cb6-1481"></a>$$</span>
<span id="cb6-1482"><a href="#cb6-1482"></a>\norm{A^H}_2 = \norm{A}_2</span>
<span id="cb6-1483"><a href="#cb6-1483"></a>$$</span>
<span id="cb6-1484"><a href="#cb6-1484"></a></span>
<span id="cb6-1485"><a href="#cb6-1485"></a>(iii)</span>
<span id="cb6-1486"><a href="#cb6-1486"></a>$$</span>
<span id="cb6-1487"><a href="#cb6-1487"></a>\norm{A^H A}_2 = \norm{A}_2^2</span>
<span id="cb6-1488"><a href="#cb6-1488"></a>$$</span>
<span id="cb6-1489"><a href="#cb6-1489"></a>:::</span>
<span id="cb6-1490"><a href="#cb6-1490"></a></span>
<span id="cb6-1491"><a href="#cb6-1491"></a>*Claim*.</span>
<span id="cb6-1492"><a href="#cb6-1492"></a></span>
<span id="cb6-1493"><a href="#cb6-1493"></a>$$</span>
<span id="cb6-1494"><a href="#cb6-1494"></a>\norm{A}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|</span>
<span id="cb6-1495"><a href="#cb6-1495"></a>$$</span>
<span id="cb6-1496"><a href="#cb6-1496"></a></span>
<span id="cb6-1497"><a href="#cb6-1497"></a>*Proof.*</span>
<span id="cb6-1498"><a href="#cb6-1498"></a></span>
<span id="cb6-1499"><a href="#cb6-1499"></a>We have:</span>
<span id="cb6-1500"><a href="#cb6-1500"></a></span>
<span id="cb6-1501"><a href="#cb6-1501"></a>$$</span>
<span id="cb6-1502"><a href="#cb6-1502"></a>\begin{align*}</span>
<span id="cb6-1503"><a href="#cb6-1503"></a>\max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}| &amp;\leq \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} \norm{\bf{y}^H}_2 \norm{A\bf{x}}_2 &amp; <span class="sc">\{</span> \text{ Cauchy-Schwarz }<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb6-1504"><a href="#cb6-1504"></a>&amp;= \max_{\norm{\bf{x}}_2 } \norm{A\bf{x}}_2<span class="sc">\\</span></span>
<span id="cb6-1505"><a href="#cb6-1505"></a>&amp;= \norm{A}_2</span>
<span id="cb6-1506"><a href="#cb6-1506"></a>\end{align*}</span>
<span id="cb6-1507"><a href="#cb6-1507"></a>$$</span>
<span id="cb6-1508"><a href="#cb6-1508"></a></span>
<span id="cb6-1509"><a href="#cb6-1509"></a>On the other hand:</span>
<span id="cb6-1510"><a href="#cb6-1510"></a></span>
<span id="cb6-1511"><a href="#cb6-1511"></a>$$</span>
<span id="cb6-1512"><a href="#cb6-1512"></a>\begin{align*}</span>
<span id="cb6-1513"><a href="#cb6-1513"></a>\max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}| &amp;\geq \max_{\norm{\bf{x}}_2 = 1} |\left(\frac{A\bf{x}}{\norm{A\bf{x}}_2}\right)^H A \bf{x}| &amp; <span class="sc">\{</span>\text{ Specific vector }<span class="sc">\}\\</span></span>
<span id="cb6-1514"><a href="#cb6-1514"></a>&amp;= \max_{\norm{\bf{x}}_2 = 1} \frac{\norm{A\bf{x}}_2^2}{\norm{A\bf{x}}_2}<span class="sc">\\</span></span>
<span id="cb6-1515"><a href="#cb6-1515"></a>&amp;=\max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2<span class="sc">\\</span></span>
<span id="cb6-1516"><a href="#cb6-1516"></a>&amp;= \norm{A}_2</span>
<span id="cb6-1517"><a href="#cb6-1517"></a>\end{align*}</span>
<span id="cb6-1518"><a href="#cb6-1518"></a>$$</span>
<span id="cb6-1519"><a href="#cb6-1519"></a></span>
<span id="cb6-1520"><a href="#cb6-1520"></a>We have the desired result.</span>
<span id="cb6-1521"><a href="#cb6-1521"></a></span>
<span id="cb6-1522"><a href="#cb6-1522"></a>*Claim*.</span>
<span id="cb6-1523"><a href="#cb6-1523"></a></span>
<span id="cb6-1524"><a href="#cb6-1524"></a>$$</span>
<span id="cb6-1525"><a href="#cb6-1525"></a>\norm{A^H}_2 = \norm{A}_2</span>
<span id="cb6-1526"><a href="#cb6-1526"></a>$$</span>
<span id="cb6-1527"><a href="#cb6-1527"></a></span>
<span id="cb6-1528"><a href="#cb6-1528"></a>*Proof.*</span>
<span id="cb6-1529"><a href="#cb6-1529"></a></span>
<span id="cb6-1530"><a href="#cb6-1530"></a>We have:</span>
<span id="cb6-1531"><a href="#cb6-1531"></a></span>
<span id="cb6-1532"><a href="#cb6-1532"></a>$$</span>
<span id="cb6-1533"><a href="#cb6-1533"></a>\begin{align*}</span>
<span id="cb6-1534"><a href="#cb6-1534"></a>\norm{A^H}_2^2 &amp;= \max_{\norm{\bf{x}}_2 = 1} \norm{A^H\bf{x}}_2^2 <span class="sc">\\</span></span>
<span id="cb6-1535"><a href="#cb6-1535"></a>&amp;= \max_{\norm{\bf{x}}_2 = 1} |(A^H \bf{x})^H (A^H \bf{x})|</span>
<span id="cb6-1536"><a href="#cb6-1536"></a>\end{align*}</span>
<span id="cb6-1537"><a href="#cb6-1537"></a>$$</span>
<span id="cb6-1538"><a href="#cb6-1538"></a></span>
<span id="cb6-1539"><a href="#cb6-1539"></a>*Claim.*</span>
<span id="cb6-1540"><a href="#cb6-1540"></a></span>
<span id="cb6-1541"><a href="#cb6-1541"></a>$$</span>
<span id="cb6-1542"><a href="#cb6-1542"></a>\norm{A^H}_2 = \norm{A}_2</span>
<span id="cb6-1543"><a href="#cb6-1543"></a>$$</span>
<span id="cb6-1544"><a href="#cb6-1544"></a></span>
<span id="cb6-1545"><a href="#cb6-1545"></a>*Proof.*</span>
<span id="cb6-1546"><a href="#cb6-1546"></a></span>
<span id="cb6-1547"><a href="#cb6-1547"></a>We have:</span>
<span id="cb6-1548"><a href="#cb6-1548"></a></span>
<span id="cb6-1549"><a href="#cb6-1549"></a>$$</span>
<span id="cb6-1550"><a href="#cb6-1550"></a>\begin{align*}</span>
<span id="cb6-1551"><a href="#cb6-1551"></a>\norm{A^H}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H \bf{x}| <span class="sc">\\</span></span>
<span id="cb6-1552"><a href="#cb6-1552"></a>&amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{x}^H A \bf{y}| &amp; <span class="sc">\{</span> |\overline \alpha| = |\alpha| <span class="sc">\}\\</span></span>
<span id="cb6-1553"><a href="#cb6-1553"></a>&amp;= \max_{\norm{\bf{y}}_2 = 1} \norm{A \bf{y}}_2 <span class="sc">\\</span></span>
<span id="cb6-1554"><a href="#cb6-1554"></a>&amp;= \norm{A}_2</span>
<span id="cb6-1555"><a href="#cb6-1555"></a>\end{align*}</span>
<span id="cb6-1556"><a href="#cb6-1556"></a>$$</span>
<span id="cb6-1557"><a href="#cb6-1557"></a></span>
<span id="cb6-1558"><a href="#cb6-1558"></a>*Claim*</span>
<span id="cb6-1559"><a href="#cb6-1559"></a></span>
<span id="cb6-1560"><a href="#cb6-1560"></a>$$</span>
<span id="cb6-1561"><a href="#cb6-1561"></a>\norm{A^H A}_2 = \norm{A}_2^2</span>
<span id="cb6-1562"><a href="#cb6-1562"></a>$$</span>
<span id="cb6-1563"><a href="#cb6-1563"></a></span>
<span id="cb6-1564"><a href="#cb6-1564"></a>*Proof.*</span>
<span id="cb6-1565"><a href="#cb6-1565"></a></span>
<span id="cb6-1566"><a href="#cb6-1566"></a>We have:</span>
<span id="cb6-1567"><a href="#cb6-1567"></a></span>
<span id="cb6-1568"><a href="#cb6-1568"></a>$$</span>
<span id="cb6-1569"><a href="#cb6-1569"></a>\begin{align*}</span>
<span id="cb6-1570"><a href="#cb6-1570"></a>\norm{A^H A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H A \bf{x}|<span class="sc">\\</span></span>
<span id="cb6-1571"><a href="#cb6-1571"></a>&amp;\leq \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} \norm{\bf{y}^H A^H}_2 \norm{A\bf{x}}_2 &amp; <span class="sc">\{</span> \text{ Cauchy-Schwarz }<span class="sc">\}\\</span></span>
<span id="cb6-1572"><a href="#cb6-1572"></a>&amp;= \max_{\norm{\bf{y}}_2 = 1} \norm{A\bf{y}}_2 \max_{\norm{\bf{x}}_2 = 1} \norm{A\bf{x}}_2  &amp; <span class="sc">\{</span> \norm{A^H}_2 = \norm{A}_2 <span class="sc">\}\\</span></span>
<span id="cb6-1573"><a href="#cb6-1573"></a>&amp;= \norm{A}_2^2</span>
<span id="cb6-1574"><a href="#cb6-1574"></a>\end{align*}</span>
<span id="cb6-1575"><a href="#cb6-1575"></a>$$</span>
<span id="cb6-1576"><a href="#cb6-1576"></a></span>
<span id="cb6-1577"><a href="#cb6-1577"></a>Moreover:</span>
<span id="cb6-1578"><a href="#cb6-1578"></a></span>
<span id="cb6-1579"><a href="#cb6-1579"></a>$$</span>
<span id="cb6-1580"><a href="#cb6-1580"></a>\begin{align*}</span>
<span id="cb6-1581"><a href="#cb6-1581"></a>\norm{A^H A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A^H A \bf{x}|<span class="sc">\\</span></span>
<span id="cb6-1582"><a href="#cb6-1582"></a>&amp;\geq \max_{\norm{\bf{x}}_2 = 1}  |\bf{x}^H A^H A \bf{x}| <span class="sc">\{</span> \text{ Restrict the choices of }\bf{y}<span class="sc">\}\\</span></span>
<span id="cb6-1583"><a href="#cb6-1583"></a>&amp;=  \max_{\norm{\bf{x}}_2 = 1}  |(A\bf{x})^H (A \bf{x})| <span class="sc">\\</span></span>
<span id="cb6-1584"><a href="#cb6-1584"></a>&amp;=  \max_{\norm{\bf{x}}_2 = 1}  \norm{A\bf{x}}_2^2<span class="sc">\\</span></span>
<span id="cb6-1585"><a href="#cb6-1585"></a>&amp;= \norm{A}_2^2</span>
<span id="cb6-1586"><a href="#cb6-1586"></a>\end{align*}</span>
<span id="cb6-1587"><a href="#cb6-1587"></a>$$</span>
<span id="cb6-1588"><a href="#cb6-1588"></a></span>
<span id="cb6-1589"><a href="#cb6-1589"></a>::: {#exr-properties-of-two-norm-4}</span>
<span id="cb6-1590"><a href="#cb6-1590"></a>Partition </span>
<span id="cb6-1591"><a href="#cb6-1591"></a></span>
<span id="cb6-1592"><a href="#cb6-1592"></a>$$</span>
<span id="cb6-1593"><a href="#cb6-1593"></a>A = \left[</span>
<span id="cb6-1594"><a href="#cb6-1594"></a>    \begin{array}{c|c|c}</span>
<span id="cb6-1595"><a href="#cb6-1595"></a>        A_{1,1} &amp; \ldots &amp; A_{1,N}<span class="sc">\\</span></span>
<span id="cb6-1596"><a href="#cb6-1596"></a>        \hline</span>
<span id="cb6-1597"><a href="#cb6-1597"></a>        \vdots &amp; &amp; \vdots<span class="sc">\\</span></span>
<span id="cb6-1598"><a href="#cb6-1598"></a>        \hline</span>
<span id="cb6-1599"><a href="#cb6-1599"></a>        A_{M,1} &amp; \ldots &amp; A_{M,N}</span>
<span id="cb6-1600"><a href="#cb6-1600"></a>    \end{array}</span>
<span id="cb6-1601"><a href="#cb6-1601"></a>\right]</span>
<span id="cb6-1602"><a href="#cb6-1602"></a>$$</span>
<span id="cb6-1603"><a href="#cb6-1603"></a></span>
<span id="cb6-1604"><a href="#cb6-1604"></a>Prove that $\norm{A_{i,j}}_2 \leq \norm{A}_2$.</span>
<span id="cb6-1605"><a href="#cb6-1605"></a>:::</span>
<span id="cb6-1606"><a href="#cb6-1606"></a></span>
<span id="cb6-1607"><a href="#cb6-1607"></a>*Proof.*</span>
<span id="cb6-1608"><a href="#cb6-1608"></a></span>
<span id="cb6-1609"><a href="#cb6-1609"></a>By definition,</span>
<span id="cb6-1610"><a href="#cb6-1610"></a></span>
<span id="cb6-1611"><a href="#cb6-1611"></a>$$</span>
<span id="cb6-1612"><a href="#cb6-1612"></a>\begin{align*}</span>
<span id="cb6-1613"><a href="#cb6-1613"></a>\norm{A_{i,j}}_2 = \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A_{i,j} \bf{x}|</span>
<span id="cb6-1614"><a href="#cb6-1614"></a>\end{align*}</span>
<span id="cb6-1615"><a href="#cb6-1615"></a>$$</span>
<span id="cb6-1616"><a href="#cb6-1616"></a></span>
<span id="cb6-1617"><a href="#cb6-1617"></a>Since $\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1$ is a compact set, the above maximum exists. There exists $\bf{w}_i$ and $\bf{v}_j$, satisfying $\norm{\bf{w}_i}_2 = \norm{\bf{v}_j}_2 = 1$ such that:</span>
<span id="cb6-1618"><a href="#cb6-1618"></a></span>
<span id="cb6-1619"><a href="#cb6-1619"></a>$$</span>
<span id="cb6-1620"><a href="#cb6-1620"></a>\begin{align*}</span>
<span id="cb6-1621"><a href="#cb6-1621"></a>\norm{A_{i,j}}_2 = |\bf{w}_i^H A_{i,j} \bf{v}_j|</span>
<span id="cb6-1622"><a href="#cb6-1622"></a>\end{align*}</span>
<span id="cb6-1623"><a href="#cb6-1623"></a>$$</span>
<span id="cb6-1624"><a href="#cb6-1624"></a></span>
<span id="cb6-1625"><a href="#cb6-1625"></a>Next, we choose </span>
<span id="cb6-1626"><a href="#cb6-1626"></a></span>
<span id="cb6-1627"><a href="#cb6-1627"></a>$$</span>
<span id="cb6-1628"><a href="#cb6-1628"></a>\bf{w} = \left[</span>
<span id="cb6-1629"><a href="#cb6-1629"></a>    \begin{array}{c}</span>
<span id="cb6-1630"><a href="#cb6-1630"></a>    0 <span class="sc">\\</span></span>
<span id="cb6-1631"><a href="#cb6-1631"></a>    \hline </span>
<span id="cb6-1632"><a href="#cb6-1632"></a>    0 <span class="sc">\\</span></span>
<span id="cb6-1633"><a href="#cb6-1633"></a>    \hline</span>
<span id="cb6-1634"><a href="#cb6-1634"></a>    \vdots <span class="sc">\\</span></span>
<span id="cb6-1635"><a href="#cb6-1635"></a>    \hline </span>
<span id="cb6-1636"><a href="#cb6-1636"></a>    \bf{w}_i<span class="sc">\\</span></span>
<span id="cb6-1637"><a href="#cb6-1637"></a>    \hline</span>
<span id="cb6-1638"><a href="#cb6-1638"></a>    0 <span class="sc">\\</span></span>
<span id="cb6-1639"><a href="#cb6-1639"></a>    \hline </span>
<span id="cb6-1640"><a href="#cb6-1640"></a>    \vdots<span class="sc">\\</span></span>
<span id="cb6-1641"><a href="#cb6-1641"></a>    0</span>
<span id="cb6-1642"><a href="#cb6-1642"></a>    \end{array}</span>
<span id="cb6-1643"><a href="#cb6-1643"></a>\right] \quad</span>
<span id="cb6-1644"><a href="#cb6-1644"></a>\bf{v} = \left[</span>
<span id="cb6-1645"><a href="#cb6-1645"></a>    \begin{array}{c}</span>
<span id="cb6-1646"><a href="#cb6-1646"></a>    0 <span class="sc">\\</span></span>
<span id="cb6-1647"><a href="#cb6-1647"></a>    \hline </span>
<span id="cb6-1648"><a href="#cb6-1648"></a>    0 <span class="sc">\\</span></span>
<span id="cb6-1649"><a href="#cb6-1649"></a>    \hline</span>
<span id="cb6-1650"><a href="#cb6-1650"></a>    \vdots <span class="sc">\\</span></span>
<span id="cb6-1651"><a href="#cb6-1651"></a>    \hline </span>
<span id="cb6-1652"><a href="#cb6-1652"></a>    \bf{v}_j<span class="sc">\\</span></span>
<span id="cb6-1653"><a href="#cb6-1653"></a>    \hline</span>
<span id="cb6-1654"><a href="#cb6-1654"></a>    0 <span class="sc">\\</span></span>
<span id="cb6-1655"><a href="#cb6-1655"></a>    \hline </span>
<span id="cb6-1656"><a href="#cb6-1656"></a>    \vdots<span class="sc">\\</span></span>
<span id="cb6-1657"><a href="#cb6-1657"></a>    0</span>
<span id="cb6-1658"><a href="#cb6-1658"></a>    \end{array}</span>
<span id="cb6-1659"><a href="#cb6-1659"></a>\right]</span>
<span id="cb6-1660"><a href="#cb6-1660"></a>$$</span>
<span id="cb6-1661"><a href="#cb6-1661"></a></span>
<span id="cb6-1662"><a href="#cb6-1662"></a>Consider:</span>
<span id="cb6-1663"><a href="#cb6-1663"></a></span>
<span id="cb6-1664"><a href="#cb6-1664"></a>$$</span>
<span id="cb6-1665"><a href="#cb6-1665"></a>\begin{align*}</span>
<span id="cb6-1666"><a href="#cb6-1666"></a>\norm{A}_2 &amp;= \max_{\norm{\bf{x}}_2 = \norm{\bf{y}}_2 = 1} |\bf{y}^H A \bf{x}|<span class="sc">\\</span></span>
<span id="cb6-1667"><a href="#cb6-1667"></a>&amp; \geq |\bf{w}^H A \bf{v}|<span class="sc">\\</span></span>
<span id="cb6-1668"><a href="#cb6-1668"></a>&amp;= |\bf{w}_j^H A_{i,j} \bf{v}_i|<span class="sc">\\</span></span>
<span id="cb6-1669"><a href="#cb6-1669"></a>&amp;= \norm{A_{i,j}}_2</span>
<span id="cb6-1670"><a href="#cb6-1670"></a>\end{align*}</span>
<span id="cb6-1671"><a href="#cb6-1671"></a>$$</span>
<span id="cb6-1672"><a href="#cb6-1672"></a></span>
<span id="cb6-1673"><a href="#cb6-1673"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-1674"><a href="#cb6-1674"></a></span>
<span id="cb6-1675"><a href="#cb6-1675"></a><span class="fu">### Computing the matrix $1$-norm and $\infty$-norm</span></span>
<span id="cb6-1676"><a href="#cb6-1676"></a></span>
<span id="cb6-1677"><a href="#cb6-1677"></a>The matrix $1$-norm and the matrix $\infty$-norm are of great importance, because, unlike the matrix $2$-norm, they are easy and relatively cheap to compute. The following exercises show how to practically compute the matrix $1$-norm and $\infty$-norm.</span>
<span id="cb6-1678"><a href="#cb6-1678"></a></span>
<span id="cb6-1679"><a href="#cb6-1679"></a>::: {#exr-exercise-on-matrix-1-norm-1}</span>
<span id="cb6-1680"><a href="#cb6-1680"></a>Let $A = \C^{m \times n}$ and partition $A = <span class="co">[</span><span class="ot">a_1 | a_2 | \ldots | a_n</span><span class="co">]</span>$. Prove that </span>
<span id="cb6-1681"><a href="#cb6-1681"></a></span>
<span id="cb6-1682"><a href="#cb6-1682"></a>$$</span>
<span id="cb6-1683"><a href="#cb6-1683"></a>\norm{A}_1 = \max_{1 \leq j \leq n}\norm{a_j}_1</span>
<span id="cb6-1684"><a href="#cb6-1684"></a>$$</span>
<span id="cb6-1685"><a href="#cb6-1685"></a>:::</span>
<span id="cb6-1686"><a href="#cb6-1686"></a></span>
<span id="cb6-1687"><a href="#cb6-1687"></a>*Proof*</span>
<span id="cb6-1688"><a href="#cb6-1688"></a></span>
<span id="cb6-1689"><a href="#cb6-1689"></a>We have:</span>
<span id="cb6-1690"><a href="#cb6-1690"></a></span>
<span id="cb6-1691"><a href="#cb6-1691"></a>$$</span>
<span id="cb6-1692"><a href="#cb6-1692"></a>\begin{align*}</span>
<span id="cb6-1693"><a href="#cb6-1693"></a>\norm{A}_1 &amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{A\bf{x}}_1 &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1694"><a href="#cb6-1694"></a>&amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{a_1 x_1 + a_2 x_2 + \ldots + a_n x_n}_1 &amp; <span class="sc">\{</span> \text{ Algebra }<span class="sc">\}\\</span></span>
<span id="cb6-1695"><a href="#cb6-1695"></a>&amp;\leq \max_{\norm{\bf{x}}_1 = 1} \norm{a_1 x_1}_1 + \norm{a_2 x_2}_1 + \ldots + \norm{a_n x_n}_1 &amp; <span class="sc">\{</span> \text{ Triangle Inequality }<span class="sc">\}\\</span></span>
<span id="cb6-1696"><a href="#cb6-1696"></a>&amp;= \max_{\norm{\bf{x}}_1 = 1} |x_1| \norm{a_1}_1 + |x_2| \norm{a_2}_1 + \ldots + |x_n| \norm{a_n}_1  &amp; <span class="sc">\{</span> \text{ Homogeneity }<span class="sc">\}\\</span></span>
<span id="cb6-1697"><a href="#cb6-1697"></a>&amp;= \max_{\norm{\bf{x}}_1 = 1}  |x_1| (\max_{1 \leq j \leq n} \norm{a_j}_1) + |x_2| (\max_{1 \leq j \leq n} \norm{a_j}_1)+ \ldots + |x_n| (\max_{1 \leq j \leq n} \norm{a_j}_1)<span class="sc">\\</span></span>
<span id="cb6-1698"><a href="#cb6-1698"></a>&amp;= \max_{1 \leq j \leq n} \norm{a_j}_1 \max_{\norm{\bf{x}}_1 = 1} \sum_{j=1}^n |x_j|<span class="sc">\\</span></span>
<span id="cb6-1699"><a href="#cb6-1699"></a>&amp;= \max_{1 \leq j \leq n} \norm{a_j}_1 </span>
<span id="cb6-1700"><a href="#cb6-1700"></a>\end{align*}</span>
<span id="cb6-1701"><a href="#cb6-1701"></a>$$</span>
<span id="cb6-1702"><a href="#cb6-1702"></a></span>
<span id="cb6-1703"><a href="#cb6-1703"></a>On the other hand,</span>
<span id="cb6-1704"><a href="#cb6-1704"></a></span>
<span id="cb6-1705"><a href="#cb6-1705"></a>$$</span>
<span id="cb6-1706"><a href="#cb6-1706"></a>\begin{align*}</span>
<span id="cb6-1707"><a href="#cb6-1707"></a>\norm{A}_1  &amp;= \max_{\norm{\bf{x}}_1 = 1} \norm{A\bf{x}}_1 &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}\\</span></span>
<span id="cb6-1708"><a href="#cb6-1708"></a>&amp;\geq \norm{A\bf{e}_j}_1 &amp; <span class="sc">\{</span> \text{ Specific vector }<span class="sc">\}\\</span></span>
<span id="cb6-1709"><a href="#cb6-1709"></a>&amp;= \norm{a_j}_1</span>
<span id="cb6-1710"><a href="#cb6-1710"></a>\end{align*}</span>
<span id="cb6-1711"><a href="#cb6-1711"></a>$$</span>
<span id="cb6-1712"><a href="#cb6-1712"></a></span>
<span id="cb6-1713"><a href="#cb6-1713"></a>This concludes the proof. $\blacksquare$</span>
<span id="cb6-1714"><a href="#cb6-1714"></a></span>
<span id="cb6-1715"><a href="#cb6-1715"></a>::: {#exr-exercise-on-matrix-infty-norm}</span>
<span id="cb6-1716"><a href="#cb6-1716"></a>Let $A = \C^{m \times n}$ and partition </span>
<span id="cb6-1717"><a href="#cb6-1717"></a></span>
<span id="cb6-1718"><a href="#cb6-1718"></a>$$</span>
<span id="cb6-1719"><a href="#cb6-1719"></a>A = \left[</span>
<span id="cb6-1720"><a href="#cb6-1720"></a>    \begin{array}{c}</span>
<span id="cb6-1721"><a href="#cb6-1721"></a>        \tilde{a}_0^T<span class="sc">\\</span></span>
<span id="cb6-1722"><a href="#cb6-1722"></a>        \hline</span>
<span id="cb6-1723"><a href="#cb6-1723"></a>        \tilde{a}_1^T<span class="sc">\\</span></span>
<span id="cb6-1724"><a href="#cb6-1724"></a>        \hline</span>
<span id="cb6-1725"><a href="#cb6-1725"></a>        \vdots<span class="sc">\\</span></span>
<span id="cb6-1726"><a href="#cb6-1726"></a>        \hline</span>
<span id="cb6-1727"><a href="#cb6-1727"></a>        \tilde{a}_{m-1}^T<span class="sc">\\</span></span>
<span id="cb6-1728"><a href="#cb6-1728"></a>    \end{array}</span>
<span id="cb6-1729"><a href="#cb6-1729"></a>\right]</span>
<span id="cb6-1730"><a href="#cb6-1730"></a>$$</span>
<span id="cb6-1731"><a href="#cb6-1731"></a></span>
<span id="cb6-1732"><a href="#cb6-1732"></a>Prove that </span>
<span id="cb6-1733"><a href="#cb6-1733"></a></span>
<span id="cb6-1734"><a href="#cb6-1734"></a>$$</span>
<span id="cb6-1735"><a href="#cb6-1735"></a>\norm{A}_\infty = \max_{0\leq i &lt; m} \norm{\tilde{a}_i}_1 = \max_{0 \leq i &lt; m} (|\alpha_{i,0}| + |\alpha_{i,1}| + \ldots + |\alpha_{i,n-1}|)</span>
<span id="cb6-1736"><a href="#cb6-1736"></a>$$</span>
<span id="cb6-1737"><a href="#cb6-1737"></a>:::</span>
<span id="cb6-1738"><a href="#cb6-1738"></a></span>
<span id="cb6-1739"><a href="#cb6-1739"></a>*Notice that in this exercise, $\tilde{a}_i$ is really $(\tilde{a}_i^T)^T$, since $\tilde{a}_i^T$ is the label for the $i$-th row of the matrix.</span>
<span id="cb6-1740"><a href="#cb6-1740"></a></span>
<span id="cb6-1741"><a href="#cb6-1741"></a>*Proof*.</span>
<span id="cb6-1742"><a href="#cb6-1742"></a></span>
<span id="cb6-1743"><a href="#cb6-1743"></a>We have:</span>
<span id="cb6-1744"><a href="#cb6-1744"></a></span>
<span id="cb6-1745"><a href="#cb6-1745"></a>$$</span>
<span id="cb6-1746"><a href="#cb6-1746"></a>\begin{align*}</span>
<span id="cb6-1747"><a href="#cb6-1747"></a>\norm{A}_\infty &amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{A\bf{x}}_\infty &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb6-1748"><a href="#cb6-1748"></a>&amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{\left[</span>
<span id="cb6-1749"><a href="#cb6-1749"></a>    \begin{array}{c}</span>
<span id="cb6-1750"><a href="#cb6-1750"></a>        \tilde{a}_0^T \bf{x}<span class="sc">\\</span></span>
<span id="cb6-1751"><a href="#cb6-1751"></a>        \hline</span>
<span id="cb6-1752"><a href="#cb6-1752"></a>        \tilde{a}_1^T \bf{x}<span class="sc">\\</span></span>
<span id="cb6-1753"><a href="#cb6-1753"></a>        \hline</span>
<span id="cb6-1754"><a href="#cb6-1754"></a>        \vdots<span class="sc">\\</span></span>
<span id="cb6-1755"><a href="#cb6-1755"></a>        \hline</span>
<span id="cb6-1756"><a href="#cb6-1756"></a>        \tilde{a}_{m-1}^T \bf{x}<span class="sc">\\</span></span>
<span id="cb6-1757"><a href="#cb6-1757"></a>    \end{array}</span>
<span id="cb6-1758"><a href="#cb6-1758"></a>\right]}_\infty &amp; <span class="sc">\{</span> \text{ Algebra }<span class="sc">\}\\</span></span>
<span id="cb6-1759"><a href="#cb6-1759"></a>&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} |\tilde{a}_i^T \bf{x}|<span class="sc">\\</span></span>
<span id="cb6-1760"><a href="#cb6-1760"></a>&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} |\alpha_{i,0}x_0 + \ldots + \alpha_{i,n-1}x_{n-1}|<span class="sc">\\</span></span>
<span id="cb6-1761"><a href="#cb6-1761"></a>&amp;\leq  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} \left( |\alpha_{i,0}x_0 | + \ldots + |\alpha_{i,n-1}x_{n-1}| \right) &amp; <span class="sc">\{</span> \text{ Triangle Inequality }<span class="sc">\}\\</span></span>
<span id="cb6-1762"><a href="#cb6-1762"></a>&amp;=  \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m} \left( |\alpha_{i,0}||x_0 | + \ldots + |\alpha_{i,n-1}||x_{n-1}| \right) &amp; <span class="sc">\{</span> \text{ Algebra }<span class="sc">\}\\</span></span>
<span id="cb6-1763"><a href="#cb6-1763"></a>&amp;\leq \max_{\norm{\bf{x}}_\infty = 1} \max_{0 \leq i &lt; m}\left( |\alpha_{i,0}|\norm{\bf{x}}_\infty + \ldots + |\alpha_{i,n-1}|\norm{\bf{x}}_\infty \right) &amp; \{ |x_i| \leq \norm{\bf{x}}_\infty <span class="sc">\}\\</span></span>
<span id="cb6-1764"><a href="#cb6-1764"></a>&amp;= \max_{0 \leq i &lt; m} ( |\alpha_{i,0}| + \ldots + |\alpha_{i,n-1}|) \max_{\norm{\bf{x}}_\infty = 1} \norm{\bf{x}}_\infty<span class="sc">\\</span></span>
<span id="cb6-1765"><a href="#cb6-1765"></a>&amp;= \max_{0 \leq i &lt; m} ( |\alpha_{i,0}| + \ldots + |\alpha_{i,n-1}|)<span class="sc">\\</span></span>
<span id="cb6-1766"><a href="#cb6-1766"></a>&amp;= \max_{0 \leq i &lt; m}</span>
<span id="cb6-1767"><a href="#cb6-1767"></a> \norm{\tilde{a}_i}_1</span>
<span id="cb6-1768"><a href="#cb6-1768"></a>\end{align*}</span>
<span id="cb6-1769"><a href="#cb6-1769"></a>$$</span>
<span id="cb6-1770"><a href="#cb6-1770"></a></span>
<span id="cb6-1771"><a href="#cb6-1771"></a>We also want to show that $\norm{A}_\infty \geq \max_{0 \leq i &lt; m} \norm{\tilde{a}_i}_1$. Let $k$ be such that $\max_{0 \leq i &lt; m}\norm{\tilde{a}_i}_1 = \norm{\tilde{a}_k}_1$ and pick $\bf{y} = \left(\begin{array}{c}\psi_0\\ \vdots\\ \psi_{n-1}\end{array}\right)$ so that $\tilde{a}_k^T \bf{y} = |\alpha_{k,0}| + |\alpha_{k,1}| + \ldots + |\alpha_{k,n-1}|=\norm{\tilde{a}_k}_1$. This is a matter of picking $\psi_j = |\alpha_{k,j}|/\alpha_{k,j}$. Then, $|\psi_j| = 1$ and hence, $\norm{\bf{y}}_\infty = 1$ and $\psi_j \alpha_{k,j} = |\alpha_{k,j}|$. Then:</span>
<span id="cb6-1772"><a href="#cb6-1772"></a></span>
<span id="cb6-1773"><a href="#cb6-1773"></a>$$</span>
<span id="cb6-1774"><a href="#cb6-1774"></a>\begin{align*}</span>
<span id="cb6-1775"><a href="#cb6-1775"></a>\norm{A}_\infty &amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{A\bf{x}}_\infty &amp; <span class="sc">\{</span> \text{ Definition }<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb6-1776"><a href="#cb6-1776"></a>&amp;= \max_{\norm{\bf{x}}_\infty = 1} \norm{\left[</span>
<span id="cb6-1777"><a href="#cb6-1777"></a>    \begin{array}{c}</span>
<span id="cb6-1778"><a href="#cb6-1778"></a>        \tilde{a}_0^T \bf{x}<span class="sc">\\</span></span>
<span id="cb6-1779"><a href="#cb6-1779"></a>        \hline</span>
<span id="cb6-1780"><a href="#cb6-1780"></a>        \tilde{a}_1^T \bf{x}<span class="sc">\\</span></span>
<span id="cb6-1781"><a href="#cb6-1781"></a>        \hline</span>
<span id="cb6-1782"><a href="#cb6-1782"></a>        \vdots<span class="sc">\\</span></span>
<span id="cb6-1783"><a href="#cb6-1783"></a>        \hline</span>
<span id="cb6-1784"><a href="#cb6-1784"></a>        \tilde{a}_{m-1}^T \bf{x}<span class="sc">\\</span></span>
<span id="cb6-1785"><a href="#cb6-1785"></a>    \end{array}</span>
<span id="cb6-1786"><a href="#cb6-1786"></a>\right]}_\infty &amp; <span class="sc">\{</span> \text{ Expose rows }<span class="sc">\}\\</span></span>
<span id="cb6-1787"><a href="#cb6-1787"></a>&amp;\geq  \norm{\left[</span>
<span id="cb6-1788"><a href="#cb6-1788"></a>    \begin{array}{c}</span>
<span id="cb6-1789"><a href="#cb6-1789"></a>        \tilde{a}_0^T \bf{y}<span class="sc">\\</span></span>
<span id="cb6-1790"><a href="#cb6-1790"></a>        \hline</span>
<span id="cb6-1791"><a href="#cb6-1791"></a>        \tilde{a}_1^T \bf{y}<span class="sc">\\</span></span>
<span id="cb6-1792"><a href="#cb6-1792"></a>        \hline</span>
<span id="cb6-1793"><a href="#cb6-1793"></a>        \vdots<span class="sc">\\</span></span>
<span id="cb6-1794"><a href="#cb6-1794"></a>        \hline</span>
<span id="cb6-1795"><a href="#cb6-1795"></a>        \tilde{a}_{m-1}^T \bf{y}<span class="sc">\\</span></span>
<span id="cb6-1796"><a href="#cb6-1796"></a>    \end{array}</span>
<span id="cb6-1797"><a href="#cb6-1797"></a>\right]}_\infty &amp; <span class="sc">\{</span> \text{ Specific vector }<span class="sc">\}\\</span></span>
<span id="cb6-1798"><a href="#cb6-1798"></a>&amp;\geq |\tilde{a}_k^T \bf{y}|<span class="sc">\\</span></span>
<span id="cb6-1799"><a href="#cb6-1799"></a>&amp;= \norm{\tilde{a}_k}_1 <span class="sc">\\</span></span>
<span id="cb6-1800"><a href="#cb6-1800"></a>&amp;= \max_{0 \leq i &lt; m} \norm{\tilde{a}_i}_1</span>
<span id="cb6-1801"><a href="#cb6-1801"></a>\end{align*}</span>
<span id="cb6-1802"><a href="#cb6-1802"></a>$$</span>
<span id="cb6-1803"><a href="#cb6-1803"></a></span>
<span id="cb6-1804"><a href="#cb6-1804"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-1805"><a href="#cb6-1805"></a></span>
<span id="cb6-1806"><a href="#cb6-1806"></a>::: {#exr-computing-matrix-norms}</span>
<span id="cb6-1807"><a href="#cb6-1807"></a>Fill out the following table:</span>
<span id="cb6-1808"><a href="#cb6-1808"></a></span>
<span id="cb6-1809"><a href="#cb6-1809"></a>$$</span>
<span id="cb6-1810"><a href="#cb6-1810"></a>\begin{array}{|c|c|c|c|}</span>
<span id="cb6-1811"><a href="#cb6-1811"></a>\hline</span>
<span id="cb6-1812"><a href="#cb6-1812"></a>A &amp; \norm{A}_1 &amp; \norm{A}_\infty &amp; \norm{A}_F &amp; \norm{A}_2<span class="sc">\\</span></span>
<span id="cb6-1813"><a href="#cb6-1813"></a>\hline</span>
<span id="cb6-1814"><a href="#cb6-1814"></a>\begin{bmatrix}</span>
<span id="cb6-1815"><a href="#cb6-1815"></a>1 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1816"><a href="#cb6-1816"></a>0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1817"><a href="#cb6-1817"></a>0 &amp; 0 &amp; 1 </span>
<span id="cb6-1818"><a href="#cb6-1818"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb6-1819"><a href="#cb6-1819"></a>\hline</span>
<span id="cb6-1820"><a href="#cb6-1820"></a>\begin{bmatrix}</span>
<span id="cb6-1821"><a href="#cb6-1821"></a>1 &amp; 1 &amp; 1<span class="sc">\\</span></span>
<span id="cb6-1822"><a href="#cb6-1822"></a>1 &amp; 1 &amp; 1<span class="sc">\\</span></span>
<span id="cb6-1823"><a href="#cb6-1823"></a>1 &amp; 1 &amp; 1<span class="sc">\\</span></span>
<span id="cb6-1824"><a href="#cb6-1824"></a>1 &amp; 1 &amp; 1</span>
<span id="cb6-1825"><a href="#cb6-1825"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb6-1826"><a href="#cb6-1826"></a>\hline</span>
<span id="cb6-1827"><a href="#cb6-1827"></a>\begin{bmatrix}</span>
<span id="cb6-1828"><a href="#cb6-1828"></a>0 &amp; 1 &amp; 0<span class="sc">\\</span></span>
<span id="cb6-1829"><a href="#cb6-1829"></a>0 &amp; 1 &amp; 0<span class="sc">\\</span></span>
<span id="cb6-1830"><a href="#cb6-1830"></a>0 &amp; 1 &amp; 0</span>
<span id="cb6-1831"><a href="#cb6-1831"></a>\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb6-1832"><a href="#cb6-1832"></a>\hline</span>
<span id="cb6-1833"><a href="#cb6-1833"></a>\end{array}</span>
<span id="cb6-1834"><a href="#cb6-1834"></a>$$</span>
<span id="cb6-1835"><a href="#cb6-1835"></a>:::</span>
<span id="cb6-1836"><a href="#cb6-1836"></a></span>
<span id="cb6-1837"><a href="#cb6-1837"></a>*Solution*.</span>
<span id="cb6-1838"><a href="#cb6-1838"></a></span>
<span id="cb6-1839"><a href="#cb6-1839"></a>Let </span>
<span id="cb6-1840"><a href="#cb6-1840"></a></span>
<span id="cb6-1841"><a href="#cb6-1841"></a>$$</span>
<span id="cb6-1842"><a href="#cb6-1842"></a>A = \begin{bmatrix}</span>
<span id="cb6-1843"><a href="#cb6-1843"></a>1 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1844"><a href="#cb6-1844"></a>0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-1845"><a href="#cb6-1845"></a>0 &amp; 0 &amp; 1 </span>
<span id="cb6-1846"><a href="#cb6-1846"></a>\end{bmatrix}</span>
<span id="cb6-1847"><a href="#cb6-1847"></a>$$</span>
<span id="cb6-1848"><a href="#cb6-1848"></a></span>
<span id="cb6-1849"><a href="#cb6-1849"></a>We have, $\norm{A}_1 = 1$, $\norm{A}_\infty = 1$, $\norm{A}_F = \sqrt{3}$. Since this is a diagonal matrix, $\norm{A}_2 = \max_{0 \leq i \leq 2} |d_{i}|$ = 1. </span>
<span id="cb6-1850"><a href="#cb6-1850"></a></span>
<span id="cb6-1851"><a href="#cb6-1851"></a>Next, consider:</span>
<span id="cb6-1852"><a href="#cb6-1852"></a></span>
<span id="cb6-1853"><a href="#cb6-1853"></a>$$</span>
<span id="cb6-1854"><a href="#cb6-1854"></a>\begin{bmatrix}</span>
<span id="cb6-1855"><a href="#cb6-1855"></a>1 &amp; 1 &amp; 1<span class="sc">\\</span></span>
<span id="cb6-1856"><a href="#cb6-1856"></a>1 &amp; 1 &amp; 1<span class="sc">\\</span></span>
<span id="cb6-1857"><a href="#cb6-1857"></a>1 &amp; 1 &amp; 1<span class="sc">\\</span></span>
<span id="cb6-1858"><a href="#cb6-1858"></a>1 &amp; 1 &amp; 1</span>
<span id="cb6-1859"><a href="#cb6-1859"></a>\end{bmatrix}</span>
<span id="cb6-1860"><a href="#cb6-1860"></a>$$</span>
<span id="cb6-1861"><a href="#cb6-1861"></a></span>
<span id="cb6-1862"><a href="#cb6-1862"></a>We have, $\norm{A}_1 = 4$, $\norm{A}_\infty = 3$, $\norm{A}_F = \sqrt{12}$. </span>
<span id="cb6-1863"><a href="#cb6-1863"></a></span>
<span id="cb6-1864"><a href="#cb6-1864"></a>Note that, we can write </span>
<span id="cb6-1865"><a href="#cb6-1865"></a></span>
<span id="cb6-1866"><a href="#cb6-1866"></a>$$</span>
<span id="cb6-1867"><a href="#cb6-1867"></a>A = \begin{bmatrix}</span>
<span id="cb6-1868"><a href="#cb6-1868"></a>1 <span class="sc">\\</span></span>
<span id="cb6-1869"><a href="#cb6-1869"></a>1 <span class="sc">\\</span></span>
<span id="cb6-1870"><a href="#cb6-1870"></a>1 <span class="sc">\\</span></span>
<span id="cb6-1871"><a href="#cb6-1871"></a>1 <span class="sc">\\</span></span>
<span id="cb6-1872"><a href="#cb6-1872"></a>\end{bmatrix} <span class="co">[</span><span class="ot">1, 1, 1, 1</span><span class="co">]</span> = \bf{x}\bf{y}^H</span>
<span id="cb6-1873"><a href="#cb6-1873"></a>$$</span>
<span id="cb6-1874"><a href="#cb6-1874"></a></span>
<span id="cb6-1875"><a href="#cb6-1875"></a>where $\bf{x} = \bf{y} = \begin{bmatrix}</span>
<span id="cb6-1876"><a href="#cb6-1876"></a>1 <span class="sc">\\</span></span>
<span id="cb6-1877"><a href="#cb6-1877"></a>1 <span class="sc">\\</span></span>
<span id="cb6-1878"><a href="#cb6-1878"></a>1 <span class="sc">\\</span></span>
<span id="cb6-1879"><a href="#cb6-1879"></a>1 <span class="sc">\\</span></span>
<span id="cb6-1880"><a href="#cb6-1880"></a>\end{bmatrix}$. Using the property that, $\norm{\bf{x}\bf{y}^H}_2 = \norm{\bf{x}}_2 \norm{\bf{y}}_2$, we have that, $\norm{A}_2 = 4$. </span>
<span id="cb6-1881"><a href="#cb6-1881"></a></span>
<span id="cb6-1882"><a href="#cb6-1882"></a>Finally, if </span>
<span id="cb6-1883"><a href="#cb6-1883"></a></span>
<span id="cb6-1884"><a href="#cb6-1884"></a>$$</span>
<span id="cb6-1885"><a href="#cb6-1885"></a>A = \begin{bmatrix}</span>
<span id="cb6-1886"><a href="#cb6-1886"></a>0 &amp; 1 &amp; 0<span class="sc">\\</span></span>
<span id="cb6-1887"><a href="#cb6-1887"></a>0 &amp; 1 &amp; 0<span class="sc">\\</span></span>
<span id="cb6-1888"><a href="#cb6-1888"></a>0 &amp; 1 &amp; 0</span>
<span id="cb6-1889"><a href="#cb6-1889"></a>\end{bmatrix}</span>
<span id="cb6-1890"><a href="#cb6-1890"></a>$$</span>
<span id="cb6-1891"><a href="#cb6-1891"></a></span>
<span id="cb6-1892"><a href="#cb6-1892"></a>we find that $\norm{A}_1 = 3$, $\norm{A}_\infty = 1$, $\norm{A}_F = \sqrt{3}$. Finally, let $\bf{x} = \begin{bmatrix}1 <span class="sc">\\</span> 1 <span class="sc">\\</span> 1\end{bmatrix}$ and $\bf{y} = \begin{bmatrix}0 <span class="sc">\\</span> 1 <span class="sc">\\</span> 0\end{bmatrix}$. Then, $A = \bf{x}\bf{y}^H$. So, $\norm{A}_2 = \norm{\bf{x}}_2 \norm{\bf{y}}_2 = \sqrt{3}$.</span>
<span id="cb6-1893"><a href="#cb6-1893"></a></span>
<span id="cb6-1894"><a href="#cb6-1894"></a><span class="fu">### Equivalence of matrix norms</span></span>
<span id="cb6-1895"><a href="#cb6-1895"></a></span>
<span id="cb6-1896"><a href="#cb6-1896"></a>We saw that vector norms are equivalent in the sense that if a vector is *small* in one norm, it is small in all other norms and if it is large in one norm, it is large in all other norms. The same is true for matrix norms.</span>
<span id="cb6-1897"><a href="#cb6-1897"></a></span>
<span id="cb6-1898"><a href="#cb6-1898"></a>::: {#thm-equivalence-of-matrix-norms}</span>
<span id="cb6-1899"><a href="#cb6-1899"></a></span>
<span id="cb6-1900"><a href="#cb6-1900"></a><span class="fu">### Equivalence of matrix norms </span></span>
<span id="cb6-1901"><a href="#cb6-1901"></a></span>
<span id="cb6-1902"><a href="#cb6-1902"></a>Let $\norm{\cdot} : \C^{m \times n} \to \R$ and $|||\cdot|||:\C^{m \times n} \to \R$ both be matrix norms. Then, there exist positive scalars $\sigma$ and $\tau$ such that for all $A \in \C^{m \times n}$</span>
<span id="cb6-1903"><a href="#cb6-1903"></a></span>
<span id="cb6-1904"><a href="#cb6-1904"></a>$$</span>
<span id="cb6-1905"><a href="#cb6-1905"></a>\sigma \norm{A} \leq |||A||| \leq \tau \norm{A}</span>
<span id="cb6-1906"><a href="#cb6-1906"></a>$$</span>
<span id="cb6-1907"><a href="#cb6-1907"></a>:::</span>
<span id="cb6-1908"><a href="#cb6-1908"></a></span>
<span id="cb6-1909"><a href="#cb6-1909"></a>*Proof.*</span>
<span id="cb6-1910"><a href="#cb6-1910"></a></span>
<span id="cb6-1911"><a href="#cb6-1911"></a>The proof again builds on the fact that the supremum over a compact set is achieved and can be replaced by the maximum. We will prove that there exists $\tau$ such that for all $A \in \C^{m \times n}$</span>
<span id="cb6-1912"><a href="#cb6-1912"></a></span>
<span id="cb6-1913"><a href="#cb6-1913"></a>$$</span>
<span id="cb6-1914"><a href="#cb6-1914"></a>|||A||| \leq \tau \norm{A}</span>
<span id="cb6-1915"><a href="#cb6-1915"></a>$$</span>
<span id="cb6-1916"><a href="#cb6-1916"></a></span>
<span id="cb6-1917"><a href="#cb6-1917"></a>Let $A \in \C^{m \times n}$ be an arbitrary matrix. Assume that $A \neq 0$ (the zero matrix). Then:</span>
<span id="cb6-1918"><a href="#cb6-1918"></a></span>
<span id="cb6-1919"><a href="#cb6-1919"></a>$$</span>
<span id="cb6-1920"><a href="#cb6-1920"></a>\begin{align*}</span>
<span id="cb6-1921"><a href="#cb6-1921"></a>|||A||| &amp;= \frac{|||A|||}{\norm{A}} \cdot \norm{A} &amp; <span class="sc">\{</span>\text{Algebra}<span class="sc">\}\\</span></span>
<span id="cb6-1922"><a href="#cb6-1922"></a>&amp;\leq \sup_{Z \neq 0} \left(\frac{|||Z|||}{\norm{Z}}\right) \norm{A} &amp; <span class="sc">\{</span>\text{Definition of supremum}<span class="sc">\}\\</span></span>
<span id="cb6-1923"><a href="#cb6-1923"></a>&amp;= \sup_{Z \neq 0} \left(\Biggl|\Biggl|\Biggl|\frac{Z}{\norm{Z}}\Biggr|\Biggr|\Biggr|\right) \norm{A} &amp; <span class="sc">\{</span>\text{Homogeneity}<span class="sc">\}\\</span></span>
<span id="cb6-1924"><a href="#cb6-1924"></a>&amp;= \left(\sup_{\norm{B} = 1} |||B||| \right) \norm{A} &amp;<span class="sc">\{</span> \text{change of variables }B=Z/\norm{Z}<span class="sc">\}\\</span></span>
<span id="cb6-1925"><a href="#cb6-1925"></a>&amp;= \left(\max_{\norm{B}=1}|||B|||\right) \norm{A} &amp; <span class="sc">\{</span>\text{the set }\norm{B} = 1\text{ is compact}<span class="sc">\}</span></span>
<span id="cb6-1926"><a href="#cb6-1926"></a>\end{align*}</span>
<span id="cb6-1927"><a href="#cb6-1927"></a>$$</span>
<span id="cb6-1928"><a href="#cb6-1928"></a></span>
<span id="cb6-1929"><a href="#cb6-1929"></a>So, we can choose $\tau = \max_{\norm{B}=1} |||B|||$. </span>
<span id="cb6-1930"><a href="#cb6-1930"></a></span>
<span id="cb6-1931"><a href="#cb6-1931"></a>Also, from the above proof, we deduce that, there exists $\sigma$ given by:</span>
<span id="cb6-1932"><a href="#cb6-1932"></a></span>
<span id="cb6-1933"><a href="#cb6-1933"></a>$$</span>
<span id="cb6-1934"><a href="#cb6-1934"></a>\sigma = \frac{1}{\max_{|||B|||=1}||B||}</span>
<span id="cb6-1935"><a href="#cb6-1935"></a>$$</span>
<span id="cb6-1936"><a href="#cb6-1936"></a></span>
<span id="cb6-1937"><a href="#cb6-1937"></a>such that:</span>
<span id="cb6-1938"><a href="#cb6-1938"></a></span>
<span id="cb6-1939"><a href="#cb6-1939"></a>$$</span>
<span id="cb6-1940"><a href="#cb6-1940"></a>\sigma \norm{A} \leq |||A|||</span>
<span id="cb6-1941"><a href="#cb6-1941"></a>$$</span>
<span id="cb6-1942"><a href="#cb6-1942"></a></span>
<span id="cb6-1943"><a href="#cb6-1943"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-1944"><a href="#cb6-1944"></a></span>
<span id="cb6-1945"><a href="#cb6-1945"></a>::: {#exr-matrix-2-norm-bounded-by-frobenius-norm}</span>
<span id="cb6-1946"><a href="#cb6-1946"></a>Given $A \in \C^{m \times n}$, show that $\norm{A}_2 \leq \norm{A}_F$. For what matrix, is the equality attained?</span>
<span id="cb6-1947"><a href="#cb6-1947"></a>:::</span>
<span id="cb6-1948"><a href="#cb6-1948"></a></span>
<span id="cb6-1949"><a href="#cb6-1949"></a>*Solution*.</span>
<span id="cb6-1950"><a href="#cb6-1950"></a></span>
<span id="cb6-1951"><a href="#cb6-1951"></a>We have:</span>
<span id="cb6-1952"><a href="#cb6-1952"></a></span>
<span id="cb6-1953"><a href="#cb6-1953"></a>$$</span>
<span id="cb6-1954"><a href="#cb6-1954"></a>\begin{align*}</span>
<span id="cb6-1955"><a href="#cb6-1955"></a>\norm{A}_2^2 &amp;= \max_{\norm{x}_2 = 1} \norm{Ax}_2^2  &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-1956"><a href="#cb6-1956"></a>&amp;= \max_{\norm{x}_2 = 1} \norm{\begin{bmatrix}</span>
<span id="cb6-1957"><a href="#cb6-1957"></a>\sum_{j=0}^{n-1} a_{0,j} x_j <span class="sc">\\</span></span>
<span id="cb6-1958"><a href="#cb6-1958"></a>\sum_{j=0}^{n-1} a_{1,j} x_j <span class="sc">\\</span></span>
<span id="cb6-1959"><a href="#cb6-1959"></a>\vdots<span class="sc">\\</span></span>
<span id="cb6-1960"><a href="#cb6-1960"></a>\sum_{j=0}^{n-1} a_{m-1,j} x_j </span>
<span id="cb6-1961"><a href="#cb6-1961"></a>\end{bmatrix}</span>
<span id="cb6-1962"><a href="#cb6-1962"></a>}_2^2<span class="sc">\\</span></span>
<span id="cb6-1963"><a href="#cb6-1963"></a>&amp;= \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \Biggl|\sum_{j=0}^{n-1} a_{i,j} x_j\Biggr|^2<span class="sc">\\</span></span>
<span id="cb6-1964"><a href="#cb6-1964"></a>&amp;\leq \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j} x_j|\right)^2 &amp; <span class="sc">\{</span>\text{Triangle Inequality}<span class="sc">\}\\</span></span>
<span id="cb6-1965"><a href="#cb6-1965"></a>&amp;\leq \max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left\{\left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right)^{1/2} \left(\sum_{j=0}^{n-1}|x_j|^2\right)^{1/2}\right<span class="sc">\}</span>^2 &amp; <span class="sc">\{</span>\text{Cauchy-Schwarz}<span class="sc">\}\\</span></span>
<span id="cb6-1966"><a href="#cb6-1966"></a>&amp;=\max_{\norm{x}_2 = 1} \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right) \left(\sum_{j=0}^{n-1}|x_j|^2\right) &amp; <span class="sc">\{</span>\text{Simplify}<span class="sc">\}\\</span></span>
<span id="cb6-1967"><a href="#cb6-1967"></a>&amp;=\sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |a_{i,j}|^2\right) &amp; <span class="sc">\{</span>\norm{x}_2 = 1<span class="sc">\}\\</span></span>
<span id="cb6-1968"><a href="#cb6-1968"></a>&amp;= \norm{A}_F</span>
<span id="cb6-1969"><a href="#cb6-1969"></a>\end{align*}</span>
<span id="cb6-1970"><a href="#cb6-1970"></a>$$</span>
<span id="cb6-1971"><a href="#cb6-1971"></a></span>
<span id="cb6-1972"><a href="#cb6-1972"></a>Also, consider </span>
<span id="cb6-1973"><a href="#cb6-1973"></a></span>
<span id="cb6-1974"><a href="#cb6-1974"></a>$$A = \begin{bmatrix}1 &amp; 0 <span class="sc">\\</span> 0 &amp; 0\end{bmatrix}$$</span>
<span id="cb6-1975"><a href="#cb6-1975"></a></span>
<span id="cb6-1976"><a href="#cb6-1976"></a>Then, $\norm{A}_2 = \norm{A}_F = 1$. So, the inequality $\norm{A}_2 \leq \norm{A}_F$ is tight. This closes the proof. $\blacksquare$</span>
<span id="cb6-1977"><a href="#cb6-1977"></a></span>
<span id="cb6-1978"><a href="#cb6-1978"></a>::: {#exr-matrix-norm-equivalences}</span>
<span id="cb6-1979"><a href="#cb6-1979"></a>Let $A \in \C^{m \times n}$. The following table summarizes the equivalences of various matrix norms:</span>
<span id="cb6-1980"><a href="#cb6-1980"></a></span>
<span id="cb6-1981"><a href="#cb6-1981"></a>$$</span>
<span id="cb6-1982"><a href="#cb6-1982"></a>\begin{array}{c|c|c|c}</span>
<span id="cb6-1983"><a href="#cb6-1983"></a> &amp; \norm{A}_1 \leq \sqrt{m}\norm{A}_2 &amp; \norm{A}_1 \leq m \norm{A}_\infty &amp; \norm{A}_1 \leq \sqrt{m}\norm{A}_F <span class="sc">\\</span></span>
<span id="cb6-1984"><a href="#cb6-1984"></a> \hline</span>
<span id="cb6-1985"><a href="#cb6-1985"></a> \norm{A}_2 \leq \sqrt{n}\norm{A}_1 &amp; &amp; \norm{A}_2 \leq \sqrt{m}\norm{A}_\infty &amp; \norm{A}_2 \leq \norm{A}_F <span class="sc">\\</span></span>
<span id="cb6-1986"><a href="#cb6-1986"></a> \hline</span>
<span id="cb6-1987"><a href="#cb6-1987"></a> \norm{A}_\infty \leq n \norm{A}_1 &amp; \norm{A}_\infty \leq \sqrt{n} \norm{A}_2 &amp; &amp; \norm{A}_\infty \leq \sqrt{n}\norm{A}_F<span class="sc">\\</span></span>
<span id="cb6-1988"><a href="#cb6-1988"></a> \hline</span>
<span id="cb6-1989"><a href="#cb6-1989"></a> \norm{A}_F \leq \sqrt{n} \norm{A}_1 &amp; \norm{A}_F \leq \tau \norm{A}_2 &amp; \norm{A}_F \leq \sqrt{m}\norm{A}_\infty </span>
<span id="cb6-1990"><a href="#cb6-1990"></a>\end{array}</span>
<span id="cb6-1991"><a href="#cb6-1991"></a>$$</span>
<span id="cb6-1992"><a href="#cb6-1992"></a></span>
<span id="cb6-1993"><a href="#cb6-1993"></a>For each, prove the inequality, including that it is a tight inequality for some nonzero $A$. (Skip $\norm{A}_F \leq \tau \norm{A}_2$, we revisit it in a later post)</span>
<span id="cb6-1994"><a href="#cb6-1994"></a>:::</span>
<span id="cb6-1995"><a href="#cb6-1995"></a></span>
<span id="cb6-1996"><a href="#cb6-1996"></a>*Solution*.</span>
<span id="cb6-1997"><a href="#cb6-1997"></a></span>
<span id="cb6-1998"><a href="#cb6-1998"></a>*Claim*. Our claim is that $\norm{A}_1 \leq \sqrt{m} \norm{A}_2$. </span>
<span id="cb6-1999"><a href="#cb6-1999"></a></span>
<span id="cb6-2000"><a href="#cb6-2000"></a>Partition $A = <span class="co">[</span><span class="ot">a_0 | a_1 | \ldots | a_{n-1}</span><span class="co">]</span>$.</span>
<span id="cb6-2001"><a href="#cb6-2001"></a></span>
<span id="cb6-2002"><a href="#cb6-2002"></a>We have:</span>
<span id="cb6-2003"><a href="#cb6-2003"></a></span>
<span id="cb6-2004"><a href="#cb6-2004"></a>$$</span>
<span id="cb6-2005"><a href="#cb6-2005"></a>\begin{align*}</span>
<span id="cb6-2006"><a href="#cb6-2006"></a>\norm{A}_1 &amp;= \max_{0 \leq j &lt; n} \norm{a_j}_1 &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2007"><a href="#cb6-2007"></a>&amp;= \max_{0 \leq j &lt; n} \norm{</span>
<span id="cb6-2008"><a href="#cb6-2008"></a>    \begin{bmatrix}</span>
<span id="cb6-2009"><a href="#cb6-2009"></a>    \alpha_{0,j}<span class="sc">\\</span></span>
<span id="cb6-2010"><a href="#cb6-2010"></a>    \alpha_{1,j}<span class="sc">\\</span></span>
<span id="cb6-2011"><a href="#cb6-2011"></a>    \vdots<span class="sc">\\</span></span>
<span id="cb6-2012"><a href="#cb6-2012"></a>    \alpha_{m-1,j}</span>
<span id="cb6-2013"><a href="#cb6-2013"></a>    \end{bmatrix}</span>
<span id="cb6-2014"><a href="#cb6-2014"></a>}_1<span class="sc">\\</span></span>
<span id="cb6-2015"><a href="#cb6-2015"></a>&amp;= \max_{0 \leq j &lt; n} \sum_{i=0}^{m-1}|\alpha_{i,j}| \cdot |1|<span class="sc">\\</span></span>
<span id="cb6-2016"><a href="#cb6-2016"></a>&amp;= \max_{0 \leq j &lt; n} \left(\sum_{i=0}^{m-1}|\alpha_{i,j}|^2\right)^{1/2} \left(\sum_{i=0}^{m-1}|1|^2\right)^{1/2} &amp; <span class="sc">\{</span>\text{Cauchy-Schwarz}<span class="sc">\}\\</span></span>
<span id="cb6-2017"><a href="#cb6-2017"></a>&amp;= \max_{0 \leq j &lt; n} \norm{a_j}_2 \sqrt{m}<span class="sc">\\</span></span>
<span id="cb6-2018"><a href="#cb6-2018"></a>&amp;= \max_{0 \leq j &lt; n} \norm{A}_2 \sqrt{m} &amp; \{\norm{A_{i,j}}_2 \leq \norm{A}_2<span class="sc">\}\\</span></span>
<span id="cb6-2019"><a href="#cb6-2019"></a>&amp;= \sqrt{m} \norm{A}_2 </span>
<span id="cb6-2020"><a href="#cb6-2020"></a>\end{align*}</span>
<span id="cb6-2021"><a href="#cb6-2021"></a>$$</span>
<span id="cb6-2022"><a href="#cb6-2022"></a></span>
<span id="cb6-2023"><a href="#cb6-2023"></a>Moreover, consider the matrix</span>
<span id="cb6-2024"><a href="#cb6-2024"></a></span>
<span id="cb6-2025"><a href="#cb6-2025"></a>$$</span>
<span id="cb6-2026"><a href="#cb6-2026"></a>A = \begin{bmatrix}</span>
<span id="cb6-2027"><a href="#cb6-2027"></a>1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb6-2028"><a href="#cb6-2028"></a>1 &amp; 0</span>
<span id="cb6-2029"><a href="#cb6-2029"></a>\end{bmatrix} = </span>
<span id="cb6-2030"><a href="#cb6-2030"></a>\begin{bmatrix}</span>
<span id="cb6-2031"><a href="#cb6-2031"></a>1 <span class="sc">\\</span></span>
<span id="cb6-2032"><a href="#cb6-2032"></a>1</span>
<span id="cb6-2033"><a href="#cb6-2033"></a>\end{bmatrix} </span>
<span id="cb6-2034"><a href="#cb6-2034"></a>\begin{bmatrix}</span>
<span id="cb6-2035"><a href="#cb6-2035"></a>1 &amp; 0</span>
<span id="cb6-2036"><a href="#cb6-2036"></a>\end{bmatrix}</span>
<span id="cb6-2037"><a href="#cb6-2037"></a>$$</span>
<span id="cb6-2038"><a href="#cb6-2038"></a></span>
<span id="cb6-2039"><a href="#cb6-2039"></a>We have, $\norm{A}_2 = \sqrt{2}$ and $\norm{A}_1 = 2$, so $\norm{A}_1 = \sqrt{2}\norm{A}_2$. Thus, the inequality is tight. This closes the proof.</span>
<span id="cb6-2040"><a href="#cb6-2040"></a></span>
<span id="cb6-2041"><a href="#cb6-2041"></a>*Claim*. Our claim is that $\norm{A}_1 \leq m \norm{A}_\infty$.</span>
<span id="cb6-2042"><a href="#cb6-2042"></a></span>
<span id="cb6-2043"><a href="#cb6-2043"></a>*Solution*.</span>
<span id="cb6-2044"><a href="#cb6-2044"></a></span>
<span id="cb6-2045"><a href="#cb6-2045"></a>We have:</span>
<span id="cb6-2046"><a href="#cb6-2046"></a></span>
<span id="cb6-2047"><a href="#cb6-2047"></a>$$</span>
<span id="cb6-2048"><a href="#cb6-2048"></a>\begin{align}</span>
<span id="cb6-2049"><a href="#cb6-2049"></a>\norm{A}_1 &amp;= \max_{0 \leq j &lt; n} \norm{a_j}_1 &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2050"><a href="#cb6-2050"></a>&amp;= \max_{0 \leq j &lt; n} \sum_{i=0}^{m-1}|\alpha_{i,j}|<span class="sc">\\</span></span>
<span id="cb6-2051"><a href="#cb6-2051"></a>&amp;\leq \sum_{j=0}^{n-1} \sum_{i=0}^{m-1} |\alpha_{i,j}| = \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}| <span class="sc">\\</span></span>
<span id="cb6-2052"><a href="#cb6-2052"></a>&amp;=\sum_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1<span class="sc">\\</span></span>
<span id="cb6-2053"><a href="#cb6-2053"></a>&amp;= m \max_{0 \leq i &lt; m} \norm{\tilde{a}_i^T}_1<span class="sc">\\</span></span>
<span id="cb6-2054"><a href="#cb6-2054"></a>&amp;= m \norm{A}_\infty</span>
<span id="cb6-2055"><a href="#cb6-2055"></a>\end{align}</span>
<span id="cb6-2056"><a href="#cb6-2056"></a>$$</span>
<span id="cb6-2057"><a href="#cb6-2057"></a></span>
<span id="cb6-2058"><a href="#cb6-2058"></a>Again, consider the matrix </span>
<span id="cb6-2059"><a href="#cb6-2059"></a></span>
<span id="cb6-2060"><a href="#cb6-2060"></a>$$</span>
<span id="cb6-2061"><a href="#cb6-2061"></a>A = \begin{bmatrix}</span>
<span id="cb6-2062"><a href="#cb6-2062"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb6-2063"><a href="#cb6-2063"></a>1 &amp; 0 </span>
<span id="cb6-2064"><a href="#cb6-2064"></a>\end{bmatrix}</span>
<span id="cb6-2065"><a href="#cb6-2065"></a>$$</span>
<span id="cb6-2066"><a href="#cb6-2066"></a></span>
<span id="cb6-2067"><a href="#cb6-2067"></a>$\norm{A}_1 = 2$ and $\norm{A}_\infty = 1$, so $\norm{A}_1 = 2 \norm{A}_\infty$. Hence, the inequality is tight. This closes the proof.</span>
<span id="cb6-2068"><a href="#cb6-2068"></a></span>
<span id="cb6-2069"><a href="#cb6-2069"></a>*Claim*. Our claim is that $\norm{A}_1 \leq \sqrt{m}\norm{A}_F$.</span>
<span id="cb6-2070"><a href="#cb6-2070"></a></span>
<span id="cb6-2071"><a href="#cb6-2071"></a>*Solution.*</span>
<span id="cb6-2072"><a href="#cb6-2072"></a></span>
<span id="cb6-2073"><a href="#cb6-2073"></a>We have shown that:</span>
<span id="cb6-2074"><a href="#cb6-2074"></a></span>
<span id="cb6-2075"><a href="#cb6-2075"></a>$$</span>
<span id="cb6-2076"><a href="#cb6-2076"></a>\begin{align}</span>
<span id="cb6-2077"><a href="#cb6-2077"></a>\norm{A}_1 &amp;\leq \sqrt{m}\norm{A}_2 <span class="sc">\\</span></span>
<span id="cb6-2078"><a href="#cb6-2078"></a>\norm{A}_2 &amp;\leq \norm{A}_F</span>
<span id="cb6-2079"><a href="#cb6-2079"></a>\end{align}</span>
<span id="cb6-2080"><a href="#cb6-2080"></a>$$</span>
<span id="cb6-2081"><a href="#cb6-2081"></a></span>
<span id="cb6-2082"><a href="#cb6-2082"></a>So, we deduce that $\norm{A}_1 \leq \sqrt{m}\norm{A}_F$. Moreover, consider </span>
<span id="cb6-2083"><a href="#cb6-2083"></a></span>
<span id="cb6-2084"><a href="#cb6-2084"></a>$$</span>
<span id="cb6-2085"><a href="#cb6-2085"></a>A = \sqrt{2}I</span>
<span id="cb6-2086"><a href="#cb6-2086"></a>$$</span>
<span id="cb6-2087"><a href="#cb6-2087"></a></span>
<span id="cb6-2088"><a href="#cb6-2088"></a>Then, $\norm{A}_1 = \sqrt{2}$ and $\norm{A}_F = 2$, so $\norm{A}_1 = \sqrt{2}\norm{A}_F$. Hence, the inequality is tight.</span>
<span id="cb6-2089"><a href="#cb6-2089"></a></span>
<span id="cb6-2090"><a href="#cb6-2090"></a>*Claim.* Our claim is that $\norm{A}_2 \leq \sqrt{n}\norm{A}_1$.</span>
<span id="cb6-2091"><a href="#cb6-2091"></a></span>
<span id="cb6-2092"><a href="#cb6-2092"></a>*Solution.*</span>
<span id="cb6-2093"><a href="#cb6-2093"></a></span>
<span id="cb6-2094"><a href="#cb6-2094"></a>We have:</span>
<span id="cb6-2095"><a href="#cb6-2095"></a></span>
<span id="cb6-2096"><a href="#cb6-2096"></a>$$</span>
<span id="cb6-2097"><a href="#cb6-2097"></a>\begin{align}</span>
<span id="cb6-2098"><a href="#cb6-2098"></a>\norm{A}_2 &amp;= \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2099"><a href="#cb6-2099"></a>&amp;= \max_{x \neq 0} \frac{\norm{Ax}_1}{\norm{x}_2}  &amp; <span class="sc">\{</span>\norm{z}_2 \leq \norm{z}_1<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb6-2100"><a href="#cb6-2100"></a>&amp;= \max_{x \neq 0} \frac{\norm{Ax}_1}{\frac{1}{\sqrt{n}}\norm{x}_1}  &amp; <span class="sc">\{</span>\norm{z}_1 \leq \sqrt{n}\norm{z}_2<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb6-2101"><a href="#cb6-2101"></a>&amp;= \sqrt{n}\norm{A}_1</span>
<span id="cb6-2102"><a href="#cb6-2102"></a>\end{align}</span>
<span id="cb6-2103"><a href="#cb6-2103"></a>$$</span>
<span id="cb6-2104"><a href="#cb6-2104"></a></span>
<span id="cb6-2105"><a href="#cb6-2105"></a>Again, consider the matrix $A = <span class="co">[</span><span class="ot">1 | 1| \ldots | 1</span><span class="co">]</span>$. Then, $\norm{A}_2 = \sqrt{n}$ and $\norm{A}_1 = 1$. So, $\norm{A}_2 = \sqrt{n}\norm{A}_1$.</span>
<span id="cb6-2106"><a href="#cb6-2106"></a></span>
<span id="cb6-2107"><a href="#cb6-2107"></a>*Claim*. Our claim is that $\norm{A}_2 \leq \sqrt{m} \norm{A}_\infty$.</span>
<span id="cb6-2108"><a href="#cb6-2108"></a></span>
<span id="cb6-2109"><a href="#cb6-2109"></a>*Solution.*</span>
<span id="cb6-2110"><a href="#cb6-2110"></a></span>
<span id="cb6-2111"><a href="#cb6-2111"></a>$$</span>
<span id="cb6-2112"><a href="#cb6-2112"></a>\begin{align*}</span>
<span id="cb6-2113"><a href="#cb6-2113"></a>\norm{A}_2 &amp;= \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} &amp;<span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2114"><a href="#cb6-2114"></a>&amp;=\max_{x \neq 0} \frac{\norm{\begin{bmatrix}\tilde{a}_0^T \\ \tilde{a}_1^T \\ \vdots \\ \tilde{a}_{m-1}^T\end{bmatrix}x}_2}{\norm{x}_2} &amp;<span class="sc">\{</span>\text{Expose rows}<span class="sc">\}\\</span></span>
<span id="cb6-2115"><a href="#cb6-2115"></a>&amp;=\max_{x \neq 0} \frac{\norm{\begin{bmatrix}\tilde{a}_0^T x \\ \tilde{a}_1^T x\\ \vdots \\ \tilde{a}_{m-1}^T x\end{bmatrix}}_2}{\norm{x}_2} &amp;<span class="sc">\{</span>\text{Algebra}<span class="sc">\}\\</span></span>
<span id="cb6-2116"><a href="#cb6-2116"></a>&amp;\leq \max_{x \neq 0} \frac{\sqrt{m}\norm{Ax}_\infty}{\norm{x}_2} &amp;\{\norm{z}_2 \leq \sqrt{n}\norm{z}_\infty<span class="sc">\}\\</span></span>
<span id="cb6-2117"><a href="#cb6-2117"></a>&amp; \leq \max_{x \neq 0} \frac{\sqrt{m}\norm{Ax}_\infty}{\norm{x}_\infty} &amp;\{\norm{z}_\infty \leq \norm{z}_2<span class="sc">\}\\</span></span>
<span id="cb6-2118"><a href="#cb6-2118"></a>&amp;= \sqrt{m} \norm{A}_\infty</span>
<span id="cb6-2119"><a href="#cb6-2119"></a>\end{align*}</span>
<span id="cb6-2120"><a href="#cb6-2120"></a>$$</span>
<span id="cb6-2121"><a href="#cb6-2121"></a></span>
<span id="cb6-2122"><a href="#cb6-2122"></a>Moreover, consider the matrix</span>
<span id="cb6-2123"><a href="#cb6-2123"></a></span>
<span id="cb6-2124"><a href="#cb6-2124"></a>$$</span>
<span id="cb6-2125"><a href="#cb6-2125"></a>A = \begin{bmatrix}</span>
<span id="cb6-2126"><a href="#cb6-2126"></a>1 <span class="sc">\\</span></span>
<span id="cb6-2127"><a href="#cb6-2127"></a>1 <span class="sc">\\</span></span>
<span id="cb6-2128"><a href="#cb6-2128"></a>\vdots <span class="sc">\\</span></span>
<span id="cb6-2129"><a href="#cb6-2129"></a>1</span>
<span id="cb6-2130"><a href="#cb6-2130"></a>\end{bmatrix}</span>
<span id="cb6-2131"><a href="#cb6-2131"></a>$$</span>
<span id="cb6-2132"><a href="#cb6-2132"></a></span>
<span id="cb6-2133"><a href="#cb6-2133"></a>We have $\norm{A}_2 = \sqrt{m}$, $\norm{A}_\infty = 1$, so $\norm{A}_2 = \sqrt{m}\norm{A}_\infty$. Hence, the inequality is tight.</span>
<span id="cb6-2134"><a href="#cb6-2134"></a></span>
<span id="cb6-2135"><a href="#cb6-2135"></a>*Claim*. Our claim is that $\norm{A}_\infty \leq n \norm{A}_1$. </span>
<span id="cb6-2136"><a href="#cb6-2136"></a></span>
<span id="cb6-2137"><a href="#cb6-2137"></a>*Solution.*</span>
<span id="cb6-2138"><a href="#cb6-2138"></a></span>
<span id="cb6-2139"><a href="#cb6-2139"></a>We have:</span>
<span id="cb6-2140"><a href="#cb6-2140"></a></span>
<span id="cb6-2141"><a href="#cb6-2141"></a>$$</span>
<span id="cb6-2142"><a href="#cb6-2142"></a>\begin{align*}</span>
<span id="cb6-2143"><a href="#cb6-2143"></a>\norm{A}_\infty &amp;= \max_{x \neq 0} \frac{\norm{Ax}_\infty}{\norm{x}_\infty} &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2144"><a href="#cb6-2144"></a>&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_1}{\norm{x}_\infty} &amp; \{\norm{x}_\infty \leq \norm{x}_1<span class="sc">\}\\</span></span>
<span id="cb6-2145"><a href="#cb6-2145"></a>&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_1}{\frac{1}{n}\norm{x}_1} &amp; \{\norm{x}_1 \leq n \norm{x}_\infty<span class="sc">\}\\</span></span>
<span id="cb6-2146"><a href="#cb6-2146"></a>&amp;= n \norm{A}_1</span>
<span id="cb6-2147"><a href="#cb6-2147"></a>\end{align*}</span>
<span id="cb6-2148"><a href="#cb6-2148"></a>$$</span>
<span id="cb6-2149"><a href="#cb6-2149"></a></span>
<span id="cb6-2150"><a href="#cb6-2150"></a>Moreover, let $A = <span class="co">[</span><span class="ot">1 | 1 | \ldots | 1</span><span class="co">]</span>$. Then, $\norm{A}_\infty = n$ and $\norm{A}_1 = 1$, so $\norm{A}_\infty = n \norm{A}_1$. Hence, the inequality is tight.</span>
<span id="cb6-2151"><a href="#cb6-2151"></a></span>
<span id="cb6-2152"><a href="#cb6-2152"></a>*Claim.* Our claim is that $\norm{A}_\infty \leq \sqrt{n} \norm{A}_2$. </span>
<span id="cb6-2153"><a href="#cb6-2153"></a></span>
<span id="cb6-2154"><a href="#cb6-2154"></a>*Solution.*</span>
<span id="cb6-2155"><a href="#cb6-2155"></a></span>
<span id="cb6-2156"><a href="#cb6-2156"></a>We have:</span>
<span id="cb6-2157"><a href="#cb6-2157"></a></span>
<span id="cb6-2158"><a href="#cb6-2158"></a>$$</span>
<span id="cb6-2159"><a href="#cb6-2159"></a>\begin{align*}</span>
<span id="cb6-2160"><a href="#cb6-2160"></a>\norm{A}_\infty &amp;= \max_{x \neq 0} \frac{\norm{Ax}_\infty}{\norm{x}_\infty} &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2161"><a href="#cb6-2161"></a>&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_\infty} &amp; \{\norm{x}_\infty \leq \norm{x}_2<span class="sc">\}\\</span></span>
<span id="cb6-2162"><a href="#cb6-2162"></a>&amp;\leq  \max_{x \neq 0} \frac{\norm{Ax}_2}{\frac{1}{\sqrt{n}}\norm{x}_2} &amp; \{\norm{x}_2 \leq \sqrt{n} \norm{x}_\infty<span class="sc">\}\\</span></span>
<span id="cb6-2163"><a href="#cb6-2163"></a>&amp;= \sqrt{n} \norm{A}_2</span>
<span id="cb6-2164"><a href="#cb6-2164"></a>\end{align*}</span>
<span id="cb6-2165"><a href="#cb6-2165"></a>$$</span>
<span id="cb6-2166"><a href="#cb6-2166"></a></span>
<span id="cb6-2167"><a href="#cb6-2167"></a>Moreover, let $A = <span class="co">[</span><span class="ot">1|1|\ldots|1</span><span class="co">]</span>$</span>
<span id="cb6-2168"><a href="#cb6-2168"></a></span>
<span id="cb6-2169"><a href="#cb6-2169"></a>Then, $\norm{A}_\infty = n$, $\norm{A}_2 = \sqrt{n}$ and $\norm{A}_\infty = \sqrt{n} \norm{A}_2$. So, the bound is tight.</span>
<span id="cb6-2170"><a href="#cb6-2170"></a></span>
<span id="cb6-2171"><a href="#cb6-2171"></a>*Claim*. Our claim is that $\norm{A}_\infty \leq \sqrt{n}\norm{A}_F$.</span>
<span id="cb6-2172"><a href="#cb6-2172"></a></span>
<span id="cb6-2173"><a href="#cb6-2173"></a>*Solution.* This is true since $\norm{A}_\infty \leq \sqrt{n}\norm{A}_2$ and $\norm{A}_2 \leq \norm{A}_F$.</span>
<span id="cb6-2174"><a href="#cb6-2174"></a></span>
<span id="cb6-2175"><a href="#cb6-2175"></a>Let $A = <span class="co">[</span><span class="ot">1 | 1 | \ldots | 1</span><span class="co">]</span>$. Then, $\norm{A}_\infty = n$ and $\norm{A}_F = \sqrt{n}$. So, $\norm{A}_\infty = \sqrt{n}\norm{A}_F$. The bound is tight.</span>
<span id="cb6-2176"><a href="#cb6-2176"></a></span>
<span id="cb6-2177"><a href="#cb6-2177"></a>*Claim*. Our claim is that $\norm{A}_F \leq \sqrt{n} \norm{A}_1$.</span>
<span id="cb6-2178"><a href="#cb6-2178"></a></span>
<span id="cb6-2179"><a href="#cb6-2179"></a>*Solution.*</span>
<span id="cb6-2180"><a href="#cb6-2180"></a></span>
<span id="cb6-2181"><a href="#cb6-2181"></a>We have:</span>
<span id="cb6-2182"><a href="#cb6-2182"></a></span>
<span id="cb6-2183"><a href="#cb6-2183"></a>$$</span>
<span id="cb6-2184"><a href="#cb6-2184"></a>\begin{align}</span>
<span id="cb6-2185"><a href="#cb6-2185"></a>\norm{A}_F^2 &amp;= \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}|^2 &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2186"><a href="#cb6-2186"></a>&amp;= \sum_{j=0}^{n-1} \sum_{i=0}^{m-1} |\alpha_{i,j}|^2 <span class="sc">\\</span></span>
<span id="cb6-2187"><a href="#cb6-2187"></a>&amp;\leq \sum_{j=0}^{n-1} \left(\sum_{i=0}^{m-1} |\alpha_{i,j}| \right)^2 <span class="sc">\\</span></span>
<span id="cb6-2188"><a href="#cb6-2188"></a>&amp;= \sum_{j=0}^{n-1} \norm{a_j}_1^2 <span class="sc">\\</span></span>
<span id="cb6-2189"><a href="#cb6-2189"></a>&amp;\leq \sum_{j=0}^{n-1} \max_{j=0}^{n-1} \norm{a_j}_1^2 <span class="sc">\\</span></span>
<span id="cb6-2190"><a href="#cb6-2190"></a>&amp;= n \norm{A}_1^2</span>
<span id="cb6-2191"><a href="#cb6-2191"></a>\end{align}</span>
<span id="cb6-2192"><a href="#cb6-2192"></a>$$</span>
<span id="cb6-2193"><a href="#cb6-2193"></a></span>
<span id="cb6-2194"><a href="#cb6-2194"></a>Consequently, $\norm{A}_F \leq \sqrt{n}\norm{A}_1$. Let $A = <span class="co">[</span><span class="ot">1 |1 | \ldots| 1</span><span class="co">]</span>$. Then, $\norm{A}_F = \sqrt{n}$ and $\norm{A}_1 = 1$, so $\norm{A}_F = \sqrt{n}\norm{A}_1$. Hence, the bound is tight.</span>
<span id="cb6-2195"><a href="#cb6-2195"></a></span>
<span id="cb6-2196"><a href="#cb6-2196"></a></span>
<span id="cb6-2197"><a href="#cb6-2197"></a>*Claim*. Our claim is that $\norm{A}_F \leq \sqrt{m} \norm{A}_\infty$.</span>
<span id="cb6-2198"><a href="#cb6-2198"></a></span>
<span id="cb6-2199"><a href="#cb6-2199"></a>*Solution.*</span>
<span id="cb6-2200"><a href="#cb6-2200"></a></span>
<span id="cb6-2201"><a href="#cb6-2201"></a>We have:</span>
<span id="cb6-2202"><a href="#cb6-2202"></a></span>
<span id="cb6-2203"><a href="#cb6-2203"></a>$$</span>
<span id="cb6-2204"><a href="#cb6-2204"></a>\begin{align}</span>
<span id="cb6-2205"><a href="#cb6-2205"></a>\norm{A}_F^2 &amp;= \sum_{i=0}^{m-1} \sum_{j=0}^{n-1} |\alpha_{i,j}|^2 &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2206"><a href="#cb6-2206"></a>&amp;\leq \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1} |\alpha_{i,j}| \right)^2 <span class="sc">\\</span></span>
<span id="cb6-2207"><a href="#cb6-2207"></a>&amp;= \sum_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1^2 <span class="sc">\\</span></span>
<span id="cb6-2208"><a href="#cb6-2208"></a>&amp;\leq \sum_{j=0}^{n-1} \max_{i=0}^{m-1} \norm{\tilde{a}_i^T}_1^2 <span class="sc">\\</span></span>
<span id="cb6-2209"><a href="#cb6-2209"></a>&amp;= m \norm{A}_\infty^2</span>
<span id="cb6-2210"><a href="#cb6-2210"></a>\end{align}</span>
<span id="cb6-2211"><a href="#cb6-2211"></a>$$</span>
<span id="cb6-2212"><a href="#cb6-2212"></a></span>
<span id="cb6-2213"><a href="#cb6-2213"></a>Consequently, $\norm{A}_F \leq \sqrt{m}\norm{A}_\infty$. Let $A = [1, 1, \ldots, 1]^T$. Then, $\norm{A}_F = \sqrt{m}$ and $\norm{A}_\infty = 1$, so $\norm{A}_F = \sqrt{m}\norm{A}_1$. Hence, the bound is tight.</span>
<span id="cb6-2214"><a href="#cb6-2214"></a></span>
<span id="cb6-2215"><a href="#cb6-2215"></a><span class="fu">### Sub-multiplicative norms</span></span>
<span id="cb6-2216"><a href="#cb6-2216"></a></span>
<span id="cb6-2217"><a href="#cb6-2217"></a>There are a number of properties that we would like a matrix norm to have(but not all matrix norms do). Given a matrix norm $\norm{\cdot} : \C^{m \times n} \to \R$, we may ask the following question. Do there exist vector norms $\norm{\cdot}_\mu : C^m \to \R$ and $\norm{\cdot}:\C^n \to R$, such that the matrix norm is an upper bound on how much the non-zero vector $x$ is stretched? That is, the following inequality is satisfied:</span>
<span id="cb6-2218"><a href="#cb6-2218"></a></span>
<span id="cb6-2219"><a href="#cb6-2219"></a>$$</span>
<span id="cb6-2220"><a href="#cb6-2220"></a>\frac{\norm{Ax}_\mu}{\norm{x}_\nu} \leq \norm{A}</span>
<span id="cb6-2221"><a href="#cb6-2221"></a>$$</span>
<span id="cb6-2222"><a href="#cb6-2222"></a></span>
<span id="cb6-2223"><a href="#cb6-2223"></a>or equivalently</span>
<span id="cb6-2224"><a href="#cb6-2224"></a></span>
<span id="cb6-2225"><a href="#cb6-2225"></a>$$</span>
<span id="cb6-2226"><a href="#cb6-2226"></a>\norm{Ax}_\mu \leq \norm{A} \norm{x}_\nu</span>
<span id="cb6-2227"><a href="#cb6-2227"></a>$$</span>
<span id="cb6-2228"><a href="#cb6-2228"></a></span>
<span id="cb6-2229"><a href="#cb6-2229"></a>where this second formulation has the benefit that it also holds if $x = 0$. </span>
<span id="cb6-2230"><a href="#cb6-2230"></a></span>
<span id="cb6-2231"><a href="#cb6-2231"></a>::: {#def-subordinate-matrix-norm}</span>
<span id="cb6-2232"><a href="#cb6-2232"></a></span>
<span id="cb6-2233"><a href="#cb6-2233"></a><span class="fu">### Subordinate matrix norm</span></span>
<span id="cb6-2234"><a href="#cb6-2234"></a></span>
<span id="cb6-2235"><a href="#cb6-2235"></a>A matrix norm $\norm{\cdot}:\C^{m \times n} \to \R$ is said to be subordinate to vector norms $\norm{\cdot}_\mu :\C^m \to \R$ and $\norm{\cdot}_\nu : \C^n \to \R$, if for all, $x \in \C^n$, </span>
<span id="cb6-2236"><a href="#cb6-2236"></a></span>
<span id="cb6-2237"><a href="#cb6-2237"></a>$$</span>
<span id="cb6-2238"><a href="#cb6-2238"></a>\norm{Ax}_\mu \leq \norm{A} \norm{x}_\nu</span>
<span id="cb6-2239"><a href="#cb6-2239"></a>$$</span>
<span id="cb6-2240"><a href="#cb6-2240"></a></span>
<span id="cb6-2241"><a href="#cb6-2241"></a>If $\norm{\cdot}_\mu$ and $\norm{\cdot}_\nu$ are the same norm (but perhaps for different $m$ and $n$), then $\norm{\cdot}$ is said to be subordinate to the given vector norm.</span>
<span id="cb6-2242"><a href="#cb6-2242"></a>:::</span>
<span id="cb6-2243"><a href="#cb6-2243"></a></span>
<span id="cb6-2244"><a href="#cb6-2244"></a>:::{#exr-matrix-2-norm-is-subordinate-to-vector-2-norm}</span>
<span id="cb6-2245"><a href="#cb6-2245"></a>Prove that the matrix $2$-norm is subordinate to the vector $2$-norm.</span>
<span id="cb6-2246"><a href="#cb6-2246"></a>:::</span>
<span id="cb6-2247"><a href="#cb6-2247"></a></span>
<span id="cb6-2248"><a href="#cb6-2248"></a>*Proof.*</span>
<span id="cb6-2249"><a href="#cb6-2249"></a></span>
<span id="cb6-2250"><a href="#cb6-2250"></a>Let $A \in C^{m \times n}$ and let $x \in \C^n$. Assume that $x \neq 0$, for if $x = 0$, then the inequality $\norm{Ax}_2 \leq \norm{A}_2 \norm{x}_2$ is vacuously true. </span>
<span id="cb6-2251"><a href="#cb6-2251"></a></span>
<span id="cb6-2252"><a href="#cb6-2252"></a>We have:</span>
<span id="cb6-2253"><a href="#cb6-2253"></a></span>
<span id="cb6-2254"><a href="#cb6-2254"></a>$$</span>
<span id="cb6-2255"><a href="#cb6-2255"></a>\begin{align}</span>
<span id="cb6-2256"><a href="#cb6-2256"></a>\norm{Ax}_2&amp;= \left(\frac{\norm{Ax}_2}{\norm{x}_2}\right) \norm{x}_2 &amp; <span class="sc">\{</span>x \neq 0<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb6-2257"><a href="#cb6-2257"></a>&amp;\leq \left(\max_{y \neq 0} \frac{\norm{Ay}_2}{\norm{y}_2}\right)\norm{x}_2<span class="sc">\\</span></span>
<span id="cb6-2258"><a href="#cb6-2258"></a>&amp;= \norm{A}_2 \norm{x}_2</span>
<span id="cb6-2259"><a href="#cb6-2259"></a>\end{align}</span>
<span id="cb6-2260"><a href="#cb6-2260"></a>$$</span>
<span id="cb6-2261"><a href="#cb6-2261"></a></span>
<span id="cb6-2262"><a href="#cb6-2262"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-2263"><a href="#cb6-2263"></a></span>
<span id="cb6-2264"><a href="#cb6-2264"></a>:::{#exr-frobenius-norm-is-subordinate-to-vector-2-norm}</span>
<span id="cb6-2265"><a href="#cb6-2265"></a>Prove that the Frobenius norm is subordinate to the vector $2$-norm.</span>
<span id="cb6-2266"><a href="#cb6-2266"></a>:::</span>
<span id="cb6-2267"><a href="#cb6-2267"></a></span>
<span id="cb6-2268"><a href="#cb6-2268"></a>*Proof.*</span>
<span id="cb6-2269"><a href="#cb6-2269"></a></span>
<span id="cb6-2270"><a href="#cb6-2270"></a>We are interested to prove the claim that, $(\forall A \in \C^{m\times n})(\forall x \in \C^n)$: </span>
<span id="cb6-2271"><a href="#cb6-2271"></a></span>
<span id="cb6-2272"><a href="#cb6-2272"></a>$$ \norm{Ax}_2 \leq \norm{A}_F \norm{x}_2 $$</span>
<span id="cb6-2273"><a href="#cb6-2273"></a></span>
<span id="cb6-2274"><a href="#cb6-2274"></a>Again, without loss of generality, we have:</span>
<span id="cb6-2275"><a href="#cb6-2275"></a></span>
<span id="cb6-2276"><a href="#cb6-2276"></a>$$</span>
<span id="cb6-2277"><a href="#cb6-2277"></a>\begin{align}</span>
<span id="cb6-2278"><a href="#cb6-2278"></a>\norm{Ax}_2^2 &amp;= \norm{</span>
<span id="cb6-2279"><a href="#cb6-2279"></a>    \begin{bmatrix}</span>
<span id="cb6-2280"><a href="#cb6-2280"></a>        \sum_{j=0}^{n-1}\alpha_{0,j} x_j <span class="sc">\\</span></span>
<span id="cb6-2281"><a href="#cb6-2281"></a>        \sum_{j=0}^{n-1}\alpha_{1,j} x_j <span class="sc">\\</span></span>
<span id="cb6-2282"><a href="#cb6-2282"></a>        \vdots</span>
<span id="cb6-2283"><a href="#cb6-2283"></a>        \sum_{j=0}^{n-1}\alpha_{m-1,j} x_j </span>
<span id="cb6-2284"><a href="#cb6-2284"></a>    \end{bmatrix}</span>
<span id="cb6-2285"><a href="#cb6-2285"></a>}_2^2 &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2286"><a href="#cb6-2286"></a>&amp;= \sum_{i=0}^{m-1} \Biggl| \sum_{j=0}^{n-1}\alpha_{i,j} x_j \Biggr|^2<span class="sc">\\</span></span>
<span id="cb6-2287"><a href="#cb6-2287"></a>&amp;= \sum_{i=0}^{m-1} \left(\sum_{j=0}^{n-1}|\alpha_{i,j} x_j| \right)^2 &amp; <span class="sc">\{</span>\text{Triangle Inequality}<span class="sc">\}\\</span></span>
<span id="cb6-2288"><a href="#cb6-2288"></a>&amp;\leq \sum_{i=0}^{m-1} \left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}|\alpha_{i,j}|^2 \right)  \left(\sum_{j=0}^{n-1} |x_j|^2\right)\right</span><span class="co">]</span> &amp; <span class="sc">\{</span>\text{Cauchy-Schwarz}<span class="sc">\}\\</span></span>
<span id="cb6-2289"><a href="#cb6-2289"></a>&amp;= \left(\sum_{j=0}^{n-1} |x_j|^2\right) \left(\sum_{i=0}^{m-1} \sum_{j=0}^{n-1}|\alpha_{i,j}|^2 \right)   &amp; <span class="sc">\{</span>\text{Algebra}<span class="sc">\}\\</span></span>
<span id="cb6-2290"><a href="#cb6-2290"></a>&amp;= \norm{A}_F^2 \norm{x}_2^2</span>
<span id="cb6-2291"><a href="#cb6-2291"></a>\end{align}</span>
<span id="cb6-2292"><a href="#cb6-2292"></a>$$</span>
<span id="cb6-2293"><a href="#cb6-2293"></a></span>
<span id="cb6-2294"><a href="#cb6-2294"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-2295"><a href="#cb6-2295"></a></span>
<span id="cb6-2296"><a href="#cb6-2296"></a>:::{#thm-induced-matrix-norms-are-subordinate-norms}</span>
<span id="cb6-2297"><a href="#cb6-2297"></a>Induced matrix norms, $\norm{\cdot}_{\mu,\nu} : \C^{m \times n} \to \R$ are subordinate to the norms, $\norm{\cdot}_\mu$ and $\norm{\cdot}_\nu$ that induce them.</span>
<span id="cb6-2298"><a href="#cb6-2298"></a>:::</span>
<span id="cb6-2299"><a href="#cb6-2299"></a></span>
<span id="cb6-2300"><a href="#cb6-2300"></a>*Proof*.</span>
<span id="cb6-2301"><a href="#cb6-2301"></a></span>
<span id="cb6-2302"><a href="#cb6-2302"></a>Without loss of generality, assume that $x \neq 0$, otherwise the proposition is vacuously true.</span>
<span id="cb6-2303"><a href="#cb6-2303"></a></span>
<span id="cb6-2304"><a href="#cb6-2304"></a>We have:</span>
<span id="cb6-2305"><a href="#cb6-2305"></a></span>
<span id="cb6-2306"><a href="#cb6-2306"></a>$$</span>
<span id="cb6-2307"><a href="#cb6-2307"></a>\begin{align}</span>
<span id="cb6-2308"><a href="#cb6-2308"></a>\norm{Ax}_\mu &amp;= \frac{\norm{Ax}_\mu}{\norm{x}_\nu} \norm{x}_\nu <span class="sc">\\</span></span>
<span id="cb6-2309"><a href="#cb6-2309"></a>&amp;\leq \left(\max_{x \neq 0} \frac{\norm{Ax}_\mu}{\norm{x}_\nu} \right) \norm{x}_\nu <span class="sc">\\</span></span>
<span id="cb6-2310"><a href="#cb6-2310"></a>&amp;= \left(\max_{y \neq 0} \frac{\norm{Ay}_\mu}{\norm{y}_\nu} \right) \norm{x}_\nu <span class="sc">\\</span></span>
<span id="cb6-2311"><a href="#cb6-2311"></a>&amp;= \norm{A} \norm{x}_\nu</span>
<span id="cb6-2312"><a href="#cb6-2312"></a>\end{align}</span>
<span id="cb6-2313"><a href="#cb6-2313"></a>$$</span>
<span id="cb6-2314"><a href="#cb6-2314"></a></span>
<span id="cb6-2315"><a href="#cb6-2315"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-2316"><a href="#cb6-2316"></a></span>
<span id="cb6-2317"><a href="#cb6-2317"></a>:::{#cor-matrix-p-norms-are-subordinate}</span>
<span id="cb6-2318"><a href="#cb6-2318"></a></span>
<span id="cb6-2319"><a href="#cb6-2319"></a>Any matrix $p$-norm is subordinate to the corresponding vector norm.</span>
<span id="cb6-2320"><a href="#cb6-2320"></a>:::</span>
<span id="cb6-2321"><a href="#cb6-2321"></a></span>
<span id="cb6-2322"><a href="#cb6-2322"></a>*Proof.*</span>
<span id="cb6-2323"><a href="#cb6-2323"></a></span>
<span id="cb6-2324"><a href="#cb6-2324"></a>Without the loss of generality, assume that $x \neq 0$. If $x = 0$, the proposition is vacuously true.</span>
<span id="cb6-2325"><a href="#cb6-2325"></a></span>
<span id="cb6-2326"><a href="#cb6-2326"></a>We have:</span>
<span id="cb6-2327"><a href="#cb6-2327"></a></span>
<span id="cb6-2328"><a href="#cb6-2328"></a>$$</span>
<span id="cb6-2329"><a href="#cb6-2329"></a>\begin{align}</span>
<span id="cb6-2330"><a href="#cb6-2330"></a>\norm{Ax}_p &amp;= \left(\frac{\norm{Ax}_p}{\norm{x}_p} \right) \norm{x}_p &amp; <span class="sc">\{</span> x \neq 0<span class="sc">\}\\</span></span>
<span id="cb6-2331"><a href="#cb6-2331"></a>&amp;\leq  \left(\max_{x \neq 0} \frac{\norm{Ax}_p}{\norm{x}_p} \right) \norm{x}_p<span class="sc">\\</span></span>
<span id="cb6-2332"><a href="#cb6-2332"></a>&amp;=  \left(\max_{y \neq 0}\frac{\norm{Ay}_p}{\norm{y}_p} \right) \norm{x}_p<span class="sc">\\</span></span>
<span id="cb6-2333"><a href="#cb6-2333"></a>&amp;= \norm{A}_p \norm{x}_p </span>
<span id="cb6-2334"><a href="#cb6-2334"></a>\end{align}</span>
<span id="cb6-2335"><a href="#cb6-2335"></a>$$</span>
<span id="cb6-2336"><a href="#cb6-2336"></a></span>
<span id="cb6-2337"><a href="#cb6-2337"></a>This closes the proof. $\blacksquare$</span>
<span id="cb6-2338"><a href="#cb6-2338"></a></span>
<span id="cb6-2339"><a href="#cb6-2339"></a>Another desirable property that not all norms have is that:</span>
<span id="cb6-2340"><a href="#cb6-2340"></a></span>
<span id="cb6-2341"><a href="#cb6-2341"></a>$$</span>
<span id="cb6-2342"><a href="#cb6-2342"></a>\norm{AB} \leq \norm{A} \norm{B}</span>
<span id="cb6-2343"><a href="#cb6-2343"></a>$$</span>
<span id="cb6-2344"><a href="#cb6-2344"></a></span>
<span id="cb6-2345"><a href="#cb6-2345"></a>This requires the given norm to be defined for all matrix sizes.</span>
<span id="cb6-2346"><a href="#cb6-2346"></a></span>
<span id="cb6-2347"><a href="#cb6-2347"></a>:::{#def-consistent-matrix-norm}</span>
<span id="cb6-2348"><a href="#cb6-2348"></a></span>
<span id="cb6-2349"><a href="#cb6-2349"></a><span class="fu">### Consistent matrix norm</span></span>
<span id="cb6-2350"><a href="#cb6-2350"></a></span>
<span id="cb6-2351"><a href="#cb6-2351"></a>A matrix norm $\norm{\cdot} : \C^{m \times n} \to \R$ is said to be consistent matrix norm if it is defined for all $m$ and $n$, using the same formula for all $m$ and $n$.</span>
<span id="cb6-2352"><a href="#cb6-2352"></a>:::</span>
<span id="cb6-2353"><a href="#cb6-2353"></a></span>
<span id="cb6-2354"><a href="#cb6-2354"></a>:::{#def-submulticative-matrix-norm}</span>
<span id="cb6-2355"><a href="#cb6-2355"></a></span>
<span id="cb6-2356"><a href="#cb6-2356"></a><span class="fu">### Submultiplicative matrix norm</span></span>
<span id="cb6-2357"><a href="#cb6-2357"></a></span>
<span id="cb6-2358"><a href="#cb6-2358"></a>A consistent matrix norm $\norm{\cdot} : \C^{m \times n} \to \R$ is said to be submultiplicative if it satisfies:</span>
<span id="cb6-2359"><a href="#cb6-2359"></a></span>
<span id="cb6-2360"><a href="#cb6-2360"></a>$$</span>
<span id="cb6-2361"><a href="#cb6-2361"></a>\norm{AB} \leq \norm{A} \norm{B}</span>
<span id="cb6-2362"><a href="#cb6-2362"></a>$$</span>
<span id="cb6-2363"><a href="#cb6-2363"></a>:::</span>
<span id="cb6-2364"><a href="#cb6-2364"></a></span>
<span id="cb6-2365"><a href="#cb6-2365"></a>:::{#thm-induced-matrix-norms-are-submultiplicative}</span>
<span id="cb6-2366"><a href="#cb6-2366"></a></span>
<span id="cb6-2367"><a href="#cb6-2367"></a>Let $\norm{\cdot} : \C^n \to \R$ be a vector norm defined for all $n$. Define the corresponding induced matrix norm as:</span>
<span id="cb6-2368"><a href="#cb6-2368"></a></span>
<span id="cb6-2369"><a href="#cb6-2369"></a>$$</span>
<span id="cb6-2370"><a href="#cb6-2370"></a>\norm{A} = \max_{x \neq 0} \frac{\norm{Ax}}{\norm{x}} = \max_{\norm{x} = 1} \norm{Ax}</span>
<span id="cb6-2371"><a href="#cb6-2371"></a>$$</span>
<span id="cb6-2372"><a href="#cb6-2372"></a></span>
<span id="cb6-2373"><a href="#cb6-2373"></a>Then, for any $A \in \C^{m \times k}$ and $B^{k \times n}$, the inequality $\norm{AB} \leq \norm{A} \norm{B}$ holds. </span>
<span id="cb6-2374"><a href="#cb6-2374"></a>:::</span>
<span id="cb6-2375"><a href="#cb6-2375"></a></span>
<span id="cb6-2376"><a href="#cb6-2376"></a>In other words, induced matrix norms are submultiplicative.</span>
<span id="cb6-2377"><a href="#cb6-2377"></a></span>
<span id="cb6-2378"><a href="#cb6-2378"></a>*Proof.*</span>
<span id="cb6-2379"><a href="#cb6-2379"></a></span>
<span id="cb6-2380"><a href="#cb6-2380"></a>We have:</span>
<span id="cb6-2381"><a href="#cb6-2381"></a></span>
<span id="cb6-2382"><a href="#cb6-2382"></a>$$</span>
<span id="cb6-2383"><a href="#cb6-2383"></a>\begin{align}</span>
<span id="cb6-2384"><a href="#cb6-2384"></a>\norm{AB} &amp;= \max_{\norm{x}=1} \norm{ABx} &amp; <span class="sc">\{</span>\text{Definition}<span class="sc">\}\\</span></span>
<span id="cb6-2385"><a href="#cb6-2385"></a>&amp;= \max_{\norm{x}=1} \norm{A(Bx)} &amp; <span class="sc">\{</span>\text{Associativity}<span class="sc">\}\\</span></span>
<span id="cb6-2386"><a href="#cb6-2386"></a>&amp;\leq \max_{\norm{x}=1} \norm{A} \norm{Bx} &amp; <span class="sc">\{</span>\text{Subordinate property}<span class="sc">\}\\</span></span>
<span id="cb6-2387"><a href="#cb6-2387"></a>&amp;\leq \max_{\norm{x}=1} \norm{A} \norm{B} \norm{x} &amp; <span class="sc">\{</span>\text{Subordinate property}<span class="sc">\}\\</span></span>
<span id="cb6-2388"><a href="#cb6-2388"></a>&amp;= \norm{A} \norm{B} &amp; <span class="sc">\{</span>\norm{x}=1<span class="sc">\}</span></span>
<span id="cb6-2389"><a href="#cb6-2389"></a>\end{align}</span>
<span id="cb6-2390"><a href="#cb6-2390"></a>$$</span>
<span id="cb6-2391"><a href="#cb6-2391"></a></span>
<span id="cb6-2392"><a href="#cb6-2392"></a>:::{#exr-induced-matrix-norm-is-submultiplicative-1}</span>
<span id="cb6-2393"><a href="#cb6-2393"></a></span>
<span id="cb6-2394"><a href="#cb6-2394"></a>Show that $\norm{Ax}_\mu \leq \norm{A}_{\mu,\nu}\norm{x}_\nu$ for all $x$.</span>
<span id="cb6-2395"><a href="#cb6-2395"></a>:::</span>
<span id="cb6-2396"><a href="#cb6-2396"></a></span>
<span id="cb6-2397"><a href="#cb6-2397"></a>*Solution.*</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>
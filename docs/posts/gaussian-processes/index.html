<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2025-10-19">

<title>quantdev.blog - Gaussian Processes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sell_side_quant_critical_path.html" rel="" target="">
 <span class="menu-text">Sell-side Quant</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../roadmap.html" rel="" target="">
 <span class="menu-text">C++ Roadmap</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/quasar-chunawala" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://linkedin.com/in/quasar-chunawala" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Gaussian Processes</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Stochastic Calculus</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 19, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gaussian-processes." id="toc-gaussian-processes." class="nav-link active" data-scroll-target="#gaussian-processes.">Gaussian Processes.</a>
  <ul class="collapse">
  <li><a href="#random-vectors." id="toc-random-vectors." class="nav-link" data-scroll-target="#random-vectors.">Random Vectors.</a></li>
  <li><a href="#basic-probabilistic-inequalities." id="toc-basic-probabilistic-inequalities." class="nav-link" data-scroll-target="#basic-probabilistic-inequalities.">Basic Probabilistic Inequalities.</a>
  <ul class="collapse">
  <li><a href="#jensens-inequality." id="toc-jensens-inequality." class="nav-link" data-scroll-target="#jensens-inequality.">Jensen’s inequality.</a></li>
  <li><a href="#jensens-inequality-for-random-variables." id="toc-jensens-inequality-for-random-variables." class="nav-link" data-scroll-target="#jensens-inequality-for-random-variables.">Jensen’s inequality for Random variables.</a></li>
  <li><a href="#youngs-inequality." id="toc-youngs-inequality." class="nav-link" data-scroll-target="#youngs-inequality.">Young’s Inequality.</a></li>
  <li><a href="#chebyshevs-inequality." id="toc-chebyshevs-inequality." class="nav-link" data-scroll-target="#chebyshevs-inequality.">Chebyshev’s inequality.</a></li>
  <li><a href="#holders-inequality." id="toc-holders-inequality." class="nav-link" data-scroll-target="#holders-inequality.">Holder’s inequality.</a></li>
  <li><a href="#minkowskis-inequality." id="toc-minkowskis-inequality." class="nav-link" data-scroll-target="#minkowskis-inequality.">Minkowski’s Inequality.</a></li>
  </ul></li>
  <li><a href="#a-quick-refresher-of-linear-algebra." id="toc-a-quick-refresher-of-linear-algebra." class="nav-link" data-scroll-target="#a-quick-refresher-of-linear-algebra.">A quick refresher of linear algebra.</a>
  <ul class="collapse">
  <li><a href="#inner-products." id="toc-inner-products." class="nav-link" data-scroll-target="#inner-products.">Inner Products.</a></li>
  <li><a href="#orthogonal-matrices." id="toc-orthogonal-matrices." class="nav-link" data-scroll-target="#orthogonal-matrices.">Orthogonal Matrices.</a></li>
  <li><a href="#quadratic-forms." id="toc-quadratic-forms." class="nav-link" data-scroll-target="#quadratic-forms.">Quadratic Forms.</a></li>
  <li><a href="#eigenthingies-and-diagonalizability." id="toc-eigenthingies-and-diagonalizability." class="nav-link" data-scroll-target="#eigenthingies-and-diagonalizability.">Eigenthingies and diagonalizability.</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="gaussian-processes." class="level1">
<h1>Gaussian Processes.</h1>
<section id="random-vectors." class="level2">
<h2 class="anchored" data-anchor-id="random-vectors.">Random Vectors.</h2>
<p>Consider a probability space <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. We can define several random variables on <span class="math inline">\(\Omega\)</span>. A <span class="math inline">\(n\)</span>-tuple of random variables on this space is called a random vector. For example, if <span class="math inline">\(X_{1},X_{2},\ldots,X_{n}\)</span> are random variables on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>, then the <span class="math inline">\(n\)</span>-tuple <span class="math inline">\((X_{1},X_{2},\ldots,X_{n})\)</span> is a random vector on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. The vector is said to be <span class="math inline">\(n\)</span>-dimensional because it contains <span class="math inline">\(n\)</span>-variables. We will sometimes denote a random vector by <span class="math inline">\(X\)</span>.</p>
<p>A good point of view is to think of a random vector <span class="math inline">\(X=(X_{1},\ldots,X_{n})\)</span> as a random variable (point) in <span class="math inline">\(\mathbf{R}^{n}\)</span>. In other words, for an outcome <span class="math inline">\(\omega\in\Omega\)</span>, <span class="math inline">\(X(\omega)\)</span> is a point sampled in <span class="math inline">\(\mathbf{R}^{n}\)</span>, where <span class="math inline">\(X_{j}(\omega)\)</span> represents the <span class="math inline">\(j\)</span>-th coordinate of the point. The distribution of <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(\mu_{X}\)</span> is the probability on <span class="math inline">\(\mathbf{R}^{n}\)</span>defined by the events related to the values of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\mathbb{P}\{X\in A\}=\mu_{X}(A)\quad\text{for a subset }A\text{ in }\mathbf{R}^{n}\]</span></p>
<p>In other words, <span class="math inline">\(\mathbb{P}(X\in A)=\mu_{X}(A)\)</span> is the probability that the random point <span class="math inline">\(X\)</span> falls in <span class="math inline">\(A\)</span>. The distribution of the vector <span class="math inline">\(X\)</span> is also called the joint distribution of <span class="math inline">\((X_{1},\ldots,X_{n})\)</span>.</p>
<div id="def-joint-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Joint Distribution) </strong></span>The <strong>joint distribution function</strong> of <span class="math inline">\(\mathbf{X}=(X,Y)\)</span> is the function <span class="math inline">\(F:\mathbf{R}^{2}\to[0,1]\)</span> given by:</p>
<p><span class="math display">\[F_{\mathbf{X}}(x,y)=\mathbb{P}(X\leq x,Y\leq y)\]</span></p>
</div>
<div id="def-joint-density-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Joint density) </strong></span>The joint <strong>PDF</strong> <span class="math inline">\(f_{\mathbf{X}}(x_{1},\ldots,x_{n})\)</span> of a random vector <span class="math inline">\(\mathbf{X}\)</span> is a function <span class="math inline">\(f_{\mathbf{X}}:\mathbf{R}^{n}\to\mathbf{R}\)</span> such that the probability that <span class="math inline">\(X\)</span> falls in a subset <span class="math inline">\(A\)</span> of <span class="math inline">\(\mathbf{R}^{n}\)</span> and is expressed as the multiple integral of <span class="math inline">\(f(x_{1},x_{2,}\ldots,x_{n})\)</span> over <span class="math inline">\(A\)</span>.</p>
<p><span class="math display">\[\mathbb{P}(X\in A)=\int_{A}f(x_{1},x_{2},\ldots,x_{n})dx_{1}dx_{2}\ldots dx_{n}\]</span></p>
</div>
<p>Note that: we must have that the integral of <span class="math inline">\(f\)</span> over the whole of <span class="math inline">\(\mathbf{R}^{n}\)</span> is <span class="math inline">\(1\)</span>.</p>
<p>If <span class="math inline">\(F\)</span> is differentiable at the point <span class="math inline">\((x,y)\)</span>, then we usually specify:</p>
<p><span class="math display">\[f(x,y)=\frac{\partial^{2}}{\partial x\partial y}F(x,y)\]</span></p>
<div id="thm-law-of-total-probability" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Law of total probability) </strong></span>Let <span class="math inline">\((X,Y)\)</span> be the random variables with joint density function <span class="math inline">\(f_{X,Y}(x,y)\)</span>. The marginal density function <span class="math inline">\(f_{X}(x)\)</span> and <span class="math inline">\(f_{Y}(y)\)</span> of the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> respectively is given by:</p>
<p><span class="math display">\[\begin{aligned}
f_{X}(x) &amp; =\int_{-\infty}^{+\infty}f_{(X,Y)}(x,y)dy\\ f_{Y}(y) &amp; =\int_{-\infty}^{+\infty}f_{(X,Y)}(x,y)dx\end{aligned}\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
F_{X}(x) &amp; =P(X\leq x)\\ &amp; =\int_{-\infty}^{x}\int_{y=-\infty}^{y=+\infty}f(x,y)dydx\end{aligned}\]</span></p>
<p>Differentiating both sides with respect to <span class="math inline">\(x\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
f_{X}(x) &amp; =\int_{y=-\infty}^{y=+\infty}f(x,y)dydx\end{aligned}\]</span>&nbsp;</p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="def-conditional-density-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Conditional density function) </strong></span>For continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with the joint density function <span class="math inline">\(f_{(X,Y)}\)</span>, the conditional density of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
f_{Y|X}(y|x) &amp; =\frac{f_{(X,Y)}(x,y)}{f_{X}(x)}\end{aligned}\]</span></p>
<p>for all <span class="math inline">\(x\)</span> with <span class="math inline">\(f_{X}(x)&gt;0\)</span>. This is considered as a function of <span class="math inline">\(y\)</span> for a fixed <span class="math inline">\(x\)</span>. As a convention, in order to make <span class="math inline">\(f_{Y|X}(y|x)\)</span> well-defined for all real <span class="math inline">\(x\)</span>, let <span class="math inline">\(f_{Y|X}(y|x)=0\)</span> for all <span class="math inline">\(x\)</span> with <span class="math inline">\(f_{X}(x)=0\)</span>.</p>
</div>
<p>We are essentially slicing the the joint density function of <span class="math inline">\(f_{(X,Y)}(x,y)\)</span> by a thin plane <span class="math inline">\(X=x\)</span>. How can we speak of conditioning on <span class="math inline">\(X=x\)</span> for <span class="math inline">\(X\)</span> being a continuous random variable, considering that this event has probability zero. Rigorously speaking, we are actually conditioning on the event that <span class="math inline">\(X\)</span> falls within a small interval containing <span class="math inline">\(x\)</span>, say <span class="math inline">\(X\in(x-\epsilon,x+\epsilon)\)</span> and then taking the limit as <span class="math inline">\(\epsilon\)</span> approaches zero from the right.</p>
<p>We can recover the joint PDF <span class="math inline">\(f_{(X,Y)}\)</span> if we have the conditional PDF <span class="math inline">\(f_{Y|X}\)</span> and the corresponding marginal <span class="math inline">\(f_{X}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
f_{(X,Y)}(x,y) &amp; =f_{Y|X}(y|x)\cdot f_{X}(x)
\end{aligned}
\]</span></p>
<div id="thm-bayes-rule-and-lotp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (Bayes rule and LOTP) </strong></span>Let <span class="math inline">\((X,Y)\)</span> be continuous random variables. We have the following continuous form of the Bayes rule:</p>
<p><span class="math display">\[f_{Y|X}(y|x)=\frac{f_{X|Y}(x|y)\cdot f_{Y}(y)}{f_{X}(x)}\]</span></p>
<p>And we have the following continuous form of the law of total probability:</p>
<p><span class="math display">\[\begin{aligned}
f_{X}(x) &amp; =\int_{y=-\infty}^{y=+\infty}f_{X|Y}(x|y)\cdot f_{Y}(y)dy\end{aligned}\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>By the definition of conditional PDFs, we have:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X|Y}(x|y)\cdot f_{Y}(y) &amp; =f_{(X,Y)}(x,y)=f_{Y|X}(y|x)\cdot f_{X}(x)\end{aligned}
\]</span></p>
<p>Dividing throughout by <span class="math inline">\(f_{X}(x)\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
f_{Y|X}(x) &amp; =\frac{f_{X|Y}(x|y)\cdot f_{Y}(y)}{f_{X}(x)}=\frac{f_{(X,Y)}(x,y)}{f_{X}(x)}\end{aligned}
\]</span>&nbsp;</p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
<div id="exm-sampling-uniformly" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 </strong></span>(Sampling uniformly in the unit disc). Consider the random vector <span class="math inline">\(\mathbf{X}=(X,Y)\)</span> corresponding to a random point chosen uniformly in the unit disc <span class="math inline">\(\{(x,y):x^{2}+y^{2}\leq1\}\)</span>. <span class="math inline">\(\mathbf{X}\)</span> is said to have uniform on the unit circle distribution. In this case the PDF is <span class="math inline">\(0\)</span> outside the disc and <span class="math inline">\(\frac{1}{\pi}\)</span> inside the disc:</p>
<p><span class="math display">\[\begin{aligned}
f(x,y) &amp; =\frac{1}{\pi}\quad\text{ if }x^{2}+y^{2}\leq1\end{aligned}\]</span></p>
<p>The random point <span class="math inline">\((X,Y)\)</span> has <span class="math inline">\(x\)</span>-coordinate <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> coordinate <span class="math inline">\(Y\)</span>. Each of these are random variables and their PDFs and CDFs can be computed. This is a valid PDF, because:</p>
<p><span class="math display">\[\begin{aligned}
\int\int_{D}f(x,y)dydx &amp; =\int_{-1}^{1}\int_{-\sqrt{1-x^{2}}}^{\sqrt{1-x^{2}}}\frac{1}{\pi}dydx\\ &amp; =\frac{1}{\pi}\int_{-1}^{1}\left[y\right]_{-\sqrt{1-x^{2}}}^{+\sqrt{1-x^{2}}}dx\\ &amp; =\frac{2}{\pi}\int_{-1}^{1}\sqrt{1-x^{2}}dx\end{aligned}\]</span></p>
<p>Substituting <span class="math inline">\(x=\sin\theta\)</span>, we have: <span class="math inline">\(dx=\cos\theta d\theta\)</span> and <span class="math inline">\(\sqrt{1-x^{2}}=\cos\theta\)</span>. The limits of integration are <span class="math inline">\(\theta=-\pi/2\)</span> to <span class="math inline">\(\theta=\pi/2\)</span>. Thus,</p>
<p><span class="math display">\[\begin{aligned}
\int\int_{D}f(x,y)dydx &amp; =\frac{2}{\pi}\int_{-\pi/2}^{\pi/2}\cos^{2}\theta d\theta\\ &amp; =\frac{1}{\pi}\int_{-\pi/2}^{\pi/2}(1+\cos2\theta)d\theta\\ &amp; =\frac{1}{\pi}\left[\theta+\frac{1}{2}\sin2\theta\right]_{-\pi/2}^{\pi/2}\\ &amp; =\frac{1}{\pi}\cdot\pi\\ &amp; =1\end{aligned}\]</span></p>
<p>The CDF of <span class="math inline">\(X\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
F_{X}(a) &amp; =\int_{-1}^{a}\int_{-\sqrt{1-x^{2}}}^{\sqrt{1-x^{2}}}\frac{1}{\pi}dydx\\ &amp; =\frac{2}{\pi}\int_{-1}^{a}\sqrt{1-x^{2}}dx\end{aligned}\]</span></p>
<p>I leave it in this integral form. The PDF of <span class="math inline">\(X\)</span> is obtained by differentiating the CDF, so it is:</p>
<p><span class="math display">\[f_{X}(x)=\frac{2}{\pi}\sqrt{1-x^{2}}\label{eq:marginal-pdf-of-X}\]</span></p>
</div>
<p>Let’s quickly plot the density of <span class="math inline">\(X\)</span> over the domain of the definition <span class="math inline">\(-1\leq x\leq1\)</span>.</p>
<p>::: center Figure. The PDF of the random variable <span class="math inline">\(X\)</span>. :::</p>
<p>Not suprisingly the distribution of the <span class="math inline">\(x\)</span>-coordinate is no longer uniform!</p>
<p>If <span class="math inline">\((X_{1},X_{2},\ldots,X_{n})\)</span> is a random vector, the distribution of a single coordinate, say <span class="math inline">\(X_{1}\)</span> is called the <em>marginal distribution</em>. In the example <a href="#Uniform-on-the-unit-circle-distribution" data-reference-type="ref" data-reference="Uniform-on-the-unit-circle-distribution">[Uniform-on-the-unit-circle-distribution]</a>, the marginal distribution of <span class="math inline">\(X\)</span> is determined by the PDF <a href="#eq:marginal-pdf-of-X" data-reference-type="ref" data-reference="eq:marginal-pdf-of-X">[eq:marginal-pdf-of-X]</a>.</p>
<p>Random variables <span class="math inline">\(X_{1},X_{2},\ldots,X_{n}\)</span> defined on the same probability space are said to be independent if for any intervals <span class="math inline">\(A_{1},A_{2},\ldots,A_{n}\)</span> in <span class="math inline">\(\mathbf{R}\)</span>, the probability factors:</p>
<p><span class="math display">\[\mathbb{P}(X_{1}\in A_{1},X_{2}\in A_{2},\ldots,X_{n}\in A_{n})=\mathbb{P}(X_{1}\in A_{1})\times\mathbb{P}(X_{2}\in A_{2})\times\ldots\times\mathbb{P}(X_{n}\in A_{n})\]</span> We say that the random variables are independent and identically distributed (IID) if they are independent and their marginal distributions are the same.</p>
<p>When the random vector <span class="math inline">\((X_{1},X_{2},\ldots,X_{n})\)</span> has a joint PDF <span class="math inline">\(f(x_{1},x_{2},\ldots,x_{n})\)</span>, the independence of random variables is equivalent to saying that the joint PDF is given by the product of the marginal PDFs:</p>
<p><span class="math display">\[f(x_{1},x_{2},\ldots,x_{n})=f_{1}(x_{1})\times f_{2}(x_{2})\times\ldots\times f_{n}(x_{n})\]</span></p>
</section>
<section id="basic-probabilistic-inequalities." class="level2">
<h2 class="anchored" data-anchor-id="basic-probabilistic-inequalities.">Basic Probabilistic Inequalities.</h2>
<p>Inequalities are extremely useful tools in the theoretical development of probability theory.</p>
<section id="jensens-inequality." class="level3">
<h3 class="anchored" data-anchor-id="jensens-inequality.">Jensen’s inequality.</h3>
<div id="thm-jensens-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 </strong></span>If <span class="math inline">\(g\)</span> is a convex function, and <span class="math inline">\(a&gt;0\)</span>, <span class="math inline">\(b&gt;0\)</span>, with <span class="math inline">\(p\in[0,1]\)</span>, it follows that:</p>
<p><span class="math display">\[g(pa+(1-p)b)\leq pg(a)+(1-p)g(b)\]</span></p>
</div>
<p><em>Proof.</em> This directly follows from the definition of convex functions.&nbsp;<span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="jensens-inequality-for-random-variables." class="level3">
<h3 class="anchored" data-anchor-id="jensens-inequality-for-random-variables.">Jensen’s inequality for Random variables.</h3>
<div id="thm-jensens-inequality-for-random-variables" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4 </strong></span>If <span class="math inline">\(g\)</span> is a convex function, then it follows that:</p>
<p><span class="math display">\[\mathbb{E}(g(X))\geq g(\mathbb{E}X)\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>Another way to express the idea, that a function is convex is to observe that the tangent line at an arbitrary point <span class="math inline">\((t,g(t))\)</span> always lies below the curve. Let <span class="math inline">\(y=a+bx\)</span> be the tangent to <span class="math inline">\(g\)</span> at the point <span class="math inline">\(t\)</span>. Then, it follows that:</p>
<p><span class="math display">\[\begin{aligned}
a+bt &amp; =g(t)\\ a+bx &amp; \leq g(x)\end{aligned}\]</span></p>
<p>for all <span class="math inline">\(x\)</span>.</p>
<p>Thus, it follows that, for any point <span class="math inline">\(t\)</span>, there exists <span class="math inline">\(b\)</span> such that:</p>
<p><span class="math display">\[\begin{aligned}
g(x)-g(t) &amp; \geq b(x-t)\end{aligned}\]</span></p>
<p>for all <span class="math inline">\(x\)</span>. Set <span class="math inline">\(t=\mathbb{E}X\)</span> and <span class="math inline">\(x=X\)</span>. Then,</p>
<p><span class="math display">\[\begin{aligned}
g(X)-g(\mathbb{E}X) &amp; \geq b(X-\mathbb{E}X)\end{aligned}\]</span></p>
<p>Taking expectations on both sides and simplifying:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left(g(X)\right)-g(\mathbb{E}X) &amp; \geq b(\mathbb{E}X-\mathbb{E}X)=0\\ \mathbb{E}g(X) &amp; \geq g(\mathbb{E}X)\end{aligned}\]</span>&nbsp;</p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="youngs-inequality." class="level3">
<h3 class="anchored" data-anchor-id="youngs-inequality.">Young’s Inequality.</h3>
<div id="thm-youngs-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 </strong></span>If <span class="math inline">\(a\geq0\)</span> and <strong><span class="math inline">\(b\geq0\)</span></strong> are non-negative real numbers and if <span class="math inline">\(p&gt;1\)</span> and <span class="math inline">\(q&gt;1\)</span> are real numbers such that <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>, then:</p>
<p><span class="math display">\[ab\leq\frac{a^{p}}{p}+\frac{b^{q}}{q}\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>Consider <span class="math inline">\(g(x)=\log x\)</span>. Being a concave function, Jensen’s inequality can be reversed. We have:</p>
<p><span class="math display">\[\begin{aligned}
g\left(\frac{1}{p}a^{p}+\frac{1}{q}b^{q}\right) &amp; \geq\frac{1}{p}g(a^{p})+\frac{1}{q}g(b^{q})\\ \log\left(\frac{1}{p}a^{p}+\frac{1}{q}b^{q}\right) &amp; \geq\frac{1}{p}\log(a^{p})+\frac{1}{q}\log(b^{q})\\ \log\left(\frac{1}{p}a^{p}+\frac{1}{q}b^{q}\right) &amp; \geq\frac{1}{p}\cdot p\log(a)+\frac{1}{q}\cdot q\log(b)\\ \log\left(\frac{1}{p}a^{p}+\frac{1}{q}b^{q}\right) &amp; \geq\log ab\end{aligned}\]</span></p>
<p>By the Monotonicity of the <span class="math inline">\(\log x\)</span> function, it follows that :</p>
<p><span class="math display">\[\begin{aligned}
ab &amp; \leq\frac{a^{p}}{p}+\frac{b^{q}}{q}\end{aligned}\]</span>&nbsp;</p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="chebyshevs-inequality." class="level3">
<h3 class="anchored" data-anchor-id="chebyshevs-inequality.">Chebyshev’s inequality.</h3>
<p>One of the simplest and very useful probabilistic inequalities is a tail bound by expectation: the so called Chebyshev’s inequality.</p>
<div id="thm-chebyshevs-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (Chebyshev’s inequality) </strong></span>If <span class="math inline">\(X\)</span> is a non-negative random variable, then for every <span class="math inline">\(t\geq0\)</span>:</p>
<p><span class="math display">\[\mathbb{P}(X\geq t)\leq\frac{1}{t}\mathbb{E}X\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
t\cdot\mathbf{1}_{\{X\geq t\}} &amp; \leq X\cdot\mathbf{1}_{\{X\geq t\}}\end{aligned}\]</span></p>
<p>By the monotonicity of expectations, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\mathbf{1}_{\{X\geq t\}} &amp; \leq\frac{1}{t}\mathbb{E}X\\ \implies\mathbb{P}\{X\geq t\} &amp; \leq\frac{1}{t}\mathbb{E}X\end{aligned}\]</span></p>
<p>This closes the proof.&nbsp;<span class="math inline">\(\blacksquare\)</span></p>
<p>There are several variants, easily deduced from Chebyshev’s inequality using monotonicity of several functions. For a non-negative random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(t&gt;0\)</span>, using the power function <span class="math inline">\(x^{p}\)</span>, <span class="math inline">\(p&gt;0\)</span>, we get:</p>
<p><span class="math display">\[\mathbb{P}(X\geq t)=\mathbb{P}(X^{p}\geq t^{p})\leq\frac{1}{t^{p}}\mathbb{E}X^{p}\]</span></p>
<p>For a real valued random variable <span class="math inline">\(X\)</span>, every <span class="math inline">\(t\in\mathbf{R}\)</span>, using the square function <span class="math inline">\(x^{2}\)</span> and variance, we have:</p>
<p><span class="math display">\[\mathbb{P}(|X-\mathbb{E}X|\geq t)\leq\frac{1}{t^{2}}\mathbb{E}|X-\mathbb{E}X|^{2}=\frac{1}{t^{2}}Var(X)\]</span></p>
<p>For a real-valued random variable <span class="math inline">\(X\)</span>, every <span class="math inline">\(t\in\mathbf{R}\)</span> and <span class="math inline">\(\lambda&gt;0\)</span>, using the exponential function <span class="math inline">\(e^{\lambda x}\)</span>(which is monotonic), we have:</p>
<p><span class="math display">\[\mathbb{P}(X\geq t)=\mathbb{P}(\lambda X\geq\lambda t)=\mathbb{P}(e^{\lambda X}\geq e^{\lambda t})\leq\frac{1}{e^{\lambda t}}\mathbb{E}e^{\lambda X}\]</span></p>
<p>Our next inequality, the so-called Holder’s inequality is a very effective inequality to factor out the expectation of a product.</p>
</section>
<section id="holders-inequality." class="level3">
<h3 class="anchored" data-anchor-id="holders-inequality.">Holder’s inequality.</h3>
<div id="thm-holders-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 </strong></span>Let <span class="math inline">\(p,q\geq1\)</span> be such that <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>, For random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}|XY| &amp; \leq\left(\mathbb{E}|X^{p}|\right)^{1/p}\left(\mathbb{E}|Y^{q}|\right)^{1/q}\end{aligned}\]</span></p>
</div>
<p><em>Proof.</em> From the Young’s inequality, for any <span class="math inline">\(a,b\in\mathbf{R}\)</span>, <span class="math inline">\(p,q\geq1\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
ab &amp; \leq\frac{a^{p}}{p}+\frac{b^{q}}{q}\end{aligned}\]</span></p>
<p>Setting <span class="math inline">\(a=\frac{|X|}{\left(\mathbb{E}|X^{p}|\right)^{1/p}}\)</span> and <span class="math inline">\(b=\frac{|Y|}{\left(\mathbb{E}|Y^{q}|\right)^{1/q}}\)</span>, we get:</p>
<p><span class="math display">\[\begin{aligned}
\frac{|XY|}{\left(\mathbb{E}|X^{p}|\right)^{1/p}\left(\mathbb{E}|Y^{q}|\right)^{1/q}} &amp; \leq\frac{1}{p}\cdot\frac{|X|^{p}}{\mathbb{E}|X^{p}|}+\frac{1}{q}\cdot\frac{|Y|^{q}}{\mathbb{E}|Y^{q}|}\end{aligned}\]</span></p>
<p>Taking expectations on both sides, and using the monotonicity of expectation property, we get:</p>
<p><span class="math display">\[\begin{aligned}
\frac{\mathbb{E}|XY|}{\left(\mathbb{E}|X^{p}|\right)^{1/p}\left(\mathbb{E}|Y^{q}|\right)^{1/q}} &amp; \leq\frac{1}{p}\cdot\frac{\mathbb{E}|X|^{p}}{\mathbb{E}|X^{p}|}+\frac{1}{q}\cdot\frac{\mathbb{E}|Y|^{q}}{\mathbb{E}|Y^{q}|}=\frac{1}{p}+\frac{1}{q}=1\end{aligned}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}|XY| &amp; \leq\left(\mathbb{E}|X^{p}|\right)^{1/p}\left(\mathbb{E}|Y^{q}|\right)^{1/q}\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(p=2\)</span> and <span class="math inline">\(q=2\)</span>. Then, we get the Cauchy-Schwarz inequality:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}|XY| &amp; \leq\left[\mathbb{E}(X^{2})\right]^{1/2}\left[\mathbb{E}(Y^{2})\right]^{1/2}\end{aligned}\]</span></p>
<p>In some ways, the <span class="math inline">\(p\)</span>-th moment of a random variable can be thought of as it’s length or <span class="math inline">\(p\)</span>-norm.</p>
<p>Define:</p>
<p><span class="math display">\[\left\Vert X\right\Vert _{p}=\left(\mathbb{E}|X|^{p}\right)^{1/p}\]</span>&nbsp;</p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="minkowskis-inequality." class="level3">
<h3 class="anchored" data-anchor-id="minkowskis-inequality.">Minkowski’s Inequality.</h3>
<div id="thm-minkowskis-inequality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8 </strong></span>For random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and for all <span class="math inline">\(p\geq1\)</span> we have:</p>
<p><span class="math display">\[\left\Vert X+Y\right\Vert _{p}\leq\left\Vert X\right\Vert _{p}+\left\Vert Y\right\Vert _{p}\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>The basic idea of the proof is to use Holder’s inequality. Let <span class="math inline">\(\frac{1}{q}=1-\frac{1}{p}\)</span> or in other words, <span class="math inline">\(q=\frac{p}{p-1}\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}|X||X+Y|^{p-1} &amp; \leq\left(\mathbb{E}|X|^{p}\right)^{1/p}\left(\mathbb{E}|X+Y|^{(p-1)q}\right)^{1/q} &amp; (a)\\ \mathbb{E}|Y||X+Y|^{p-1} &amp; \leq\left(\mathbb{E}|Y|^{p}\right)^{1/p}\left(\mathbb{E}|X+Y|^{(p-1)q}\right)^{1/q} &amp; (b)\end{aligned}\]</span></p>
<p>Adding the above two equations, we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}(|X+Y||X+Y|^{p-1})\leq\mathbb{E}(|X|+|Y|)(|X+Y|^{p-1}) &amp; \leq\left\{ \left(\mathbb{E}|X|^{p}\right)^{1/p}+\left(\mathbb{E}|Y|^{p}\right)^{1/p}\right\} \left(\mathbb{E}|X+Y|^{(p-1)q}\right)^{1/q}\\ \mathbb{E}|X+Y|^{p} &amp; \leq\left\{ \left\Vert X\right\Vert _{p}+\left\Vert Y\right\Vert _{p}\right\} \left(\mathbb{E}|X+Y|^{p}\right)^{1/q}\\ \left(\mathbb{E}|X+Y|^{p}\right)^{1/p} &amp; \leq\left\Vert X\right\Vert _{p}+\left\Vert Y\right\Vert _{p}\\ \left\Vert X+Y\right\Vert _{p} &amp; \leq\left\Vert X\right\Vert _{p}+\left\Vert Y\right\Vert _{p}\end{aligned}\]</span>&nbsp;</p>
<p>This closes the proof. <span class="math inline">\(\blacksquare\)</span></p>
</section>
</section>
<section id="a-quick-refresher-of-linear-algebra." class="level2">
<h2 class="anchored" data-anchor-id="a-quick-refresher-of-linear-algebra.">A quick refresher of linear algebra.</h2>
<p>Many of the concepts in this chapter have very elegant interpretations, if we think of real-valued random variables on a probability space as vectors in a vector space. In particular, variance is related to the concept of norm and distance, while covariance is related to inner-products. These concepts can help unify some of the ideas in this chapter from a geometric point of view. Of course, real-valued random variables are simply measurable, real-valued functions on the abstract space <span class="math inline">\(\Omega.\)</span></p>
<div id="def-vector-space" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Vector Space.) </strong></span>By a vector space, we mean a non-empty set <span class="math inline">\(V\)</span> with two operations: :::</p>
<ul>
<li><p>Vector addition: <span class="math inline">\(+:(\mathbf{x},\mathbf{y})\to\mathbf{x}+\mathbf{y}\)</span></p></li>
<li><p>Scalar multiplication: <span class="math inline">\(\cdot:(\alpha,\mathbf{x})\to\alpha\mathbf{x}\)</span></p></li>
</ul>
<p>such that the following conditions are satisfied:</p>
<p>(A1) Commutativity. <span class="math inline">\(\mathbf{x}+\mathbf{y}=\mathbf{y}+\mathbf{x}\)</span> for all <span class="math inline">\(\mathbf{x},\mathbf{y}\in V\)</span></p>
<p>(A2) Associativity: <span class="math inline">\((\mathbf{x}+\mathbf{y})+\mathbf{z}=\mathbf{x}+(\mathbf{y}+\mathbf{z})\)</span> for all <span class="math inline">\(\mathbf{x},\mathbf{y},\mathbf{z}\in V\)</span></p>
<p>(A3) Zero Element: There exists a zero element, denoted <span class="math inline">\(\mathbf{0}\)</span> in <span class="math inline">\(V\)</span>, for all <span class="math inline">\(\mathbf{x}\in V\)</span>, such that <span class="math inline">\(\mathbf{x}+\mathbf{0}=\mathbf{x}\)</span>.</p>
<p>(A4) Additive Inverse: For all <span class="math inline">\(\mathbf{x}\in V\)</span>, there exists an additive inverse(negative element) denoted <span class="math inline">\(-\mathbf{x}\)</span> in <span class="math inline">\(V\)</span>, such that <span class="math inline">\(\mathbf{x}+(-\mathbf{x})=\mathbf{0}\)</span>.</p>
<p>(M1) Scalar multiplication by identity element in <span class="math inline">\(F\)</span>: For all <span class="math inline">\(\mathbf{x}\in V\)</span>, <span class="math inline">\(1\cdot\mathbf{x}=\mathbf{x}\)</span>, where <span class="math inline">\(1\)</span> denotes the multiplicative identity in <span class="math inline">\(F\)</span>.</p>
<p>(M2) Scalar multiplication and field multiplication mix well: For all <span class="math inline">\(\alpha,\beta\in F\)</span> and <span class="math inline">\(\mathbf{v}\in V\)</span>, <span class="math inline">\((\alpha\beta)\mathbf{v}=\alpha(\beta\mathbf{v})\)</span>.</p>
<p>(D1) Distribution of scalar multiplication over vector addition: For all <span class="math inline">\(\alpha\in F\)</span>, and <span class="math inline">\(\mathbf{u},\mathbf{v}\in V\)</span>, <span class="math inline">\(\alpha(\mathbf{u}+\mathbf{v})=\alpha\mathbf{u}+\alpha\mathbf{v}\)</span>.</p>
<p>(D2) Distribution of field addition over scalar multiplication: For all <span class="math inline">\(\alpha,\beta\in F\)</span>, and <span class="math inline">\(\mathbf{v}\in V\)</span>, <span class="math inline">\((\alpha+\beta)\mathbf{v}=\alpha\mathbf{v}+\beta\mathbf{v}\)</span>.</p>
</div>
<p>As usual, our starting point is a random experiment modeled by a probability space <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>, so that <span class="math inline">\(\Omega\)</span> is the set of outcomes, <span class="math inline">\(\mathscr{\mathcal{F}}\)</span> is the <span class="math inline">\(\sigma\)</span>-algebra of events and <span class="math inline">\(\mathbb{P}\)</span> is the probability measure on the measurable space <span class="math inline">\((\Omega,\mathcal{F})\)</span>. Our basic vector space <span class="math inline">\(V\)</span> consists of all real-valued random variables defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. We define vector addition and scalar multiplication in the usual way point-wise.</p>
<ul>
<li><p>Vector addition: <span class="math inline">\((X+Y)(\omega)=X(\omega)+Y(\omega)\)</span>.</p></li>
<li><p>Scalar multiplication: <span class="math inline">\((\alpha X)(\omega)=\alpha X(\omega)\)</span></p></li>
</ul>
<p>Clearly, any function <span class="math inline">\(g\)</span> of a random variable <span class="math inline">\(X(\omega)\)</span> is also a random variable on the same probability space and any linear combination of random variables on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> also define a new random variable on the same probability space. Thus, <span class="math inline">\(V\)</span> is closed under vector addition and scalar-multiplication. Since vector-addition and scalar multiplication is defined point-wise, it is easy to see that - all the axioms of a vector space (A1)-(A4), (M1-M2), (D1), (D2) are satisfied. The constantly zero random variable <span class="math inline">\(0(\omega)=0\)</span> and the indicator random variable <span class="math inline">\(I_{\Omega}(\omega)\)</span> can be thought of as the zero and identity vectors in this vector space.</p>
<p>Clearly, any function <span class="math inline">\(g\)</span> of a random variable <span class="math inline">\(X(\omega)\)</span> is also a random variable on the same probability space and any linear combination of random variables on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> also define a new random variable on the same probability space. Thus, <span class="math inline">\(V\)</span> is closed under vector addition and scalar-multiplication. Since vector-addition and scalar multiplication is defined point-wise, it is easy to see that - all the axioms of a vector space (A1)-(A4), (M1-M2), (D1), (D2) are satisfied. The constantly zero random variable <span class="math inline">\(0(\omega)=0\)</span> and the indicator random variable <span class="math inline">\(I_{\Omega}(\omega)\)</span> can be thought of as the zero and identity vectors in this vector space.</p>
<section id="inner-products." class="level3">
<h3 class="anchored" data-anchor-id="inner-products.">Inner Products.</h3>
<p>In Euclidean geometry, the angle between two vectors is specified by their dot product, which is itself formalized by the abstract concept of inner products.</p>
<div id="def-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (Inner Product.) </strong></span>An inner product on the real vector space <span class="math inline">\(V\)</span> is a pairing that takes two vectors <span class="math inline">\(\mathbf{v},\mathbf{w}\in V\)</span> and produces a real number <span class="math inline">\(\left\langle \mathbf{v},\mathbf{w}\right\rangle \in\mathbf{R}\)</span>. The inner product is required to satisfy the following three axioms for all <span class="math inline">\(\mathbf{u},\mathbf{v},\mathbf{w}\in V\)</span> and scalars <span class="math inline">\(c,d\in\mathbf{R}\)</span>.</p>
<ol type="1">
<li>Bilinearity: <span class="math display">\[
\left\langle c\mathbf{u}+d\mathbf{v},\mathbf{w}\right\rangle =c\left\langle \mathbf{u},\mathbf{w}\right\rangle +d\left\langle \mathbf{v},\mathbf{w}\right\rangle
\]</span></li>
</ol>
<p><span class="math display">\[
\left\langle \mathbf{u},c\mathbf{v}+d\mathbf{w}\right\rangle =c\left\langle \mathbf{u},\mathbf{v}\right\rangle +d\left\langle \mathbf{u},\mathbf{w}\right\rangle
\]</span></p>
<ol start="2" type="1">
<li>Symmetry:</li>
</ol>
<p><span class="math display">\[
\left\langle \mathbf{v},\mathbf{w}\right\rangle =\left\langle \mathbf{w},\mathbf{v}\right\rangle
\]</span></p>
<ol start="3" type="1">
<li>Positive Definiteness:</li>
</ol>
<p><span class="math display">\[
\left\langle \mathbf{v},\mathbf{v}\right\rangle &gt;0\quad\text{ whenever }\mathbf{v\neq\mathbf{0}}
\]</span></p>
<p><span class="math display">\[
\left\langle \mathbf{v},\mathbf{v}\right\rangle =0\quad\text{ whenever }\mathbf{v=0}
\]</span></p>
</div>
<div id="def-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 </strong></span>(Norm). A norm on a real vector space <span class="math inline">\(V\)</span> is a function <span class="math inline">\(\left\Vert \cdot\right\Vert :V\to\mathbf{R}\)</span> satisfying :</p>
<p>(i) Positive Definiteness.</p>
<p><span class="math display">\[\left\Vert \mathbf{v}\right\Vert \geq0\]</span></p>
<p>and <span class="math display">\[\left\Vert \mathbf{v}\right\Vert =0\quad\text{if and only if }\mathbf{v}=\mathbf{0}\]</span></p>
<p>(ii) Scalar multiplication.</p>
<p><span class="math display">\[\left\Vert \alpha\mathbf{v}\right\Vert =|\alpha|\left\Vert \mathbf{v}\right\Vert\]</span></p>
<p>(iii) Triangle Inequality.</p>
<p><span class="math display">\[\left\Vert \mathbf{x+y}\right\Vert \leq\left\Vert \mathbf{x}\right\Vert +\left\Vert \mathbf{y}\right\Vert\]</span></p>
</div>
<p>As mentioned earlier, we can define the <span class="math inline">\(p\)</span>-norm of a random variable as:</p>
<p><span class="math display">\[
\begin{aligned}
\left\Vert X\right\Vert _{p} &amp; =\left(\mathbb{E}|X|^{p}\right)^{1/p}
\end{aligned}
\]</span></p>
<p>(i) Positive semi-definiteness: Since <span class="math inline">\(|X|\)</span> is a non-negative random variable, <span class="math inline">\(|X|^{p}\geq0\)</span> and the expectation of a non-negative random variable is also non-negative. Hence, <span class="math inline">\((\mathbb{E}|X|^{p})^{1/p}\geq0\)</span>. Moreover, <span class="math inline">\(\left\Vert X\right\Vert _{p}=0\)</span> implies that <span class="math inline">\(\mathbb{E}|X|^{p}=0\)</span>. From property (iv) of expectations, <span class="math inline">\(X=0\)</span>.</p>
<p>(ii) Scalar-multiplication: We have:</p>
<p><span class="math display">\[
\begin{aligned}
\left\Vert cX\right\Vert _{p} &amp; =\left(\mathbb{E}|cX|^{p}\right)^{1/p}\\
&amp; =\left(|c|^{p}\right)^{1/p}\left(\mathbb{E}|X|^{p}\right)^{1/p}\\
&amp; =|c|\cdot\left\Vert X\right\Vert _{p}
\end{aligned}
\]</span></p>
<p>(iii) Triangle Inequality. This followed from the Minkowski’s inequality.</p>
<p>The space of all random variables defined on <span class="math inline">\((\Omega,\mathcal{\mathcal{F}},\mathbb{P})\)</span> such that <span class="math inline">\(||X||_{p}&lt;\infty\)</span> is finite is called the <span class="math inline">\(L^{p}\)</span> space.</p>
</section>
<section id="orthogonal-matrices." class="level3">
<h3 class="anchored" data-anchor-id="orthogonal-matrices.">Orthogonal Matrices.</h3>
<div id="def-orthogonal-matrices" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (Orthogonal Matrix.) </strong></span>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n\times n\)</span> square matrix. We say that the matrix <span class="math inline">\(A\)</span> is orthogonal, if its transpose is equal toits inverse.</p>
<p><span class="math display">\[
\begin{aligned}
A' &amp; =A^{-1}
\end{aligned}
\]</span></p>
</div>
<p>This may seem like an odd property to study, but the following theorem explains why it is so useful. Essentially, an orthogonal matrix rotates (or reflects) vectors without distorting angles or distances.</p>
<div id="prp-properties-of-orthogonal-matrix" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 (Properties of an orthogonal matrix) </strong></span>Faor an <span class="math inline">\(n\times n\)</span> square matrix <span class="math inline">\(A\)</span>, the following are equivalent:</p>
<p>(1) <span class="math inline">\(A\)</span> is orthogonal. That is, <span class="math inline">\(A'A=I\)</span>.</p>
<p>(2) <span class="math inline">\(A\)</span> preserves norms. That is, for all <span class="math inline">\(\mathbf{x}\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\left\Vert A\mathbf{x}\right\Vert &amp;=\left\Vert \mathbf{x}\right\Vert \end{aligned}\]</span></p>
<p>(3) <span class="math inline">\(A\)</span> preserves inner products, that is, for every <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\mathbf{y}\)</span><span class="math inline">\(\in\mathbf{R}^{n}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
(A\mathbf{x})\cdot(A\mathbf{y}) &amp;=\mathbf{x}\cdot\mathbf{y}\end{aligned}\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
\left\Vert A\mathbf{x}\right\Vert ^{2} &amp; =\left(A\mathbf{x}\right)'(A\mathbf{x})\\
&amp; =\mathbf{x}'(A'A)\mathbf{x}\\
&amp; =\mathbf{x}'I\mathbf{x}\\
&amp; =\mathbf{x}'\mathbf{x}\\
&amp; =||\mathbf{x}||^{2}\end{aligned}\]</span></p>
<p>Consequently, <span class="math inline">\(||A\mathbf{x}||=||\mathbf{x}||\)</span>. The matrix <span class="math inline">\(A\)</span> preserves norms. Thus, (1) implies (2).</p>
<p>Moreover, consider</p>
<p><span class="math display">\[\begin{aligned}
||A(\mathbf{x}+\mathbf{y})||^{2} &amp; =\left(A\mathbf{x}+A\mathbf{y}\right)\cdot\left(A\mathbf{x}+A\mathbf{y}\right)\\
&amp; =(A\mathbf{x})\cdot(A\mathbf{x})+(A\mathbf{x})\cdot(A\mathbf{y})+(A\mathbf{y})\cdot(A\mathbf{x})+(A\mathbf{y})\cdot(A\mathbf{y})\\
&amp; =||A\mathbf{x}||^{2}+2(A\mathbf{x})\cdot(A\mathbf{y})+||A\mathbf{y}||^{2} &amp; \{\mathbf{x}\cdot\mathbf{y}=\mathbf{y}\cdot\mathbf{x}\}\\
&amp; =||\mathbf{x}||^{2}+2(A\mathbf{x})\cdot(A\mathbf{y})+||\mathbf{y}||^{2} &amp; \{A\text{ preserves norms}\}\end{aligned}\]</span></p>
<p>But, <span class="math inline">\(||A(\mathbf{x}+\mathbf{y})||^{2}=||\mathbf{x}+\mathbf{y}||^{2}=||\mathbf{x}||^{2}+2\mathbf{x}\cdot\mathbf{y}+||\mathbf{y}||^{2}\)</span>.</p>
<p>Equating the two expressions, we have the desired result. Hence, (2) implies (3).</p>
<p>Lastly, if <span class="math inline">\(A\)</span> preserves inner products, we may write:</p>
<p><span class="math display">\[\begin{aligned}
\left\langle A\mathbf{x},A\mathbf{x}\right\rangle  &amp; =\left\langle \mathbf{x},\mathbf{x}\right\rangle \\
\left(A\mathbf{x}\right)'(A\mathbf{x}) &amp; =\mathbf{x}'\mathbf{x}\\
\mathbf{x}'A'A\mathbf{x} &amp; =0\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(\mathbf{x}\neq\mathbf{0}\)</span>, it must be true that <span class="math inline">\(\mathbf{x}'A'A-\mathbf{x}'=0\)</span>. Again, since <span class="math inline">\(\mathbf{x}'\neq\mathbf{0}\)</span>, it follows that <span class="math inline">\(A'A-I=0\)</span>.</p>
<div id="thm-linear-independence-of-orthogonal-vectors" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9 (Linear Independence of orthogonal vectors) </strong></span>If <span class="math inline">\(\mathbf{q}_{1},\mathbf{q}_{2},\ldots,\mathbf{q}_{k}\in V\)</span> be mutually orthogonal elements, such that <span class="math inline">\(\mathbf{q}_{i}\neq\mathbf{0}\)</span> for all <span class="math inline">\(i\)</span>, then <span class="math inline">\(\mathbf{q}_{1},\mathbf{q}_{2},\ldots,\mathbf{q}_{k}\)</span> are linearly independent.</p>
</div>
<p><em>Proof.</em></p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
c_{1}\mathbf{q}_{1}+c_{2}\mathbf{q}_{2}+\ldots+c_{k}\mathbf{q}_{k} &amp; =\mathbf{0}\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(\left\langle \mathbf{q}_{i},\mathbf{q}_{i}\right\rangle =1\)</span> and <span class="math inline">\(\left\langle \mathbf{q}_{i},\mathbf{q}_{j}\right\rangle =0\)</span> where <span class="math inline">\(i\neq j\)</span>, we can take the inner product of the vector <span class="math inline">\(c_1 \mathbf{q}_{1} + c_2 \mathbf{q}_{2} + \ldots + c_i \mathbf{q}_{i}+\ldots + c_{k}\mathbf{q}_{k}\)</span> with <span class="math inline">\(\mathbf{q}_{i}\)</span> for each <span class="math inline">\(i=1,2,3,\ldots,k\)</span>. It results in <span class="math inline">\(c_{i}||\mathbf{q}_{i}||^{2}=0\)</span>. Since <span class="math inline">\(\mathbf{q}_{i}\neq\mathbf{0}\)</span>, <span class="math inline">\(||\mathbf{q}_{i}||^{2}&gt;0\)</span>. So, <span class="math inline">\(c_{i}=0\)</span>. We conclude that <span class="math inline">\(c_{1}=c_{2}=\ldots=c_{k} =0\)</span>. Consequently, <span class="math inline">\(\mathbf{q}_{1},\mathbf{q}_{2},\ldots,\mathbf{q}_{k}\)</span> are linearly independent.&nbsp;<span class="math inline">\(\blacksquare\)</span></p>
<div id="thm-orthogonal-vectors-form-a-basis" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10 (Orthogonal vectors form a basis) </strong></span>Let <span class="math inline">\(Q=\left[\begin{array}{cccc} \mathbf{q}_{1} &amp; \mathbf{q}_{2} &amp; \ldots &amp; \mathbf{q}_{n}\end{array}\right]\)</span> be an <span class="math inline">\(n\times n\)</span> orthogonal matrix. Then, <span class="math inline">\(\{\mathbf{q}_{1},\ldots,\mathbf{q}_{n}\}\)</span> form an orthonormal basis for <span class="math inline">\(\mathbf{R}^{n}\)</span>.</p>
</div>
<p><em>Proof.</em></p>
<p>We have <span class="math inline">\(Q\mathbf{e}_{i}=\mathbf{q}_{i}\)</span>. Consequently,</p>
<p><span class="math display">\[\begin{aligned}
\left\langle \mathbf{q}_{i},\mathbf{q}_{i}\right\rangle  &amp; =\mathbf{q}_{i}'\mathbf{q}_{i}\\
&amp; =(Q\mathbf{e}_{i})'(Q\mathbf{e}_{i})\\
&amp; =\mathbf{e}_{i}'Q'Q\mathbf{e}_{i}\\
&amp; =\mathbf{e}_{i}'I\mathbf{e}_{i}\\
&amp; =\mathbf{e}_{i}'\mathbf{e}_{i}\\
&amp; =1\end{aligned}\]</span></p>
<p>Assume that <span class="math inline">\(i\neq j\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\left\langle \mathbf{q}_{i},\mathbf{q}_{j}\right\rangle  &amp; =\mathbf{q}_{i}'\mathbf{q}_{j}\\
&amp; =\mathbf{e}_{i}'Q'Q\mathbf{e}_{j}\\
&amp; =\mathbf{e}_{i}'\mathbf{e}_{j}\\
&amp; =0\end{aligned}\]</span></p>
<p>From <a href="#thm-linear-independence-of-orthogonal-vectors">Theorem&nbsp;9</a>, <span class="math inline">\(\{\mathbf{q}_{1},\ldots,\mathbf{q}_{n}\}\)</span> are linearly independent and hence form an orthonormal basis for <span class="math inline">\(\mathbf{R}^{n}\)</span>.</p>
</section>
<section id="quadratic-forms." class="level3">
<h3 class="anchored" data-anchor-id="quadratic-forms.">Quadratic Forms.</h3>
<p>An expression of the form:</p>
<p><span class="math display">\[\mathbf{x}'A\mathbf{x}\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}\)</span> is a <span class="math inline">\(n\times1\)</span> column vector and <span class="math inline">\(A\)</span> is an <span class="math inline">\(n\times n\)</span> matrix is called a quadratic form in <span class="math inline">\(\mathbf{x}\)</span> and</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{x}'A\mathbf{x} &amp; =\sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}x_{i}x_{j}\end{aligned}\]</span></p>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <span class="math inline">\(n\times n\)</span> and <span class="math inline">\(\mathbf{x},\mathbf{y}\)</span> are <span class="math inline">\(n\)</span>-vectors, then</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{x}'(A+B)\mathbf{y} &amp; =\mathbf{x}'A\mathbf{y}+\mathbf{x}'B\mathbf{y}\end{aligned}\]</span></p>
<p>The quadratic form of the matrix <span class="math inline">\(A\)</span> is called positive definite if:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{x}'A\mathbf{x} &amp; &gt;0\quad\text{whenever }\mathbf{x}\neq\mathbf{0}\end{aligned}\]</span></p>
<p>and positive semidefinite if:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{x}'A\mathbf{x} &amp; \geq0\quad\text{whenever }\mathbf{x}\neq\mathbf{0}\end{aligned}\]</span></p>
<p>Letting <span class="math inline">\(\mathbf{e}_{i}\)</span> be the unit vector with it’s <span class="math inline">\(i\)</span>th coordinate vector <span class="math inline">\(1\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{e}_{i}'A\mathbf{e}_{i} &amp; =\left[a_{i1}a_{i2}\ldots a_{ii}\ldots a_{in}\right]\left[\begin{array}{c}
0\\
0\\
\vdots\\
1\\
\vdots\\
0
\end{array}\right]=a_{ii}
\end{aligned}
\]</span></p>
</section>
<section id="eigenthingies-and-diagonalizability." class="level3">
<h3 class="anchored" data-anchor-id="eigenthingies-and-diagonalizability.">Eigenthingies and diagonalizability.</h3>
<p>Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be finite dimensional vector spaces with <span class="math inline">\(dim(V)=n\)</span> and <span class="math inline">\(dim(W)=m\)</span>. A linear transformation <span class="math inline">\(T:V\to W\)</span>, is defined by its action on the basis vectors. Suppose:</p>
<p><span class="math display">\[
\begin{aligned}
T(\mathbf{v}_{j}) &amp; =\sum_{i=1}^{n}a_{ij}\mathbf{w}_{i}
\end{aligned}
\]</span></p>
<p>for all <span class="math inline">\(1\leq i\leq m\)</span>.</p>
<p>Then, the matrix <span class="math inline">\(A=[T]_{\mathcal{B}_{V}}^{\mathcal{B}_{W}}\)</span> of the linear transformation is defined as:</p>
<p><span class="math display">\[\begin{aligned}
A &amp; =\left[\begin{array}{cccc}
a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2n}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
a_{m1} &amp; a_{m2} &amp; \ldots &amp; a_{mn}
\end{array}\right]\end{aligned}\]</span></p>
<div id="def-diagonalizable" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8 </strong></span>A linear transformation <span class="math inline">\(T:V\to V\)</span> is said to be <strong>diagonalizable</strong> if there exists an ordered basis <span class="math inline">\(\mathcal{B}=\{\mathbf{v}_{1},\ldots,\mathbf{v}_{n}\}\)</span> for <span class="math inline">\(V\)</span> so that the matrix for <span class="math inline">\(T\)</span> with respect to <span class="math inline">\(\mathcal{B}\)</span> is diagonal. This means precisely that, for some scalars <span class="math inline">\(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
T(\mathbf{v}_{1}) &amp; =\lambda_{1}\mathbf{v}_{1}\\
T(\mathbf{v}_{2}) &amp; =\lambda_{2}\mathbf{v}_{2}\\
\vdots\\
T(\mathbf{v}_{n}) &amp; =\lambda_{n}\mathbf{v}_{n}
\end{aligned}
\]</span></p>
</div>
<p>In other words, if <span class="math inline">\(A=[T]_{\mathcal{B}}\)</span>, then we have:</p>
<p><span class="math display">\[\begin{aligned}
A\mathbf{v}_{i} &amp; =\lambda_{i}\mathbf{v}_{i}\end{aligned}\]</span></p>
<p>Thus, if we let <span class="math inline">\(P\)</span> be the <span class="math inline">\(n\times n\)</span> matrix whose columns are the vectors <span class="math inline">\(\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{n}\)</span> and <span class="math inline">\(\Lambda\)</span> be the <span class="math inline">\(n\times n\)</span> diagonal matrix with diagonal entries <span class="math inline">\(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\)</span>, then we have:</p>
<p><span class="math display">\[\begin{aligned}
A\left[\begin{array}{cccc}
\mathbf{v}_{1} &amp; \mathbf{v}_{2} &amp; \ldots &amp; \mathbf{v}_{n}\end{array}\right] &amp; =\left[\begin{array}{cccc}
\mathbf{v}_{1} &amp; \mathbf{v}_{2} &amp; \ldots &amp; \mathbf{v}_{n}\end{array}\right]\left[\begin{array}{cccc}
\lambda_{1}\\
&amp; \lambda_{2}\\
&amp;  &amp; \ddots\\
&amp;  &amp;  &amp; \lambda_{n}
\end{array}\right]\\
AP &amp; =P\Lambda\\
A &amp; =P\Lambda P^{-1}\end{aligned}\]</span></p>
<p>There exists a large class of diagonalizable matrices - the symmetric matrices. A square matrix <span class="math inline">\(A\)</span> is symmetric, if <span class="math inline">\(A=A'\)</span>.</p>
<div id="def-eigenvector" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9 (Eigenvectors) </strong></span>Let <span class="math inline">\(T:V\to V\)</span> be a linear transformation. A <strong>non-zero</strong> vector <span class="math inline">\(\mathbf{v}\in V\)</span> is called the eigenvector of <span class="math inline">\(T\)</span>, if there is a scalar <span class="math inline">\(\lambda\)</span> so that <span class="math inline">\(T(\mathbf{v})=\lambda\mathbf{v}\)</span>. The scalar <span class="math inline">\(\lambda\)</span> is called the eigenvalue of <span class="math inline">\(T\)</span>.</p>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-07-12">

<title>quantdev.blog - Martingales</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sell_side_quant_critical_path.html" rel="" target="">
 <span class="menu-text">Sell-side Quant</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../roadmap.html" rel="" target="">
 <span class="menu-text">C++ Roadmap</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/quasar-chunawala" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://linkedin.com/in/quasar-chunawala" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Martingales</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Stochastic Calculus</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 12, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#martingales." id="toc-martingales." class="nav-link active" data-scroll-target="#martingales.">Martingales.</a>
  <ul class="collapse">
  <li><a href="#elementary-conditional-expectation." id="toc-elementary-conditional-expectation." class="nav-link" data-scroll-target="#elementary-conditional-expectation.">Elementary conditional expectation.</a></li>
  <li><a href="#conditional-expectation-as-a-projection." id="toc-conditional-expectation-as-a-projection." class="nav-link" data-scroll-target="#conditional-expectation-as-a-projection.">Conditional Expectation as a projection.</a>
  <ul class="collapse">
  <li><a href="#properties-of-conditional-expectation." id="toc-properties-of-conditional-expectation." class="nav-link" data-scroll-target="#properties-of-conditional-expectation.">Properties of Conditional Expectation.</a></li>
  </ul></li>
  <li><a href="#martingales.-1" id="toc-martingales.-1" class="nav-link" data-scroll-target="#martingales.-1">Martingales.</a></li>
  <li><a href="#computations-with-martingales." id="toc-computations-with-martingales." class="nav-link" data-scroll-target="#computations-with-martingales.">Computations with Martingales.</a></li>
  <li><a href="#reflection-principle-for-brownian-motion." id="toc-reflection-principle-for-brownian-motion." class="nav-link" data-scroll-target="#reflection-principle-for-brownian-motion.">Reflection principle for Brownian motion.</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="martingales." class="level1">
<h1>Martingales.</h1>
<section id="elementary-conditional-expectation." class="level2">
<h2 class="anchored" data-anchor-id="elementary-conditional-expectation.">Elementary conditional expectation.</h2>
<p>In elementary probability, the conditional expectation of a variable <span class="math inline">\(Y\)</span> given another random variable <span class="math inline">\(X\)</span> refers to the expectation of <span class="math inline">\(Y\)</span> given the conditional distribution <span class="math inline">\(f_{Y|X}(y|x)\)</span> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. To illustrate this, let’s go through a simple example. Consider <span class="math inline">\(\mathcal{B}_{1}\)</span>, <span class="math inline">\(\mathcal{B}_{2}\)</span> to be two independent Bernoulli-distributed random variables with <span class="math inline">\(p=1/2\)</span>. Then, construct:</p>
<p><span class="math display">\[\begin{aligned}
X=\mathcal{B}_{1}, &amp; \quad Y=\mathcal{B}_{1}+\mathcal{B}_{2}
\end{aligned}\]</span></p>
<p>It is easy to compute <span class="math inline">\(\mathbb{E}[Y|X=0]\)</span> and <span class="math inline">\(\mathbb{E}[Y|X=1]\)</span>. By definition, it is given by:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X=0] &amp; =\sum_{j=0}^{2}j\mathbb{P}(Y=j|X=0)\\
&amp; =\sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(Y=j,X=0)}{P(X=0)}\\
&amp; =0+1\cdot\frac{(1/4)}{(1/2)}+2\cdot\frac{0}{(1/2)}\\
&amp; =\frac{1}{2}
\end{aligned}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X=1] &amp; =\sum_{j=0}^{2}j\mathbb{P}(Y=j|X=1)\\
&amp; =\sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(Y=j,X=1)}{P(X=1)}\\
&amp; =0+1\cdot\frac{(1/4)}{(1/2)}+2\cdot\frac{(1/4)}{(1/2)}\\
&amp; =\frac{3}{2}
\end{aligned}\]</span></p>
<p>With this point of view, the conditional expectation is computed given the information that the event <span class="math inline">\(\{X=0\}\)</span> occurred or the event <span class="math inline">\(\{X=1\}\)</span> occurred. It is possible to regroup both conditional expectations in a single object, if we think of the conditional expectation as a random variable and denote it by <span class="math inline">\(\mathbb{E}[Y|X]\)</span>. Namely, we take:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X](\omega) &amp; =\begin{cases}
\frac{1}{2} &amp; \text{if }X(\omega)=0\\
\frac{3}{2} &amp; \text{if }X(\omega)=1
\end{cases}\label{eq:elementary-conditional-expectation-example}
\end{aligned}\]</span></p>
<p>This random variable is called the <em>conditional expectation</em> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. We make two important observations:</p>
<p>(i) If the value of <span class="math inline">\(X\)</span> is known, then the value of <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is determined.</p>
<p>(ii) If we have another random variable <span class="math inline">\(g(X)\)</span> constructed from <span class="math inline">\(X\)</span>, then we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[g(X)Y] &amp; =\mathbb{E}[g(X)\mathbb{E}[Y|X]]
\end{aligned}\]</span></p>
<p>In other words, as far as <span class="math inline">\(X\)</span> is concerned, the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is a proxy for <span class="math inline">\(Y\)</span> in the expectation. We sometimes say that <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is the best estimate of <span class="math inline">\(Y\)</span> given the information of <span class="math inline">\(X\)</span>.</p>
<p>The last observation is easy to verify since:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[g(X)Y] &amp; =\sum_{i=0}^{1}\sum_{j=0}^{2}g(i)\cdot j\cdot\mathbb{P}(X=i,Y=j)\\
&amp; =\sum_{i=0}^{1}\mathbb{P}(X=i)g(i)\left\{ \sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(X=i,Y=j)}{\mathbb{P}(X=i)}\right\} \\
&amp; =\mathbb{E}[g(X)\mathbb{E}[Y|X]]
\end{aligned}\]</span></p>
<div class="example">
<p><span id="ex:elementary-definitions-of-conditional-expectation" label="ex:elementary-definitions-of-conditional-expectation"></span>(Elementary Definitions of Conditional Expectation).</p>
<p>(1) <span class="math inline">\((X,Y)\)</span> discrete. The treatment is similar to the above. If a random variable <span class="math inline">\(X\)</span> takes values <span class="math inline">\((x_{i},i\geq1)\)</span> and <span class="math inline">\(Y\)</span> takes values <span class="math inline">\((y_{j},j\geq1)\)</span>, we have by definition that the conditional expectation as a random variable is:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X](\omega) &amp; =\sum_{j\geq1}y_{j}\mathbb{P}(Y=y_{j}|X=x_{i})\quad\text{for }\omega\text{ such that }X(\omega)=x_{i}
\end{aligned}\]</span> (2) <span class="math inline">\((X,Y)\)</span> continuous with joint PDF <span class="math inline">\(f_{X,Y}(x,y)\)</span>: In this case, the conditional expectation is the random variable given by</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X] &amp; =h(X)
\end{aligned}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{aligned}
h(x) &amp; =\int_{\mathbf{R}}yf_{Y|X}(y|x)dy=\int_{\mathbf{R}}y\frac{f_{X,Y}(x,y)}{f_{X}(x)}dy=\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}
\end{aligned}\]</span></p>
</div>
<p>In the two examples above, the expectation of the random variable <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is equal to <span class="math inline">\(\mathbb{E}[Y]\)</span>. Indeed in the discrete case, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[\mathbb{E}[Y|X]] &amp; =\sum_{i=0}^{1}P(X=x_{i})\cdot\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j}|X=x_{i})\\
&amp; =\sum_{i=0}^{1}\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j},X=x_{i})\\
&amp; =\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j})\\
&amp; =\mathbb{E}[Y]
\end{aligned}\]</span></p>
<div class="example">
<p>(Conditional Probability vs Conditional expectation). The conditional probability of the event <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> can be recast in terms of conditional expectation using indicator functions. If <span class="math inline">\(0&lt;\mathbb{P}(B)&lt;1\)</span>, it is not hard to check that: <span class="math inline">\(\mathbb{P}(A|B)=\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=1]\)</span> and <span class="math inline">\(\mathbb{P}(A|B^{C})=\mathbb{E}[\mathbf{1}_{A}|1_{B}=0]\)</span>. Indeed the random variables <span class="math inline">\(\mathbf{1}_{A}\)</span> and <span class="math inline">\(\mathbf{1}_{B}\)</span> are discrete. If we proceed as in the discrete case above, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=1] &amp; =1\cdot\mathbb{P}(\mathbf{1}_{A}=1|\mathbf{1}_{B}=1)\\
&amp; =\frac{\mathbb{P}(\mathbf{1}_{A}=1,\mathbf{1}_{B}=1)}{\mathbb{P}(\mathbf{1}_{B}=1)}\\
&amp; =\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\\
&amp; =\mathbb{P}(A|B)
\end{aligned}\]</span></p>
<p>A similar calculation gives <span class="math inline">\(\mathbb{P}(A|B^{C})\)</span>. In particular, the formula for total probability for <span class="math inline">\(A\)</span> is a rewriting of the expectation of the random variable <span class="math inline">\(\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}]\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}]] &amp; =\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=1]\mathbb{P}(\mathbf{1}_{B}=1)+\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=0]\mathbb{P}(\mathbf{1}_{B}=0)\\
&amp; =\mathbb{P}(A|B)\cdot\mathbb{P}(B)+\mathbb{P}(A|B^{C})\cdot\mathbb{P}(B^{C})\\
&amp; =\mathbb{P}(A)
\end{aligned}\]</span></p>
</div>
</section>
<section id="conditional-expectation-as-a-projection." class="level2">
<h2 class="anchored" data-anchor-id="conditional-expectation-as-a-projection.">Conditional Expectation as a projection.</h2>
<section id="conditioning-on-one-variable." class="level4">
<h4 class="anchored" data-anchor-id="conditioning-on-one-variable.">Conditioning on one variable.</h4>
<p>We start by giving the definition of conditional expectation given a single variable. This relates to the two observations (A) and (B) made previously. We assume that the random variable is integrable for the expectations to be well-defined.</p>
<div class="defn">
<p><span id="def:conditional-expectation" label="def:conditional-expectation"></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be integrable random variables on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. The conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is the random variable denoted by <span class="math inline">\(\mathbb{E}[Y|X]\)</span> with the following two properties:</p>
<p>(A) There exists a function <span class="math inline">\(h:\mathbf{R}\to\mathbf{R}\)</span> such that <span class="math inline">\(\mathbb{E}[Y|X]=h(X)\)</span>.</p>
<p>(B) For any bounded random variable of the form <span class="math inline">\(g(X)\)</span> for some function <span class="math inline">\(g\)</span>,</p>
<p><span class="math display">\[\mathbb{E}[g(X)Y]=\mathbb{E}[g(X)\mathbb{E}[Y|X]]\label{eq:definition-conditional-expectation}\]</span></p>
<p>We can intepret the second property as follows. The conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> serves as a proxy for <span class="math inline">\(Y\)</span> as far as <span class="math inline">\(X\)</span> is concerned. Note that in equation (<a href="#eq:definition-conditional-expectation" data-reference-type="ref" data-reference="eq:definition-conditional-expectation">[eq:definition-conditional-expectation]</a>), the expectation on the left can be seen as an average over the joint values of <span class="math inline">\((X,Y)\)</span>, whereas the one on the right is an average over the values of <span class="math inline">\(X\)</span> only! Another way to see this property is to write is as:</p>
<p><span class="math display">\[\mathbb{E}[g(X)(Y-\mathbb{E}[Y|X])]=0\]</span></p>
<p>In other words, the <em>random variable <span class="math inline">\(Y-\mathbb{E}[Y|X]\)</span> is orthogonal to any random variable constructed from <span class="math inline">\(X\)</span>.</em></p>
<p>Finally, it is important to notice that if we take <span class="math inline">\(g(X)=1\)</span>, then the second property implies :</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y] &amp; =\mathbb{E}[\mathbb{E}[Y|X]]
\end{aligned}\]</span></p>
<p>In other words, the expectation of the conditional expectation of <span class="math inline">\(Y\)</span> is simply the expectation of <span class="math inline">\(Y\)</span>.</p>
<p>The existence of the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is not obvious. We know, it exists in particular cases given in example (<a href="#ex:elementary-definitions-of-conditional-expectation" data-reference-type="ref" data-reference="ex:elementary-definitions-of-conditional-expectation">[ex:elementary-definitions-of-conditional-expectation]</a>). We will show more generally, that it exists, it is unique whenever <span class="math inline">\(Y\)</span> is in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> (In fact, it can be shown to exist whenever <span class="math inline">\(Y\)</span> is integrable). Before doing so, let’s warm up by looking at the case of Gaussian vectors.</p>
</div>
<div class="example">
<p><span id="ex:conditional-expectation-of-gaussian-vectors" label="ex:conditional-expectation-of-gaussian-vectors"></span>(Conditional expectation of Gaussian vectors - I). Let <span class="math inline">\((X,Y)\)</span> be a Gaussian vector of mean <span class="math inline">\(0\)</span>. Then:</p>
<p><span class="math display">\[\mathbb{E}[Y|X]=\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X\label{eq:conditional-expectation-of-gaussian-vector}\]</span></p>
<p>This candidate satisfies the two defining properties of conditional expectation : (A) It is clearly a function of <span class="math inline">\(X\)</span>; in fact it is a simple multiple of <span class="math inline">\(X\)</span>. (B) We have that the random variable <span class="math inline">\(\left(Y-\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X\right)\)</span> is orthogonal and thus independent to <span class="math inline">\(X\)</span>. This is a consequence of the proposition (<a href="#prop:diagonal-cov-matrix-implies-independence-of-gaussians" data-reference-type="ref" data-reference="prop:diagonal-cov-matrix-implies-independence-of-gaussians">[prop:diagonal-cov-matrix-implies-independence-of-gaussians]</a>), since:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[X\left(Y-\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X\right)\right] &amp; =\mathbb{E}XY-\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}\mathbb{E}X^{2}\\
&amp; =\mathbb{E}XY-\frac{\mathbb{E}[XY]}{\cancel{\mathbb{E}[X^{2}]}}\cancel{\mathbb{E}X^{2}}\\
&amp; =0
\end{aligned}\]</span></p>
<p>Therefore, we have for any bounded function <span class="math inline">\(g(X)\)</span> of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[g(X)(Y-\mathbb{E}(Y|X))] &amp; =\mathbb{E}[g(X)]\mathbb{E}[Y-\mathbb{E}[Y|X]]=0
\end{aligned}\]</span></p>
</div>
<div class="example">
<p><span id="ex:brownian-conditioning-I" label="ex:brownian-conditioning-I"></span>(Brownian conditioning-I) Let <span class="math inline">\((B_{t},t\geq0)\)</span> be a standard Brownian motion. Consider the Gaussian vector <span class="math inline">\((B_{1/2},B_{1})\)</span>. Its covariance matrix is:</p>
<p><span class="math display">\[\begin{aligned}
C &amp; =\left[\begin{array}{cc}
1/2 &amp; 1/2\\
1/2 &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>Let’s compute <span class="math inline">\(\mathbb{E}[B_{1}|B_{1/2}]\)</span> and <span class="math inline">\(\mathbb{E}[B_{1/2}|B_{1}]\)</span>. This is easy using the equation (<a href="#eq:conditional-expectation-of-gaussian-vector" data-reference-type="ref" data-reference="eq:conditional-expectation-of-gaussian-vector">[eq:conditional-expectation-of-gaussian-vector]</a>). We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[B_{1}|B_{1/2}] &amp; =\frac{\mathbb{E}[B_{1}B_{1/2}]}{\mathbb{E}[B_{1/2}^{2}]}B_{1/2}\\
&amp; =\frac{(1/2)}{(1/2)}B_{1/2}\\
&amp; =B_{1/2}
\end{aligned}\]</span></p>
<p>In other words, the best approximation of <span class="math inline">\(B_{1}\)</span> given the information of <span class="math inline">\(B_{1/2}\)</span> is <span class="math inline">\(B_{1/2}\)</span>. There is no problem in computing <span class="math inline">\(\mathbb{E}[B_{1/2}|B_{1}]\)</span>, even though we are conditioning on a future position. Indeed the same formula gives</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[B_{1/2}|B_{1}] &amp; =\frac{\mathbb{E}[B_{1}B_{1/2}]}{\mathbb{E}[B_{1}^{2}]}B_{1}=\frac{1}{2}B_{1}
\end{aligned}\]</span></p>
<p>This means that the best approximation of <span class="math inline">\(B_{1/2}\)</span> given the position at time <span class="math inline">\(1\)</span>, is <span class="math inline">\(\frac{1}{2}B_{1}\)</span> which makes a whole lot of sense!</p>
</div>
<p>In example (<a href="#eq:conditional-expectation-of-gaussian-vector" data-reference-type="ref" data-reference="eq:conditional-expectation-of-gaussian-vector">[eq:conditional-expectation-of-gaussian-vector]</a>) for the Gaussian vector <span class="math inline">\((X,Y)\)</span>, the conditional expectation was equal to the <em>orthogonal projection</em> of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X\)</span> in <span class="math inline">\(L^{2}\)</span>. In particular, the conditional expectation was a multiple of <span class="math inline">\(X\)</span>. Is this always the case? Unfortunately, it is not. For example, in the equation (<a href="#eq:elementary-conditional-expectation-example" data-reference-type="ref" data-reference="eq:elementary-conditional-expectation-example">[eq:elementary-conditional-expectation-example]</a>), the conditional expectation is clearly not a multiple of the random variable <span class="math inline">\(X\)</span>. However, it is a function of <span class="math inline">\(X\)</span>, as is always the case by definition (<a href="#def:conditional-expectation" data-reference-type="ref" data-reference="def:conditional-expectation">[def:conditional-expectation]</a>).</p>
<p>The idea to construct the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> in general is to <em>project <span class="math inline">\(Y\)</span> on the space of all random variables that can be constructed from <span class="math inline">\(X\)</span>. To make this precise, consider the following subspace of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> :</em></p>
<div class="defn">
<p>Let <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> be a probability space and <span class="math inline">\(X\)</span> a random variable defined on it. The space <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> is the linear subspace of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> consisting of the square-integrable random variables of the form <span class="math inline">\(g(X)\)</span> for some function <span class="math inline">\(g:\mathbf{R}\to\mathbf{R}\)</span>.</p>
</div>
<p>This is a linear subspace of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>: It contains the random variable <span class="math inline">\(0\)</span>, and any linear combination of random variables of this kind is also a function of <span class="math inline">\(X\)</span> and must have a finite second moment. We note the following:</p>
<div class="rem*">
<p><span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> is a subspace of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>, very much how a plane or line (going through the origin) is a subspace of <span class="math inline">\(\mathbf{R}^{3}\)</span>.</p>
<p>In particular, as in the case of a line or a plane, we can project an element of <span class="math inline">\(Y\)</span> of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> onto <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>. The resulting projection is an element of <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>, a square-integrable random-variable that is a function of <span class="math inline">\(X\)</span>. For a subspace <span class="math inline">\(\mathcal{S}\)</span> of <span class="math inline">\(\mathbf{R}^{3}\)</span> (e.g.&nbsp;a line or a plane), the projection of the vector <span class="math inline">\(\mathbf{v}\in\mathbf{R}^{3}\)</span> onto the subspace <span class="math inline">\(\mathcal{S}\)</span>, denoted <span class="math inline">\(\text{Proj}_{\mathcal{S}}(\mathbf{v})\)</span> is the closest point to <span class="math inline">\(\mathbf{v}\)</span> lying in the subspace <span class="math inline">\(\mathcal{S}\)</span>. Moreover, <span class="math inline">\(\mathbf{v}-\text{Proj}_{\mathcal{S}}(\mathbf{v})\)</span> is orthogonal to the subspace. This picture of orthogonal projection also holds in <span class="math inline">\(L^{2}\)</span>. Let <span class="math inline">\(Y\)</span> be a random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> and let <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> be the subspace of those random variables that are functions of <span class="math inline">\(X\)</span>. We write <span class="math inline">\(Y^{\star}\)</span> for the random variable in <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> that is <em>closest</em> to <span class="math inline">\(Y\)</span>. In other words, we have (using the definition of the <span class="math inline">\(L^{2}\)</span>-distance square):</p>
<p><span class="math display">\[\inf_{Z\in L^{2}(\Omega,\sigma(X),\mathbb{P})}\mathbb{E}[(Y-Z)^{2}]=\mathbb{E}[(Y-Y^{\star})^{2}]\label{eq:Y-star-is-the-closest-to-Y-in-L2-sense}\]</span></p>
</div>
<p>It turns out that <span class="math inline">\(Y^{\star}\)</span> is the right candidate for the conditional expectation.</p>
<div class="center">
<p>Figure. An illustration of the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> as an orthogonal projection of <span class="math inline">\(Y\)</span> onto the subspace <span class="math inline">\(L^2(\Omega,\sigma(X),\mathbb{P})\)</span>.</p>
</div>
<div class="thm">
<p><span id="th:existence-and-uniqueness-of-the-conditional-expectation" label="th:existence-and-uniqueness-of-the-conditional-expectation"></span>(Existence and uniqueness of the conditional expectation) Let <span class="math inline">\(X\)</span> be a random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Let <span class="math inline">\(Y\)</span> be a random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>. Then the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is the random variable <span class="math inline">\(Y^{\star}\)</span> given in the equation (<a href="#eq:Y-star-is-the-closest-to-Y-in-L2-sense" data-reference-type="ref" data-reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense">[eq:Y-star-is-the-closest-to-Y-in-L2-sense]</a>). Namely, it is the random variable in <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> that is closest to <span class="math inline">\(Y\)</span> in the <span class="math inline">\(L^{2}\)</span>-distance.</p>
<p>In particular we have the following:</p>
<p>1) It is the orthogonal projection of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>, that is <span class="math inline">\(Y-Y^{\star}\)</span> is orthogonal to any random variables in the subspace <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>.</p>
<p>2) It is unique.</p>
</div>
<div class="rem*">
<p>This result reinforces the meaning of the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> as the best estimation of <span class="math inline">\(Y\)</span> given the information of <span class="math inline">\(X\)</span>: it is the closest random variable to <span class="math inline">\(Y\)</span> among all the functions of <span class="math inline">\(X\)</span> in the sense of <span class="math inline">\(L^{2}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> We write for short <span class="math inline">\(L^{2}(X)\)</span> for the subspace <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>. Let <span class="math inline">\(Y^{\star}\)</span> be as in equation (<a href="#eq:Y-star-is-the-closest-to-Y-in-L2-sense" data-reference-type="ref" data-reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense">[eq:Y-star-is-the-closest-to-Y-in-L2-sense]</a>). We show successively that (1) <span class="math inline">\(Y-Y^{\star}\)</span> is orthogonal to any element of <span class="math inline">\(L^{2}(X)\)</span>, so it is the orthogonal projection (2) <span class="math inline">\(Y^{\star}\)</span> has the properties of conditional expectation in definition (<a href="#eq:definition-conditional-expectation" data-reference-type="ref" data-reference="eq:definition-conditional-expectation">[eq:definition-conditional-expectation]</a>) (3) <span class="math inline">\(Y^{\star}\)</span> is unique.</p>
<p>(1) Let <span class="math inline">\(W=g(X)\)</span> be a random variable in <span class="math inline">\(L^{2}(X)\)</span>. We show that <span class="math inline">\(W\)</span> is orthogonal to <span class="math inline">\(Y-Y^{\star}\)</span>; that is <span class="math inline">\(\mathbb{E}[(Y-Y^{\star})W]=0\)</span>. This should be intuitively clear from figure above. On the one hand, we have by developing the square:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[(W-(Y-Y^{\star}))^{2}] &amp; =\mathbb{E}[W^{2}-2W(Y-Y^{\star})+(Y-Y^{\star})^{2}]\nonumber \\
&amp; =\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})]+\mathbb{E}(Y-Y^{\star})^{2}]\label{eq:developing-the-square}
\end{aligned}\]</span></p>
<p>On the other hand, <span class="math inline">\(Y^{\star}+W\)</span> is an arbitrary vector in <span class="math inline">\(L^{2}(X)\)</span>(it is a linear combination of the elements in <span class="math inline">\(L^{2}(X)\)</span>), we must have from equation (<a href="#eq:Y-star-is-the-closest-to-Y-in-L2-sense" data-reference-type="ref" data-reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense">[eq:Y-star-is-the-closest-to-Y-in-L2-sense]</a>):</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[(W-(Y-Y^{\star}))^{2}] &amp; =\mathbb{E}[(Y-(Y^{\star}+W))^{2}]\nonumber \\
&amp; \geq\inf_{Z\in L^{2}(X)}\mathbb{E}[(Y-Z)^{2}]\nonumber \\
&amp; =\mathbb{E}[(Y-Y^{\star})^{2}]\label{eq:lower-bound}
\end{aligned}\]</span></p>
<p>Putting the last two equations (<a href="#eq:developing-the-square" data-reference-type="ref" data-reference="eq:developing-the-square">[eq:developing-the-square]</a>), (<a href="#eq:lower-bound" data-reference-type="ref" data-reference="eq:lower-bound">[eq:lower-bound]</a>) together, we get that for any <span class="math inline">\(W\in L^{2}(X)\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})] &amp; \geq0
\end{aligned}\]</span></p>
<p>In particular, this also holds for <span class="math inline">\(aW\)</span>, in which case we get:</p>
<p><span class="math display">\[\begin{aligned}
a^{2}\mathbb{E}[W^{2}]-2a\mathbb{E}[W(Y-Y^{\star})] &amp; \geq0\\
\implies a\left\{ a\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})]\right\}  &amp; \geq0
\end{aligned}\]</span></p>
<p>If <span class="math inline">\(a&gt;0\)</span>, then:</p>
<p><span class="math display">\[a\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})]\geq0\label{eq:case-when-a-gt-zero}\]</span></p>
<p>whereas if <span class="math inline">\(a&lt;0\)</span>, then the sign changes upon dividing throughout by <span class="math inline">\(a\)</span>, and we have:</p>
<p><span class="math display">\[a\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})]\leq0\label{eq:case-when-a-lt-zero}\]</span></p>
<p>Rearranging (<a href="#eq:case-when-a-gt-zero" data-reference-type="ref" data-reference="eq:case-when-a-gt-zero">[eq:case-when-a-gt-zero]</a>) yields:</p>
<p><span class="math display">\[\mathbb{E}[W(Y-Y^{\star})]\leq a\mathbb{E}[W^{2}]/2\label{eq:case-when-a-gt-zero-rearranged}\]</span></p>
<p>Rearranging (<a href="#eq:case-when-a-lt-zero" data-reference-type="ref" data-reference="eq:case-when-a-lt-zero">[eq:case-when-a-lt-zero]</a>) yields:</p>
<p><span class="math display">\[\mathbb{E}[W(Y-Y^{\star})]\geq a\mathbb{E}[W^{2}]/2\label{eq:case-when-a-lt-zero-rearranged}\]</span></p>
<p>Since (<a href="#eq:case-when-a-gt-zero-rearranged" data-reference-type="ref" data-reference="eq:case-when-a-gt-zero-rearranged">[eq:case-when-a-gt-zero-rearranged]</a>) holds for all <span class="math inline">\(a&gt;0\)</span>, the stronger inequality, <span class="math inline">\(\mathbb{E}[W(Y-Y^{\star})]\leq0\)</span> must hold. Since, (<a href="#eq:case-when-a-lt-zero-rearranged" data-reference-type="ref" data-reference="eq:case-when-a-lt-zero-rearranged">[eq:case-when-a-lt-zero-rearranged]</a>) holds for all <span class="math inline">\(a&lt;0\)</span>, the stronger inequality <span class="math inline">\(\mathbb{E}[W(Y-Y^{\star})]\geq0\)</span> must hold. Consequently,</p>
<p><span class="math display">\[\mathbb{E}[W(Y-Y^{\star})]=0\]</span></p>
<p>(2) It is clear that <span class="math inline">\(Y^{\star}\)</span> is a function of <span class="math inline">\(X\)</span> by construction, since it is in <span class="math inline">\(L^{2}(X)\)</span>. Moreover, for any <span class="math inline">\(W\in L^{2}(X)\)</span>, we have from (1) that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[W(Y-Y^{\star})] &amp; =0
\end{aligned}\]</span></p>
<p>which is the second defining property of conditional expectations.</p>
<p>(3) Lastly, suppose there is another element <span class="math inline">\(Y'\)</span> that is in <span class="math inline">\(L^{2}(X)\)</span> that minimizes the distance to <span class="math inline">\(Y\)</span>. Then we would get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[(Y-Y')^{2}] &amp; =\mathbb{E}[(Y-Y^{\star}+Y^{\star}-Y')^{2}]\\
&amp; =\mathbb{E}[(Y-Y^{\star})^{2}]+2\mathbb{E}[(Y-Y^{\star})(Y^{\star}-Y')]+\mathbb{E}[(Y^{\star}-Y')^{2}]\\
&amp; =\mathbb{E}[(Y-Y^{\star})^{2}]+0+\mathbb{E}[(Y^{\star}-Y')^{2}]\\
&amp; \quad\left\{ (Y^{\star}-Y')\in L^{2}(X)\perp(Y-Y^{\star})\right\}
\end{aligned}\]</span></p>
<p>where we used the fact, that <span class="math inline">\(Y^{\star}-Y'\)</span> is a vector in <span class="math inline">\(L^{2}(X)\)</span> and the orthogonality of <span class="math inline">\(Y-Y^{\star}\)</span> with <span class="math inline">\(L^{2}(X)\)</span> as in (1). But, this implies that:</p>
<p><span class="math display">\[\begin{aligned}
\cancel{\mathbb{E}[(Y-Y')^{2}]} &amp; =\cancel{\mathbb{E}[(Y-Y^{\star})^{2}]}+\mathbb{E}[(Y^{\star}-Y')^{2}]\\
\mathbb{E}[(Y^{\star}-Y')^{2}] &amp; =0
\end{aligned}\]</span></p>
<p>So, <span class="math inline">\(Y^{\star}=Y'\)</span> almost surely. ◻</p>
</div>
<div class="example">
<p><strong>Conditional Expectation of continuous random variables.</strong> Let <span class="math inline">\((X,Y)\)</span> be two random variables with joint density <span class="math inline">\(f_{X,Y}(x,y)\)</span> on <span class="math inline">\(\mathbf{R}^{2}\)</span>. Suppose for simplicity, that <span class="math inline">\(\int_{\mathbf{R}}f(x,y)dx&gt;0\)</span> for every <span class="math inline">\(y\)</span> belonging to <span class="math inline">\(\mathbf{R}\)</span>. Show that the conditional expectation <span class="math inline">\(\mathbf{E}[Y|X]\)</span> equals <span class="math inline">\(h(X)\)</span> where <span class="math inline">\(h\)</span> is the function:</p>
<p><span class="math display">\[\begin{aligned}
h(x) &amp; =\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\label{eq:conditional-expectation-of-continuous-random-variables}
\end{aligned}\]</span></p>
<p>In particular, verify that <span class="math inline">\(\mathbf{E}[\mathbf{E}[Y|X]]=\mathbf{E}[Y]\)</span>.</p>
<p><em>Hint</em>: To prove this, verify that the above formula satisfies both the properties of conditional expectations; then invoke uniqueness to finish it off.</p>
</div>
<div class="sol*">
<p>(i) The density function <span class="math inline">\(f_{X,Y}(x,y)\)</span> is a map <span class="math inline">\(f:\mathbf{R}^{2}\to\mathbf{R}\)</span>. The integral <span class="math inline">\(\int_{y=-\infty}^{y=+\infty}yf_{X,Y}(x_{0},y)dy\)</span> is the area under the curve <span class="math inline">\(yf(x,y)\)</span> at the point <span class="math inline">\(x=x_{0}\)</span>. Let’s call it <span class="math inline">\(A(x_{0})\)</span>. If instead, we have an arbitrary <span class="math inline">\(x\)</span>, <span class="math inline">\(\int_{y=-\infty}^{y=+\infty}yf_{X,Y}(x,y)dy\)</span> represents the area <span class="math inline">\(A(x)\)</span> of an arbitrary slice of the surface <span class="math inline">\(yf_{X,Y}\)</span> at the point <span class="math inline">\(x\)</span>. Hence, it is a function of <span class="math inline">\(x\)</span>. The denominator <span class="math inline">\(\int_{\mathbf{R}}f_{X,Y}(x,y)dy=f_{X}(x)\)</span>, the density of <span class="math inline">\(X\)</span>, which is a function of <span class="math inline">\(x\)</span>. Hence, the ratio is a function of <span class="math inline">\(x\)</span>.</p>
<p>(ii) Let <span class="math inline">\(g(X)\)</span> is a bounded random variable. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[g(X)(Y-h(X))] &amp; =\mathbf{E}[Yg(X)]-\mathbf{E}[g(X)h(X)]\\
&amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx-\int_{\mathbf{R}}g(x)h(x)f(x)dx\\
&amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx\\
&amp; -\int_{\mathbf{R}}\begin{array}{c}
g(x)\cdot\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\end{array}\cdot\int_{\mathbf{R}}f_{X,Y}(x,y)dy\ dx\\
&amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx\\
&amp; -\int_{\mathbf{R}}\begin{array}{c}
g(x)\cdot\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\cancel{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}}\end{array}\cdot\cancel{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\ dx\\
&amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx-\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)\cdot dx\cdot dy\\
&amp; =0
\end{aligned}\]</span></p>
</div>
<p>Thus, <span class="math inline">\(h(X)\)</span> is a valid candidate for the conditional expectation <span class="math inline">\(\mathbf{E}[Y|X]\)</span>. Moreover, by the existence and uniqueness theorem (<a href="#th:existence-and-uniqueness-of-the-conditional-expectation" data-reference-type="ref" data-reference="th:existence-and-uniqueness-of-the-conditional-expectation">[th:existence-and-uniqueness-of-the-conditional-expectation]</a>), <span class="math inline">\(\mathbf{E}[Y|X]\)</span> is unique and equals <span class="math inline">\(h(X)\)</span>.</p>
</section>
<section id="conditioning-on-several-random-variables." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="conditioning-on-several-random-variables.">Conditioning on several random variables.</h4>
<p>We would like to generalize the conditional expectation to the case when we condition on the information of more than one random variable. Taking the <span class="math inline">\(L^{2}\)</span> point of view, we should expect that the conditional expectation is the orthogonal projection of the given random variable on the subspace generated by square integrable functions of all the variables on which we condition.</p>
<p>It is now useful to study sigma-fields, an object that was defined in chapter 1.</p>
<div class="defn">
<p><span id="def:sigma-field" label="def:sigma-field"></span>(Sigma-Field) A sigma-field or sigma-algebra <span class="math inline">\(\mathcal{F}\)</span> of a sample space <span class="math inline">\(\Omega\)</span> is a collection of all measurable events with the following properties:</p>
<p>(1) <span class="math inline">\(\Omega\)</span> is in <span class="math inline">\(\mathcal{F}\)</span>.</p>
<p>(2) Closure under complement. If <span class="math inline">\(A\in\mathcal{F}\)</span>, then <span class="math inline">\(A^{C}\in\mathcal{F}\)</span>.</p>
<p>(3) Closure under countable unions. If <span class="math inline">\(A_{1},A_{2},\ldots,\in\mathcal{F}\)</span>, then <span class="math inline">\(\bigcup_{n=1}^{\infty}A_{n}\in\mathcal{F}\)</span>.</p>
</div>
<p>Such objects play a fundamental role in the rigorous study of probability and real analysis in general. We will focus on the intuition behind them. First let’s mention some examples of sigma-fields of a given sample space <span class="math inline">\(\Omega\)</span> to get acquainted with the concept.</p>
<div class="example">
<p>(Examples of sigma-fields).</p>
<p>(1) <em>The trivial sigma-field</em>. Note that the collection of events <span class="math inline">\(\{\emptyset,\Omega\}\)</span> is a sigma-field of <span class="math inline">\(\Omega\)</span>. We generally denote it by <span class="math inline">\(\mathcal{F}_{0}\)</span>.</p>
<p>(2) <em>The <span class="math inline">\(\sigma\)</span>-field generated by an event <span class="math inline">\(A\)</span>.</em> Let <span class="math inline">\(A\)</span> be an event that is not <span class="math inline">\(\emptyset\)</span> and not the entire <span class="math inline">\(\Omega\)</span>. Then the smallest sigma-field containing <span class="math inline">\(A\)</span> ought to be:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{F}_{1} &amp; =\{\emptyset,A,A^{C},\Omega\}
\end{aligned}\]</span></p>
<p>This sigma-field is denoted by <span class="math inline">\(\sigma(A)\)</span>.</p>
<p>(3) The <em>sigma-field generated by a random variable <span class="math inline">\(X\)</span>.</em></p>
<p>We now define the <span class="math inline">\(\mathcal{F}_{X}\)</span> as follows:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{F}_{X} &amp; =X^{-1}(\mathcal{B}):=\{\omega:X(\omega)\in B\},\forall B\in\mathcal{B}(\mathbf{R})
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\mathcal{B}\)</span> is the Borel <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbf{R}\)</span>. <span class="math inline">\(\mathcal{F}_{X}\)</span> is sometimes denoted as <span class="math inline">\(\sigma(X)\)</span>. <span class="math inline">\(\mathcal{F}_{X}\)</span>is the set of all events pertaining to <span class="math inline">\(X\)</span>. It is a sigma-algebra because:</p>
<p>(i) <span class="math inline">\(\Omega\in\sigma(X)\)</span> because <span class="math inline">\(\Omega=\{\omega:X(\omega)\in\mathbf{R}\}\)</span> and <span class="math inline">\(\mathbf{R}\in\mathcal{B}(\mathbf{R})\)</span>.</p>
<p>(ii) Let any event <span class="math inline">\(C\in\sigma(X)\)</span>. We need to show that <span class="math inline">\(\Omega\setminus C\in\sigma(X)\)</span>.</p>
<p>Since <span class="math inline">\(C\in\sigma(X)\)</span>, there exists <span class="math inline">\(A\in\mathcal{B}(\mathbf{R})\)</span>, such that:</p>
<p><span class="math display">\[\begin{aligned}
C &amp; =\{\omega\in\Omega:X(\omega)\in A\}
\end{aligned}\]</span></p>
<p>Now, we calculate:</p>
<p><span class="math display">\[\begin{aligned}
\Omega\setminus C &amp; =\{\omega\in\Omega:X(\omega)\in\mathbf{R}\setminus A\}
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(\mathcal{B}(\mathbf{R})\)</span> is a sigma-algebra, it is closed under complementation. Hence, if <span class="math inline">\(A\in\mathcal{B}(\mathbf{R})\)</span>, it implies that <span class="math inline">\(\mathbf{R}\setminus A\in\mathcal{B}(\mathbf{R})\)</span>. So, <span class="math inline">\(\Omega\setminus C\in\sigma(X)\)</span>.</p>
<p>(iii) Consider a sequence of events <span class="math inline">\(C_{1},C_{2},\ldots,C_{n},\ldots\in\sigma(X)\)</span>. We need to prove that <span class="math inline">\(\bigcup_{n=1}^{\infty}C_{n}\in\sigma(X)\)</span>.</p>
<p>Since <span class="math inline">\(C_{n}\in\sigma(X)\)</span>, there exists <span class="math inline">\(A_{n}\in\mathcal{B}(\mathbf{R})\)</span> such that:</p>
<p><span class="math display">\[\begin{aligned}
C_{n} &amp; =\{\omega\in\Omega:X(\omega)\in A_{n}\}
\end{aligned}\]</span></p>
<p>Now, we calculuate:</p>
<p><span class="math display">\[\begin{aligned}
\bigcup_{n=1}C_{n} &amp; =\{\omega\in\Omega:X(\omega)\in\bigcup_{n=1}^{\infty}A_{n}\}
\end{aligned}\]</span></p>
<p>But, <span class="math inline">\(\bigcup_{n=1}^{\infty}A_{n}\in\mathcal{B}(\mathbf{R})\)</span>. So, <span class="math inline">\(\bigcup_{n=1}^{\infty}C_{n}\in\sigma(X)\)</span>.</p>
<p>Consequently, <span class="math inline">\(\sigma(X)\)</span> is indeed a <span class="math inline">\(\sigma\)</span>-algebra.</p>
<p>Intuitively, we think of <span class="math inline">\(\sigma(X)\)</span> as containing all information about <span class="math inline">\(X\)</span>.</p>
<p>(4) <em>The sigma-field generated by a stochastic process <span class="math inline">\((X_{s},s\leq t)\)</span>.</em> Let <span class="math inline">\((X_{s},s\geq0)\)</span> be a stochastic process. Consider the process restricted to <span class="math inline">\([0,t]\)</span>, <span class="math inline">\((X_{s},s\leq t)\)</span>. We consider the smallest sigma-field containing all events pertaining to the random variables <span class="math inline">\(X_{s},s\leq t\)</span>. We denote it by <span class="math inline">\(\sigma(X_{s},s\leq t)\)</span> or <span class="math inline">\(\mathcal{F}_{t}\)</span>.</p>
</div>
<p>The sigma-fields on <span class="math inline">\(\Omega\)</span> have a natural (partial) ordering: two sigma-fields <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{F}\)</span> of <span class="math inline">\(\Omega\)</span> are such that <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> if all the events in <span class="math inline">\(\mathcal{G}\)</span> are in <span class="math inline">\(\mathcal{F}\)</span>. For example, the trivial <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{F}_{0}=\{\emptyset,\Omega\}\)</span> is contained in all the <span class="math inline">\(\sigma\)</span>-fields of <span class="math inline">\(\Omega\)</span>. Clearly, the <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{F}_{t}=\sigma(X_{s},s\leq t)\)</span> is contained in <span class="math inline">\(\mathcal{F}_{t'}\)</span> if <span class="math inline">\(t\leq t'\)</span>.</p>
<p>If all the events pertaining to a random variable <span class="math inline">\(X\)</span> are in the <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{G}\)</span> (and thus we can compute <span class="math inline">\(\mu(X^{-1}((a,b]))\)</span>), we will say that <span class="math inline">\(X\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable. This means that all information about <span class="math inline">\(X\)</span> is contained in <span class="math inline">\(\mathcal{G}\)</span>.</p>
<div class="defn">
<p>Let <span class="math inline">\(X\)</span> be a random variable defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Consider another <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span>. Then <span class="math inline">\(X\)</span> is said to be <span class="math inline">\(\mathcal{G}\)</span>-measurable, if and only if:</p>
<p><span class="math display">\[\begin{aligned}
\{\omega:X(\omega)\in(a,b]\} &amp; \in\mathcal{G}\text{ for all intervals }(a,b]\in\mathbf{R}
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>(<span class="math inline">\(\mathcal{F}_{0}\)</span>-measurable random variables). Consider the trivial sigma-field <span class="math inline">\(\mathcal{F}_{0}=\{\emptyset,\Omega\}\)</span>. A random variable that is <span class="math inline">\(\mathcal{F}_{0}\)</span>-measurable must be a constant. Indeed, we have that for any interval <span class="math inline">\((a,b]\)</span>, <span class="math inline">\(\{\omega:X(\omega)\in(a,b]\}=\emptyset\)</span> or <span class="math inline">\(\{\omega:X(\omega)\in(a,b]\}=\Omega\)</span>. This can only hold if <span class="math inline">\(X\)</span> takes a single value.</p>
</div>
<div class="example">
<p>[]{#ex:sigma(X)-measurable-random-variables-example label=“ex:sigma(X)-measurable-random-variables-example”}(<span class="math inline">\(\sigma(X)\)</span>-measurable random variables). Let <span class="math inline">\(X\)</span> be a given random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Roughly speaking, a <span class="math inline">\(\sigma(X)\)</span>-measurable random variable is determined by the information of <span class="math inline">\(X\)</span> only. Here is the simplest example of a <span class="math inline">\(\sigma(X)\)</span>-measurable random variable. Take the indicator function <span class="math inline">\(Y=\mathbf{1}_{\{X\in B\}}\)</span> for some event <span class="math inline">\(\{X\in B\}\)</span> pertaining to <span class="math inline">\(X\)</span>. Then the pre-images <span class="math inline">\(\{\omega:Y(\omega)\in(a,b]\}\)</span> are either <span class="math inline">\(\emptyset\)</span>, <span class="math inline">\(\{X\in B\}\)</span>, <span class="math inline">\(\{X\in B^{C}\}\)</span> or <span class="math inline">\(\Omega\)</span> depending on whether <span class="math inline">\(0,1\)</span> are in <span class="math inline">\((a,b]\)</span> or not. All of these events are in <span class="math inline">\(\sigma(X)\)</span>. More generally, one can construct a <span class="math inline">\(\sigma(X)\)</span>-measurable random variable by taking linear combinations of indicator functions of events of the form <span class="math inline">\(\{X\in B\}\)</span>.</p>
<p>It turns out that any (Borel measurable) function of <span class="math inline">\(X\)</span> can be approximated by taking limits of such simple functions.</p>
<p>Concretely, this translates to the following statement:</p>
<p><span class="math display">\[\text{If }Y\text{ is \ensuremath{\sigma}(X)-measurable, then Y=g(X) for some function g}\]</span></p>
<p>In the same way, if <span class="math inline">\(Z\)</span> is <span class="math inline">\(\sigma(X,Y)\)</span>-measurable, then <span class="math inline">\(Z=h(X,Y)\)</span> for some <span class="math inline">\(h\)</span>. These facts can be proved rigorously using measure theory.</p>
</div>
<p>We are ready to give the general definition of conditional expectation.</p>
<div class="example">
<p>(Coin-Tossing Space). Suppose a coin is tossed infinitely many times. Let <span class="math inline">\(\Omega\)</span> be the set of all infinite sequences of <span class="math inline">\(H\)</span>s and <span class="math inline">\(T\)</span>s. A generic element of <span class="math inline">\(\Omega\)</span> is denoted by <span class="math inline">\(\omega_{1}\omega_{2}\ldots\)</span>, where <span class="math inline">\(\omega_{n}\)</span> indicates the result of the <span class="math inline">\(n\)</span>th coin toss. <span class="math inline">\(\Omega\)</span> is an uncountable sample space. The trivial sigma-field <span class="math inline">\(\mathcal{F}_{0}=\{\emptyset,\Omega\}\)</span>. Assume that we don’t know anything about the outcome of the experiement. Even without any information, we know that the true <span class="math inline">\(\omega\)</span> belongs to <span class="math inline">\(\Omega\)</span> and does not belong to <span class="math inline">\(\emptyset\)</span>. It is the information learned at time <span class="math inline">\(0\)</span>.</p>
<p>Next, assume that we know the outcome of the first coin toss. Define <span class="math inline">\(A_{H}=\{\omega:\omega_{1}=H\}\)</span>=set of all sequences beginning with <span class="math inline">\(H\)</span> and <span class="math inline">\(A_{T}=\{\omega:\omega_{1}=T\}\)</span>=set of all sequences beginning with <span class="math inline">\(T\)</span>. The four sets resolved by the first coin-toss form the the <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{F}_{1}=\{\emptyset,A_{H},A_{T},\Omega\}\)</span>. We shall think of this <span class="math inline">\(\sigma\)</span>-field as containing the information learned by knowing the outcome of the first coin toss. More precisely, if instead of being told about the first coin toss, we are told for each set in <span class="math inline">\(\mathcal{F}_{1}\)</span>, whether or not the true <span class="math inline">\(\omega\)</span> belongs to that set, then we know the outcome of the first coin toss and nothing more.</p>
<p>If we are told the first two coin tosses, we obtain a finer resolution. In particular, the four sets:</p>
<p><span class="math display">\[\begin{aligned}
A_{HH} &amp; =\{\omega:\omega_{1}=H,\omega_{2}=H\}\\
A_{HT} &amp; =\{\omega:\omega_{1}=H,\omega_{2}=T\}\\
A_{TH} &amp; =\{\omega:\omega_{1}=T,\omega_{2}=H\}\\
A_{TT} &amp; =\{\omega:\omega_{1}=T,\omega_{2}=T\}
\end{aligned}\]</span></p>
<p>are resolved. Of course, the sets in <span class="math inline">\(\mathcal{F}_{1}\)</span> are resolved. Whenever a set is resolved, so is its complement, which means that <span class="math inline">\(A_{HH}^{C}\)</span>, <span class="math inline">\(A_{HT}^{C}\)</span>, <span class="math inline">\(A_{TH}^{C}\)</span> and <span class="math inline">\(A_{TT}^{C}\)</span> are resolved, so is their union which means that <span class="math inline">\(A_{HH}\cup A_{TH}\)</span>, <span class="math inline">\(A_{HH}\cup A_{TT}\)</span>, <span class="math inline">\(A_{HT}\cup A_{TH}\)</span> and <span class="math inline">\(A_{HT}\cup A_{TT}\)</span> are resolved. The other two pair-wise unions <span class="math inline">\(A_{HH}\cup A_{HT}=A_{H}\)</span> and <span class="math inline">\(A_{TH}\cup A_{TT}=A_{T}\)</span> are already resolved. Finally, the triple unions are also resolved, because <span class="math inline">\(A_{HH}\cup A_{HT}\cup A_{TH}=A_{TT}^{C}\)</span> and so forth. Hence, the information pertaining to the second coin-toss is contained in:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{F}_{2} &amp; =\{\emptyset,\Omega,\\
&amp; A_{H},A_{T},\\
&amp; A_{HH},A_{HT},A_{TH},A_{TT},\\
&amp; A_{HH}^{C},A_{HT}^{C},A_{TH}^{C},A_{TT}^{C},\\
&amp; A_{HH}\cup A_{TH},A_{HH}\cup A_{TT},A_{HT}\cup A_{TH},A_{HT}\cup A_{TT}\}
\end{aligned}\]</span></p>
<p>Hence, if the outcome of the first two coin tosses is known, all of the events in <span class="math inline">\(\mathcal{F}_{2}\)</span> are resolved - we exactly know, if each event has ocurred or not. <span class="math inline">\(\mathcal{F}_{2}\)</span> is the information learned by observing the first two coin tosses.</p>
</div>
<div class="xca">
<p>(<strong>Exercises on sigma-fields</strong>).</p>
<p>(a) Let <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> be two proper subsets of <span class="math inline">\(\Omega\)</span> such that <span class="math inline">\(A\cap B\neq\emptyset\)</span> and <span class="math inline">\(A\cup B\neq\Omega\)</span>. Write down <span class="math inline">\(\sigma(\{A,B\})\)</span>, the smallest sigma-field containing <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> explicitly. What if <span class="math inline">\(A\cap B=\emptyset\)</span>?</p>
<p>(b) The Borel sigma-field is the smallest sigma-field containing intervals of the form <span class="math inline">\((a,b]\)</span> in <span class="math inline">\(\mathbf{R}\)</span>. Show that all singletons <span class="math inline">\(\{b\}\)</span> are in <span class="math inline">\(\mathcal{B}(\mathbf{R})\)</span> by writing <span class="math inline">\(\{b\}\)</span> as a countable intersection of intervals <span class="math inline">\((a,b]\)</span>. Conclude that all open intervals <span class="math inline">\((a,b)\)</span> and all closed intervals <span class="math inline">\([a,b]\)</span> are in <span class="math inline">\(\mathcal{B}(\mathbf{R})\)</span>. Is the subset <span class="math inline">\(\mathbf{Q}\)</span> of rational numbers a Borel set?</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (a) The sigma-field generated by the two events <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
\sigma(\{A,B\}) &amp; =\{\emptyset,\Omega,\\
&amp; A,B,A^{C},B^{C},\\
&amp; A\cup B,A\cap B,\\
&amp; A\cup B^{C},A^{C}\cup B,A^{C}\cup B^{C},\\
&amp; A\cap B^{C},A^{C}\cap B,A^{C}\cap B^{C},\\
&amp; (A\cup B)\cap(A\cap B)^{C},\\
&amp; (A\cup B)^{C}\cup(A\cap B)\}
\end{aligned}\]</span></p>
<p>(b) Firstly, recall that:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{B}(\mathbf{R}) &amp; =\bigcap_{\alpha\in\Lambda}\mathcal{F}_{\alpha}=\bigcap\sigma(\{I:I\text{ is an interval }(a,b]\subseteq\mathbf{R}\})
\end{aligned}\]</span></p>
<p>We can write:</p>
<p><span class="math display">\[\begin{aligned}
\{b\} &amp; =\bigcap_{n=1}^{\infty}\left(b-\frac{1}{n},b\right]
\end{aligned}\]</span></p>
<p>As <span class="math inline">\(\mathcal{B}(\mathbf{R})\)</span> is a sigma-field, it is closed under countable intersections. Hence, the singleton set <span class="math inline">\(\{b\}\)</span>is a Borel set.</p>
<p>Similarly, we can write, any open interval as the countable union:</p>
<p><span class="math display">\[\begin{aligned}
(a,b) &amp; =\bigcup_{n=1}^{\infty}\left(a,b-\frac{1}{n}\right]
\end{aligned}\]</span></p>
<p>We can convince ourselves, that equality indeed holds. Let <span class="math inline">\(x\in(a,b)\)</span> and choose <span class="math inline">\(N\)</span>, such that <span class="math inline">\(\frac{1}{N}&lt;|b-x|\)</span>. Then, for all <span class="math inline">\(n\geq N\)</span>, <span class="math inline">\(x\in(a,b-1/n]\)</span>. Thus, it belongs to the RHS. In the reverse direction, let <span class="math inline">\(x\)</span> belong to <span class="math inline">\(\bigcup_{n=1}^{\infty}\left(a,b-\frac{1}{n}\right]\)</span>. So, <span class="math inline">\(x\)</span> belongs to atleast one of these sets. Therefore, <span class="math inline">\(x\in(a,b)\)</span> is trivially true. So, the two sets are equal.</p>
<p>Hence, open intervals are Borel sets.</p>
<p>Similarly, we may write:</p>
<p><span class="math display">\[\begin{aligned}
[a,b] &amp; =\bigcap_{n=1}^{\infty}\left(a-\frac{1}{n},b+\frac{1}{n}\right)
\end{aligned}\]</span></p>
<p>Consequently, closed intervals are Borel sets. Since <span class="math inline">\(\mathbf{Q}\)</span> is countable, it is a Borel set. Moreover, the empty set <span class="math inline">\(\emptyset\)</span> and <span class="math inline">\(\mathbf{R}\)</span> are Borel sets. So, <span class="math inline">\(\mathbf{R}\backslash\mathbf{Q}\)</span> is also a Borel set. ◻</p>
</div>
<div class="xca">
<p>Let <span class="math inline">\((X,Y)\)</span> be a Gaussian vector with mean <span class="math inline">\(0\)</span> and covariance matrix</p>
<p><span class="math display">\[\begin{aligned}
C &amp; =\left[\begin{array}{cc}
1 &amp; \rho\\
\rho &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>for <span class="math inline">\(\rho\in(-1,1)\)</span>. We verify that the example (<a href="#ex:conditional-expectation-of-gaussian-vectors" data-reference-type="ref" data-reference="ex:conditional-expectation-of-gaussian-vectors">[ex:conditional-expectation-of-gaussian-vectors]</a>) and exercise (<a href="#ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables" data-reference-type="ref" data-reference="ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables">[ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables]</a>) yield the same conditional expectation.</p>
<p>(a) Use equation (<a href="#eq:conditional-expectation-of-gaussian-vector" data-reference-type="ref" data-reference="eq:conditional-expectation-of-gaussian-vector">[eq:conditional-expectation-of-gaussian-vector]</a>) to show that <span class="math inline">\(\mathbf{E}[Y|X]=\rho X\)</span>.</p>
<p>(b) Write down the joint PDF <span class="math inline">\(f(x,y)\)</span> of <span class="math inline">\((X,Y)\)</span>.</p>
<p>(c) Show that <span class="math inline">\(\int_{\mathbf{R}}yf(x,y)dy=\rho x\)</span> and that <span class="math inline">\(\int_{\mathbf{R}}f(x,y)dy=1\)</span>.</p>
<p>(d) Deduce that <span class="math inline">\(\mathbf{E}[Y|X]=\rho X\)</span> using the equation (<a href="#eq:conditional-expectation-of-continuous-random-variables" data-reference-type="ref" data-reference="eq:conditional-expectation-of-continuous-random-variables">[eq:conditional-expectation-of-continuous-random-variables]</a>).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (a) Since <span class="math inline">\((X,Y)\)</span> have mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>, it follows that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(X-EX)(Y-EY)] &amp; =\mathbf{E}(XY)\\
\sqrt{(\mathbf{E}[X^{2}]-(\mathbf{E}X)^{2})}\cdot\sqrt{(\mathbf{E}[Y^{2}]-(\mathbf{E}Y)^{2})} &amp; =\sqrt{(1-0)(1-0)}\\
&amp; =1
\end{aligned}\]</span></p>
<p>and therefore,</p>
<p><span class="math display">\[\begin{aligned}
\rho &amp; =\frac{\mathbf{E}(XY)}{1}=\frac{\mathbf{E}[XY]}{\mathbf{E}[X^{2}]}
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\((X,Y)\)</span> is a Gaussian vector, using (<a href="#eq:conditional-expectation-of-gaussian-vector" data-reference-type="ref" data-reference="eq:conditional-expectation-of-gaussian-vector">[eq:conditional-expectation-of-gaussian-vector]</a>), we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|X] &amp; =\frac{\mathbf{E}[XY]}{\mathbf{E}[X^{2}]}X=\rho X
\end{aligned}\]</span></p>
<p>(b) Consider the augmented matrix <span class="math inline">\([C|I]\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
[C|I] &amp; =\left[\left.\begin{array}{cc}
1 &amp; \rho\\
\rho &amp; 1
\end{array}\right|\begin{array}{cc}
1 &amp; 0\\
0 &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>Performing <span class="math inline">\(R_{2}=R_{2}-\rho R_{1}\)</span>, the above system is row-equivalent to:</p>
<p><span class="math display">\[\left[\left.\begin{array}{cc}
1 &amp; \rho\\
0 &amp; 1-\rho^{2}
\end{array}\right|\begin{array}{cc}
1 &amp; 0\\
-\rho &amp; 1
\end{array}\right]\]</span></p>
<p>Performing <span class="math inline">\(R_{2}=\frac{1}{1-\rho^{2}}R_{2}\)</span>, the above system is row-equivalent to:</p>
<p><span class="math display">\[\left[\begin{array}{cc}
1 &amp; \rho\\
0 &amp; 1
\end{array}\left|\begin{array}{cc}
1 &amp; 0\\
\frac{-\rho}{1-\rho^{2}} &amp; \frac{1}{1-\rho^{2}}
\end{array}\right.\right]\]</span></p>
<p>Performing <span class="math inline">\(R_{1}=R_{1}-\rho R_{2}\)</span>, we have:</p>
<p><span class="math display">\[\left[\begin{array}{cc}
1 &amp; 0\\
0 &amp; 1
\end{array}\left|\begin{array}{cc}
\frac{1}{1-\rho^{2}} &amp; -\frac{\rho}{1-\rho^{2}}\\
\frac{-\rho}{1-\rho^{2}} &amp; \frac{1}{1-\rho^{2}}
\end{array}\right.\right]\]</span></p>
<p>Thus, <span class="math display">\[\begin{aligned}
C^{-1} &amp; =\frac{1}{1-\rho^{2}}\left[\begin{array}{cc}
1 &amp; -\rho\\
-\rho &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>Moreover, <span class="math inline">\(\det C=1-\rho^{2}.\)</span></p>
<p>Therefore, the joint density of <span class="math inline">\((X,Y)\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
f(x,y) &amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}\left[\begin{array}{cc}
x &amp; y\end{array}\right]\left[\begin{array}{cc}
1 &amp; -\rho\\
-\rho &amp; 1
\end{array}\right]\left[\begin{array}{c}
x\\
y
\end{array}\right]\right]\\
&amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}\left[\begin{array}{cc}
x-\rho y &amp; -\rho x+y\end{array}\right]\left[\begin{array}{c}
x\\
y
\end{array}\right]\right]\\
&amp; \frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}(x^{2}-2\rho xy+y^{2})\right]
\end{aligned}\]</span></p>
<p>(c) Claim I. <span class="math inline">\(\int_{\mathbf{R}}yf(x,y)dy=\rho x\)</span>.</p>
<p>Completing the square, we have:</p>
<p><span class="math display">\[\begin{aligned}
(x^{2}-2\rho xy+y^{2}) &amp; =(y-\rho x)^{2}+x^{2}(1-\rho^{2})
\end{aligned}\]</span></p>
<p>Thus, we can write:</p>
<p><span class="math display">\[\begin{aligned}
\int_{\mathbf{R}}yf(x,y)dy &amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}e^{-\frac{1}{2}x^{2}}\int_{\mathbf{R}}ye^{-\frac{1}{2}\frac{(y-\rho x)^{2}}{(1-\rho^{2})}}dy
\end{aligned}\]</span></p>
<p>Let’s substitute</p>
<p><span class="math display">\[\begin{aligned}
z &amp; =\frac{(y-\rho x)}{\sqrt{1-\rho^{2}}}\\
dz &amp; =\frac{dy}{\sqrt{1-\rho^{2}}}
\end{aligned}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{aligned}
\int_{\mathbf{R}}ye^{-\frac{1}{2}\frac{(y-\rho x)^{2}}{(1-\rho^{2})}}dy &amp; =\sqrt{1-\rho^{2}}\int_{\mathbf{R}}(\rho x+\sqrt{1-\rho^{2}}z)e^{-\frac{z^{2}}{2}}dz\\
&amp; =\rho x\cdot\sqrt{1-\rho^{2}}\int_{\mathbf{R}}e^{-\frac{z^{2}}{2}}dz+(1-\rho^{2})\int_{\mathbf{R}}ze^{-\frac{z^{2}}{2}}dz\\
&amp; =\rho x\cdot\sqrt{1-\rho^{2}}\cdot\sqrt{2\pi}+(1-\rho^{2})\cdot0\\
&amp; =\rho x\cdot\sqrt{1-\rho^{2}}\cdot\sqrt{2\pi}
\end{aligned}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{aligned}
\int_{\mathbf{R}}yf(x,y)dy &amp; =\frac{1}{2\pi\cancel{\sqrt{1-\rho^{2}}}}e^{-\frac{1}{2}x^{2}}\rho x\cdot\cancel{\sqrt{1-\rho^{2}}}\cdot\sqrt{2\pi}\\
&amp; =\rho x\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}}\\
&amp; =\rho x\cdot f_{X}(x)\\
\frac{\int_{\mathbf{R}}yf(x,y)dy}{f_{X}(x)} &amp; =\frac{\int_{\mathbf{R}}yf(x,y)dy}{\int_{\mathbf{R}}f(x,y)}=\rho x
\end{aligned}\]</span></p>
<p>(d) For a Gaussian vector <span class="math inline">\((X,Y),\)</span> the conditional expectation <span class="math inline">\(\mathbf{E}[Y|X]=h(X)\)</span>. Hence, <span class="math inline">\(\mathbf{E}[Y|X]=\rho X\)</span>. ◻</p>
</div>
<div class="defn">
<p>(Conditional Expectation) Let <span class="math inline">\(Y\)</span> be an integrable random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> and let <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> be a sigma-field of <span class="math inline">\(\Omega\)</span>. The conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\mathcal{G}\)</span> is the random variable denoted by <span class="math inline">\(\mathbb{E}[Y|\mathcal{G}]\)</span> such that the following hold:</p>
<p>(a) <span class="math inline">\(\mathbb{E}[Y|\mathcal{G}]\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable.</p>
<p>In other words, all events pertaining to the random variable <span class="math inline">\(\mathbb{E}[Y|\mathcal{G}]\)</span> are in <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p>(b) For any (bounded) random variable <span class="math inline">\(W\)</span>, that is <span class="math inline">\(\mathcal{G}\)</span>-measurable,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[WY] &amp; =\mathbb{E}[W\mathbb{E}[Y|\mathcal{G}]]
\end{aligned}\]</span></p>
<p>In other words, <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is a proxy for <span class="math inline">\(Y\)</span> as far as the events in <span class="math inline">\(\mathcal{G}\)</span> are concerned.</p>
<p>Note that, by taking <span class="math inline">\(W=1\)</span> in the property (B), we recover:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[\mathbf{E}[Y|\mathcal{G}]] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
</div>
<div class="rem*">
<p>Beware of the notation! If <span class="math inline">\(\mathcal{G}=\sigma(X)\)</span>, then the conditional expectation <span class="math inline">\(\mathbf{E}[Y|\sigma(X)]\)</span> is usually denoted by <span class="math inline">\(\mathbf{E}[Y|X]\)</span> for short. However, one should always keep in mind that conditioning on <span class="math inline">\(X\)</span> is in fact projecting on the linear subspace <em>generated by all variables constructed from <span class="math inline">\(X\)</span></em> and not on the linear space generated by generated by <span class="math inline">\(X\)</span> alone. In the same way, the conditional expectation <span class="math inline">\(\mathbf{E}[Z|\sigma(X,Y)]\)</span> is often written <span class="math inline">\(\mathbf{E}[Z|X,Y]\)</span> for short.</p>
<p>As expected, if <span class="math inline">\(Y\)</span> is in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>, then <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is given by the orthogonal projection of <span class="math inline">\(Y\)</span> onto the subspace <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>, the subspace of square integrable random variables that are <span class="math inline">\(\mathcal{G}\)</span>-measurable. We write <span class="math inline">\(Y^{\star}\)</span> for the random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span> that is closest to <span class="math inline">\(Y\)</span> that is:</p>
<p><span class="math display">\[\begin{aligned}
\min_{Z\in L^{2}(\Omega,\mathcal{G},\mathbb{P})}\mathbf{E}[(Y-Z)^{2}] &amp; =\mathbf{E}[(Y-Y^{\star})^{2}]\label{eq:conditional-expectation}
\end{aligned}\]</span></p>
</div>
<div class="thm">
<p><span id="th:existence-and-uniqueness-of-conditional-expectations-II" label="th:existence-and-uniqueness-of-conditional-expectations-II"></span>(Existence and Uniqueness of Conditional Expectations) Let <span class="math inline">\(\mathcal{G}\subset\mathcal{F}\)</span> be a sigma-field of <span class="math inline">\(\Omega\)</span>. Let <span class="math inline">\(Y\)</span> be a random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>. Then, the conditional expectation <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is the random variable <span class="math inline">\(Y^{\star}\)</span> given in the equation (<a href="#eq:conditional-expectation" data-reference-type="ref" data-reference="eq:conditional-expectation">[eq:conditional-expectation]</a>). Namely, it is the random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span> that is closest to <span class="math inline">\(Y\)</span> in the <span class="math inline">\(L^{2}\)</span>-distance. In particular we have the following:</p>
</div>
<ul>
<li><p>It is the orthogonal projection of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>, that is, <span class="math inline">\(Y-Y^{\star}\)</span> is orthogonal to the random variables in <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>.</p></li>
<li><p>It is unique.</p></li>
</ul>
<p>Again, the result should be interpreted as follows: The conditional expectation <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is the best approximation of <span class="math inline">\(Y\)</span> given the information included in <span class="math inline">\(\mathcal{G}\)</span>.</p>
<div class="rem*">
<p>The conditional expectation in fact exists and is unique for any integrable random variable <span class="math inline">\(Y\)</span>(i.e. <span class="math inline">\(Y\in L^{1}(\Omega,\mathcal{F},\mathbb{P})\)</span> as the definition suggests. However, there is no orthogonal projection in <span class="math inline">\(L^{1}\)</span>, so the intuitive geometric picture is lost.</p>
</div>
<div class="center">
<p>Figure. An illustration of the conditional expectation <span class="math inline">\(\mathbb{E}[Y|\mathcal{G}]\)</span> as an orthogonal projection of <span class="math inline">\(Y\)</span> onto the subspace <span class="math inline">\(L^2(\Omega,\mathcal{G},\mathbb{P})\)</span>.</p>
</div>
<div class="example">
<p>(Conditional Expectation for Gaussian Vectors. II.) Consider the Gaussian vector <span class="math inline">\((X_{1},\ldots,X_{n})\)</span>. Without loss of generality, suppose it has mean <span class="math inline">\(0\)</span> and is non-degenerate. What is the best approximation of <span class="math inline">\(X_{n}\)</span> given the information <span class="math inline">\(X_{1},\ldots,X_{n-1}\)</span>? In other words, what is:</p>
<p><span class="math display">\[\mathbf{E}[X_{n}|\sigma(X_{1},\ldots,X_{n-1})\]</span></p>
<p>With example (<a href="#ex:sigma(X)-measurable-random-variables-example" data-reference-type="ref" data-reference="ex:sigma(X)-measurable-random-variables-example">[ex:sigma(X)-measurable-random-variables-example]</a>) in mind, let’s write <span class="math inline">\(\mathbf{E}[X_{n}|X_{1}\ldots X_{n-1}]\)</span> for short. From example (<a href="#ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables" data-reference-type="ref" data-reference="ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables">[ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables]</a>), we know that if <span class="math inline">\((X,Y)\)</span> is a Gaussian vector with mean <span class="math inline">\(0\)</span>, then <span class="math inline">\(\mathbf{E}[Y|X]\)</span> is a multiple of <span class="math inline">\(X\)</span>. Thus, we expect, that <span class="math inline">\(\mathbf{E}[X_{n}|X_{1}X_{2}\ldots X_{n-1}]\)</span> is a linear combination of <span class="math inline">\(X_{1},X_{2},\ldots,X_{n-1}\)</span>. That is, there exists <span class="math inline">\(a_{1},\ldots,a_{n-1}\)</span> such that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{n}|X_{1}X_{2}\ldots X_{n-1}] &amp; =a_{1}X_{1}+a_{2}X_{2}+\ldots+a_{n-1}X_{n-1}
\end{aligned}\]</span> In particular, since the conditional expectation is a linear combination of the <span class="math inline">\(X\)</span>’s, it is itself a Gaussian random variable. The best way to find the coefficient <span class="math inline">\(a\)</span>’s is to go back to IID decomposition of Gaussian vectors.</p>
<p>Let <span class="math inline">\((Z_{1},Z_{2},\ldots,Z_{n-1})\)</span> be IID standard Gaussians constructed from the linear combination of <span class="math inline">\((X_{1},X_{2},\ldots,X_{n-1})\)</span>. Then, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{n}|X_{1}X_{2}\ldots X_{n-1}] &amp; =b_{1}Z_{1}+\ldots+b_{n-1}Z_{n-1}
\end{aligned}\]</span></p>
<p>Now, recall, that we construct the random variables <span class="math inline">\(Z_{1}\)</span>, <span class="math inline">\(Z_{2}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(Z_{n}\)</span> using Gram-Schmidt orthogonalization:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{Z_{1}} &amp; =X_{1}, &amp; Z_{1} &amp; =\frac{\tilde{Z_{1}}}{\mathbf{E}(\tilde{Z}_{1}^{2})}\\
\tilde{Z_{2}} &amp; =X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1} &amp; Z_{2} &amp; =\frac{\tilde{Z}_{2}}{\mathbf{E}(\tilde{Z}_{2}^{2})}\\
\tilde{Z_{3}} &amp; =X_{3}-\sum_{i=1}^{2}\mathbf{E}(X_{3}Z_{i})Z_{i} &amp; Z_{3} &amp; =\frac{\tilde{Z}_{3}}{\mathbf{E}(\tilde{Z}_{3}^{2})}\\
&amp; \vdots
\end{aligned}\]</span></p>
</div>
<p><strong>The simple case for <span class="math inline">\(n=2\)</span> random variables.</strong></p>
<p>We have already seen before:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})] &amp; =\mathbf{E}[\tilde{Z}_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})]\\
&amp; =\frac{\mathbf{E}[\tilde{Z}_{1}^{2}]}{\mathbf{E}[\tilde{Z}_{1}^{2}]}\times\mathbf{E}\left[\tilde{Z}_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})\right]\\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}\left[\frac{\tilde{Z}_{1}}{\mathbf{E}[\tilde{Z}_{1}^{2}]}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})\right]\\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}[Z_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})]\\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\left(\mathbf{E}[Z_{1}X_{2}]-\mathbf{E}(X_{2}Z_{1})\mathbf{E}[Z_{1}^{2}]\right)\\
&amp; =0
\end{aligned}\]</span></p>
<p>So,<span class="math inline">\(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1}\)</span> is orthogonal to <span class="math inline">\(X_{1}\)</span>.</p>
<p>Moreover, <span class="math inline">\(\mathbf{E}(X_{2}Z_{1})Z_{1}\)</span> is a function of <span class="math inline">\(X_{1}\)</span>. Thus, both the properties of conditional expectation are satisfied. Since conditional expectations are unique, we must have, <span class="math inline">\(\mathbf{E}[X_{2}|X_{1}]=\mathbf{E}(X_{2}Z_{1})Z_{1}\)</span>.</p>
<p><strong>The case for <span class="math inline">\(n=3\)</span> random variables.</strong></p>
<p>We have seen that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})] &amp; =\frac{\mathbf{E}[\tilde{Z}_{1}^{2}]}{\mathbf{E}[\tilde{Z}_{1}^{2}]}\times\mathbf{E}[\tilde{Z}_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})]\\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}\left\{ \frac{\tilde{Z}_{1}}{\mathbf{E}[\tilde{Z}_{1}^{2}]}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})\right\} \\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}\left\{ Z_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})\right\} \\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}[X_{3}Z_{1}]-\mathbf{E}[X_{3}Z_{1}]\mathbf{E}[Z_{1}^{2}]-\mathbf{E}[X_{3}Z_{2}]\mathbf{E}[Z_{1}Z_{2}]\\
&amp; =0
\end{aligned}\]</span></p>
<p>It is an easy exercise to show that it is orthogonal to <span class="math inline">\(X_{2}\)</span>.</p>
<p>Hence, <span class="math inline">\(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2}\)</span> is orthogonal to <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>. Moreover, <span class="math inline">\(\mathbf{E}(X_{3}Z_{1})Z_{1}+\mathbf{E}(X_{3}Z_{2})Z_{2}\)</span> is a function of <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>. Thus, we must have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{3}|X_{1}X_{2}] &amp; =\mathbf{E}(X_{3}Z_{1})Z_{1}+\mathbf{E}(X_{3}Z_{2})Z_{2}
\end{aligned}\]</span></p>
<p>In general, <span class="math inline">\(X_{n}-\sum_{i=1}^{n-1}\mathbf{E}(X_{n}Z_{i})Z_{i}\)</span> is orthogonal to <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_{n-1}\)</span>. Hence,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{n}|X_{1}X_{2}\ldots X_{n-1}] &amp; =\sum_{i=1}^{n-1}\mathbf{E}(X_{n}Z_{i})Z_{i}
\end{aligned}\]</span></p>
</section>
<section id="properties-of-conditional-expectation." class="level3">
<h3 class="anchored" data-anchor-id="properties-of-conditional-expectation.">Properties of Conditional Expectation.</h3>
<p>We now list the properties of conditional expectation that follow from the two defining properties (A), (B) in the definition. They are extremely useful, when doing explicit computations on martingales. A good way to remember them is to understand how they relate to the interpretation of conditional expectation as an orthogonal projection onto a subspace or, equivalently, as the best approximation of the variable given the information available.</p>
<div class="prop">
<p><span id="prop:properties-of-conditional-expectation" label="prop:properties-of-conditional-expectation"></span>Let <span class="math inline">\(Y\)</span> be an integrable random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Let <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> be another sigma-field of <span class="math inline">\(\Omega\)</span>. Then, the conditional expectation <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> has the following properties:</p>
<p>(1) If <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable, then :</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{G}] &amp; =Y
\end{aligned}\]</span></p>
<p>(2) Taking out what is known. More generally, if <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G-}\)</span>measurable and <span class="math inline">\(X\)</span> is another integrable random variable (with <span class="math inline">\(XY\)</span> also integrable), then :</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[XY|\mathcal{G}] &amp; =Y\mathbf{E}[X|\mathcal{G}]
\end{aligned}\]</span></p>
<p>This makes sense, since <span class="math inline">\(Y\)</span> is determined by <span class="math inline">\(\mathcal{G}\)</span>, so we can take out what is known; it can be treated as a constant for the conditional expectation.</p>
<p>(3) Independence. If <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(\mathcal{G}\)</span>, that is, for any events <span class="math inline">\(\{Y\in(a,b]\}\)</span> and <span class="math inline">\(A\in\mathcal{G}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(\{Y\in I\}\cap A) &amp; =\mathbb{P}(\{Y\in I\})\cdot\mathbb{P}(A)
\end{aligned}\]</span></p>
<p>then</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{G}] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
<p>In other words, if you have no information on <span class="math inline">\(Y\)</span>, your best guess for its value is simply plain expectation.</p>
<p>(4) Linearity of conditional expectations. Let <span class="math inline">\(X\)</span> be another integrable random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Then,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[aX+bY|\mathcal{G}] &amp; =a\mathbf{E}[X|\mathcal{G}]+b\mathbf{E}[Y|\mathcal{G}],\quad\text{for any }a,b\in\mathbf{R}
\end{aligned}\]</span></p>
<p>The linearity justifies the cumbersom choice of notation <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> for the random variable.</p>
<p>(5) Tower Property : If <span class="math inline">\(\mathcal{H}\subseteq\mathcal{G}\)</span> is another sigma-field of <span class="math inline">\(\Omega\)</span>, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{H}] &amp; =\mathbf{E}[\mathbf{E}[Y|\mathcal{G}]|\mathcal{H}]
\end{aligned}\]</span></p>
<p>Think in terms of two successive projections: first on a plane, then on a line in the plane.</p>
<p>(6) Pythagoras Theorem. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y^{2}] &amp; =\mathbf{E}\left[\left(\mathbf{E}[Y|\mathcal{G}]\right)^{2}\right]+\mathbf{E}\left[\left(Y-\mathbf{E}[Y|\mathcal{G}]\right)^{2}\right]
\end{aligned}\]</span></p>
<p>In particular:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[\left(\mathbf{E}\left[Y|\mathcal{G}\right]\right)^{2}\right] &amp; \leq\mathbf{E}[Y^{2}]
\end{aligned}\]</span></p>
<p>In words, the <span class="math inline">\(L^{2}\)</span> norm of <span class="math inline">\(\mathbf{E}[X|\mathcal{G}]\)</span> is smaller than the one of <span class="math inline">\(X\)</span>, which is clear if you think in terms of orthogonal projection.</p>
<p>(7) Expectation of the conditional expectation.</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[\mathbf{E}[Y|\mathcal{G}]\right] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>The uniqueness property of conditional expectations in theorem (<a href="#th:existence-and-uniqueness-of-conditional-expectations-II" data-reference-type="ref" data-reference="th:existence-and-uniqueness-of-conditional-expectations-II">[th:existence-and-uniqueness-of-conditional-expectations-II]</a>) might appear to be an academic curiosity. On the contrary, it is very practical, since it ensures, that if we find a candidate for the conditional expectation that has the two properties in Definition (<a href="#def:conditional-expectation" data-reference-type="ref" data-reference="def:conditional-expectation">[def:conditional-expectation]</a>), then it must be <em>the</em> conditional expectation. To see this, let’s prove property (1).</p>
<div class="claim">
<p>If <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable, then <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]=Y\)</span>.</p>
<p>It suffices to show that <span class="math inline">\(Y\)</span> has the two defining properties of conditional expectation.</p>
<p>(1) We are given that, <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable. So, property (A) is satisfied.</p>
<p>(2) For any bounded random variable <span class="math inline">\(W\)</span> that is <span class="math inline">\(\mathcal{G}\)</span>-measurable, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[W(Y-Y)] &amp; =\mathbf{E}[0]=0
\end{aligned}\]</span></p>
<p>So, property (B) is also a triviality.</p>
</div>
<div class="claim">
<p>(Taking out what is known.) If <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable and <span class="math inline">\(X\)</span> is another integrable random variable, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[XY|\mathcal{G}] &amp; =Y\mathbf{E}[X|\mathcal{G}]
\end{aligned}\]</span></p>
<p>In a similar vein, it suffices to show that, <span class="math inline">\(Y\mathbf{E}[X|\mathcal{G}]\)</span> has the two defining properties of conditional expectation.</p>
<p>(1) We are given that <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable; from property (1), <span class="math inline">\(\mathbf{E}[X|\mathcal{G}]\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable. It follows that, <span class="math inline">\(Y\mathbf{E}[X|\mathcal{G}]\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable.</p>
<p>(2) From theorem (<a href="#th:existence-and-uniqueness-of-conditional-expectations-II" data-reference-type="ref" data-reference="th:existence-and-uniqueness-of-conditional-expectations-II">[th:existence-and-uniqueness-of-conditional-expectations-II]</a>), <span class="math inline">\(X-\mathbf{E}[X|\mathcal{G}]\)</span> is orthogonal to the random variables <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>. So, if <span class="math inline">\(W\)</span> is any bounded <span class="math inline">\(\mathcal{G}\)</span>-measurable random variable, it follows that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[WY(X-\mathbf{E}[X|\mathcal{G}])] &amp; =0\\
\implies\mathbf{E}[W\cdot XY] &amp; =\mathbf{E}[WY\mathbf{E}[X|\mathcal{G}]]
\end{aligned}\]</span></p>
<p>This closes the proof.</p>
</div>
<div class="claim">
<p>(Independence.) If <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(\mathcal{G}\)</span>, that is, for all events <span class="math inline">\(\{Y\in(a,b]\}\)</span> and <span class="math inline">\(A\in\mathcal{G}\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{\mathbb{P}}\{Y\in(a,b]\cap A\} &amp; =\mathbb{P}\{Y\in(a,b]\}\cdot\mathbb{P}(A)
\end{aligned}\]</span></p>
<p>then</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{G}] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
<p>Let us show that <span class="math inline">\(\mathbf{E}[Y]\)</span> has the two defining properties of conditional expectations.</p>
<p>(1) <span class="math inline">\(\mathbf{E}[Y]\)</span> is a constant and so it is <span class="math inline">\(\mathcal{F}_{0}\)</span> measurable. Hence, it is <span class="math inline">\(\mathcal{G}\)</span> measurable.</p>
<p>(2) If <span class="math inline">\(W\)</span> is another <span class="math inline">\(\mathcal{G}\)</span>-measurable random variable,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[WY] &amp; =\mathbf{E}[W]\cdot\mathbf{E}[Y]
\end{aligned}\]</span></p>
<p>since <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(\mathcal{G}\)</span> and therefore it is independent of <span class="math inline">\(Y\)</span>. Hence,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[W(Y-\mathbf{E}[Y])] &amp; =0
\end{aligned}\]</span></p>
<p>Consequently, <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]=\mathbf{E}[Y]\)</span>.</p>
</div>
<div class="claim">
<p>(Linearity of conditional expectations) Let <span class="math inline">\(X\)</span> be another integrable random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Then,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[aX+bY|\mathcal{G}] &amp; =a\mathbf{E}[X|\mathcal{G}]+b\mathbf{E}[Y|\mathcal{G}],\quad\text{for any }a,b\in\mathbf{R}
\end{aligned}\]</span></p>
</div>
<p>Since <span class="math inline">\(\mathbf{E}[X|\mathcal{G}]\)</span> and <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> are <span class="math inline">\(\mathcal{G}-\)</span>measurable, any linear combination of these two random variables is also <span class="math inline">\(\mathcal{G}\)</span>-measurable.</p>
<p>Also, if <span class="math inline">\(W\)</span> is any bounded <span class="math inline">\(\mathcal{G}-\)</span>measurable random variable, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[W(aX+bY-(a\mathbf{E}[X|\mathcal{G}]+b\mathbf{E}[Y|\mathcal{G}]))] &amp; =a\mathbf{E}[W(X-\mathbf{E}[X|\mathcal{G}])]\\
&amp; +b\mathbf{E}[W(Y-\mathbf{E}[Y|\mathcal{G}])]
\end{aligned}\]</span></p>
<p>By definition, <span class="math inline">\(X-\mathbf{E}(X|\mathcal{G})\)</span> is orthogonal t o the subspace <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span> and hence to all <span class="math inline">\(\mathcal{G}\)</span>-measurable random-variables. Hence, the two expectations on the right hand side of the above expression are <span class="math inline">\(0\)</span>. Since, conditional expectations are unique, we have the desired result.</p>
<div class="claim">
<p>If <span class="math inline">\(\mathcal{H}\subseteq\mathcal{G}\)</span> is another sigma-field of <span class="math inline">\(\Omega\)</span>, then</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{H}] &amp; =\mathbf{E}[\mathbf{E}[Y|\mathcal{G}]|\mathcal{H}]
\end{aligned}\]</span></p>
<p>Define <span class="math inline">\(U:=\mathbf{E}[Y|\mathcal{G}]\)</span>. By definition, <span class="math inline">\(\mathbf{E}[U|\mathcal{H}]\)</span> is <span class="math inline">\(\mathcal{H}\)</span>-measurable.</p>
<p>Let <span class="math inline">\(W\)</span> be any bounded <span class="math inline">\(\mathcal{H}\)</span>-measurable random variable. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[W\{\mathbf{E}(Y|\mathcal{G})-\mathbf{E}(\mathbf{E}(Y|\mathcal{G})|\mathcal{H})\}] &amp; =\mathbf{E}[W(U-\mathbf{E}(U|\mathcal{H})]
\end{aligned}\]</span></p>
<p>But, by definition <span class="math inline">\(U-\mathbf{E}(U|\mathcal{H})\)</span> is always orthogonal to the subspace <span class="math inline">\(L^{2}(\Omega,\mathcal{H},\mathbb{P})\)</span> and hence, <span class="math inline">\(\mathbf{E}[W(U-\mathbf{\mathbf{E}}(U|\mathcal{H})]=0\)</span>. Since, conditional expectations are unique, we have the desired result.</p>
</div>
<div class="claim">
<p><strong>Pythagoras’s theorem</strong>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y^{2}] &amp; =\mathbf{E}[(\mathbf{E}[Y|\mathcal{G}])^{2}]+\mathbf{E}[(Y-\mathbf{E}(Y|\mathcal{G}))^{2}]
\end{aligned}\]</span></p>
<p>In particular,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(\mathbf{E}[Y|\mathcal{G}])^{2}] &amp; \leq\mathbf{E}[Y^{2}]
\end{aligned}\]</span></p>
<p>Consider the orthogonal decomposition:</p>
<p><span class="math display">\[\begin{aligned}
Y &amp; =\mathbf{E}[Y|\mathcal{G}]+(Y-\mathbf{E}[Y|\mathcal{G}])
\end{aligned}\]</span></p>
<p>Squaring on both sides and taking expectations, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y^{2}] &amp; =\mathbf{E}[(\mathbf{E}(Y|\mathcal{G}))^{2}]+\mathbf{E}[(Y-\mathbf{E}[Y|\mathcal{G}])^{2}]+2\mathbf{E}\left[\mathbf{E}[Y|\mathcal{G}](Y-\mathbf{E}[Y|\mathcal{G}])\right]
\end{aligned}\]</span></p>
<p>By definition of conditional expectation, <span class="math inline">\((Y-\mathbf{E}[Y|\mathcal{G}])\)</span> is orthogonal to the subspace <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>. By the properties of conditional expectation, <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is <span class="math inline">\(\mathcal{G}-\)</span>measurable, so it belongs to <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>. Hence, the dot-product on the right-hand side is <span class="math inline">\(0\)</span>. Consequently, we have the desired result.</p>
<p>Moreover, since <span class="math inline">\((Y-\mathbf{E}[Y|\mathcal{G}])^{2}\)</span> is a non-negative random variable, <span class="math inline">\(\mathbf{E}[(Y-\mathbf{E}[Y|\mathcal{G}])^{2}]\geq0\)</span>. It follows that: <span class="math inline">\(\mathbf{E}[Y^{2}]\geq\mathbf{E}[(\mathbf{E}(Y|\mathcal{G}))^{2}]\)</span>.</p>
</div>
<div class="claim">
<p>Our claim is:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[\mathbf{E}[Y|\mathcal{G}]\right] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
<p>We know that, if <span class="math inline">\(W\)</span> is any bounded <span class="math inline">\(\mathcal{G}\)</span>-measurable random variable:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[WY\right] &amp; =\mathbf{E}[W\mathbf{E}[Y|\mathcal{G}]]
\end{aligned}\]</span></p>
<p>Taking <span class="math inline">\(W=1\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[Y\right] &amp; =\mathbf{E}[\mathbf{E}[Y|\mathcal{G}]]
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>(Brownian Conditioning II). We continue the example (<a href="#ex:brownian-conditioning-I" data-reference-type="ref" data-reference="ex:brownian-conditioning-I">[ex:brownian-conditioning-I]</a>). Let’s now compute the conditional expectations <span class="math inline">\(\mathbf{E}[e^{aB_{1}}|B_{1/2}]\)</span> and <span class="math inline">\(\mathbf{E}[e^{aB_{1/2}}|B_{1}]\)</span> for some parameter <span class="math inline">\(a\)</span>. We shall need the properties of conditional expectation in proposition (<a href="#prop:properties-of-conditional-expectation" data-reference-type="ref" data-reference="prop:properties-of-conditional-expectation">[prop:properties-of-conditional-expectation]</a>). For the first one we use the fact that <span class="math inline">\(B_{1/2}\)</span> is independent of <strong><span class="math inline">\(B_{1}-B_{1/2}\)</span></strong> to get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[e^{aB_{1}}|B_{1/2}] &amp; =\mathbf{E}[e^{a((B_{1}-B_{1/2})+B_{1/2})}|B_{1/2}]\\
&amp; =\mathbf{E}[e^{a(B_{1}-B_{2})}\cdot e^{aB_{1/2}}|B_{1/2}]\\
&amp; \quad\left\{ \text{Taking out what is known}\right\} \\
&amp; =e^{aB_{1/2}}\mathbf{E}[e^{a(B_{1}-B_{1/2})}|B_{1/2}]\\
&amp; =e^{aB_{1/2}}\cdot\mathbf{E}[e^{a(B_{1}-B_{1/2})}]\\
&amp; \quad\{\text{Independence}\}
\end{aligned}\]</span></p>
<p>We know that, <span class="math inline">\(a(B_{1}-B_{1/2})\)</span> is a gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(a^{2}/2\)</span>. We also know that, <span class="math inline">\(\mathbf{E}[e^{tZ}]=e^{t^{2}/2}\)</span>. So, <span class="math inline">\(\mathbf{E}[e^{a(B_{1}-B_{1/2})}]=e^{a^{2}/4}\)</span>. Consequently, <span class="math inline">\(\mathbf{E}[e^{aB_{1}}|B_{1/2}]=e^{aB_{1/2}+a^{2}/4}\)</span>.</p>
<p>The result itself has the form of the MGF of a Gaussian with mean <span class="math inline">\(B_{1/2}\)</span> and variance <span class="math inline">\(1/2\)</span>. (The MGF of <span class="math inline">\(X=\mu+\sigma Z\)</span>, <span class="math inline">\(Z=N(0,1)\)</span> is <span class="math inline">\(M_{X}(a)=\exp\left[\mu+\frac{1}{2}\sigma^{2}a^{2}\right]\)</span>.) In fact, this shows that the conditional distribution of <span class="math inline">\(B_{1}\)</span> given <span class="math inline">\(B_{1/2}\)</span> is Gaussian of mean <span class="math inline">\(B_{1/2}\)</span> and variance <span class="math inline">\(1/2\)</span>.</p>
<p>For the other expectation, note that <span class="math inline">\(B_{1/2}-\frac{1}{2}B_{1}\)</span> is independent of <span class="math inline">\(B_{1}\)</span>. We have: <span class="math display">\[\begin{aligned}
\mathbf{E}\left[\left(B_{1/2}-\frac{1}{2}B_{1}\right)B_{1}\right] &amp; =\mathbf{E}(B_{1/2}B_{1})-\frac{1}{2}\mathbf{E}[B_{1}^{2}]\\
&amp; =\frac{1}{2}-\frac{1}{2}\cdot1\\
&amp; =0
\end{aligned}\]</span></p>
<p>Therefore, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[e^{aB_{1/2}}|B_{1}] &amp; =\mathbf{E}[e^{a(B_{1/2}-\frac{1}{2}B_{1})+\frac{a}{2}B_{1}}|B_{1}]\\
&amp; =\mathbf{E}[e^{a(B_{1/2}-\frac{1}{2}B_{1})}\cdot e^{\frac{a}{2}B_{1}}|B_{1}]\\
&amp; =e^{\frac{a}{2}B_{1}}\mathbf{E}[e^{a(B_{1/2}-\frac{1}{2}B_{1})}|B_{1}]\\
&amp; \quad\{\text{Taking out what is known }\}\\
&amp; =e^{\frac{a}{2}B_{1}}\mathbf{E}[e^{a(B_{1/2}-\frac{1}{2}B_{1})}]\\
&amp; \quad\{\text{Independence}\}
\end{aligned}\]</span></p>
<p>Now, <span class="math inline">\(a(B_{1/2}-\frac{1}{2}B_{1})\)</span> is a random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(a^{2}(\frac{1}{2}-\frac{1}{4})=\frac{a^{2}}{4}\)</span>. Consequently, <span class="math inline">\(\mathbf{E}[e^{(a/2)Z}]=e^{\frac{a^{2}}{8}}\)</span>. Thus, <span class="math inline">\(\mathbf{E}[e^{aB_{1/2}}|B_{1}]=e^{\frac{a}{2}B_{1}+\frac{a^{2}}{8}}\)</span>.</p>
</div>
<div class="example">
<p>(Brownian bridge is conditioned Brownian motion). We know that the Brownian bridge <span class="math inline">\(M_{t}=B_{t}-tB_{1}\)</span>, <span class="math inline">\(t\in[0,1]\)</span> is independent of <span class="math inline">\(B_{1}\)</span>. We use this to show that the conditional distribution of the Brownian motion given the value at the end-point <span class="math inline">\(B_{1}\)</span> is the one of a Brownian bridge shifted by the straight line going from <span class="math inline">\(0\)</span> to <span class="math inline">\(B_{1}\)</span>. To see this, we compute the conditional MGF of <span class="math inline">\((B_{t_{1}},B_{t_{2}},\ldots,B_{t_{n}})\)</span> given <span class="math inline">\(B_{1}\)</span> for some arbitrary choices of <span class="math inline">\(t_{1},t_{2},\ldots,t_{n}\)</span> in <span class="math inline">\([0,1]\)</span>. We get the following by adding and subtracting <span class="math inline">\(t_{j}B_{1}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[e^{a_{1}B_{t_{1}}+\ldots+a_{n}B_{t_{n}}}|B_{1}] &amp; =\mathbf{E}[e^{a_{1}(B_{t_{1}}-t_{1}B_{1})+\ldots+a_{n}(B_{t_{n}}-t_{n}B_{1})}\cdot e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}|B_{1}]\\
&amp; =e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}\mathbf{E}[e^{a_{1}M_{t_{1}}+\ldots+a_{n}M_{t_{n}}}|B_{1}]\\
&amp; \quad\{\text{Taking out what is known}\}\\
&amp; =e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}\mathbf{E}[e^{a_{1}M_{t_{1}}+\ldots+a_{n}M_{t_{n}}}]\\
&amp; \quad\{\text{Independence}\}
\end{aligned}\]</span></p>
<p>The right side is exactly the MGF of the process <span class="math inline">\(M_{t}+tB_{1},t\in[0,1]\)</span> (for a fixed value <span class="math inline">\(B_{1})\)</span>, where <span class="math inline">\((M_{t},t\in[0,1])\)</span> is a Brownian bridge. This proves the claim.</p>
</div>
<div class="lem">
<p>(Conditional Jensen’s Inequality) If <span class="math inline">\(c\)</span> is a convex function on <span class="math inline">\(\mathbf{R}\)</span> and <span class="math inline">\(X\)</span> is a random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[c(X)] &amp; \geq c(\mathbf{E}[X])
\end{aligned}\]</span></p>
<p>More generally, if <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> is a sigma-field, then:</p>
<p><span class="math display">\[\mathbf{E}[c(X)|\mathcal{G}]\geq c(\mathbf{E}[X|\mathcal{G}])\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> We know that, if <span class="math inline">\(c(x)\)</span> is a convex function, the tangent to the curve <span class="math inline">\(c\)</span> at any point lies below the curve. The tangent to the cuve at this point, is a straight-line of the form:</p>
<p><span class="math display">\[\begin{aligned}
c(t)=y &amp; =mt+c
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(m(t)=c'(t)\)</span>. This holds for all <span class="math inline">\(t\in\mathbf{R}\)</span>. At an arbitrary point <span class="math inline">\(x\)</span> we have:</p>
<p><span class="math display">\[\begin{aligned}
c(x)\geq &amp; y=mx+c
\end{aligned}\]</span></p>
<p>Therefore, we have:</p>
<p><span class="math display">\[\begin{aligned}
c(x)-c(t) &amp; \geq m(t)(x-t)
\end{aligned}\]</span></p>
<p>for any <span class="math inline">\(x\)</span> and any point of tangency <span class="math inline">\(t\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
c(X)-c(Y) &amp; \geq m(Y)(X-Y)
\end{aligned}\]</span></p>
<p>Substituting <span class="math inline">\(Y=\mathbf{E}[X|\mathcal{G}]\)</span>, we get:</p>
<p><span class="math display">\[\begin{aligned}
c(X)-c(\mathbf{E}[X|\mathcal{G}]) &amp; \geq m(\mathbf{E}[X|\mathcal{G}])(X-\mathbf{E}[X|\mathcal{G}])
\end{aligned}\]</span></p>
<p>Taking expectations on both sides, we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(c(X)-c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}] &amp; \geq\mathbf{E}[m(\mathbf{E}[X|\mathcal{G}])(X-\mathbf{E}[X|\mathcal{G}])|\mathcal{G}]
\end{aligned}\]</span></p>
<p>The left-hand side simplifies as:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(c(X)-c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}] &amp; =\mathbf{E}[c(X)|\mathcal{G}]-\mathbf{E}[c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}]\\
&amp; \quad\{\text{Linearity}\}\\
&amp; =\mathbf{E}[c(X)|\mathcal{G}]-c(\mathbf{E}[X|\mathcal{G}])\\
&amp; \quad\{\text{c(\ensuremath{\mathbf{E}}[X|\ensuremath{\mathcal{G}}])) is \ensuremath{\mathcal{G}}-measurable}\}
\end{aligned}\]</span></p>
<p>On the right hand side, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[m(\mathbf{E}[X|\mathcal{G}])(X-\mathbf{E}[X|\mathcal{G}])|\mathcal{G}] &amp; =\mathbf{E}[m(\mathbf{E}[X|\mathcal{G}])\cdot X|\mathcal{G}]-\mathbf{E}[m(\mathbf{E}[X|\mathcal{G}])\cdot\mathbf{E}[X|\mathcal{G}]|\mathcal{G}]\\
&amp; =\mathbf{E}[X|\mathcal{G}]m(\mathbf{E}[X|\mathcal{G}])-m(\mathbf{E}[X|\mathcal{G}])\cdot\mathbf{E}[X|\mathcal{G}]\\
&amp; =0
\end{aligned}\]</span></p>
<p>Consequently, it follows that <span class="math inline">\(\mathbf{E}[c(X)|\mathcal{G}]\geq c(\mathbf{E}[X|\mathcal{G}])\)</span>. ◻</p>
</div>
<div class="example">
<p>(Embeddings of <span class="math inline">\(L^{p}\)</span> spaces) Square-integrable random variables are in fact integrable. In other words, there is always the inclusion <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\subseteq L^{1}(\Omega,\mathcal{F},\mathbb{P})\)</span>. In particular, square integrable random variables always have a well-defined variance. This embedding is a simple consequence of Jensen’s inequality since:</p>
<p><span class="math display">\[\begin{aligned}
|\mathbf{E}[X]|^{2} &amp; \leq\mathbf{E}[|X|^{2}]
\end{aligned}\]</span></p>
<p>as <span class="math inline">\(f(x)=|x|^{2}\)</span> is convex. By taking the square root on both sides, we get:</p>
<p><span class="math display">\[\begin{aligned}
\left\Vert X\right\Vert _{1} &amp; \leq\left\Vert X\right\Vert _{2}
\end{aligned}\]</span></p>
<p>More generally, for any <span class="math inline">\(1&lt;p&lt;\infty\)</span>, we can define <span class="math inline">\(L^{p}(\Omega,\mathcal{F},\mathbb{P})\)</span> to be the linear space of random variables such that <span class="math inline">\(\mathbf{E}[|X|^{p}]&lt;\infty\)</span>. Then for <span class="math inline">\(p&lt;q\)</span>, since <span class="math inline">\(x^{q/p}\)</span> is convex, we get by Jensen’s inequality :</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[|X|^{q}] &amp; =\mathbf{E}[(|X|^{p})^{\frac{q}{p}}]\geq\left(\mathbf{E}[|X|^{p}]\right)^{\frac{q}{p}}
\end{aligned}\]</span></p>
<p>Taking the <span class="math inline">\(q\)</span>-th root on both sides:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[|X|^{p}]^{1/p} &amp; \leq\mathbf{E}[|X|^{q}]^{1/q}
\end{aligned}\]</span></p>
<p>So, if <span class="math inline">\(X\in L^{q}\)</span>, then it must also be in <span class="math inline">\(L^{p}\)</span>. Concretely, this means that any random variable with a finite <span class="math inline">\(q\)</span>-moment will also have a finite <span class="math inline">\(p\)</span>-moment, for <span class="math inline">\(q&gt;p\)</span>.</p>
</div>
</section>
</section>
<section id="martingales.-1" class="level2">
<h2 class="anchored" data-anchor-id="martingales.-1">Martingales.</h2>
<p>We now have all the tools to define martingales.</p>
<div class="defn">
<p><span id="def:Filtration" label="def:Filtration"></span>(Filtration). A <em>filtration</em> <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> of <span class="math inline">\(\Omega\)</span> is an increasing sequence of <span class="math inline">\(\sigma\)</span>-fields of <span class="math inline">\(\Omega\)</span>. That is,</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{F}_{s} &amp; \subseteq\mathcal{F}_{t},\quad\forall s\leq t
\end{aligned}\]</span></p>
<p>We will usually take <span class="math inline">\(\mathcal{F}_{0}=\{\emptyset,\Omega\}\)</span>. The canonical example of a filtration is the natural filtration of a given process <span class="math inline">\((M_{s}:s\geq0)\)</span>. This is the filtration given by <span class="math inline">\(\mathcal{F}_{t}=\sigma(M_{s},s\leq t)\)</span>. The inclusions of the <span class="math inline">\(\sigma\)</span>-fields are then clear. For a given Brownian motion <span class="math inline">\((B_{t},t\geq0)\)</span>, the filtration <span class="math inline">\(\mathcal{F}_{t}=\sigma(B_{s},s\leq t)\)</span> is sometimes called the <em>Brownian filtration</em>. We think of the filtration as the <em>flow of information of the process</em>.</p>
</div>
<div class="defn">
<p><span id="def:Adapted-process" label="def:Adapted-process"></span>A stochastic process <span class="math inline">\((X_{t}:t\geq0)\)</span> is said to be adapted to <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>, if for each <span class="math inline">\(t\)</span>, the random variable <span class="math inline">\(X_{t}\)</span> is <span class="math inline">\(\mathcal{F}_{t}-\)</span>measurable.</p>
</div>
<div class="defn">
<p>(Martingale). A process <span class="math inline">\((M_{t}:t\geq0)\)</span> is a martingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> if the following hold:</p>
<p>(1) The process is <em>adapted</em>, that is <span class="math inline">\(M_{t}\)</span> is <span class="math inline">\(\mathcal{F}_{t}-\)</span>measurable for all <span class="math inline">\(t\geq0\)</span>.</p>
<p>(2) <span class="math inline">\(\mathbf{E}[|M_{t}|]&lt;\infty\)</span> for all <span class="math inline">\(t\geq0\)</span>. (This ensures that the conditional expectation is well defined.)</p>
<p>(3) <em>Martingale property:</em></p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}|\mathcal{F}_{s}] &amp; =M_{s}\quad\forall s\leq t
\end{aligned}\]</span></p>
<p>Roughly, speaking this means that the best approximation of a process at a future time <span class="math inline">\(t\)</span> is its value at the present.</p>
</div>
<p>In particular, the martingale property implies that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}|\mathcal{F}_{0}] &amp; =M_{0}\nonumber \\
\mathbf{E}[\mathbf{E}[M_{t}|\mathcal{F}_{0}]] &amp; =\mathbf{E}[M_{0}]\nonumber \\
\mathbf{E}[M_{t}] &amp; =\mathbf{E}[M_{0}]\label{eq:expected-value-of-martingale-at-any-time-is-constant}\\
&amp; \quad\{\text{Tower Property}\}\nonumber
\end{aligned}\]</span></p>
<p>Usually, we take <span class="math inline">\(\mathcal{F}_{0}\)</span> to be the trivial sigma-field <span class="math inline">\(\{\emptyset,\Omega\}\)</span>. A random variable that is <span class="math inline">\(\mathcal{F}_{0}\)</span>-measurable must be a constant, so <span class="math inline">\(M_{0}\)</span> is a constant. In this case, <span class="math inline">\(\mathbf{E}[M_{t}]=M_{0}\)</span> for all <span class="math inline">\(t\)</span>. If properties (1) and (2) are satisfied, but the best approximation is larger, <span class="math inline">\(\mathbf{E}[M_{t}|\mathcal{F}_{s}]\geq M_{s}\)</span>, the process is called a <em>submartingale</em>. If it is smaller on average, <span class="math inline">\(\mathbf{E}[M_{t}|\mathcal{F}_{s}]\leq\mathbf{E}[M_{s}]\)</span>, we say it is a supermartingale.</p>
<p>We will be mostly interested in martingales that are continuous and square-integrable. Continuous martingales are martingales whose paths <span class="math inline">\(t\mapsto M_{t}(\omega)\)</span> are continuous almost surely. Square-integrable martingales are such that <span class="math inline">\(\mathbf{E}[|M_{t}|^{2}]&lt;\infty\)</span> for all <span class="math inline">\(t\)</span>’s. This condition is stronger than <span class="math inline">\(\mathbf{E}[|M_{t}|]&lt;\infty\)</span> due to Jensen’s inequality.</p>
<div class="rem*">
<p>(Martingales in Discrete-time). Martingales can be defined the same way if the index set of the process is discrete. For example, the filtration <span class="math inline">\((\mathcal{F}_{n}:n\in\mathbf{N})\)</span> is a countable set and the martingale property is then replaced by <span class="math inline">\(\mathbf{E}[M_{n+1}|\mathcal{F}_{n}]=M_{n}\)</span> as expected. The tower-property then yields the martingale property <span class="math inline">\(\mathbf{E}[M_{n+k}|\mathcal{F}_{n}]=M_{n}\)</span> for <span class="math inline">\(k\geq1\)</span>.</p>
</div>
<div class="rem*">
<p>(Continuous Filtrations). Filtrations with continuous time can be tricky to handle rigorously. For example, one has to make sense of what it means for <span class="math inline">\(\mathcal{F}_{s}\)</span> as <span class="math inline">\(s\)</span> approaches <span class="math inline">\(t\)</span> from the left. Is it equal to <span class="math inline">\(\mathcal{F}_{t}\)</span>? Or is there actually less information in <span class="math inline">\(\lim_{s\to t^{-}}\mathcal{F}_{s}\)</span> than in <span class="math inline">\(\mathcal{F}_{t}\)</span>? This is a bit of headache when dealing with processes with jumps, like the Poisson process. However, if the paths are continuous, the technical problems are not as heavy.</p>
<p>Let’s look at some of the important examples of martingales constructed from Brownian Motion.</p>
</div>
<div class="example">
<p>(Examples of Brownian Martingales)</p>
<p>(i) <em>Standard Brownian Motion.</em> Let <span class="math inline">\((B_{t}:t\geq0)\)</span> be a standard Brownian motion and let <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> be a <em>Brownian filtration</em>. Then <span class="math inline">\((B_{t}:t\geq0)\)</span> is a square integrable martingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>. Property (1) is obvious, because all the sets in <span class="math inline">\(\mathcal{F}_{t}\)</span> are resolved, upon observing the outcome of <span class="math inline">\(B_{t}\)</span>. Similarly, <span class="math inline">\(\mathbf{E}[|B_{t}|]=0\)</span>. As for the martingale property, note that, by the properties of conditional expectation in proposition (<a href="#prop:properties-of-conditional-expectation" data-reference-type="ref" data-reference="prop:properties-of-conditional-expectation">[prop:properties-of-conditional-expectation]</a>), we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[B_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}[B_{t}|B_{s}]\\
&amp; =\mathbf{E}[B_{t}-B_{s}+B_{s}|B_{s}]\\
&amp; =\mathbf{E}[B_{t}-B_{s}|B_{s}]+\mathbf{E}[B_{s}|B_{s}]\\
&amp; \quad\{\text{Linearity}\}\\
&amp; =\mathbf{E}[B_{t}-B_{s}]+B_{s}\\
&amp; \quad\{\text{Independence}\}\\
&amp; =B_{s}
\end{aligned}\]</span></p>
<p>(ii) <em>Geometric Brownian Motion.</em> Let <span class="math inline">\((B_{t},t\ge0)\)</span> be a standard brownian motion, and <span class="math inline">\(\mathcal{F}_{t}=\sigma(B_{s},s\leq t)\)</span>. A <em>geometric brownian motion</em> is a process <span class="math inline">\((S_{t},t\geq0)\)</span> defined by:</p>
<p><span class="math display">\[\begin{aligned}
S_{t} &amp; =S_{0}\exp\left(\sigma B_{t}+\mu t\right)
\end{aligned}\]</span></p>
<p>for some parameter <span class="math inline">\(\sigma&gt;0\)</span> and <span class="math inline">\(\mu\in\mathbf{R}\)</span>. This is simply the exponential of the Brownian motion with drift. This is not a martingale for most choices of <span class="math inline">\(\mu\)</span>! In fact, one must take</p>
<p><span class="math display">\[\begin{aligned}
\mu &amp; =-\frac{1}{2}\sigma^{2}
\end{aligned}\]</span> for the process to be a martingale for the Brownian filtration. Let’s verify this. Property (1) is obvious since <span class="math inline">\(S_{t}\)</span> is a function of <span class="math inline">\(B_{t}\)</span> for each <span class="math inline">\(t\)</span>. So, it is <span class="math inline">\(\mathcal{F}_{t}\)</span> measurable. Moreover, property (2) is clear: <span class="math inline">\(\mathbf{E}[\exp(\sigma B_{t}+\mu t)]=\mathbf{E}[\exp(\sigma\sqrt{t}Z+\mu t)]=\exp(\mu t+\frac{1}{2}\sigma^{2}t)\)</span>. So, its a finite quantity. As for the martingale property, note that by the properties of conditional expectation, and the MGF of Gaussians, we have for <span class="math inline">\(s\leq t\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[S_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}\left[S_{0}\exp\left(\sigma B_{t}-\frac{1}{2}\sigma^{2}t\right)|\mathcal{F}_{s}\right]\\
&amp; =S_{0}\exp(-\frac{1}{2}\sigma^{2}t)\mathbf{E}[\exp(\sigma(B_{t}-B_{s}+B_{s}))|\mathcal{F}_{s}]\\
&amp; =S_{0}\exp(-\frac{1}{2}\sigma^{2}t)\exp(\sigma B_{s})\mathbf{E}[\exp(\sigma(B_{t}-B_{s}))|\mathcal{F}_{s}]\\
&amp; \quad\{\text{Taking out what is known}\}\\
&amp; =S_{0}\exp\left(\sigma B_{s}-\frac{1}{2}\sigma^{2}t\right)\mathbf{E}\left[\exp\left(\sigma(B_{t}-B_{s})\right)\right]\\
&amp; \quad\{\text{Independence}\}\\
&amp; =S_{0}\exp\left(\sigma B_{s}-\frac{1}{2}\sigma^{2}t+\frac{1}{2}\sigma^{2}(t-s)\right)\\
&amp; =S_{0}\exp(\sigma B_{s}-\frac{1}{2}\sigma^{2}s)\\
&amp; =S_{s}
\end{aligned}\]</span></p>
<p>We will sometimes abuse terminology and refer to the martingale case of geometric brownian motion simply as geometric Brownian Motion when the context is clear.</p>
<p>(iii) <em>The square of the Brownian motion, compensated</em>. It is easy to check <span class="math inline">\((B_{t}^{2},t\geq0)\)</span> is a submartingale by direct computation using increments or by Jensen’s inequality: <span class="math inline">\(\mathbf{E}[B_{t}^{2}|\mathcal{F}_{s}]&gt;(\mathbf{E}[B_{t}|\mathcal{F}_{s}])^{2}=B_{s}^{2}\)</span>, <span class="math inline">\(s&lt;t\)</span>. It is nevertheless possible to compensate to get a martingale:</p>
<p><span class="math display">\[\begin{aligned}
M_{t} &amp; =B_{t}^{2}-t
\end{aligned}\]</span></p>
<p>It is an easy exercise to verify that <span class="math inline">\((M_{t}:t\geq0)\)</span> is a martingale for the Brownian filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}[B_{t}^{2}-t|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[B_{t}^{2}|\mathcal{F}_{s}]-t\\
&amp; =\mathbf{E}[(B_{t}-B_{s}+B_{s})^{2}|\mathcal{F}_{s}]-t\\
&amp; =\mathbf{E}[(B_{t}-B_{s})^{2}|\mathcal{F}_{s}]+2\mathbf{E}[(B_{t}-B_{s})B_{s}|\mathcal{F}_{s}]+\mathbf{E}[B_{s}^{2}|\mathcal{F}_{s}]-t\\
&amp; =\mathbf{E}[(B_{t}-B_{s})^{2}]+2B_{s}\mathbf{E}[(B_{t}-B_{s})|\mathcal{F}_{s}]+B_{s}^{2}-t\\
&amp; =\mathbf{E}[(B_{t}-B_{s})^{2}]+2B_{s}\mathbf{E}[(B_{t}-B_{s})]+B_{s}^{2}-t\\
&amp; \left\{ \begin{array}{c}
\text{\ensuremath{(B_{t}-B_{s})} is independent of \ensuremath{\mathcal{F}_{s}}}\\
\text{Also, \ensuremath{B_{s}} is known at time \ensuremath{s}}
\end{array}\right\} \\
&amp; =(t-s)+2B_{s}\cdot0+B_{s}^{2}-t\\
&amp; =B_{s}^{2}-s\\
&amp; =M_{s}
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>(Other important martingales).</p>
<p>(1) <em>Symmetric random walks.</em> This is an example of a martingale in discrete time. Take <span class="math inline">\((X_{i}:i\in\mathbf{N})\)</span> to be IID random variables with <span class="math inline">\(\mathbf{E}[X_{i}]=0\)</span> and <span class="math inline">\(\mathbf{E}[|X_{i}|]&lt;\infty\)</span>. Take <span class="math inline">\(\mathcal{F}_{n}=\sigma(X_{i},i\leq n)\)</span> and</p>
<p><span class="math display">\[\begin{aligned}
S_{n} &amp; =X_{1}+X_{2}+\ldots+X_{n},\quad S_{0}=0
\end{aligned}\]</span></p>
<p>Firstly, the information learned by observing the outcomes of <span class="math inline">\(X_{1}\)</span>,<span class="math inline">\(\ldots\)</span>,<span class="math inline">\(X_{n}\)</span> is enough to completely determine <span class="math inline">\(S_{n}\)</span>. Hence, <span class="math inline">\(S_{n}\)</span> is <span class="math inline">\(\mathcal{F}_{n}-\)</span>measurable.</p>
<p>Next, <span class="math display">\[\begin{aligned}
|S_{n}| &amp; =\left|\sum_{i=1}^{n}X_{i}\right|\\
&amp; \leq\sum_{i=1}^{n}|X_{i}|
\end{aligned}\]</span></p>
<p>Consequently, by the montonocity of expectations, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[|S_{n}|] &amp; \leq\sum_{i=1}^{n}\mathbf{E}[|X_{i}|]&lt;\infty
\end{aligned}\]</span></p>
<p>The martingale property is also satisfied. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[S_{n+1}|\mathcal{F}_{n}] &amp; =\mathbf{E}[S_{n}+X_{n+1}|\mathcal{F}_{n}]\\
&amp; =\mathbf{E}[S_{n}|\mathcal{F}_{n}]+\mathbf{E}[X_{n+1}|\mathcal{F}_{n}]\\
&amp; =S_{n}+\mathbf{E}[X_{n+1}]\\
&amp; \left\{ \begin{array}{c}
\text{\ensuremath{S_{n}} is \ensuremath{\mathcal{F}_{n}}-measurable}\\
\text{\ensuremath{X_{n+1}} is independent of \ensuremath{\mathcal{F}_{n}}}
\end{array}\right\} \\
&amp; =S_{n}+0\\
&amp; =S_{n}
\end{aligned}\]</span></p>
<p>(2) <em>Compensated Poisson process</em>. Let <span class="math inline">\((N_{t}:t\geq0)\)</span> be a Poisson process with rate <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\mathcal{F}_{t}=\sigma(N_{s},s\leq t)\)</span>. Then, <span class="math inline">\(N_{t}\)</span> is a submartingale for its natural filtration. Again, properties (1) and (2) are easily checked. <span class="math inline">\(N_{t}\)</span> is <span class="math inline">\(\mathcal{F}_{t}\)</span> measurable. Moreover, <span class="math inline">\(\mathbf{E}[|N_{t}|]=\mathbf{E}[N_{t}]=\frac{1}{\lambda t}&lt;\infty\)</span>. The submartingale property follows by the independence of increments : for <span class="math inline">\(s\leq t\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[N_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}[N_{t}-N_{s}+N_{s}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[N_{t}-N_{s}|\mathcal{F}_{s}]+\mathbf{E}[N_{s}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[N_{t}-N_{s}]+N_{s}\\
&amp; =\lambda(t-s)+N_{s}\\
&amp; \left\{ \because\mathbf{E}[N_{t}]=\lambda t\right\}
\end{aligned}\]</span></p>
<p>More importantly, we get a martingale by slightly modifying the process. Indeed, if we subtract <span class="math inline">\(\lambda t\)</span>, we have that the process :</p>
<p><span class="math display">\[\begin{aligned}
M_{t} &amp; =N_{t}-\lambda t
\end{aligned}\]</span></p>
<p>is a martingale. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}[N_{t}-\lambda t|\mathcal{F}_{s}]\\
&amp; =\lambda t-\lambda s+N_{s}-\lambda t\\
&amp; =N_{s}-\lambda s\\
&amp; =M_{s}
\end{aligned}\]</span></p>
<p>This is called the <em>compensated Poisson process</em>. Let us simulate <span class="math inline">\(10\)</span> paths of the compensated poisson process on <span class="math inline">\([0,10]\)</span>.</p>
</div>
<pre data-caption="Generating 10 paths of a compensated Poisson process"><code>import numpy as np
import matplotlib.pyplot as plt

# Generates a sample path of a compensated poisson process 
# with rate : `lambda_` per unit time
# on the interval [0,T], and subintervals of size `stepSize`.

def generateCompensatedPoissonPath(lambda_,T,stepSize):
    N = int(T/stepSize)   

    poissonParam = lambda_ * stepSize        

    x = np.random.poisson(lam=poissonParam,size=N)  
    x = np.concatenate([[0.0], x])
    N_t = np.cumsum(x)  
    t = np.linspace(start=0.0,stop=10.0,num=1001)

    M_t = np.subtract(N_t,lambda_ * t)  
    return M_t


t = np.linspace(0,10,1001)
plt.grid(True)

plt.xlabel(r'Time $t$')
plt.ylabel(r'Compensated poisson process $M(t)$')
plt.grid(True)
plt.title(r'$10$ paths of the compensated Poisson process on $[0,10]$')

for i in range(10):
    # Generate a poisson path with rate 1 /sec = 0.01 /millisec
    n_t = generateCompensatedPoissonPath(lambda_=1.0, T=10, stepSize=0.01)
    plt.plot(t, n_t)


plt.show()
plt.close()</code></pre>
<p>We saw in the two examples, that, even though a process is not itself a martingale, we can sometimes <em>compensate</em> to obtain a martingale! Ito Calculus will greatly extend this perspective. We will have systematic rules that show when a function of Brownian motion is a martingale and if not, how to modify it to get one.</p>
<p>For now, we observe that a convex function of a martingale is always a submartingale by Jensen’s inequality.</p>
<div class="cor">
<p><span id="corollary:the-convex-function-of-martingale-is-a-submartingale" label="corollary:the-convex-function-of-martingale-is-a-submartingale"></span>If <span class="math inline">\(c\)</span> is a convex function on <span class="math inline">\(\mathbf{R}\)</span> and <span class="math inline">\((M_{t}:t\geq0)\)</span> is a martingale for <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>, then the process <span class="math inline">\((c(M_{t}):t\geq0)\)</span> is a submartingale for the same filtration, granted that <span class="math inline">\(\mathbf{E}[|c(M_{t})|]&lt;\infty\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> The fact that <span class="math inline">\(c(M_{t})\)</span> is adapted to the filtration is clear since it is an explicit function of <span class="math inline">\(M_{t}\)</span>. The integrability is by assumption. The submartingale property is checked as follows:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[c(M_{t})|\mathcal{F}_{s}] &amp; \geq c(\mathbf{E}[M_{t}|\mathcal{F}_{s}])=c(M_{s})
\end{aligned}\]</span> ◻</p>
</div>
<div class="rem*">
<p>(The Doob-Meyer Decomposition Theorem). Let <span class="math inline">\((X_{n}:n\in\mathbf{N})\)</span> be a submartingale with respect to a filtration <span class="math inline">\((\mathcal{F}_{n}:n\in\mathbf{N})\)</span>. Define a sequence of random variables <span class="math inline">\((A_{n}:n\in\mathbf{N})\)</span> by <span class="math inline">\(A_{0}=0\)</span> and</p>
<p><span class="math display">\[\begin{aligned}
A_{n} &amp; =\sum_{i=1}^{n}(\mathbf{E}[X_{i}|\mathcal{F}_{i-1}]-X_{i-1}),\quad n\geq1
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(A_{n}\)</span> is <span class="math inline">\(\mathcal{F}_{n-1}\)</span>-measurable. Moreover, since <span class="math inline">\((X_{n}:n\in\mathbf{N})\)</span> is a submartingale, we have <span class="math inline">\(\mathbf{E}[X_{i}|\mathcal{F}_{i-1}]-X_{i-1}\geq0\)</span> almost surely. Hence, <span class="math inline">\((A_{n}:n\in\mathbf{N})\)</span> is an increasing sequence almost surely. Let <span class="math inline">\(M_{n}=X_{n}-A_{n}\)</span>.</p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{n}|\mathcal{F}_{n-1}] &amp; =\mathbf{E}[X_{n}-A_{n}|\mathcal{F}_{n-1}]\\
&amp; =\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-\mathbf{E}[A_{n}|\mathcal{F}_{n-1}]\\
&amp; =\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-\mathbf{E}\left[\left.\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-X_{n-1}+A_{n-1}\right|\mathcal{F}_{n-1}\right]\\
&amp; =\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]+\mathbf{E}[X_{n-1}|\mathcal{F}_{n-1}]-\mathbf{E}[A_{n-1}|\mathcal{F}_{n-1}]\\
&amp; =\cancel{\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]}-\cancel{\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]}+X_{n-1}-A_{n-1}\\
&amp; =M_{n-1}
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\((M_{n}:n\in\mathbf{N})\)</span> is a martingale. Thus, we have obtained the Doob decomposition:</p>
<p><span class="math display">\[\begin{aligned}
X_{n} &amp; =M_{n}+A_{n}\label{eq:doob-decomposition}
\end{aligned}\]</span></p>
<p>This decomposition of a submartingale as a sum of a martingale and an adapted increasing sequence is unique, if we require that <span class="math inline">\(A_{0}=0\)</span> and that <span class="math inline">\(A_{n}\)</span> is <span class="math inline">\(\mathcal{F}_{n-1}\)</span>-measurable.</p>
<p>For the continuous-time case, the situation is much more complicated. The analogue of equation (<a href="#eq:doob-decomposition" data-reference-type="ref" data-reference="eq:doob-decomposition">[eq:doob-decomposition]</a>) is called the <em>Doob-Meyer decomposition</em>. We briefly describe this decomposition and avoid the technical details. All stochastic processes <span class="math inline">\(X(t)\)</span> are assumed to be right-continuous with left-hand limits <span class="math inline">\(X(t-)\)</span>.</p>
<p>Let <span class="math inline">\(X(t)\)</span>, <span class="math inline">\(a\leq t\leq b\)</span> be a submartingale with respect to a right-continuous filtration <span class="math inline">\((\mathcal{F}_{t}:a\leq t\leq b)\)</span>. If <span class="math inline">\(X(t)\)</span> satisfies certain conditions, then it can be uniquely decomposed as:</p>
<p><span class="math display">\[\begin{aligned}
X(t) &amp; =M(t)+C(t),\quad a\leq t\leq b
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(M(t)\)</span>, <span class="math inline">\(a\leq t\leq b\)</span> is a martingale with respect to <span class="math inline">\((\mathcal{F}_{t};a\leq t\leq b)\)</span>, <span class="math inline">\(C(t)\)</span> is right-continuous and increasing almost surely with <span class="math inline">\(\mathbf{E}[C(t)]&lt;\infty\)</span>.</p>
</div>
<div class="example">
<p>(Square of a Poisson Process). Let <span class="math inline">\((N_{t}:t\geq0)\)</span> be a Poisson process with rate <span class="math inline">\(\lambda\)</span>. We consider the compensated process <span class="math inline">\(M_{t}=N_{t}-\lambda t\)</span>. By (<a href="#corollary:the-convex-function-of-martingale-is-a-submartingale" data-reference-type="ref" data-reference="corollary:the-convex-function-of-martingale-is-a-submartingale">[corollary:the-convex-function-of-martingale-is-a-submartingale]</a>), the process <span class="math inline">\((M_{t}^{2}:t\geq0)\)</span> is a submartingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> of the Poisson process. How should we compensated <span class="math inline">\(M_{t}^{2}\)</span> to get a martingale? A direct computation using the properties of conditional expectation yields:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}^{2}|\mathcal{F}_{s}] &amp; =\mathbf{E}[(M_{t}-M_{s}+M_{s})^{2}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[(M_{t}-M_{s})^{2}+2(M_{t}-M_{s})M_{s}+M_{s}^{2}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[(M_{t}-M_{s})^{2}|\mathcal{F}_{s}]+2\mathbf{E}[(M_{t}-M_{s})M_{s}|\mathcal{F}_{s}]+\mathbf{E}[M_{s}^{2}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[(M_{t}-M_{s})^{2}]+2M_{s}\underbrace{\mathbf{E}[M_{t}-M_{s}]}_{\text{equals \ensuremath{0}}}+M_{s}^{2}\\
&amp; =\mathbf{E}[(M_{t}-M_{s})^{2}]+M_{s}^{2}
\end{aligned}\]</span></p>
<p>Now, if <span class="math inline">\(X\sim\text{Poisson\ensuremath{(\lambda t)}}\)</span>, then <span class="math inline">\(\mathbf{E}[X]=\lambda t\)</span> and <span class="math inline">\(\mathbf{E}\ensuremath{[X^{2}]}=\lambda t(\lambda t+1)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(M_{t}-M_{s})^{2}] &amp; =\mathbf{E}\left[\left\{ (N_{t}-N_{s})-\lambda(t-s)\right\} ^{2}\right]\\
&amp; =\mathbf{E}\left[(N_{t}-N_{s})^{2}\right]-2\lambda(t-s)\mathbf{E}\left[(N_{t}-N_{s})\right]+\lambda^{2}(t-s)^{2}\\
&amp; =\lambda^{2}(t-s)^{2}+\lambda(t-s)-2\lambda(t-s)\cdot\lambda(t-s)+\lambda^{2}(t-s)^{2}\\
&amp; =\lambda(t-s)
\end{aligned}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}^{2}-\lambda t|\mathcal{F}_{s}] &amp; =M_{s}^{2}-\lambda s
\end{aligned}\]</span></p>
<p>We conclude that the process <span class="math inline">\((M_{t}^{2}-\lambda t:t\geq0)\)</span> is a martingale. The Doob-Meyer decomposition of the submartingale <span class="math inline">\(M_{t}^{2}\)</span> is then:</p>
<p><span class="math display">\[\begin{aligned}
M_{t}^{2} &amp; =(M_{t}^{2}-\lambda t)+\lambda t
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>Consider a Brownian motion <span class="math inline">\(B(t)\)</span>. The quadratic variation of the process <span class="math inline">\((B(t):t\geq0)\)</span> over the interval <span class="math inline">\([0,t]\)</span> is given by <span class="math inline">\([B]_{t}=t\)</span>. On the other hand, we saw, that the square of Brownian motion compensated, <span class="math inline">\((B_{t}^{2}-t:t\geq0)\)</span> is a martingale. Hence, the Doob-Meyer decomposition of <span class="math inline">\(B(t)^{2}\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
B(t)^{2} &amp; =(B(t)^{2}-t)+t
\end{aligned}\]</span></p>
</div>
</section>
<section id="computations-with-martingales." class="level2">
<h2 class="anchored" data-anchor-id="computations-with-martingales.">Computations with Martingales.</h2>
<p>Martingales are not only conceptually interesting, they are also formidable tools to compute probabilities and expectations of processes. For example, in this section, we will solve the <em>gambler’s ruin</em> problem for Brownian motion. For convenience, we introduce the notion of <em>stopping time</em> before doing so.</p>
<div class="defn">
<p>A random variable <span class="math inline">\(\tau:\Omega\to\mathbf{N}\cup\{+\infty\}\)</span> is said to be a <em>stopping time</em> for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> if and only if:</p>
<p><span class="math display">\[\begin{aligned}
\{\omega:\tau(\omega)\leq t\} &amp; \in\mathcal{F}_{t},\quad\forall t\geq0
\end{aligned}\]</span> Note that since <span class="math inline">\(\mathcal{F}_{t}\)</span> is a sigma-field, if <span class="math inline">\(\tau\)</span> is a stopping time, then we must also have that <span class="math inline">\(\{\omega:\tau(\omega)&gt;t\}\in\mathcal{F}_{t}\)</span>.</p>
<p>In other words, <span class="math inline">\(\tau\)</span> is a stopping time, if we can decide if the events <span class="math inline">\(\{\tau\leq t\}\)</span> occurred or not based on the information available at time <span class="math inline">\(t\)</span>.</p>
<p>The term <em>stopping time</em> comes from gambling: a gambler can decide to stop playing at a random time (depending for example on previous gains or losses), but when he or she decides to stop, his/her decision is based solely upon the knowledge of what happened before, and does not depend on future outcomes. In other words, the stopping policy/strategy can only depend on past outcomes. Otherwise, it would mean that he/she has a crystall ball.</p>
</div>
<div class="example">
<p>(Examples of stopping times).</p>
<p>(i) <em>First passage time</em>. This is the first time when a process reaches a certain value. To be precise, let <span class="math inline">\(X=(X_{t}:t\geq0)\)</span> be a process and <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> be its natural filtration. For <span class="math inline">\(a&gt;0\)</span>, we define the first passage time at <span class="math inline">\(a\)</span> to be:</p>
<p><span class="math display">\[\begin{aligned}
\tau(\omega) &amp; =\inf\{s\geq0:X_{s}(\omega)\geq a\}
\end{aligned}\]</span></p>
<p>If the path <span class="math inline">\(\omega\)</span> never reaches <span class="math inline">\(a\)</span>, we set <span class="math inline">\(\tau(\omega)=\infty\)</span>. Now, for <span class="math inline">\(t\)</span> fixed and for a given path <span class="math inline">\(X(\omega)\)</span>, it is possible to know if <span class="math inline">\(\{\tau(\omega)\leq t\}\)</span> (the path has reached <span class="math inline">\(a\)</span> before time <span class="math inline">\(t\)</span>) or <span class="math inline">\(\{\tau(\omega)&gt;t\}\)</span> (the path has not reached <span class="math inline">\(a\)</span> before time <span class="math inline">\(t\)</span>) with the information available at time <span class="math inline">\(t\)</span>, since we are looking at the first time the process reaches <span class="math inline">\(a\)</span>. Hence, we conclude that <span class="math inline">\(\tau\)</span> is a stopping time.</p>
<p>(ii) <em>Hitting time</em>. More generally, we can consider the first time (if ever) that the path of a process <span class="math inline">\((X_{t}:t\geq0)\)</span> enters or hits a subset <span class="math inline">\(B\)</span> of <span class="math inline">\(\mathbf{R}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\tau(\omega) &amp; =\min\{s\geq0:X_{s}(\omega)\in B\}
\end{aligned}\]</span></p>
<p>The first passage time is the particular case in which <span class="math inline">\(B=[a,\infty)\)</span>.</p>
<p>(iii) <em>Minimum of two stopping times.</em> If <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\tau'\)</span> are two stopping times for the same filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>, then so is the minimum <span class="math inline">\(\tau\land\tau'\)</span> between the two, where</p>
<p><span class="math display">\[\begin{aligned}
(\tau\land\tau')(\omega) &amp; =\min\{\tau(\omega),\tau'(\omega)\}
\end{aligned}\]</span></p>
<p>This is because for any <span class="math inline">\(t\geq0\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\{\omega: &amp; (\tau\land\tau')(\omega)\leq t\}=\{\omega:\tau(\omega)\leq t\}\cup\{\omega:\tau'(\omega)\leq t\}
\end{aligned}\]</span></p>
<p>Since the right hand side is the union of two events in <span class="math inline">\(\mathcal{F}_{t}\)</span>, it must also be in <span class="math inline">\(\mathcal{F}_{t}\)</span> by the properties of a sigma-field. We conclude that <span class="math inline">\(\tau\land\tau'\)</span> is a stopping time. Is it also the case that the maximum <span class="math inline">\(\tau\lor\tau'\)</span> is a stopping time?</p>
<p>For any fixed <span class="math inline">\(t\geq0\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\{\omega:(\tau\lor\tau')(\omega)\leq t\} &amp; =\{\omega:\tau(\omega)\leq t\}\cap\{\omega:\tau'(\omega)\leq t\}
\end{aligned}\]</span></p>
<p>Since the right hand side is the intersection of two events in <span class="math inline">\(\mathcal{F}_{t}\)</span>, it must also be in <span class="math inline">\(\mathcal{F}_{t}\)</span> by the properties of a sigma-field. We conclude that <span class="math inline">\(\tau\lor\tau'\)</span> is a stopping time.</p>
</div>
<div class="example">
<p>(Last passage time is not a stopping time). What if we look at the last time the process reaches <span class="math inline">\(a\)</span>, that is:</p>
<p><span class="math display">\[\begin{aligned}
\rho(\omega) &amp; =\sup\{t\geq0:X_{t}(\omega)\geq a\}
\end{aligned}\]</span></p>
<p>This is a well-defined random variable, but it is not a stopping time. Based on the information available at time <span class="math inline">\(t\)</span>, we are not able to decide whether or not <span class="math inline">\(\{\rho(\omega)\leq t\}\)</span> occurred or not, as the path can always reach <span class="math inline">\(a\)</span> one more time after <span class="math inline">\(t\)</span>.</p>
</div>
<p>It turns out that a martingale that is stopped when the stopping time is attained remains a martingale.</p>
<div class="prop">
<p>(Stopped Martingale). If <span class="math inline">\((M_{t}:t\geq0)\)</span> is a continuous martingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> and <span class="math inline">\(\tau\)</span> is a stopping time for the same filtration, then the stopped process defined by <span class="math display">\[\begin{aligned}
M_{t\land\tau} &amp; =\begin{cases}
M_{t} &amp; t\leq\tau\\
M_{\tau} &amp; t&gt;\tau
\end{cases}
\end{aligned}\]</span></p>
<p>is also a continuous martingale for the same filtration.</p>
</div>
<div class="thm">
<p>[]{#th:doob’s-optional-sampling-theorem label=“th:doob’s-optional-sampling-theorem”}(Doob’s Optional sampling theorem). If <span class="math inline">\((M_{t}:t\geq0)\)</span> is a continuous martingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> and <span class="math inline">\(\tau\)</span> is a stopping time such that <span class="math inline">\(\tau&lt;\infty\)</span> and the stopped process <span class="math inline">\((M_{t\land\tau}:t\geq0)\)</span> is bounded, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{\tau}] &amp; =M_{0}
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Since <span class="math inline">\((M_{\tau\land t}:t\geq0)\)</span> is a martingale, we always have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{\tau\land t}] &amp; =M_{0}
\end{aligned}\]</span></p>
<p>Now, since <span class="math inline">\(\tau(\omega)&lt;\infty\)</span>, we must</p>
<p>have that <span class="math inline">\(\lim_{t\to\infty}M_{\tau\land t}=M_{\tau}\)</span> almost surely. In particular, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{\tau}] &amp; =\mathbf{E}\left[\lim_{t\to\infty}M_{\tau\land t}\right]=\lim_{t\to\infty}\mathbf{E}[M_{\tau\land t}]=\lim_{t\to\infty}M_{0}
\end{aligned}\]</span></p>
<p>where we passed to the limit, using the dominated convergence theorem (<a href="#th:dominated-convergence-theorem" data-reference-type="ref" data-reference="th:dominated-convergence-theorem">[th:dominated-convergence-theorem]</a>). ◻</p>
</div>
<div class="example">
<p><span id="example:probability-of-hitting-times" label="example:probability-of-hitting-times"></span>(Gambler’s ruin with Brownian motion). The <em>gambler’s ruin problem</em> is known in different forms. Roughly speaking, it refers to the problem of computing the probability of a gambler making a series of bets reaching a certain amount before going broke. In terms of Brownian motion (and stochastic processes in general), it translates to the following questions: Let <span class="math inline">\((B_{t}:t\geq0)\)</span> be a standard brownian motion starting at <span class="math inline">\(B_{0}=0\)</span> and <span class="math inline">\(a,b&gt;0\)</span>.</p>
<p>(1) What is the probability that a Brownian path reaches <span class="math inline">\(a\)</span> before <span class="math inline">\(-b\)</span>?</p>
<p>(2) What is the expected waiting time for the path to reach <span class="math inline">\(a\)</span> or <span class="math inline">\(-b\)</span>?</p>
<p>For the first question, it is a simple computation using stopping time and martingale properties. Define the hitting time:</p>
<p><span class="math display">\[\begin{aligned}
\tau(\omega) &amp; =\inf\{t\geq0:B_{t}(\omega)\geq a\text{ or }B_{t}(\omega)\leq-b\}
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(\tau\)</span> is the minimum between the first passage time at <span class="math inline">\(a\)</span> and the one at <span class="math inline">\(-b\)</span>.</p>
<p>We first show that <span class="math inline">\(\tau&lt;\infty\)</span> almost surely. In other words, all Brownian paths reach <span class="math inline">\(a\)</span> or <span class="math inline">\(-b\)</span> eventually. To see this, consider the event <span class="math inline">\(E_{n}\)</span> that the <span class="math inline">\(n\)</span>-th increment exceeds <span class="math inline">\(a+b\)</span></p>
<p><span class="math display">\[\begin{aligned}
E_{n} &amp; :=\left\{ |B_{n}-B_{n-1}|&gt;a+b\right\}
\end{aligned}\]</span></p>
<p>Note that, if <span class="math inline">\(E_{n}\)</span> occurs, then we must have that the Brownian motion path exits the interval <span class="math inline">\([-b,a].\)</span> Moreover, we have <span class="math inline">\(\mathbb{P}(E_{n})=\mathbb{P}(E_{1})\)</span> for all <span class="math inline">\(n\)</span>. Call this probability <span class="math inline">\(p\)</span>.</p>
<p>Since the events <span class="math inline">\(E_{n}\)</span> are independent, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}) &amp; =(1-p)^{n}
\end{aligned}\]</span></p>
<p>As <span class="math inline">\(n\to\infty\)</span> we have:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\mathbb{P}(E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}) &amp; =0
\end{aligned}\]</span></p>
<p>The sequence of events <span class="math inline">\((F_{n})\)</span> where <span class="math inline">\(F_{n}=E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}\)</span> is a decreasing sequence of events. By the continuity of probability measure lemma (<a href="#th:continuity-property-of-lebesgue-measure" data-reference-type="ref" data-reference="th:continuity-property-of-lebesgue-measure">[th:continuity-property-of-lebesgue-measure]</a>), we conclude that:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\mathbb{P}\left(F_{n}\right) &amp; =\mathbb{P}\left(\bigcap_{n=1}^{\infty}F_{n}\right)=0
\end{aligned}\]</span></p>
<p>Therefore, it must be the case <span class="math inline">\(\mathbb{P}(\cup_{n=1}^{\infty}E_{n})=1\)</span>. So, <span class="math inline">\(E_{n}\)</span> must occur for some <span class="math inline">\(n\)</span>, so all brownian motion paths reach <span class="math inline">\(a\)</span> or <span class="math inline">\(-b\)</span> almost surely.</p>
<p>Since <span class="math inline">\(\tau&lt;\infty\)</span> with probability one, the random variable <span class="math inline">\(B_{\tau}\)</span> is well-defined : <span class="math inline">\(B_{\tau}(\omega)=B_{t}(\omega)\)</span> if <span class="math inline">\(\tau(\omega)=t\)</span>. It can only take two values: <span class="math inline">\(a\)</span> or <span class="math inline">\(-b\)</span>. Question (1) above translates into computing <span class="math inline">\(\mathbb{P}(B_{\tau}=a)\)</span>. On one hand, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[B_{\tau}] &amp; =a\mathbb{P}(B_{\tau}=a)+(-b)(1-\mathbb{P}(B_{\tau}=a))
\end{aligned}\]</span></p>
<p>On the other hand, by corollary (<a href="#th:doob's-optional-sampling-theorem" data-reference-type="ref" data-reference="th:doob's-optional-sampling-theorem">[th:doob's-optional-sampling-theorem]</a>), we have <span class="math inline">\(\mathbf{E}[B_{\tau}]=\mathbf{E}[B_{0}]=0\)</span>. (Note that the stopped process <span class="math inline">\((B_{t\land\tau}:t\geq0)\)</span> is bounded above by <span class="math inline">\(a\)</span> and by <span class="math inline">\(-b\)</span> below). Putting these two observations together, we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(B_{\tau}=a) &amp; =\frac{b}{a+b}
\end{aligned}\]</span></p>
<p>A very simple and elegant answer!</p>
<p>We will revisit this problem again and again. In particular, we will answer the question above for Brownian motion with a drift at length further ahead.</p>
</div>
<div class="example">
<p><span id="ex:expected-waiting-times" label="ex:expected-waiting-times"></span>(Expected Waiting Time). Let <span class="math inline">\(\tau\)</span> be as in the last example. We now answer question (2) of the gambler’s ruin problem:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[\tau] &amp; =ab
\end{aligned}\]</span></p>
<p>Note that the expected waiting time is consistent with the rough heuristic that Brownian motion travels a distance <span class="math inline">\(\sqrt{t}\)</span> by time <span class="math inline">\(t\)</span>. We now use the martingale <span class="math inline">\(M_{t}=B_{t}^{2}-t\)</span>. On the one hand, if we apply optional stopping in corollary (<a href="#th:doob's-optional-sampling-theorem" data-reference-type="ref" data-reference="th:doob's-optional-sampling-theorem">[th:doob's-optional-sampling-theorem]</a>), we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{\tau}] &amp; =M_{0}=0
\end{aligned}\]</span></p>
<p>Moreover, we know the distribution of <span class="math inline">\(B_{\tau}\)</span>, thanks to the probability calculated in the last example. We can therefore compute <span class="math inline">\(\mathbf{E}[M_{\tau}]\)</span> directly:</p>
<p><span class="math display">\[\begin{aligned}
0 &amp; =\mathbf{E}[M_{\tau}]\\
&amp; =\mathbf{E}[B_{\tau}^{2}-\tau]\\
&amp; =\mathbf{E}[B_{\tau}^{2}]-\mathbf{E}[\tau]\\
&amp; =a^{2}\cdot\frac{b}{a+b}+b^{2}\cdot\frac{a}{a+b}-\mathbf{E}[\tau]\\
\mathbf{E}[\tau] &amp; =\frac{a^{2}b+b^{2}a}{a+b}\\
&amp; =\frac{ab\cancel{(a+b)}}{\cancel{(a+b)}}=ab
\end{aligned}\]</span></p>
<p>Why can we apply optional stopping here? The random variable <span class="math inline">\(\tau\)</span> is finite with probability <span class="math inline">\(1\)</span> as before. However, the stopped martingale is not necessarily bounded as before: <span class="math inline">\(B_{\tau\land t}\)</span> is bounded but <span class="math inline">\(\tau\)</span> is not. However, the conclusion of optional stopping still holds. Indeed, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t\land\tau}] &amp; =\mathbf{E}[B_{t\land\tau}^{2}]-\mathbf{E}[t\land\tau]
\end{aligned}\]</span></p>
<p>By the bounded convergence theorem, we get <span class="math inline">\(\lim_{t\to\infty}\mathbf{E}[B_{t\land\tau}^{2}]=\mathbf{E}[\lim_{t\to\infty}B_{t\land\tau}^{2}]=\mathbf{E}[B_{\tau}^{2}]\)</span>. Since <span class="math inline">\(\tau\land t\)</span> is a non-decreasing sequence and as <span class="math inline">\(t\to\infty\)</span>, <span class="math inline">\(t\land\tau\to\tau\)</span> almost surely, as <span class="math inline">\(\tau&lt;\infty\)</span>, by the monotone convergence theorem, <span class="math inline">\(\lim_{t\to\infty}\mathbf{E}[t\land\tau]=\mathbf{E}[\tau]\)</span>.</p>
</div>
<div class="example">
<p>(First passage time of Brownian Motion.) We can use the previous two examples to get some very interesting information on the first passage time:</p>
<p><span class="math display">\[\begin{aligned}
\tau_{a} &amp; =\inf\{t\geq0:B_{t}\geq a\}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(\tau=\tau_{a}\land\tau_{-b}\)</span> be as in the previous examples with <span class="math inline">\(\tau_{-b}=\inf\{t\geq0:B_{t}\leq-b\}\)</span>. Note that <span class="math inline">\((\tau_{-b},b\in\mathbf{R}_{+})\)</span> is a sequence of random variables that is increasing in <span class="math inline">\(b\)</span>. A brownian motion path must cross through <span class="math inline">\(-1\)</span> before it hits <span class="math inline">\(-2\)</span> for the first time and in general <span class="math inline">\(\tau_{-n}(\omega)\leq\tau_{-(n+1)}(\omega)\)</span>. Moreover, we have <span class="math inline">\(\tau_{-b}\to\infty\)</span> almost surely as <span class="math inline">\(b\to\infty\)</span>. That’s because, <span class="math inline">\(\mathbb{P}\{\tau&lt;\infty\}=1\)</span>. Moreover, the event <span class="math inline">\(\{B_{\tau}=a\}\)</span> is the same as <span class="math inline">\(\{\tau_{a}&lt;\tau_{-b}\}\)</span>. Now, the events <span class="math inline">\(\{\tau_{a}&lt;\tau_{-b}\}\)</span> are increasing in <span class="math inline">\(b\)</span>, since if a path reaches <span class="math inline">\(a\)</span> before <span class="math inline">\(-b\)</span>, it will do so as well for a more negative value of <span class="math inline">\(-b\)</span>. On one hand, this means by the continuity of probability measure lemma (<a href="#th:continuity-property-of-lebesgue-measure" data-reference-type="ref" data-reference="th:continuity-property-of-lebesgue-measure">[th:continuity-property-of-lebesgue-measure]</a>) that:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{b\to\infty}\mathbb{P}\left\{ \tau_{a}&lt;\tau_{-b}\right\}  &amp; =\mathbb{P}\{\lim_{b\to\infty}\tau_{a}&lt;\tau_{-b}\}\\
&amp; =\mathbb{P}\{\tau_{a}&lt;\infty\}
\end{aligned}\]</span></p>
<p>On the other hand, we have by example (<a href="#example:probability-of-hitting-times" data-reference-type="ref" data-reference="example:probability-of-hitting-times">[example:probability-of-hitting-times]</a>)</p>
<p><span class="math display">\[\begin{aligned}
\lim_{b\to\infty}\mathbb{P}\left\{ \tau_{a}&lt;\tau_{-b}\right\}  &amp; =\lim_{b\to\infty}\mathbb{P}\{B_{\tau}=a\}\\
&amp; =\lim_{b\to\infty}\frac{b}{b+a}\\
&amp; =1
\end{aligned}\]</span></p>
<p>We just showed that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left\{ \tau_{a}&lt;\infty\right\}  &amp; =1\label{eq:first-passage-time-to-a-is-finite-almost-surely}
\end{aligned}\]</span></p>
<p>In other words, every Brownian path will reach <span class="math inline">\(a\)</span>, no matter how large <span class="math inline">\(a\)</span> is!</p>
<p>How long will it take to reach <span class="math inline">\(a\)</span> on average? Well, we know from example (<a href="#ex:expected-waiting-times" data-reference-type="ref" data-reference="ex:expected-waiting-times">[ex:expected-waiting-times]</a>) that <span class="math inline">\(\mathbf{E}[\tau_{a}\land\tau_{-b}]=ab\)</span>. On one hand this means,</p>
<p><span class="math display">\[\begin{aligned}
\lim_{b\to\infty}\mathbf{E}[\tau_{a}\land\tau_{-b}] &amp; =\lim_{b\to\infty}ab=\infty
\end{aligned}\]</span></p>
<p>On the other hand, since the random variables <span class="math inline">\(\tau_{-b}\)</span> are increasing,</p>
<p><span class="math display">\[\begin{aligned}
\lim_{b\to\infty}\mathbf{E}[\tau_{a}\land\tau_{-b}] &amp; =\mathbf{E}\left[\lim_{b\to\infty}\tau_{a}\land\tau_{-b}\right]=\mathbf{E}[\tau_{a}]
\end{aligned}\]</span></p>
<p>by the monotone convergence theorem (<a href="#th:monotone-convergence-theorem" data-reference-type="ref" data-reference="th:monotone-convergence-theorem">[th:monotone-convergence-theorem]</a>). We just proved that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[\tau_{a}] &amp; =\infty
\end{aligned}\]</span></p>
<p>In other words, any Brownian motion path will reach <span class="math inline">\(a\)</span>, but the expected waiting time for this to occur is infinite, no matter, how small <span class="math inline">\(a\)</span> is! What is happening here? No matter, how small <span class="math inline">\(a\)</span> is, there is always paths that reach very large negative values before hitting <span class="math inline">\(a\)</span>. These paths might be unlikely. However, the first passage time for these paths is so large that they affect the value of the expectation substantially. In other words, <span class="math inline">\(\tau_{a}\)</span> is a <em>heavy-tailed random variable</em>. We look at the distribution of <span class="math inline">\(\tau_{a}\)</span> in more detail in the next section.</p>
</div>
<div class="example">
<p>(When option stopping fails). Consider <span class="math inline">\(\tau_{a}\)</span>, the first passage time at <span class="math inline">\(a&gt;0\)</span>. The random variable <span class="math inline">\(B_{\tau_{a}}\)</span> is well-defined since <span class="math inline">\(\tau_{a}&lt;\infty\)</span>. In fact, we have <span class="math inline">\(B_{\tau_{a}}=a\)</span> with probability one. Therefore, the following must hold:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[B_{\tau_{a}}] &amp; =a\neq B_{0}
\end{aligned}\]</span></p>
<p>Optional stopping theorem corollary (<a href="#th:doob's-optional-sampling-theorem" data-reference-type="ref" data-reference="th:doob's-optional-sampling-theorem">[th:doob's-optional-sampling-theorem]</a>) does not apply here, since the stopped process <span class="math inline">\((B_{t\land\tau_{a}}:t\geq0)\)</span> is not bounded. <span class="math inline">\(B_{t\land\tau_{a}}\)</span> can become infinitely negative before hitting <span class="math inline">\(a\)</span>.</p>
</div>
</section>
<section id="reflection-principle-for-brownian-motion." class="level2">
<h2 class="anchored" data-anchor-id="reflection-principle-for-brownian-motion.">Reflection principle for Brownian motion.</h2>
<div class="prop">
<p><span id="prop:bacheliers-formula" label="prop:bacheliers-formula"></span>(Bachelier’s formula). Let <span class="math inline">\((B_{t}:t\leq T)\)</span> be a standard brownian motion on <span class="math inline">\([0,T].\)</span> Then, the CDF of the random variable <span class="math inline">\(\sup_{0\leq t\leq T}B_{t}\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\sup_{0\leq t\leq T}B_{t}\leq a\right) &amp; =\mathbb{P}\left(|B_{T}|\leq a\right)
\end{aligned}\]</span></p>
<p>In particular, its PDF is:</p>
<p><span class="math display">\[\begin{aligned}
f_{\max}(a) &amp; =\frac{2}{\sqrt{2\pi T}}e^{-\frac{a^{2}}{2T}}
\end{aligned}\]</span></p>
</div>
<div class="rem*">
<p>We can verify these results empirically. Note that the paths of the random variables <span class="math inline">\(\max_{0\leq s\leq t}B_{s}\)</span> and <span class="math inline">\(|B_{t}|\)</span> are very different as <span class="math inline">\(t\)</span> varies for a given <span class="math inline">\(\omega\)</span>. One is increasing and the other is not. The equality holds in distribution for a fixed <span class="math inline">\(t\)</span>. As a bonus corollary, we get the distribution of the first passage time at <span class="math inline">\(a\)</span>.</p>
</div>
<div class="cor">
<p>Let <span class="math inline">\(a\geq0\)</span> and <span class="math inline">\(\tau_{a}=\inf\{t\geq0:B_{t}\geq a\}\)</span>. Then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\tau_{a}\leq T\right) &amp; =\mathbb{P}\left(\max_{0\leq t\leq T}B_{t}\geq a\right)=\int_{a}^{\infty}\frac{2}{\sqrt{2\pi T}}e^{-\frac{x^{2}}{2T}}dx
\end{aligned}\]</span></p>
<p>In particular, the random variable <span class="math inline">\(\tau_{a}\)</span> has the PDF:</p>
<p><span class="math display">\[\begin{aligned}
f_{\tau_{a}}(t) &amp; =\frac{a}{\sqrt{2\pi}}\frac{e^{-\frac{a^{2}}{2t}}}{t^{3/2}},\quad t&gt;0
\end{aligned}\]</span></p>
<p>This implies that it is heavy-tailed with <span class="math inline">\(\mathbf{E}[\tau_{a}]=\infty\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> The maximum on <span class="math inline">\([0,T]\)</span> is larger than or equal to <span class="math inline">\(a\)</span> if and only if <span class="math inline">\(\tau_{a}\leq T\)</span>. Therefore, the events <span class="math inline">\(\{\max_{0\leq t\leq T}B_{t}\geq a\}\)</span> and <span class="math inline">\(\{\tau_{a}\leq T\}\)</span> are the same. So, the CDF <span class="math inline">\(\mathbb{P}(\tau_{a}\leq t)\)</span> of <span class="math inline">\(\tau_{a}\)</span>, by proposition (<a href="#prop:bacheliers-formula" data-reference-type="ref" data-reference="prop:bacheliers-formula">[prop:bacheliers-formula]</a>) <span class="math inline">\(\int_{a}^{\infty}f_{\max}(x)dx=\int_{a}^{\infty}\frac{2}{\sqrt{2\pi T}}e^{-\frac{x^{2}}{2T}}dx\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
f_{\tau_{a}}(t) &amp; =-2\phi(a/\sqrt{t})\cdot a\cdot\left(-\frac{1}{2t^{3/2}}\right)\\
&amp; =\frac{a}{t^{3/2}}\phi\left(\frac{a}{\sqrt{t}}\right)\\
&amp; =\frac{a}{t^{3/2}}\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{a^{2}}{2t}}
\end{aligned}\]</span></p>
<p>To estimate the expectation, it suffices to realize that for <span class="math inline">\(t\geq1\)</span>, <span class="math inline">\(e^{-\frac{a^{2}}{2t}}\)</span> is larger than <span class="math inline">\(e^{-\frac{a^{2}}{2}}\)</span>. Therefore, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[\tau_{a}] &amp; =\int_{0}^{\infty}t\frac{a}{\sqrt{2\pi}}\frac{e^{-a^{2}/2t}}{t^{3/2}}dt\geq\frac{ae^{-a^{2}/2}}{\sqrt{2\pi}}\int_{1}^{\infty}t^{-1/2}dt
\end{aligned}\]</span></p>
<p>This is an improper integral and it diverges like <span class="math inline">\(\sqrt{t}\)</span> and is infinite as claimed. ◻</p>
</div>
<p>To prove proposition (<a href="#prop:bacheliers-formula" data-reference-type="ref" data-reference="prop:bacheliers-formula">[prop:bacheliers-formula]</a>), we will need an important property of Brownian motion called the <em>reflection principle</em>. To motivate it, recall the reflection symmetry of Brownian motion at time <span class="math inline">\(s\)</span> in proposition (<a href="#prop:brownian-motion-symmetry-of-reflection-at-time-s" data-reference-type="ref" data-reference="prop:brownian-motion-symmetry-of-reflection-at-time-s">[prop:brownian-motion-symmetry-of-reflection-at-time-s]</a>). It turns out that this reflection property also holds if <span class="math inline">\(s\)</span> is replaced by a stopping time.</p>
<div class="lem">
<p><span id="lemma:BM-reflection-principle" label="lemma:BM-reflection-principle"></span>(Reflection principle). Let <span class="math inline">\((B_{t}:t\geq0)\)</span> be a standard Brownian motion and let <span class="math inline">\(\tau\)</span> be a stopping time for its filtration. Then, the process <span class="math inline">\((\tilde{B}_{t}:t\geq0)\)</span> defined by the reflection at time <span class="math inline">\(\tau\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{B}_{t} &amp; =\begin{cases}
B_{t} &amp; \text{if \ensuremath{t\leq\tau}}\\
B_{\tau}-(B_{t}-B_{\tau}) &amp; \text{if \ensuremath{t&gt;\tau}}
\end{cases}
\end{aligned}\]</span></p>
<p>is also a standard brownian motion.</p>
</div>
<div class="rem*">
<p>We defer the proof of the reflection property of Brownian motion to a further section. It is intuitive and instructive to quickly picture this in the discrete-time setting. I adopt the approach as in Shreve-I.</p>
<p>We repeatedly toss a fair coin (<span class="math inline">\(p\)</span>, the probability of <span class="math inline">\(H\)</span> on each toss, and <span class="math inline">\(q=1-p\)</span>, the probability of <span class="math inline">\(T\)</span> on each toss, are both equal to <span class="math inline">\(\frac{1}{2}\)</span>). We denote the successive outcomes of the tosses by <span class="math inline">\(\omega_{1}\omega_{2}\omega_{3}\ldots\)</span>. Let</p>
<p><span class="math display">\[\begin{aligned}
X_{j} &amp; =\begin{cases}
-1 &amp; \text{if \ensuremath{\omega_{j}=H}}\\
+1 &amp; \text{if \ensuremath{\omega_{j}=T}}
\end{cases}
\end{aligned}\]</span></p>
<p>and define <span class="math inline">\(M_{0}=0\)</span>, <span class="math inline">\(M_{n}=\sum_{j=1}^{n}X_{n}\)</span>. The process <span class="math inline">\((M_{n}:n\in\mathbf{N})\)</span> is a symmetric random walk.</p>
<p>Suppose we toss a coin an odd number <span class="math inline">\((2j-1)\)</span> of times. Some of the paths will reach level <span class="math inline">\(1\)</span> in the first <span class="math inline">\(2j-1\)</span> steps and other will not reach. In the case of <span class="math inline">\(3\)</span> tosses, there are <span class="math inline">\(2^{3}=8\)</span> possible paths and <span class="math inline">\(5\)</span> of these reach level <span class="math inline">\(1\)</span> at some time <span class="math inline">\(\tau_{1}\leq2j-1\)</span>. From that moment on, we can create a reflected path, which steps up each time the original path steps down and steps down each time the original path steps up. If the original path ends above <span class="math inline">\(1\)</span> at the final time <span class="math inline">\(2j-1\)</span>, the reflected path ends below <span class="math inline">\(1\)</span> and vice versa. If the original path ends at <span class="math inline">\(1\)</span>, the reflected path does also. In fact, the reflection at the first hitting time has the same distribution as the original random walk.</p>
<p>The key here is, out of the <span class="math inline">\(5\)</span> paths that reach level <span class="math inline">\(1\)</span> at some time, there are as many reflected paths that exceed <span class="math inline">\(1\)</span> at time <span class="math inline">\((2j-1)\)</span> as there are original paths that exceed <span class="math inline">\(1\)</span> at time <span class="math inline">\((2j-1)\)</span>. So, to count the total number of paths that reach level <span class="math inline">\(1\)</span> by time <span class="math inline">\((2j-1)\)</span>, we can count the paths that are at <span class="math inline">\(1\)</span> at time <span class="math inline">\((2j-1)\)</span> and then add on <em>twice</em> the number of paths that exceed <span class="math inline">\(1\)</span> at time <span class="math inline">\((2j-1)\)</span>.</p>
</div>
<p>With this new tool, we can now prove proposition (<a href="#prop:bacheliers-formula" data-reference-type="ref" data-reference="prop:bacheliers-formula">[prop:bacheliers-formula]</a>).</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Consider <span class="math inline">\(\mathbb{P}(\max_{t\leq T}B_{t}\geq a)\)</span>. By splitting this probability over the event of the endpoint, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}&gt;a\right)+\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right)
\end{aligned}\]</span></p>
<p>Note also, that <span class="math inline">\(\mathbb{P}(B_{T}=a)=0\)</span>. Hence, the first probability equals <span class="math inline">\(\mathbb{P}(B_{T}\geq a)\)</span>. As for the second, consider the time <span class="math inline">\(\tau_{a}\)</span>. On the event considered, we have <span class="math inline">\(\tau_{a}\leq T\)</span> and using lemma (<a href="#lemma:BM-reflection-principle" data-reference-type="ref" data-reference="lemma:BM-reflection-principle">[lemma:BM-reflection-principle]</a>) at that time, we get</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,\tilde{B}_{T}\geq a\right)
\end{aligned}\]</span></p>
<p>Observe that the event <span class="math inline">\(\{\max_{t\leq T}B_{t}\geq a\}\)</span> is the same as <span class="math inline">\(\{\max_{t\leq T}\tilde{B}_{T}\geq a\}\)</span>. (A rough picture might help here.) Thereforem the above probability is</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}\tilde{B}_{t}\geq a,\tilde{B}_{T}\geq a\right)=\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\geq a\right)
\end{aligned}\]</span></p>
<p>where the last equality follows from the reflection principle (<span class="math inline">\(\tilde{B}_{t}\)</span> is also a standard brownian motion, and <span class="math inline">\(B_{T}\)</span> and <span class="math inline">\(\tilde{B}_{T}\)</span> have the same distribution.) But, as above, the last probability is equal to <span class="math inline">\(\mathbb{P}(B_{T}\geq a)\)</span>. We conclude that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a\right) &amp; =2\mathbb{P}(B_{T}\geq a)=\frac{2}{\sqrt{2\pi T}}\int_{a}^{\infty}e^{-\frac{x^{2}}{2T}}dx=\mathbb{P}(|B_{T}|\geq a)
\end{aligned}\]</span></p>
<p>This implies in particular that <span class="math inline">\(\mathbb{P}\left(\max_{t\leq T}B_{t}=a\right)=0\)</span>. Thus, we also have <span class="math inline">\(\mathbb{P}(\max_{t\leq T}B_{t}\leq a)=\mathbb{P}(|B_{T}|\leq a)\)</span> as claimed. ◻</p>
</div>
<div class="example">
<p>(Simulating Martingales) Sample <span class="math inline">\(10\)</span> paths of the following process with a step-size of <span class="math inline">\(0.01\)</span>:</p>
<p>(a) <span class="math inline">\(B_{t}^{2}-t\)</span>, <span class="math inline">\(t\in[0,1]\)</span></p>
<p>(b) Geometric Brownian motion : <span class="math inline">\(S_{t}=\exp(B_{t}-t/2)\)</span>, <span class="math inline">\(t\in[0,1]\)</span>.</p>
<p>Let’s write a simple <span class="math inline">\(\texttt{BrownianMotion}\)</span> class, that we shall use to generate sample paths.</p>
</div>
<pre data-caption="10 paths of $B_t^2 - t$"><code>import numpy as np
import matplotlib.pyplot as plt

import attrs
from attrs import define, field

@define
class BrownianMotion:
    _step_size = field(validator=attrs.validators.and_(attrs.validators.instance_of(float),
                                                       attrs.validators.ge(0.0)))
    # Time T
    _T = field(validator=attrs.validators.and_(attrs.validators.instance_of(float),
                                               attrs.validators.ge(0.0)))
    # number of paths
    _N = field(validator=attrs.validators.and_(attrs.validators.instance_of(int),
                                               attrs.validators.gt(0)))

    _num_steps = field(init=False)

    def __attrs_post_init__(self):
        self._num_steps = int(self._T/self._step_size)

    def covariance_matrix(self):
        C = np.zeros((self._num_steps,self._num_steps))

        for i in range(self._num_steps):
            for j in range(self._num_steps):
                s = (i+1) * self._step_size
                t = (j+1) * self._step_size
                C[i,j] = min(s,t)
        return C

    # Each column vector represents a sample path
    def generate_paths(self):
        C = self.covariance_matrix()
        A = np.linalg.cholesky(C)
        Z = np.random.standard_normal((self._num_steps, self._N))
        X = np.matmul(A,Z)
        X = np.concatenate((np.zeros((1,self._N)),X),axis=0)
        return X.transpose()</code></pre>
<p>Now, the process <span class="math inline">\(B_{t}^{2}-t\)</span> can be sampled as follows:</p>
<pre data-caption="10 paths of $B_t^2 - t$"><code>
def generateSquareOfBMCompensated(numOfPaths,stepSize,T):
    N = int(T/stepSize)

    X = []
    brownianMotion = BrownianMotion(stepSize,T)
    for n in range(numOfPaths):

        B_t = brownianMotion.samplePath()

        B_t_sq = np.square(B_t)

        t = np.linspace(start=0.0,stop=1.0,num=N+1)
        M_t = np.subtract(B_t_sq,t)
        X.append(M_t)

    return X</code></pre>
<p>The gBM process can be sampled similarly, with <span class="math inline">\(\texttt{\ensuremath{M_{t}} = np.exp(np.subtract(\ensuremath{B_{t}},t/2))}\)</span>.</p>
<div class="example">
<p><strong>(Maximum of Brownian Motion.)</strong> Consider the maximum of Brownian motion on <span class="math inline">\([0,1]\)</span>: <span class="math inline">\(\max_{s\leq1}B_{s}\)</span>.</p>
<p>(a) Draw the histogram of the random variable <span class="math inline">\(\max_{s\leq1}B_{s}\)</span>using <span class="math inline">\(10,0000\)</span> sampled Brownian paths with a step size of <span class="math inline">\(0.01\)</span>.</p>
<p>(b) Compare this to the PDF of the random variable <span class="math inline">\(|B_{1}|\)</span>.</p>
</div>
<p><em>Solution.</em></p>
<p>I use the <span class="math inline">\(\texttt{itertools}\)</span> python library to compute the running maximum of a brownian motion path.</p>
<pre data-caption="The process $\sup_{s\leq 1}B_s$"><code>
brownianMotion = BrownianMotion(stepSize=0.01,T=1)
data = []

for i in range(10000):
    B_t = brownianMotion.samplePath()
    max_B_t = list(itertools.accumulate(B_t,max))
    data.append(max_B_t[100])</code></pre>
<p>Analytically, we know that <span class="math inline">\(B_{1}\)</span> is a gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(|B_{1}|\leq z) &amp; =\mathbb{P}(|Z|\leq z)\\
&amp; =\mathbb{P}(-z\leq Z\leq z)\\
&amp; =\mathbb{P}(Z\leq z)-\mathbb{P}(Z\leq-z)\\
&amp; =\mathbb{P}(Z\leq z)-(1-\mathbb{P}(Z\leq z))\\
F_{|B_{1}|}(z) &amp; =2\Phi(z)-1
\end{aligned}\]</span></p>
<p>Differentiating on both sides, we get:</p>
<p><span class="math display">\[\begin{aligned}
f_{|B_{1}|}(z) &amp; =2\phi(z)=\frac{2}{\sqrt{2\pi}}e^{-\frac{z^{2}}{2}},\quad z\in[0,\infty)
\end{aligned}\]</span></p>
<div class="example">
<p>(First passage time.) Let <span class="math inline">\((B_{t}:t\geq0)\)</span> be a standard brownian motion. Consider the random variable:</p>
<p><span class="math display">\[\begin{aligned}
\tau &amp; =\min\{t\geq0:B_{t}\geq1\}
\end{aligned}\]</span></p>
<p>This is the first time that <span class="math inline">\(B_{t}\)</span> reaches <span class="math inline">\(1\)</span>.</p>
<p>(a) Draw a histogram for the distribution of <span class="math inline">\(\tau\land10\)</span> on the time-interval <span class="math inline">\([0,10]\)</span> using <span class="math inline">\(10,000\)</span> brownian motion paths on <span class="math inline">\([0,10]\)</span> with discretization <span class="math inline">\(0.01\)</span>.</p>
<p><em>The notation <span class="math inline">\(\tau\land10\)</span> means that if the path does not reach <span class="math inline">\(1\)</span> on <span class="math inline">\([0,10]\)</span>, then give the value <span class="math inline">\(10\)</span> to the stopping time.</em></p>
<p>(b) Estimate <span class="math inline">\(\mathbf{E}[\tau\land10]\)</span>.</p>
<p>(c) What proportion of paths never reach <span class="math inline">\(1\)</span> in the time interval <span class="math inline">\([0,10]\)</span>?</p>
</div>
<p><em>Solution.</em></p>
<p>To compute the expectation, we classify the hitting times of all paths into <span class="math inline">\(50\)</span> bins. I simply did</p>
<p><span class="math inline">\(\texttt{frequency, bins = np.histogram(firstPassageTimes,bins=50,range=(0,10))}\)</span></p>
<p>and then computed</p>
<p><span class="math inline">\(\texttt{expectation=np.dot(frequency,bins[1:])/10000}\)</span>.</p>
<p>This expectation estimate on my machine is <span class="math inline">\(\mathbf{E}[\tau\land10]=4.34\)</span> secs. There were approximately <span class="math inline">\(2600\)</span> paths out of <span class="math inline">\(10,000\)</span> that did not reach <span class="math inline">\(1\)</span>.</p>
<div class="example">
<p><strong>Gambler’s ruin at the French Roulette.</strong> Consider the scenario in which you are gambling <span class="math inline">\(\$1\)</span> at the French roulette on the reds: You gain <span class="math inline">\(\$1\)</span> with probability <span class="math inline">\(18/38\)</span> and you lose a dollar with probability <span class="math inline">\(20/38\)</span>. We estimate the probability of your fortune reaching <span class="math inline">\(\$200\)</span> before it reaches <span class="math inline">\(0\)</span>.</p>
<p>(a) Write a function that samples the simple random walk path from time <span class="math inline">\(0\)</span> to time <span class="math inline">\(5,000\)</span> with a given starting point.</p>
<p>(b) Use the above to estimate the probability of reaching <span class="math inline">\(\$200\)</span> before <span class="math inline">\(\$0\)</span> on a sample of <span class="math inline">\(100\)</span> paths if you start with <span class="math inline">\(\$100\)</span>.</p>
</div>
<div class="example">
<p><strong>[]{#ex:doob’s-maximal-inequality label=“ex:doob’s-maximal-inequality”}Doob’s maximal inequalities</strong>. We prove the following: Let <span class="math inline">\((M_{k}:k\geq1)\)</span> be positive submartingale for the filtration <span class="math inline">\((\mathcal{F}_{k}:k\in\mathbf{N})\)</span>. Then, for any <span class="math inline">\(1\leq p&lt;\infty\)</span> and <span class="math inline">\(a&gt;0\)</span></p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{k\leq n}M_{k}&gt;a\right) &amp; \leq\frac{1}{a^{p}}\mathbf{E}[M_{n}^{p}]
\end{aligned}\]</span></p>
<p>(a) Use Jensen’s inequality to show that if <span class="math inline">\((M_{k}:k\geq1)\)</span> is a positive submartingale, then so is <span class="math inline">\((M_{k}^{p}:k\geq1)\)</span> for <span class="math inline">\(1\leq p&lt;\infty\)</span>. Conclude that it suffices to prove the statement for <span class="math inline">\(p=1\)</span>.</p>
</div>
<p><em>Solution.</em></p>
<p>The function <span class="math inline">\(f(x)=x^{p}\)</span> is convex. By conditional Jensen’s inequality,</p>
<p><span class="math display">\[\begin{aligned}
\left(\mathbf{E}[M_{k+1}|\mathcal{F}_{k}]\right)^{p} &amp; \leq\mathbf{E}[M_{k}^{p}|\mathcal{F}_{k}]
\end{aligned}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{k+1}^{p}|\mathcal{F}_{k}] &amp; \geq\left(\mathbf{E}[M_{k+1}|\mathcal{F}_{k}]\right)^{p}\geq M_{k}^{p}
\end{aligned}\]</span></p>
<p>where the last inequality follows from the fact that <span class="math inline">\((M_{k}:k\geq1)\)</span> is a positive submartingale, so <span class="math inline">\(\mathbf{E}[M_{k+1}|\mathcal{F}_{k}]\geq M_{k}\)</span>. Consequently, <span class="math inline">\((M_{k}^{p}:k\geq1)\)</span> is also a positive submartingale.</p>
<p>(b) Consider the events</p>
<p><span class="math display">\[\begin{aligned}
B_{k} &amp; =\bigcap_{j&lt;k}\{\omega:M_{j}(\omega)\leq a\}\cap\{\omega:M_{k}(\omega)&gt;a\}
\end{aligned}\]</span></p>
<p>Argue that the <span class="math inline">\(B_{k}\)</span>’s are disjoint and that <span class="math inline">\(\bigcup_{k\leq n}B_{k}=\{\max_{k\leq n}M_{k}&gt;a\}=B\)</span>.</p>
<p><em>Solution.</em></p>
<p>Clearly, <span class="math inline">\(B_{k}\)</span> is the event that the first time to cross <span class="math inline">\(a\)</span> is <span class="math inline">\(k\)</span>. If <span class="math inline">\(B_{k}\)</span> occurs, <span class="math inline">\(B_{k+1},B_{k+2},\ldots\)</span> fail to occur. Hence, all <span class="math inline">\(B_{k}'s\)</span> are pairwise disjoint. The event <span class="math inline">\(\bigcup_{k\leq n}B_{k}\)</span> is the event that the random walk crosses <span class="math inline">\(a\)</span> at any time <span class="math inline">\(k\leq n\)</span>. Thus, the running maximum of the Brownian motion at time <span class="math inline">\(n\)</span> exceeds <span class="math inline">\(a\)</span>.</p>
<p>(c) Show that</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{n}]\geq\mathbf{E}[M_{n}\mathbf{1}_{B}] &amp; \geq a\sum_{k\leq n}\mathbb{P}(B_{k})=a\mathbb{P}(B)
\end{aligned}\]</span></p>
<p>by decomposing <span class="math inline">\(B\)</span> in <span class="math inline">\(B_{k}\)</span>’s and by using the properties of expectations, as well as the submartingale property.</p>
<p><em>Solution.</em></p>
<p>Clearly, <span class="math inline">\(M_{n}\geq M_{n}\mathbf{1}_{B}\geq a\mathbf{1}_{B}\)</span>. And <span class="math inline">\(M_{n}\)</span> is a positive random variable. By monotonicity of expectations, <span class="math inline">\(\mathbf{E}[M_{n}]\geq\mathbf{E}[M_{n}\mathbf{1}_{B}]\geq a\mathbf{E}[\mathbf{1}_{B}]=a\mathbb{P}(B)=a\sum_{k\leq n}\mathbb{P}(B_{k})\)</span>, where the last equality holds because the <span class="math inline">\(B_{k}\)</span>’s are disjoint.</p>
<p>(d) Argue that the inequality holds for continuous paths by discretizing time and using convergence theorems : If <span class="math inline">\((M_{t}:t\geq0)\)</span> is a positive submartingale with continuous paths for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>, then for any <span class="math inline">\(1\leq p&lt;\infty\)</span> and <span class="math inline">\(a&gt;0\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{s\leq t}M_{s}&gt;a\right) &amp; \leq\frac{1}{a^{p}}\mathbf{E}[M_{t}^{p}]
\end{aligned}\]</span></p>
<p><em>Solution.</em></p>
<p>Let <span class="math inline">\((M_{t}:t\geq0)\)</span> be a positive submartingale with continuous paths for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>. Consider a sequence of partitions of the interval <span class="math inline">\([0,t]\)</span> into <span class="math inline">\(2^{r}\)</span> subintervals :</p>
<p><span class="math display">\[\begin{aligned}
D_{r} &amp; =\left\{ \frac{kt}{2^{r}}:k=0,1,2,\ldots,2^{n}\right\}
\end{aligned}\]</span></p>
<p>And consider a sequence of discrete positive sub-martingales:</p>
<p><span class="math display">\[\begin{aligned}
M_{kt/2^{r}}^{(r)} &amp; =M_{kt/2^{r}},\quad k\in\mathbf{N},0\leq k\leq2^{r}
\end{aligned}\]</span></p>
<p>Next, we define for <span class="math inline">\(r=1,2,3,\ldots\)</span></p>
<p><span class="math display">\[\begin{aligned}
A_{r} &amp; =\left\{ \sup_{s\in D_{r}}|M_{s}^{(r)}|&gt;a\right\}
\end{aligned}\]</span></p>
<p>By using the maximal inequality in discrete time, gives us:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(A_{r})=\mathbb{P}\left\{ \sup_{s\in D_{r}}|M_{s}^{(r)}|&gt;a\right\}  &amp; \leq\frac{1}{a^{p}}\mathbf{E}\left[\left(M_{s}^{(r)}\right)^{p}\right]=\frac{1}{a^{p}}\mathbf{E}\left[M_{t}^{p}\right]
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{s\leq t}M_{s}&gt;a\right) &amp; =\mathbb{P}\left(\bigcup_{r=1}^{\infty}A_{r}\right)\\
&amp; =\lim_{r\to\infty}\mathbb{P}\left(A_{r}\right)\\
&amp; \left\{ \text{Continuity of probability measure}\right\} \\
&amp; \leq\lim_{r\to\infty}\frac{1}{a^{p}}\mathbf{E}\left[M_{t}^{p}\right]
\end{aligned}\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-07-12">

<title>Martingales – quantdev.blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-d129a44951930463e8a313df5966fbea.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-52232225ae3909a0ea66e9fd7c84c945.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Martingales</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Stochastic Calculus</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 12, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#martingales." id="toc-martingales." class="nav-link active" data-scroll-target="#martingales.">Martingales.</a>
  <ul class="collapse">
  <li><a href="#elementary-conditional-expectation." id="toc-elementary-conditional-expectation." class="nav-link" data-scroll-target="#elementary-conditional-expectation.">Elementary conditional expectation.</a></li>
  <li><a href="#conditional-expectation-as-a-projection." id="toc-conditional-expectation-as-a-projection." class="nav-link" data-scroll-target="#conditional-expectation-as-a-projection.">Conditional Expectation as a projection.</a>
  <ul class="collapse">
  <li><a href="#properties-of-conditional-expectation." id="toc-properties-of-conditional-expectation." class="nav-link" data-scroll-target="#properties-of-conditional-expectation.">Properties of Conditional Expectation.</a></li>
  </ul></li>
  <li><a href="#martingales.-1" id="toc-martingales.-1" class="nav-link" data-scroll-target="#martingales.-1">Martingales.</a></li>
  <li><a href="#computations-with-martingales." id="toc-computations-with-martingales." class="nav-link" data-scroll-target="#computations-with-martingales.">Computations with Martingales.</a></li>
  <li><a href="#reflection-principle-for-brownian-motion." id="toc-reflection-principle-for-brownian-motion." class="nav-link" data-scroll-target="#reflection-principle-for-brownian-motion.">Reflection principle for Brownian motion.</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="martingales." class="level1">
<h1>Martingales.</h1>
<section id="elementary-conditional-expectation." class="level2">
<h2 class="anchored" data-anchor-id="elementary-conditional-expectation.">Elementary conditional expectation.</h2>
<p>In elementary probability, the conditional expectation of a variable <span class="math inline">\(Y\)</span> given another random variable <span class="math inline">\(X\)</span> refers to the expectation of <span class="math inline">\(Y\)</span> given the conditional distribution <span class="math inline">\(f_{Y|X}(y|x)\)</span> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. To illustrate this, let’s go through a simple example. Consider <span class="math inline">\(\mathcal{B}_{1}\)</span>, <span class="math inline">\(\mathcal{B}_{2}\)</span> to be two independent Bernoulli-distributed random variables with <span class="math inline">\(p=1/2\)</span>. Then, construct:</p>
<p><span class="math display">\[\begin{aligned}
X=\mathcal{B}_{1}, &amp; \quad Y=\mathcal{B}_{1}+\mathcal{B}_{2}
\end{aligned}\]</span></p>
<p>It is easy to compute <span class="math inline">\(\mathbb{E}[Y|X=0]\)</span> and <span class="math inline">\(\mathbb{E}[Y|X=1]\)</span>. By definition, it is given by:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X=0] &amp; =\sum_{j=0}^{2}j\mathbb{P}(Y=j|X=0)\\
&amp; =\sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(Y=j,X=0)}{P(X=0)}\\
&amp; =0+1\cdot\frac{(1/4)}{(1/2)}+2\cdot\frac{0}{(1/2)}\\
&amp; =\frac{1}{2}
\end{aligned}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X=1] &amp; =\sum_{j=0}^{2}j\mathbb{P}(Y=j|X=1)\\
&amp; =\sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(Y=j,X=1)}{P(X=1)}\\
&amp; =0+1\cdot\frac{(1/4)}{(1/2)}+2\cdot\frac{(1/4)}{(1/2)}\\
&amp; =\frac{3}{2}
\end{aligned}\]</span></p>
<p>With this point of view, the conditional expectation is computed given the information that the event <span class="math inline">\(\{X=0\}\)</span> occurred or the event <span class="math inline">\(\{X=1\}\)</span> occurred. It is possible to regroup both conditional expectations in a single object, if we think of the conditional expectation as a random variable and denote it by <span class="math inline">\(\mathbb{E}[Y|X]\)</span>. Namely, we take:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X](\omega) &amp; =\begin{cases}
\frac{1}{2} &amp; \text{if }X(\omega)=0\\
\frac{3}{2} &amp; \text{if }X(\omega)=1
\end{cases}\label{eq:elementary-conditional-expectation-example}
\end{aligned}\]</span></p>
<p>This random variable is called the <em>conditional expectation</em> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. We make two important observations:</p>
<p>(i) If the value of <span class="math inline">\(X\)</span> is known, then the value of <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is determined.</p>
<p>(ii) If we have another random variable <span class="math inline">\(g(X)\)</span> constructed from <span class="math inline">\(X\)</span>, then we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[g(X)Y] &amp; =\mathbb{E}[g(X)\mathbb{E}[Y|X]]
\end{aligned}\]</span></p>
<p>In other words, as far as <span class="math inline">\(X\)</span> is concerned, the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is a proxy for <span class="math inline">\(Y\)</span> in the expectation. We sometimes say that <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is the best estimate of <span class="math inline">\(Y\)</span> given the information of <span class="math inline">\(X\)</span>.</p>
<p>The last observation is easy to verify since:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[g(X)Y] &amp; =\sum_{i=0}^{1}\sum_{j=0}^{2}g(i)\cdot j\cdot\mathbb{P}(X=i,Y=j)\\
&amp; =\sum_{i=0}^{1}\mathbb{P}(X=i)g(i)\left\{ \sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(X=i,Y=j)}{\mathbb{P}(X=i)}\right\} \\
&amp; =\mathbb{E}[g(X)\mathbb{E}[Y|X]]
\end{aligned}\]</span></p>
<div class="example">
<p><span id="ex:elementary-definitions-of-conditional-expectation" data-label="ex:elementary-definitions-of-conditional-expectation"></span>(Elementary Definitions of Conditional Expectation).</p>
<p>(1) <span class="math inline">\((X,Y)\)</span> discrete. The treatment is similar to the above. If a random variable <span class="math inline">\(X\)</span> takes values <span class="math inline">\((x_{i},i\geq1)\)</span> and <span class="math inline">\(Y\)</span> takes values <span class="math inline">\((y_{j},j\geq1)\)</span>, we have by definition that the conditional expectation as a random variable is:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X](\omega) &amp; =\sum_{j\geq1}y_{j}\mathbb{P}(Y=y_{j}|X=x_{i})\quad\text{for }\omega\text{ such that }X(\omega)=x_{i}
\end{aligned}\]</span> (2) <span class="math inline">\((X,Y)\)</span> continuous with joint PDF <span class="math inline">\(f_{X,Y}(x,y)\)</span>: In this case, the conditional expectation is the random variable given by</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y|X] &amp; =h(X)
\end{aligned}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{aligned}
h(x) &amp; =\int_{\mathbf{R}}yf_{Y|X}(y|x)dy=\int_{\mathbf{R}}y\frac{f_{X,Y}(x,y)}{f_{X}(x)}dy=\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}
\end{aligned}\]</span></p>
</div>
<p>In the two examples above, the expectation of the random variable <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is equal to <span class="math inline">\(\mathbb{E}[Y]\)</span>. Indeed in the discrete case, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[\mathbb{E}[Y|X]] &amp; =\sum_{i=0}^{1}P(X=x_{i})\cdot\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j}|X=x_{i})\\
&amp; =\sum_{i=0}^{1}\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j},X=x_{i})\\
&amp; =\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j})\\
&amp; =\mathbb{E}[Y]
\end{aligned}\]</span></p>
<div class="example">
<p>(Conditional Probability vs Conditional expectation). The conditional probability of the event <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> can be recast in terms of conditional expectation using indicator functions. If <span class="math inline">\(0&lt;\mathbb{P}(B)&lt;1\)</span>, it is not hard to check that: <span class="math inline">\(\mathbb{P}(A|B)=\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=1]\)</span> and <span class="math inline">\(\mathbb{P}(A|B^{C})=\mathbb{E}[\mathbf{1}_{A}|1_{B}=0]\)</span>. Indeed the random variables <span class="math inline">\(\mathbf{1}_{A}\)</span> and <span class="math inline">\(\mathbf{1}_{B}\)</span> are discrete. If we proceed as in the discrete case above, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=1] &amp; =1\cdot\mathbb{P}(\mathbf{1}_{A}=1|\mathbf{1}_{B}=1)\\
&amp; =\frac{\mathbb{P}(\mathbf{1}_{A}=1,\mathbf{1}_{B}=1)}{\mathbb{P}(\mathbf{1}_{B}=1)}\\
&amp; =\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\\
&amp; =\mathbb{P}(A|B)
\end{aligned}\]</span></p>
<p>A similar calculation gives <span class="math inline">\(\mathbb{P}(A|B^{C})\)</span>. In particular, the formula for total probability for <span class="math inline">\(A\)</span> is a rewriting of the expectation of the random variable <span class="math inline">\(\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}]\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}]] &amp; =\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=1]\mathbb{P}(\mathbf{1}_{B}=1)+\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=0]\mathbb{P}(\mathbf{1}_{B}=0)\\
&amp; =\mathbb{P}(A|B)\cdot\mathbb{P}(B)+\mathbb{P}(A|B^{C})\cdot\mathbb{P}(B^{C})\\
&amp; =\mathbb{P}(A)
\end{aligned}\]</span></p>
</div>
</section>
<section id="conditional-expectation-as-a-projection." class="level2">
<h2 class="anchored" data-anchor-id="conditional-expectation-as-a-projection.">Conditional Expectation as a projection.</h2>
<section id="conditioning-on-one-variable." class="level4">
<h4 class="anchored" data-anchor-id="conditioning-on-one-variable.">Conditioning on one variable.</h4>
<p>We start by giving the definition of conditional expectation given a single variable. This relates to the two observations (A) and (B) made previously. We assume that the random variable is integrable for the expectations to be well-defined.</p>
<div class="defn">
<p><span id="def:conditional-expectation" data-label="def:conditional-expectation"></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be integrable random variables on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. The conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is the random variable denoted by <span class="math inline">\(\mathbb{E}[Y|X]\)</span> with the following two properties:</p>
<p>(A) There exists a function <span class="math inline">\(h:\mathbf{R}\to\mathbf{R}\)</span> such that <span class="math inline">\(\mathbb{E}[Y|X]=h(X)\)</span>.</p>
<p>(B) For any bounded random variable of the form <span class="math inline">\(g(X)\)</span> for some function <span class="math inline">\(g\)</span>,</p>
<p><span class="math display">\[\mathbb{E}[g(X)Y]=\mathbb{E}[g(X)\mathbb{E}[Y|X]]\label{eq:definition-conditional-expectation}\]</span></p>
<p>We can intepret the second property as follows. The conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> serves as a proxy for <span class="math inline">\(Y\)</span> as far as <span class="math inline">\(X\)</span> is concerned. Note that in equation (<a href="#eq:definition-conditional-expectation" data-reference-type="ref" data-reference="eq:definition-conditional-expectation">[eq:definition-conditional-expectation]</a>), the expectation on the left can be seen as an average over the joint values of <span class="math inline">\((X,Y)\)</span>, whereas the one on the right is an average over the values of <span class="math inline">\(X\)</span> only! Another way to see this property is to write is as:</p>
<p><span class="math display">\[\mathbb{E}[g(X)(Y-\mathbb{E}[Y|X])]=0\]</span></p>
<p>In other words, the <em>random variable <span class="math inline">\(Y-\mathbb{E}[Y|X]\)</span> is orthogonal to any random variable constructed from <span class="math inline">\(X\)</span>.</em></p>
<p>Finally, it is important to notice that if we take <span class="math inline">\(g(X)=1\)</span>, then the second property implies :</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[Y] &amp; =\mathbb{E}[\mathbb{E}[Y|X]]
\end{aligned}\]</span></p>
<p>In other words, the expectation of the conditional expectation of <span class="math inline">\(Y\)</span> is simply the expectation of <span class="math inline">\(Y\)</span>.</p>
<p>The existence of the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is not obvious. We know, it exists in particular cases given in example (<a href="#ex:elementary-definitions-of-conditional-expectation" data-reference-type="ref" data-reference="ex:elementary-definitions-of-conditional-expectation">[ex:elementary-definitions-of-conditional-expectation]</a>). We will show more generally, that it exists, it is unique whenever <span class="math inline">\(Y\)</span> is in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> (In fact, it can be shown to exist whenever <span class="math inline">\(Y\)</span> is integrable). Before doing so, let’s warm up by looking at the case of Gaussian vectors.</p>
</div>
<div class="example">
<p><span id="ex:conditional-expectation-of-gaussian-vectors" data-label="ex:conditional-expectation-of-gaussian-vectors"></span>(Conditional expectation of Gaussian vectors - I). Let <span class="math inline">\((X,Y)\)</span> be a Gaussian vector of mean <span class="math inline">\(0\)</span>. Then:</p>
<p><span class="math display">\[\mathbb{E}[Y|X]=\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X\label{eq:conditional-expectation-of-gaussian-vector}\]</span></p>
<p>This candidate satisfies the two defining properties of conditional expectation : (A) It is clearly a function of <span class="math inline">\(X\)</span>; in fact it is a simple multiple of <span class="math inline">\(X\)</span>. (B) We have that the random variable <span class="math inline">\(\left(Y-\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X\right)\)</span> is orthogonal and thus independent to <span class="math inline">\(X\)</span>. This is a consequence of the proposition (<a href="#prop:diagonal-cov-matrix-implies-independence-of-gaussians" data-reference-type="ref" data-reference="prop:diagonal-cov-matrix-implies-independence-of-gaussians">[prop:diagonal-cov-matrix-implies-independence-of-gaussians]</a>), since:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[X\left(Y-\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X\right)\right] &amp; =\mathbb{E}XY-\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}\mathbb{E}X^{2}\\
&amp; =\mathbb{E}XY-\frac{\mathbb{E}[XY]}{\cancel{\mathbb{E}[X^{2}]}}\cancel{\mathbb{E}X^{2}}\\
&amp; =0
\end{aligned}\]</span></p>
<p>Therefore, we have for any bounded function <span class="math inline">\(g(X)\)</span> of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[g(X)(Y-\mathbb{E}(Y|X))] &amp; =\mathbb{E}[g(X)]\mathbb{E}[Y-\mathbb{E}[Y|X]]=0
\end{aligned}\]</span></p>
</div>
<div class="example">
<p><span id="ex:brownian-conditioning-I" data-label="ex:brownian-conditioning-I"></span>(Brownian conditioning-I) Let <span class="math inline">\((B_{t},t\geq0)\)</span> be a standard Brownian motion. Consider the Gaussian vector <span class="math inline">\((B_{1/2},B_{1})\)</span>. Its covariance matrix is:</p>
<p><span class="math display">\[\begin{aligned}
C &amp; =\left[\begin{array}{cc}
1/2 &amp; 1/2\\
1/2 &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>Let’s compute <span class="math inline">\(\mathbb{E}[B_{1}|B_{1/2}]\)</span> and <span class="math inline">\(\mathbb{E}[B_{1/2}|B_{1}]\)</span>. This is easy using the equation (<a href="#eq:conditional-expectation-of-gaussian-vector" data-reference-type="ref" data-reference="eq:conditional-expectation-of-gaussian-vector">[eq:conditional-expectation-of-gaussian-vector]</a>). We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[B_{1}|B_{1/2}] &amp; =\frac{\mathbb{E}[B_{1}B_{1/2}]}{\mathbb{E}[B_{1/2}^{2}]}B_{1/2}\\
&amp; =\frac{(1/2)}{(1/2)}B_{1/2}\\
&amp; =B_{1/2}
\end{aligned}\]</span></p>
<p>In other words, the best approximation of <span class="math inline">\(B_{1}\)</span> given the information of <span class="math inline">\(B_{1/2}\)</span> is <span class="math inline">\(B_{1/2}\)</span>. There is no problem in computing <span class="math inline">\(\mathbb{E}[B_{1/2}|B_{1}]\)</span>, even though we are conditioning on a future position. Indeed the same formula gives</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[B_{1/2}|B_{1}] &amp; =\frac{\mathbb{E}[B_{1}B_{1/2}]}{\mathbb{E}[B_{1}^{2}]}B_{1}=\frac{1}{2}B_{1}
\end{aligned}\]</span></p>
<p>This means that the best approximation of <span class="math inline">\(B_{1/2}\)</span> given the position at time <span class="math inline">\(1\)</span>, is <span class="math inline">\(\frac{1}{2}B_{1}\)</span> which makes a whole lot of sense!</p>
</div>
<p>In example (<a href="#eq:conditional-expectation-of-gaussian-vector" data-reference-type="ref" data-reference="eq:conditional-expectation-of-gaussian-vector">[eq:conditional-expectation-of-gaussian-vector]</a>) for the Gaussian vector <span class="math inline">\((X,Y)\)</span>, the conditional expectation was equal to the <em>orthogonal projection</em> of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X\)</span> in <span class="math inline">\(L^{2}\)</span>. In particular, the conditional expectation was a multiple of <span class="math inline">\(X\)</span>. Is this always the case? Unfortunately, it is not. For example, in the equation (<a href="#eq:elementary-conditional-expectation-example" data-reference-type="ref" data-reference="eq:elementary-conditional-expectation-example">[eq:elementary-conditional-expectation-example]</a>), the conditional expectation is clearly not a multiple of the random variable <span class="math inline">\(X\)</span>. However, it is a function of <span class="math inline">\(X\)</span>, as is always the case by definition (<a href="#def:conditional-expectation" data-reference-type="ref" data-reference="def:conditional-expectation">[def:conditional-expectation]</a>).</p>
<p>The idea to construct the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> in general is to <em>project <span class="math inline">\(Y\)</span> on the space of all random variables that can be constructed from <span class="math inline">\(X\)</span>. To make this precise, consider the following subspace of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> :</em></p>
<div class="defn">
<p>Let <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> be a probability space and <span class="math inline">\(X\)</span> a random variable defined on it. The space <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> is the linear subspace of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> consisting of the square-integrable random variables of the form <span class="math inline">\(g(X)\)</span> for some function <span class="math inline">\(g:\mathbf{R}\to\mathbf{R}\)</span>.</p>
</div>
<p>This is a linear subspace of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>: It contains the random variable <span class="math inline">\(0\)</span>, and any linear combination of random variables of this kind is also a function of <span class="math inline">\(X\)</span> and must have a finite second moment. We note the following:</p>
<div class="rem*">
<p><span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> is a subspace of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>, very much how a plane or line (going through the origin) is a subspace of <span class="math inline">\(\mathbf{R}^{3}\)</span>.</p>
<p>In particular, as in the case of a line or a plane, we can project an element of <span class="math inline">\(Y\)</span> of <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> onto <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>. The resulting projection is an element of <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>, a square-integrable random-variable that is a function of <span class="math inline">\(X\)</span>. For a subspace <span class="math inline">\(\mathcal{S}\)</span> of <span class="math inline">\(\mathbf{R}^{3}\)</span> (e.g.&nbsp;a line or a plane), the projection of the vector <span class="math inline">\(\mathbf{v}\in\mathbf{R}^{3}\)</span> onto the subspace <span class="math inline">\(\mathcal{S}\)</span>, denoted <span class="math inline">\(\text{Proj}_{\mathcal{S}}(\mathbf{v})\)</span> is the closest point to <span class="math inline">\(\mathbf{v}\)</span> lying in the subspace <span class="math inline">\(\mathcal{S}\)</span>. Moreover, <span class="math inline">\(\mathbf{v}-\text{Proj}_{\mathcal{S}}(\mathbf{v})\)</span> is orthogonal to the subspace. This picture of orthogonal projection also holds in <span class="math inline">\(L^{2}\)</span>. Let <span class="math inline">\(Y\)</span> be a random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span> and let <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> be the subspace of those random variables that are functions of <span class="math inline">\(X\)</span>. We write <span class="math inline">\(Y^{\star}\)</span> for the random variable in <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> that is <em>closest</em> to <span class="math inline">\(Y\)</span>. In other words, we have (using the definition of the <span class="math inline">\(L^{2}\)</span>-distance square):</p>
<p><span class="math display">\[\inf_{Z\in L^{2}(\Omega,\sigma(X),\mathbb{P})}\mathbb{E}[(Y-Z)^{2}]=\mathbb{E}[(Y-Y^{\star})^{2}]\label{eq:Y-star-is-the-closest-to-Y-in-L2-sense}\]</span></p>
</div>
<p>It turns out that <span class="math inline">\(Y^{\star}\)</span> is the right candidate for the conditional expectation.</p>
<div class="center">
<p>Figure. An illustration of the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> as an orthogonal projection of <span class="math inline">\(Y\)</span> onto the subspace <span class="math inline">\(L^2(\Omega,\sigma(X),\mathbb{P})\)</span>.</p>
</div>
<div class="thm">
<p><span id="th:existence-and-uniqueness-of-the-conditional-expectation" data-label="th:existence-and-uniqueness-of-the-conditional-expectation"></span>(Existence and uniqueness of the conditional expectation) Let <span class="math inline">\(X\)</span> be a random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Let <span class="math inline">\(Y\)</span> be a random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>. Then the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> is the random variable <span class="math inline">\(Y^{\star}\)</span> given in the equation (<a href="#eq:Y-star-is-the-closest-to-Y-in-L2-sense" data-reference-type="ref" data-reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense">[eq:Y-star-is-the-closest-to-Y-in-L2-sense]</a>). Namely, it is the random variable in <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span> that is closest to <span class="math inline">\(Y\)</span> in the <span class="math inline">\(L^{2}\)</span>-distance.</p>
<p>In particular we have the following:</p>
<p>1) It is the orthogonal projection of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>, that is <span class="math inline">\(Y-Y^{\star}\)</span> is orthogonal to any random variables in the subspace <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>.</p>
<p>2) It is unique.</p>
</div>
<div class="rem*">
<p>This result reinforces the meaning of the conditional expectation <span class="math inline">\(\mathbb{E}[Y|X]\)</span> as the best estimation of <span class="math inline">\(Y\)</span> given the information of <span class="math inline">\(X\)</span>: it is the closest random variable to <span class="math inline">\(Y\)</span> among all the functions of <span class="math inline">\(X\)</span> in the sense of <span class="math inline">\(L^{2}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> We write for short <span class="math inline">\(L^{2}(X)\)</span> for the subspace <span class="math inline">\(L^{2}(\Omega,\sigma(X),\mathbb{P})\)</span>. Let <span class="math inline">\(Y^{\star}\)</span> be as in equation (<a href="#eq:Y-star-is-the-closest-to-Y-in-L2-sense" data-reference-type="ref" data-reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense">[eq:Y-star-is-the-closest-to-Y-in-L2-sense]</a>). We show successively that (1) <span class="math inline">\(Y-Y^{\star}\)</span> is orthogonal to any element of <span class="math inline">\(L^{2}(X)\)</span>, so it is the orthogonal projection (2) <span class="math inline">\(Y^{\star}\)</span> has the properties of conditional expectation in definition (<a href="#eq:definition-conditional-expectation" data-reference-type="ref" data-reference="eq:definition-conditional-expectation">[eq:definition-conditional-expectation]</a>) (3) <span class="math inline">\(Y^{\star}\)</span> is unique.</p>
<p>(1) Let <span class="math inline">\(W=g(X)\)</span> be a random variable in <span class="math inline">\(L^{2}(X)\)</span>. We show that <span class="math inline">\(W\)</span> is orthogonal to <span class="math inline">\(Y-Y^{\star}\)</span>; that is <span class="math inline">\(\mathbb{E}[(Y-Y^{\star})W]=0\)</span>. This should be intuitively clear from figure above. On the one hand, we have by developing the square:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[(W-(Y-Y^{\star}))^{2}] &amp; =\mathbb{E}[W^{2}-2W(Y-Y^{\star})+(Y-Y^{\star})^{2}]\nonumber \\
&amp; =\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})]+\mathbb{E}(Y-Y^{\star})^{2}]\label{eq:developing-the-square}
\end{aligned}\]</span></p>
<p>On the other hand, <span class="math inline">\(Y^{\star}+W\)</span> is an arbitrary vector in <span class="math inline">\(L^{2}(X)\)</span>(it is a linear combination of the elements in <span class="math inline">\(L^{2}(X)\)</span>), we must have from equation (<a href="#eq:Y-star-is-the-closest-to-Y-in-L2-sense" data-reference-type="ref" data-reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense">[eq:Y-star-is-the-closest-to-Y-in-L2-sense]</a>):</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[(W-(Y-Y^{\star}))^{2}] &amp; =\mathbb{E}[(Y-(Y^{\star}+W))^{2}]\nonumber \\
&amp; \geq\inf_{Z\in L^{2}(X)}\mathbb{E}[(Y-Z)^{2}]\nonumber \\
&amp; =\mathbb{E}[(Y-Y^{\star})^{2}]\label{eq:lower-bound}
\end{aligned}\]</span></p>
<p>Putting the last two equations (<a href="#eq:developing-the-square" data-reference-type="ref" data-reference="eq:developing-the-square">[eq:developing-the-square]</a>), (<a href="#eq:lower-bound" data-reference-type="ref" data-reference="eq:lower-bound">[eq:lower-bound]</a>) together, we get that for any <span class="math inline">\(W\in L^{2}(X)\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})] &amp; \geq0
\end{aligned}\]</span></p>
<p>In particular, this also holds for <span class="math inline">\(aW\)</span>, in which case we get:</p>
<p><span class="math display">\[\begin{aligned}
a^{2}\mathbb{E}[W^{2}]-2a\mathbb{E}[W(Y-Y^{\star})] &amp; \geq0\\
\implies a\left\{ a\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})]\right\}  &amp; \geq0
\end{aligned}\]</span></p>
<p>If <span class="math inline">\(a&gt;0\)</span>, then:</p>
<p><span class="math display">\[a\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})]\geq0\label{eq:case-when-a-gt-zero}\]</span></p>
<p>whereas if <span class="math inline">\(a&lt;0\)</span>, then the sign changes upon dividing throughout by <span class="math inline">\(a\)</span>, and we have:</p>
<p><span class="math display">\[a\mathbb{E}[W^{2}]-2\mathbb{E}[W(Y-Y^{\star})]\leq0\label{eq:case-when-a-lt-zero}\]</span></p>
<p>Rearranging (<a href="#eq:case-when-a-gt-zero" data-reference-type="ref" data-reference="eq:case-when-a-gt-zero">[eq:case-when-a-gt-zero]</a>) yields:</p>
<p><span class="math display">\[\mathbb{E}[W(Y-Y^{\star})]\leq a\mathbb{E}[W^{2}]/2\label{eq:case-when-a-gt-zero-rearranged}\]</span></p>
<p>Rearranging (<a href="#eq:case-when-a-lt-zero" data-reference-type="ref" data-reference="eq:case-when-a-lt-zero">[eq:case-when-a-lt-zero]</a>) yields:</p>
<p><span class="math display">\[\mathbb{E}[W(Y-Y^{\star})]\geq a\mathbb{E}[W^{2}]/2\label{eq:case-when-a-lt-zero-rearranged}\]</span></p>
<p>Since (<a href="#eq:case-when-a-gt-zero-rearranged" data-reference-type="ref" data-reference="eq:case-when-a-gt-zero-rearranged">[eq:case-when-a-gt-zero-rearranged]</a>) holds for all <span class="math inline">\(a&gt;0\)</span>, the stronger inequality, <span class="math inline">\(\mathbb{E}[W(Y-Y^{\star})]\leq0\)</span> must hold. Since, (<a href="#eq:case-when-a-lt-zero-rearranged" data-reference-type="ref" data-reference="eq:case-when-a-lt-zero-rearranged">[eq:case-when-a-lt-zero-rearranged]</a>) holds for all <span class="math inline">\(a&lt;0\)</span>, the stronger inequality <span class="math inline">\(\mathbb{E}[W(Y-Y^{\star})]\geq0\)</span> must hold. Consequently,</p>
<p><span class="math display">\[\mathbb{E}[W(Y-Y^{\star})]=0\]</span></p>
<p>(2) It is clear that <span class="math inline">\(Y^{\star}\)</span> is a function of <span class="math inline">\(X\)</span> by construction, since it is in <span class="math inline">\(L^{2}(X)\)</span>. Moreover, for any <span class="math inline">\(W\in L^{2}(X)\)</span>, we have from (1) that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[W(Y-Y^{\star})] &amp; =0
\end{aligned}\]</span></p>
<p>which is the second defining property of conditional expectations.</p>
<p>(3) Lastly, suppose there is another element <span class="math inline">\(Y'\)</span> that is in <span class="math inline">\(L^{2}(X)\)</span> that minimizes the distance to <span class="math inline">\(Y\)</span>. Then we would get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[(Y-Y')^{2}] &amp; =\mathbb{E}[(Y-Y^{\star}+Y^{\star}-Y')^{2}]\\
&amp; =\mathbb{E}[(Y-Y^{\star})^{2}]+2\mathbb{E}[(Y-Y^{\star})(Y^{\star}-Y')]+\mathbb{E}[(Y^{\star}-Y')^{2}]\\
&amp; =\mathbb{E}[(Y-Y^{\star})^{2}]+0+\mathbb{E}[(Y^{\star}-Y')^{2}]\\
&amp; \quad\left\{ (Y^{\star}-Y')\in L^{2}(X)\perp(Y-Y^{\star})\right\}
\end{aligned}\]</span></p>
<p>where we used the fact, that <span class="math inline">\(Y^{\star}-Y'\)</span> is a vector in <span class="math inline">\(L^{2}(X)\)</span> and the orthogonality of <span class="math inline">\(Y-Y^{\star}\)</span> with <span class="math inline">\(L^{2}(X)\)</span> as in (1). But, this implies that:</p>
<p><span class="math display">\[\begin{aligned}
\cancel{\mathbb{E}[(Y-Y')^{2}]} &amp; =\cancel{\mathbb{E}[(Y-Y^{\star})^{2}]}+\mathbb{E}[(Y^{\star}-Y')^{2}]\\
\mathbb{E}[(Y^{\star}-Y')^{2}] &amp; =0
\end{aligned}\]</span></p>
<p>So, <span class="math inline">\(Y^{\star}=Y'\)</span> almost surely. ◻</p>
</div>
<div class="example">
<p><strong>Conditional Expectation of continuous random variables.</strong> Let <span class="math inline">\((X,Y)\)</span> be two random variables with joint density <span class="math inline">\(f_{X,Y}(x,y)\)</span> on <span class="math inline">\(\mathbf{R}^{2}\)</span>. Suppose for simplicity, that <span class="math inline">\(\int_{\mathbf{R}}f(x,y)dx&gt;0\)</span> for every <span class="math inline">\(y\)</span> belonging to <span class="math inline">\(\mathbf{R}\)</span>. Show that the conditional expectation <span class="math inline">\(\mathbf{E}[Y|X]\)</span> equals <span class="math inline">\(h(X)\)</span> where <span class="math inline">\(h\)</span> is the function:</p>
<p><span class="math display">\[\begin{aligned}
h(x) &amp; =\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\label{eq:conditional-expectation-of-continuous-random-variables}
\end{aligned}\]</span></p>
<p>In particular, verify that <span class="math inline">\(\mathbf{E}[\mathbf{E}[Y|X]]=\mathbf{E}[Y]\)</span>.</p>
<p><em>Hint</em>: To prove this, verify that the above formula satisfies both the properties of conditional expectations; then invoke uniqueness to finish it off.</p>
</div>
<div class="sol*">
<p>(i) The density function <span class="math inline">\(f_{X,Y}(x,y)\)</span> is a map <span class="math inline">\(f:\mathbf{R}^{2}\to\mathbf{R}\)</span>. The integral <span class="math inline">\(\int_{y=-\infty}^{y=+\infty}yf_{X,Y}(x_{0},y)dy\)</span> is the area under the curve <span class="math inline">\(yf(x,y)\)</span> at the point <span class="math inline">\(x=x_{0}\)</span>. Let’s call it <span class="math inline">\(A(x_{0})\)</span>. If instead, we have an arbitrary <span class="math inline">\(x\)</span>, <span class="math inline">\(\int_{y=-\infty}^{y=+\infty}yf_{X,Y}(x,y)dy\)</span> represents the area <span class="math inline">\(A(x)\)</span> of an arbitrary slice of the surface <span class="math inline">\(yf_{X,Y}\)</span> at the point <span class="math inline">\(x\)</span>. Hence, it is a function of <span class="math inline">\(x\)</span>. The denominator <span class="math inline">\(\int_{\mathbf{R}}f_{X,Y}(x,y)dy=f_{X}(x)\)</span>, the density of <span class="math inline">\(X\)</span>, which is a function of <span class="math inline">\(x\)</span>. Hence, the ratio is a function of <span class="math inline">\(x\)</span>.</p>
<p>(ii) Let <span class="math inline">\(g(X)\)</span> is a bounded random variable. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[g(X)(Y-h(X))] &amp; =\mathbf{E}[Yg(X)]-\mathbf{E}[g(X)h(X)]\\
&amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx-\int_{\mathbf{R}}g(x)h(x)f(x)dx\\
&amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx\\
&amp; -\int_{\mathbf{R}}\begin{array}{c}
g(x)\cdot\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\end{array}\cdot\int_{\mathbf{R}}f_{X,Y}(x,y)dy\ dx\\
&amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx\\
&amp; -\int_{\mathbf{R}}\begin{array}{c}
g(x)\cdot\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\cancel{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}}\end{array}\cdot\cancel{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\ dx\\
&amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx-\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)\cdot dx\cdot dy\\
&amp; =0
\end{aligned}\]</span></p>
</div>
<p>Thus, <span class="math inline">\(h(X)\)</span> is a valid candidate for the conditional expectation <span class="math inline">\(\mathbf{E}[Y|X]\)</span>. Moreover, by the existence and uniqueness theorem (<a href="#th:existence-and-uniqueness-of-the-conditional-expectation" data-reference-type="ref" data-reference="th:existence-and-uniqueness-of-the-conditional-expectation">[th:existence-and-uniqueness-of-the-conditional-expectation]</a>), <span class="math inline">\(\mathbf{E}[Y|X]\)</span> is unique and equals <span class="math inline">\(h(X)\)</span>.</p>
</section>
<section id="conditioning-on-several-random-variables." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="conditioning-on-several-random-variables.">Conditioning on several random variables.</h4>
<p>We would like to generalize the conditional expectation to the case when we condition on the information of more than one random variable. Taking the <span class="math inline">\(L^{2}\)</span> point of view, we should expect that the conditional expectation is the orthogonal projection of the given random variable on the subspace generated by square integrable functions of all the variables on which we condition.</p>
<p>It is now useful to study sigma-fields, an object that was defined in chapter 1.</p>
<div class="defn">
<p><span id="def:sigma-field" data-label="def:sigma-field"></span>(Sigma-Field) A sigma-field or sigma-algebra <span class="math inline">\(\mathcal{F}\)</span> of a sample space <span class="math inline">\(\Omega\)</span> is a collection of all measurable events with the following properties:</p>
<p>(1) <span class="math inline">\(\Omega\)</span> is in <span class="math inline">\(\mathcal{F}\)</span>.</p>
<p>(2) Closure under complement. If <span class="math inline">\(A\in\mathcal{F}\)</span>, then <span class="math inline">\(A^{C}\in\mathcal{F}\)</span>.</p>
<p>(3) Closure under countable unions. If <span class="math inline">\(A_{1},A_{2},\ldots,\in\mathcal{F}\)</span>, then <span class="math inline">\(\bigcup_{n=1}^{\infty}A_{n}\in\mathcal{F}\)</span>.</p>
</div>
<p>Such objects play a fundamental role in the rigorous study of probability and real analysis in general. We will focus on the intuition behind them. First let’s mention some examples of sigma-fields of a given sample space <span class="math inline">\(\Omega\)</span> to get acquainted with the concept.</p>
<div class="example">
<p>(Examples of sigma-fields).</p>
<p>(1) <em>The trivial sigma-field</em>. Note that the collection of events <span class="math inline">\(\{\emptyset,\Omega\}\)</span> is a sigma-field of <span class="math inline">\(\Omega\)</span>. We generally denote it by <span class="math inline">\(\mathcal{F}_{0}\)</span>.</p>
<p>(2) <em>The <span class="math inline">\(\sigma\)</span>-field generated by an event <span class="math inline">\(A\)</span>.</em> Let <span class="math inline">\(A\)</span> be an event that is not <span class="math inline">\(\emptyset\)</span> and not the entire <span class="math inline">\(\Omega\)</span>. Then the smallest sigma-field containing <span class="math inline">\(A\)</span> ought to be:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{F}_{1} &amp; =\{\emptyset,A,A^{C},\Omega\}
\end{aligned}\]</span></p>
<p>This sigma-field is denoted by <span class="math inline">\(\sigma(A)\)</span>.</p>
<p>(3) The <em>sigma-field generated by a random variable <span class="math inline">\(X\)</span>.</em></p>
<p>We now define the <span class="math inline">\(\mathcal{F}_{X}\)</span> as follows:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{F}_{X} &amp; =X^{-1}(\mathcal{B}):=\{\omega:X(\omega)\in B\},\forall B\in\mathcal{B}(\mathbf{R})
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\mathcal{B}\)</span> is the Borel <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbf{R}\)</span>. <span class="math inline">\(\mathcal{F}_{X}\)</span> is sometimes denoted as <span class="math inline">\(\sigma(X)\)</span>. <span class="math inline">\(\mathcal{F}_{X}\)</span>is the set of all events pertaining to <span class="math inline">\(X\)</span>. It is a sigma-algebra because:</p>
<p>(i) <span class="math inline">\(\Omega\in\sigma(X)\)</span> because <span class="math inline">\(\Omega=\{\omega:X(\omega)\in\mathbf{R}\}\)</span> and <span class="math inline">\(\mathbf{R}\in\mathcal{B}(\mathbf{R})\)</span>.</p>
<p>(ii) Let any event <span class="math inline">\(C\in\sigma(X)\)</span>. We need to show that <span class="math inline">\(\Omega\setminus C\in\sigma(X)\)</span>.</p>
<p>Since <span class="math inline">\(C\in\sigma(X)\)</span>, there exists <span class="math inline">\(A\in\mathcal{B}(\mathbf{R})\)</span>, such that:</p>
<p><span class="math display">\[\begin{aligned}
C &amp; =\{\omega\in\Omega:X(\omega)\in A\}
\end{aligned}\]</span></p>
<p>Now, we calculate:</p>
<p><span class="math display">\[\begin{aligned}
\Omega\setminus C &amp; =\{\omega\in\Omega:X(\omega)\in\mathbf{R}\setminus A\}
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(\mathcal{B}(\mathbf{R})\)</span> is a sigma-algebra, it is closed under complementation. Hence, if <span class="math inline">\(A\in\mathcal{B}(\mathbf{R})\)</span>, it implies that <span class="math inline">\(\mathbf{R}\setminus A\in\mathcal{B}(\mathbf{R})\)</span>. So, <span class="math inline">\(\Omega\setminus C\in\sigma(X)\)</span>.</p>
<p>(iii) Consider a sequence of events <span class="math inline">\(C_{1},C_{2},\ldots,C_{n},\ldots\in\sigma(X)\)</span>. We need to prove that <span class="math inline">\(\bigcup_{n=1}^{\infty}C_{n}\in\sigma(X)\)</span>.</p>
<p>Since <span class="math inline">\(C_{n}\in\sigma(X)\)</span>, there exists <span class="math inline">\(A_{n}\in\mathcal{B}(\mathbf{R})\)</span> such that:</p>
<p><span class="math display">\[\begin{aligned}
C_{n} &amp; =\{\omega\in\Omega:X(\omega)\in A_{n}\}
\end{aligned}\]</span></p>
<p>Now, we calculuate:</p>
<p><span class="math display">\[\begin{aligned}
\bigcup_{n=1}C_{n} &amp; =\{\omega\in\Omega:X(\omega)\in\bigcup_{n=1}^{\infty}A_{n}\}
\end{aligned}\]</span></p>
<p>But, <span class="math inline">\(\bigcup_{n=1}^{\infty}A_{n}\in\mathcal{B}(\mathbf{R})\)</span>. So, <span class="math inline">\(\bigcup_{n=1}^{\infty}C_{n}\in\sigma(X)\)</span>.</p>
<p>Consequently, <span class="math inline">\(\sigma(X)\)</span> is indeed a <span class="math inline">\(\sigma\)</span>-algebra.</p>
<p>Intuitively, we think of <span class="math inline">\(\sigma(X)\)</span> as containing all information about <span class="math inline">\(X\)</span>.</p>
<p>(4) <em>The sigma-field generated by a stochastic process <span class="math inline">\((X_{s},s\leq t)\)</span>.</em> Let <span class="math inline">\((X_{s},s\geq0)\)</span> be a stochastic process. Consider the process restricted to <span class="math inline">\([0,t]\)</span>, <span class="math inline">\((X_{s},s\leq t)\)</span>. We consider the smallest sigma-field containing all events pertaining to the random variables <span class="math inline">\(X_{s},s\leq t\)</span>. We denote it by <span class="math inline">\(\sigma(X_{s},s\leq t)\)</span> or <span class="math inline">\(\mathcal{F}_{t}\)</span>.</p>
</div>
<p>The sigma-fields on <span class="math inline">\(\Omega\)</span> have a natural (partial) ordering: two sigma-fields <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{F}\)</span> of <span class="math inline">\(\Omega\)</span> are such that <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> if all the events in <span class="math inline">\(\mathcal{G}\)</span> are in <span class="math inline">\(\mathcal{F}\)</span>. For example, the trivial <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{F}_{0}=\{\emptyset,\Omega\}\)</span> is contained in all the <span class="math inline">\(\sigma\)</span>-fields of <span class="math inline">\(\Omega\)</span>. Clearly, the <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{F}_{t}=\sigma(X_{s},s\leq t)\)</span> is contained in <span class="math inline">\(\mathcal{F}_{t'}\)</span> if <span class="math inline">\(t\leq t'\)</span>.</p>
<p>If all the events pertaining to a random variable <span class="math inline">\(X\)</span> are in the <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{G}\)</span> (and thus we can compute <span class="math inline">\(\mu(X^{-1}((a,b]))\)</span>), we will say that <span class="math inline">\(X\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable. This means that all information about <span class="math inline">\(X\)</span> is contained in <span class="math inline">\(\mathcal{G}\)</span>.</p>
<div class="defn">
<p>Let <span class="math inline">\(X\)</span> be a random variable defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Consider another <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span>. Then <span class="math inline">\(X\)</span> is said to be <span class="math inline">\(\mathcal{G}\)</span>-measurable, if and only if:</p>
<p><span class="math display">\[\begin{aligned}
\{\omega:X(\omega)\in(a,b]\} &amp; \in\mathcal{G}\text{ for all intervals }(a,b]\in\mathbf{R}
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>(<span class="math inline">\(\mathcal{F}_{0}\)</span>-measurable random variables). Consider the trivial sigma-field <span class="math inline">\(\mathcal{F}_{0}=\{\emptyset,\Omega\}\)</span>. A random variable that is <span class="math inline">\(\mathcal{F}_{0}\)</span>-measurable must be a constant. Indeed, we have that for any interval <span class="math inline">\((a,b]\)</span>, <span class="math inline">\(\{\omega:X(\omega)\in(a,b]\}=\emptyset\)</span> or <span class="math inline">\(\{\omega:X(\omega)\in(a,b]\}=\Omega\)</span>. This can only hold if <span class="math inline">\(X\)</span> takes a single value.</p>
</div>
<div class="example">
<p>[]{#ex:sigma(X)-measurable-random-variables-example label=“ex:sigma(X)-measurable-random-variables-example”}(<span class="math inline">\(\sigma(X)\)</span>-measurable random variables). Let <span class="math inline">\(X\)</span> be a given random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Roughly speaking, a <span class="math inline">\(\sigma(X)\)</span>-measurable random variable is determined by the information of <span class="math inline">\(X\)</span> only. Here is the simplest example of a <span class="math inline">\(\sigma(X)\)</span>-measurable random variable. Take the indicator function <span class="math inline">\(Y=\mathbf{1}_{\{X\in B\}}\)</span> for some event <span class="math inline">\(\{X\in B\}\)</span> pertaining to <span class="math inline">\(X\)</span>. Then the pre-images <span class="math inline">\(\{\omega:Y(\omega)\in(a,b]\}\)</span> are either <span class="math inline">\(\emptyset\)</span>, <span class="math inline">\(\{X\in B\}\)</span>, <span class="math inline">\(\{X\in B^{C}\}\)</span> or <span class="math inline">\(\Omega\)</span> depending on whether <span class="math inline">\(0,1\)</span> are in <span class="math inline">\((a,b]\)</span> or not. All of these events are in <span class="math inline">\(\sigma(X)\)</span>. More generally, one can construct a <span class="math inline">\(\sigma(X)\)</span>-measurable random variable by taking linear combinations of indicator functions of events of the form <span class="math inline">\(\{X\in B\}\)</span>.</p>
<p>It turns out that any (Borel measurable) function of <span class="math inline">\(X\)</span> can be approximated by taking limits of such simple functions.</p>
<p>Concretely, this translates to the following statement:</p>
<p><span class="math display">\[\text{If }Y\text{ is \ensuremath{\sigma}(X)-measurable, then Y=g(X) for some function g}\]</span></p>
<p>In the same way, if <span class="math inline">\(Z\)</span> is <span class="math inline">\(\sigma(X,Y)\)</span>-measurable, then <span class="math inline">\(Z=h(X,Y)\)</span> for some <span class="math inline">\(h\)</span>. These facts can be proved rigorously using measure theory.</p>
</div>
<p>We are ready to give the general definition of conditional expectation.</p>
<div class="example">
<p>(Coin-Tossing Space). Suppose a coin is tossed infinitely many times. Let <span class="math inline">\(\Omega\)</span> be the set of all infinite sequences of <span class="math inline">\(H\)</span>s and <span class="math inline">\(T\)</span>s. A generic element of <span class="math inline">\(\Omega\)</span> is denoted by <span class="math inline">\(\omega_{1}\omega_{2}\ldots\)</span>, where <span class="math inline">\(\omega_{n}\)</span> indicates the result of the <span class="math inline">\(n\)</span>th coin toss. <span class="math inline">\(\Omega\)</span> is an uncountable sample space. The trivial sigma-field <span class="math inline">\(\mathcal{F}_{0}=\{\emptyset,\Omega\}\)</span>. Assume that we don’t know anything about the outcome of the experiement. Even without any information, we know that the true <span class="math inline">\(\omega\)</span> belongs to <span class="math inline">\(\Omega\)</span> and does not belong to <span class="math inline">\(\emptyset\)</span>. It is the information learned at time <span class="math inline">\(0\)</span>.</p>
<p>Next, assume that we know the outcome of the first coin toss. Define <span class="math inline">\(A_{H}=\{\omega:\omega_{1}=H\}\)</span>=set of all sequences beginning with <span class="math inline">\(H\)</span> and <span class="math inline">\(A_{T}=\{\omega:\omega_{1}=T\}\)</span>=set of all sequences beginning with <span class="math inline">\(T\)</span>. The four sets resolved by the first coin-toss form the the <span class="math inline">\(\sigma\)</span>-field <span class="math inline">\(\mathcal{F}_{1}=\{\emptyset,A_{H},A_{T},\Omega\}\)</span>. We shall think of this <span class="math inline">\(\sigma\)</span>-field as containing the information learned by knowing the outcome of the first coin toss. More precisely, if instead of being told about the first coin toss, we are told for each set in <span class="math inline">\(\mathcal{F}_{1}\)</span>, whether or not the true <span class="math inline">\(\omega\)</span> belongs to that set, then we know the outcome of the first coin toss and nothing more.</p>
<p>If we are told the first two coin tosses, we obtain a finer resolution. In particular, the four sets:</p>
<p><span class="math display">\[\begin{aligned}
A_{HH} &amp; =\{\omega:\omega_{1}=H,\omega_{2}=H\}\\
A_{HT} &amp; =\{\omega:\omega_{1}=H,\omega_{2}=T\}\\
A_{TH} &amp; =\{\omega:\omega_{1}=T,\omega_{2}=H\}\\
A_{TT} &amp; =\{\omega:\omega_{1}=T,\omega_{2}=T\}
\end{aligned}\]</span></p>
<p>are resolved. Of course, the sets in <span class="math inline">\(\mathcal{F}_{1}\)</span> are resolved. Whenever a set is resolved, so is its complement, which means that <span class="math inline">\(A_{HH}^{C}\)</span>, <span class="math inline">\(A_{HT}^{C}\)</span>, <span class="math inline">\(A_{TH}^{C}\)</span> and <span class="math inline">\(A_{TT}^{C}\)</span> are resolved, so is their union which means that <span class="math inline">\(A_{HH}\cup A_{TH}\)</span>, <span class="math inline">\(A_{HH}\cup A_{TT}\)</span>, <span class="math inline">\(A_{HT}\cup A_{TH}\)</span> and <span class="math inline">\(A_{HT}\cup A_{TT}\)</span> are resolved. The other two pair-wise unions <span class="math inline">\(A_{HH}\cup A_{HT}=A_{H}\)</span> and <span class="math inline">\(A_{TH}\cup A_{TT}=A_{T}\)</span> are already resolved. Finally, the triple unions are also resolved, because <span class="math inline">\(A_{HH}\cup A_{HT}\cup A_{TH}=A_{TT}^{C}\)</span> and so forth. Hence, the information pertaining to the second coin-toss is contained in:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{F}_{2} &amp; =\{\emptyset,\Omega,\\
&amp; A_{H},A_{T},\\
&amp; A_{HH},A_{HT},A_{TH},A_{TT},\\
&amp; A_{HH}^{C},A_{HT}^{C},A_{TH}^{C},A_{TT}^{C},\\
&amp; A_{HH}\cup A_{TH},A_{HH}\cup A_{TT},A_{HT}\cup A_{TH},A_{HT}\cup A_{TT}\}
\end{aligned}\]</span></p>
<p>Hence, if the outcome of the first two coin tosses is known, all of the events in <span class="math inline">\(\mathcal{F}_{2}\)</span> are resolved - we exactly know, if each event has ocurred or not. <span class="math inline">\(\mathcal{F}_{2}\)</span> is the information learned by observing the first two coin tosses.</p>
</div>
<div class="xca">
<p>(<strong>Exercises on sigma-fields</strong>).</p>
<p>(a) Let <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> be two proper subsets of <span class="math inline">\(\Omega\)</span> such that <span class="math inline">\(A\cap B\neq\emptyset\)</span> and <span class="math inline">\(A\cup B\neq\Omega\)</span>. Write down <span class="math inline">\(\sigma(\{A,B\})\)</span>, the smallest sigma-field containing <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> explicitly. What if <span class="math inline">\(A\cap B=\emptyset\)</span>?</p>
<p>(b) The Borel sigma-field is the smallest sigma-field containing intervals of the form <span class="math inline">\((a,b]\)</span> in <span class="math inline">\(\mathbf{R}\)</span>. Show that all singletons <span class="math inline">\(\{b\}\)</span> are in <span class="math inline">\(\mathcal{B}(\mathbf{R})\)</span> by writing <span class="math inline">\(\{b\}\)</span> as a countable intersection of intervals <span class="math inline">\((a,b]\)</span>. Conclude that all open intervals <span class="math inline">\((a,b)\)</span> and all closed intervals <span class="math inline">\([a,b]\)</span> are in <span class="math inline">\(\mathcal{B}(\mathbf{R})\)</span>. Is the subset <span class="math inline">\(\mathbf{Q}\)</span> of rational numbers a Borel set?</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (a) The sigma-field generated by the two events <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
\sigma(\{A,B\}) &amp; =\{\emptyset,\Omega,\\
&amp; A,B,A^{C},B^{C},\\
&amp; A\cup B,A\cap B,\\
&amp; A\cup B^{C},A^{C}\cup B,A^{C}\cup B^{C},\\
&amp; A\cap B^{C},A^{C}\cap B,A^{C}\cap B^{C},\\
&amp; (A\cup B)\cap(A\cap B)^{C},\\
&amp; (A\cup B)^{C}\cup(A\cap B)\}
\end{aligned}\]</span></p>
<p>(b) Firstly, recall that:</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{B}(\mathbf{R}) &amp; =\bigcap_{\alpha\in\Lambda}\mathcal{F}_{\alpha}=\bigcap\sigma(\{I:I\text{ is an interval }(a,b]\subseteq\mathbf{R}\})
\end{aligned}\]</span></p>
<p>We can write:</p>
<p><span class="math display">\[\begin{aligned}
\{b\} &amp; =\bigcap_{n=1}^{\infty}\left(b-\frac{1}{n},b\right]
\end{aligned}\]</span></p>
<p>As <span class="math inline">\(\mathcal{B}(\mathbf{R})\)</span> is a sigma-field, it is closed under countable intersections. Hence, the singleton set <span class="math inline">\(\{b\}\)</span>is a Borel set.</p>
<p>Similarly, we can write, any open interval as the countable union:</p>
<p><span class="math display">\[\begin{aligned}
(a,b) &amp; =\bigcup_{n=1}^{\infty}\left(a,b-\frac{1}{n}\right]
\end{aligned}\]</span></p>
<p>We can convince ourselves, that equality indeed holds. Let <span class="math inline">\(x\in(a,b)\)</span> and choose <span class="math inline">\(N\)</span>, such that <span class="math inline">\(\frac{1}{N}&lt;|b-x|\)</span>. Then, for all <span class="math inline">\(n\geq N\)</span>, <span class="math inline">\(x\in(a,b-1/n]\)</span>. Thus, it belongs to the RHS. In the reverse direction, let <span class="math inline">\(x\)</span> belong to <span class="math inline">\(\bigcup_{n=1}^{\infty}\left(a,b-\frac{1}{n}\right]\)</span>. So, <span class="math inline">\(x\)</span> belongs to atleast one of these sets. Therefore, <span class="math inline">\(x\in(a,b)\)</span> is trivially true. So, the two sets are equal.</p>
<p>Hence, open intervals are Borel sets.</p>
<p>Similarly, we may write:</p>
<p><span class="math display">\[\begin{aligned}
[a,b] &amp; =\bigcap_{n=1}^{\infty}\left(a-\frac{1}{n},b+\frac{1}{n}\right)
\end{aligned}\]</span></p>
<p>Consequently, closed intervals are Borel sets. Since <span class="math inline">\(\mathbf{Q}\)</span> is countable, it is a Borel set. Moreover, the empty set <span class="math inline">\(\emptyset\)</span> and <span class="math inline">\(\mathbf{R}\)</span> are Borel sets. So, <span class="math inline">\(\mathbf{R}\backslash\mathbf{Q}\)</span> is also a Borel set. ◻</p>
</div>
<div class="xca">
<p>Let <span class="math inline">\((X,Y)\)</span> be a Gaussian vector with mean <span class="math inline">\(0\)</span> and covariance matrix</p>
<p><span class="math display">\[\begin{aligned}
C &amp; =\left[\begin{array}{cc}
1 &amp; \rho\\
\rho &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>for <span class="math inline">\(\rho\in(-1,1)\)</span>. We verify that the example (<a href="#ex:conditional-expectation-of-gaussian-vectors" data-reference-type="ref" data-reference="ex:conditional-expectation-of-gaussian-vectors">[ex:conditional-expectation-of-gaussian-vectors]</a>) and exercise (<a href="#ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables" data-reference-type="ref" data-reference="ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables">[ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables]</a>) yield the same conditional expectation.</p>
<p>(a) Use equation (<a href="#eq:conditional-expectation-of-gaussian-vector" data-reference-type="ref" data-reference="eq:conditional-expectation-of-gaussian-vector">[eq:conditional-expectation-of-gaussian-vector]</a>) to show that <span class="math inline">\(\mathbf{E}[Y|X]=\rho X\)</span>.</p>
<p>(b) Write down the joint PDF <span class="math inline">\(f(x,y)\)</span> of <span class="math inline">\((X,Y)\)</span>.</p>
<p>(c) Show that <span class="math inline">\(\int_{\mathbf{R}}yf(x,y)dy=\rho x\)</span> and that <span class="math inline">\(\int_{\mathbf{R}}f(x,y)dy=1\)</span>.</p>
<p>(d) Deduce that <span class="math inline">\(\mathbf{E}[Y|X]=\rho X\)</span> using the equation (<a href="#eq:conditional-expectation-of-continuous-random-variables" data-reference-type="ref" data-reference="eq:conditional-expectation-of-continuous-random-variables">[eq:conditional-expectation-of-continuous-random-variables]</a>).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (a) Since <span class="math inline">\((X,Y)\)</span> have mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>, it follows that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(X-EX)(Y-EY)] &amp; =\mathbf{E}(XY)\\
\sqrt{(\mathbf{E}[X^{2}]-(\mathbf{E}X)^{2})}\cdot\sqrt{(\mathbf{E}[Y^{2}]-(\mathbf{E}Y)^{2})} &amp; =\sqrt{(1-0)(1-0)}\\
&amp; =1
\end{aligned}\]</span></p>
<p>and therefore,</p>
<p><span class="math display">\[\begin{aligned}
\rho &amp; =\frac{\mathbf{E}(XY)}{1}=\frac{\mathbf{E}[XY]}{\mathbf{E}[X^{2}]}
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\((X,Y)\)</span> is a Gaussian vector, using (<a href="#eq:conditional-expectation-of-gaussian-vector" data-reference-type="ref" data-reference="eq:conditional-expectation-of-gaussian-vector">[eq:conditional-expectation-of-gaussian-vector]</a>), we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|X] &amp; =\frac{\mathbf{E}[XY]}{\mathbf{E}[X^{2}]}X=\rho X
\end{aligned}\]</span></p>
<p>(b) Consider the augmented matrix <span class="math inline">\([C|I]\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
[C|I] &amp; =\left[\left.\begin{array}{cc}
1 &amp; \rho\\
\rho &amp; 1
\end{array}\right|\begin{array}{cc}
1 &amp; 0\\
0 &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>Performing <span class="math inline">\(R_{2}=R_{2}-\rho R_{1}\)</span>, the above system is row-equivalent to:</p>
<p><span class="math display">\[\left[\left.\begin{array}{cc}
1 &amp; \rho\\
0 &amp; 1-\rho^{2}
\end{array}\right|\begin{array}{cc}
1 &amp; 0\\
-\rho &amp; 1
\end{array}\right]\]</span></p>
<p>Performing <span class="math inline">\(R_{2}=\frac{1}{1-\rho^{2}}R_{2}\)</span>, the above system is row-equivalent to:</p>
<p><span class="math display">\[\left[\begin{array}{cc}
1 &amp; \rho\\
0 &amp; 1
\end{array}\left|\begin{array}{cc}
1 &amp; 0\\
\frac{-\rho}{1-\rho^{2}} &amp; \frac{1}{1-\rho^{2}}
\end{array}\right.\right]\]</span></p>
<p>Performing <span class="math inline">\(R_{1}=R_{1}-\rho R_{2}\)</span>, we have:</p>
<p><span class="math display">\[\left[\begin{array}{cc}
1 &amp; 0\\
0 &amp; 1
\end{array}\left|\begin{array}{cc}
\frac{1}{1-\rho^{2}} &amp; -\frac{\rho}{1-\rho^{2}}\\
\frac{-\rho}{1-\rho^{2}} &amp; \frac{1}{1-\rho^{2}}
\end{array}\right.\right]\]</span></p>
<p>Thus, <span class="math display">\[\begin{aligned}
C^{-1} &amp; =\frac{1}{1-\rho^{2}}\left[\begin{array}{cc}
1 &amp; -\rho\\
-\rho &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>Moreover, <span class="math inline">\(\det C=1-\rho^{2}.\)</span></p>
<p>Therefore, the joint density of <span class="math inline">\((X,Y)\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
f(x,y) &amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}\left[\begin{array}{cc}
x &amp; y\end{array}\right]\left[\begin{array}{cc}
1 &amp; -\rho\\
-\rho &amp; 1
\end{array}\right]\left[\begin{array}{c}
x\\
y
\end{array}\right]\right]\\
&amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}\left[\begin{array}{cc}
x-\rho y &amp; -\rho x+y\end{array}\right]\left[\begin{array}{c}
x\\
y
\end{array}\right]\right]\\
&amp; \frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}(x^{2}-2\rho xy+y^{2})\right]
\end{aligned}\]</span></p>
<p>(c) Claim I. <span class="math inline">\(\int_{\mathbf{R}}yf(x,y)dy=\rho x\)</span>.</p>
<p>Completing the square, we have:</p>
<p><span class="math display">\[\begin{aligned}
(x^{2}-2\rho xy+y^{2}) &amp; =(y-\rho x)^{2}+x^{2}(1-\rho^{2})
\end{aligned}\]</span></p>
<p>Thus, we can write:</p>
<p><span class="math display">\[\begin{aligned}
\int_{\mathbf{R}}yf(x,y)dy &amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}e^{-\frac{1}{2}x^{2}}\int_{\mathbf{R}}ye^{-\frac{1}{2}\frac{(y-\rho x)^{2}}{(1-\rho^{2})}}dy
\end{aligned}\]</span></p>
<p>Let’s substitute</p>
<p><span class="math display">\[\begin{aligned}
z &amp; =\frac{(y-\rho x)}{\sqrt{1-\rho^{2}}}\\
dz &amp; =\frac{dy}{\sqrt{1-\rho^{2}}}
\end{aligned}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{aligned}
\int_{\mathbf{R}}ye^{-\frac{1}{2}\frac{(y-\rho x)^{2}}{(1-\rho^{2})}}dy &amp; =\sqrt{1-\rho^{2}}\int_{\mathbf{R}}(\rho x+\sqrt{1-\rho^{2}}z)e^{-\frac{z^{2}}{2}}dz\\
&amp; =\rho x\cdot\sqrt{1-\rho^{2}}\int_{\mathbf{R}}e^{-\frac{z^{2}}{2}}dz+(1-\rho^{2})\int_{\mathbf{R}}ze^{-\frac{z^{2}}{2}}dz\\
&amp; =\rho x\cdot\sqrt{1-\rho^{2}}\cdot\sqrt{2\pi}+(1-\rho^{2})\cdot0\\
&amp; =\rho x\cdot\sqrt{1-\rho^{2}}\cdot\sqrt{2\pi}
\end{aligned}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{aligned}
\int_{\mathbf{R}}yf(x,y)dy &amp; =\frac{1}{2\pi\cancel{\sqrt{1-\rho^{2}}}}e^{-\frac{1}{2}x^{2}}\rho x\cdot\cancel{\sqrt{1-\rho^{2}}}\cdot\sqrt{2\pi}\\
&amp; =\rho x\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}}\\
&amp; =\rho x\cdot f_{X}(x)\\
\frac{\int_{\mathbf{R}}yf(x,y)dy}{f_{X}(x)} &amp; =\frac{\int_{\mathbf{R}}yf(x,y)dy}{\int_{\mathbf{R}}f(x,y)}=\rho x
\end{aligned}\]</span></p>
<p>(d) For a Gaussian vector <span class="math inline">\((X,Y),\)</span> the conditional expectation <span class="math inline">\(\mathbf{E}[Y|X]=h(X)\)</span>. Hence, <span class="math inline">\(\mathbf{E}[Y|X]=\rho X\)</span>. ◻</p>
</div>
<div class="defn">
<p>(Conditional Expectation) Let <span class="math inline">\(Y\)</span> be an integrable random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> and let <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> be a sigma-field of <span class="math inline">\(\Omega\)</span>. The conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\mathcal{G}\)</span> is the random variable denoted by <span class="math inline">\(\mathbb{E}[Y|\mathcal{G}]\)</span> such that the following hold:</p>
<p>(a) <span class="math inline">\(\mathbb{E}[Y|\mathcal{G}]\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable.</p>
<p>In other words, all events pertaining to the random variable <span class="math inline">\(\mathbb{E}[Y|\mathcal{G}]\)</span> are in <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p>(b) For any (bounded) random variable <span class="math inline">\(W\)</span>, that is <span class="math inline">\(\mathcal{G}\)</span>-measurable,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[WY] &amp; =\mathbb{E}[W\mathbb{E}[Y|\mathcal{G}]]
\end{aligned}\]</span></p>
<p>In other words, <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is a proxy for <span class="math inline">\(Y\)</span> as far as the events in <span class="math inline">\(\mathcal{G}\)</span> are concerned.</p>
<p>Note that, by taking <span class="math inline">\(W=1\)</span> in the property (B), we recover:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[\mathbf{E}[Y|\mathcal{G}]] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
</div>
<div class="rem*">
<p>Beware of the notation! If <span class="math inline">\(\mathcal{G}=\sigma(X)\)</span>, then the conditional expectation <span class="math inline">\(\mathbf{E}[Y|\sigma(X)]\)</span> is usually denoted by <span class="math inline">\(\mathbf{E}[Y|X]\)</span> for short. However, one should always keep in mind that conditioning on <span class="math inline">\(X\)</span> is in fact projecting on the linear subspace <em>generated by all variables constructed from <span class="math inline">\(X\)</span></em> and not on the linear space generated by generated by <span class="math inline">\(X\)</span> alone. In the same way, the conditional expectation <span class="math inline">\(\mathbf{E}[Z|\sigma(X,Y)]\)</span> is often written <span class="math inline">\(\mathbf{E}[Z|X,Y]\)</span> for short.</p>
<p>As expected, if <span class="math inline">\(Y\)</span> is in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>, then <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is given by the orthogonal projection of <span class="math inline">\(Y\)</span> onto the subspace <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>, the subspace of square integrable random variables that are <span class="math inline">\(\mathcal{G}\)</span>-measurable. We write <span class="math inline">\(Y^{\star}\)</span> for the random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span> that is closest to <span class="math inline">\(Y\)</span> that is:</p>
<p><span class="math display">\[\begin{aligned}
\min_{Z\in L^{2}(\Omega,\mathcal{G},\mathbb{P})}\mathbf{E}[(Y-Z)^{2}] &amp; =\mathbf{E}[(Y-Y^{\star})^{2}]\label{eq:conditional-expectation}
\end{aligned}\]</span></p>
</div>
<div class="thm">
<p><span id="th:existence-and-uniqueness-of-conditional-expectations-II" data-label="th:existence-and-uniqueness-of-conditional-expectations-II"></span>(Existence and Uniqueness of Conditional Expectations) Let <span class="math inline">\(\mathcal{G}\subset\mathcal{F}\)</span> be a sigma-field of <span class="math inline">\(\Omega\)</span>. Let <span class="math inline">\(Y\)</span> be a random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\)</span>. Then, the conditional expectation <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is the random variable <span class="math inline">\(Y^{\star}\)</span> given in the equation (<a href="#eq:conditional-expectation" data-reference-type="ref" data-reference="eq:conditional-expectation">[eq:conditional-expectation]</a>). Namely, it is the random variable in <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span> that is closest to <span class="math inline">\(Y\)</span> in the <span class="math inline">\(L^{2}\)</span>-distance. In particular we have the following:</p>
</div>
<ul>
<li><p>It is the orthogonal projection of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>, that is, <span class="math inline">\(Y-Y^{\star}\)</span> is orthogonal to the random variables in <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>.</p></li>
<li><p>It is unique.</p></li>
</ul>
<p>Again, the result should be interpreted as follows: The conditional expectation <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is the best approximation of <span class="math inline">\(Y\)</span> given the information included in <span class="math inline">\(\mathcal{G}\)</span>.</p>
<div class="rem*">
<p>The conditional expectation in fact exists and is unique for any integrable random variable <span class="math inline">\(Y\)</span>(i.e. <span class="math inline">\(Y\in L^{1}(\Omega,\mathcal{F},\mathbb{P})\)</span> as the definition suggests. However, there is no orthogonal projection in <span class="math inline">\(L^{1}\)</span>, so the intuitive geometric picture is lost.</p>
</div>
<div class="center">
<p>Figure. An illustration of the conditional expectation <span class="math inline">\(\mathbb{E}[Y|\mathcal{G}]\)</span> as an orthogonal projection of <span class="math inline">\(Y\)</span> onto the subspace <span class="math inline">\(L^2(\Omega,\mathcal{G},\mathbb{P})\)</span>.</p>
</div>
<div class="example">
<p>(Conditional Expectation for Gaussian Vectors. II.) Consider the Gaussian vector <span class="math inline">\((X_{1},\ldots,X_{n})\)</span>. Without loss of generality, suppose it has mean <span class="math inline">\(0\)</span> and is non-degenerate. What is the best approximation of <span class="math inline">\(X_{n}\)</span> given the information <span class="math inline">\(X_{1},\ldots,X_{n-1}\)</span>? In other words, what is:</p>
<p><span class="math display">\[\mathbf{E}[X_{n}|\sigma(X_{1},\ldots,X_{n-1})\]</span></p>
<p>With example (<a href="#ex:sigma(X)-measurable-random-variables-example" data-reference-type="ref" data-reference="ex:sigma(X)-measurable-random-variables-example">[ex:sigma(X)-measurable-random-variables-example]</a>) in mind, let’s write <span class="math inline">\(\mathbf{E}[X_{n}|X_{1}\ldots X_{n-1}]\)</span> for short. From example (<a href="#ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables" data-reference-type="ref" data-reference="ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables">[ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables]</a>), we know that if <span class="math inline">\((X,Y)\)</span> is a Gaussian vector with mean <span class="math inline">\(0\)</span>, then <span class="math inline">\(\mathbf{E}[Y|X]\)</span> is a multiple of <span class="math inline">\(X\)</span>. Thus, we expect, that <span class="math inline">\(\mathbf{E}[X_{n}|X_{1}X_{2}\ldots X_{n-1}]\)</span> is a linear combination of <span class="math inline">\(X_{1},X_{2},\ldots,X_{n-1}\)</span>. That is, there exists <span class="math inline">\(a_{1},\ldots,a_{n-1}\)</span> such that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{n}|X_{1}X_{2}\ldots X_{n-1}] &amp; =a_{1}X_{1}+a_{2}X_{2}+\ldots+a_{n-1}X_{n-1}
\end{aligned}\]</span> In particular, since the conditional expectation is a linear combination of the <span class="math inline">\(X\)</span>’s, it is itself a Gaussian random variable. The best way to find the coefficient <span class="math inline">\(a\)</span>’s is to go back to IID decomposition of Gaussian vectors.</p>
<p>Let <span class="math inline">\((Z_{1},Z_{2},\ldots,Z_{n-1})\)</span> be IID standard Gaussians constructed from the linear combination of <span class="math inline">\((X_{1},X_{2},\ldots,X_{n-1})\)</span>. Then, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{n}|X_{1}X_{2}\ldots X_{n-1}] &amp; =b_{1}Z_{1}+\ldots+b_{n-1}Z_{n-1}
\end{aligned}\]</span></p>
<p>Now, recall, that we construct the random variables <span class="math inline">\(Z_{1}\)</span>, <span class="math inline">\(Z_{2}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(Z_{n}\)</span> using Gram-Schmidt orthogonalization:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{Z_{1}} &amp; =X_{1}, &amp; Z_{1} &amp; =\frac{\tilde{Z_{1}}}{\mathbf{E}(\tilde{Z}_{1}^{2})}\\
\tilde{Z_{2}} &amp; =X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1} &amp; Z_{2} &amp; =\frac{\tilde{Z}_{2}}{\mathbf{E}(\tilde{Z}_{2}^{2})}\\
\tilde{Z_{3}} &amp; =X_{3}-\sum_{i=1}^{2}\mathbf{E}(X_{3}Z_{i})Z_{i} &amp; Z_{3} &amp; =\frac{\tilde{Z}_{3}}{\mathbf{E}(\tilde{Z}_{3}^{2})}\\
&amp; \vdots
\end{aligned}\]</span></p>
</div>
<p><strong>The simple case for <span class="math inline">\(n=2\)</span> random variables.</strong></p>
<p>We have already seen before:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})] &amp; =\mathbf{E}[\tilde{Z}_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})]\\
&amp; =\frac{\mathbf{E}[\tilde{Z}_{1}^{2}]}{\mathbf{E}[\tilde{Z}_{1}^{2}]}\times\mathbf{E}\left[\tilde{Z}_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})\right]\\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}\left[\frac{\tilde{Z}_{1}}{\mathbf{E}[\tilde{Z}_{1}^{2}]}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})\right]\\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}[Z_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})]\\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\left(\mathbf{E}[Z_{1}X_{2}]-\mathbf{E}(X_{2}Z_{1})\mathbf{E}[Z_{1}^{2}]\right)\\
&amp; =0
\end{aligned}\]</span></p>
<p>So,<span class="math inline">\(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1}\)</span> is orthogonal to <span class="math inline">\(X_{1}\)</span>.</p>
<p>Moreover, <span class="math inline">\(\mathbf{E}(X_{2}Z_{1})Z_{1}\)</span> is a function of <span class="math inline">\(X_{1}\)</span>. Thus, both the properties of conditional expectation are satisfied. Since conditional expectations are unique, we must have, <span class="math inline">\(\mathbf{E}[X_{2}|X_{1}]=\mathbf{E}(X_{2}Z_{1})Z_{1}\)</span>.</p>
<p><strong>The case for <span class="math inline">\(n=3\)</span> random variables.</strong></p>
<p>We have seen that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})] &amp; =\frac{\mathbf{E}[\tilde{Z}_{1}^{2}]}{\mathbf{E}[\tilde{Z}_{1}^{2}]}\times\mathbf{E}[\tilde{Z}_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})]\\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}\left\{ \frac{\tilde{Z}_{1}}{\mathbf{E}[\tilde{Z}_{1}^{2}]}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})\right\} \\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}\left\{ Z_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})\right\} \\
&amp; =\mathbf{E}[\tilde{Z}_{1}^{2}]\times\mathbf{E}[X_{3}Z_{1}]-\mathbf{E}[X_{3}Z_{1}]\mathbf{E}[Z_{1}^{2}]-\mathbf{E}[X_{3}Z_{2}]\mathbf{E}[Z_{1}Z_{2}]\\
&amp; =0
\end{aligned}\]</span></p>
<p>It is an easy exercise to show that it is orthogonal to <span class="math inline">\(X_{2}\)</span>.</p>
<p>Hence, <span class="math inline">\(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2}\)</span> is orthogonal to <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>. Moreover, <span class="math inline">\(\mathbf{E}(X_{3}Z_{1})Z_{1}+\mathbf{E}(X_{3}Z_{2})Z_{2}\)</span> is a function of <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>. Thus, we must have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{3}|X_{1}X_{2}] &amp; =\mathbf{E}(X_{3}Z_{1})Z_{1}+\mathbf{E}(X_{3}Z_{2})Z_{2}
\end{aligned}\]</span></p>
<p>In general, <span class="math inline">\(X_{n}-\sum_{i=1}^{n-1}\mathbf{E}(X_{n}Z_{i})Z_{i}\)</span> is orthogonal to <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_{n-1}\)</span>. Hence,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[X_{n}|X_{1}X_{2}\ldots X_{n-1}] &amp; =\sum_{i=1}^{n-1}\mathbf{E}(X_{n}Z_{i})Z_{i}
\end{aligned}\]</span></p>
</section>
<section id="properties-of-conditional-expectation." class="level3">
<h3 class="anchored" data-anchor-id="properties-of-conditional-expectation.">Properties of Conditional Expectation.</h3>
<p>We now list the properties of conditional expectation that follow from the two defining properties (A), (B) in the definition. They are extremely useful, when doing explicit computations on martingales. A good way to remember them is to understand how they relate to the interpretation of conditional expectation as an orthogonal projection onto a subspace or, equivalently, as the best approximation of the variable given the information available.</p>
<div class="prop">
<p><span id="prop:properties-of-conditional-expectation" data-label="prop:properties-of-conditional-expectation"></span>Let <span class="math inline">\(Y\)</span> be an integrable random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Let <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> be another sigma-field of <span class="math inline">\(\Omega\)</span>. Then, the conditional expectation <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> has the following properties:</p>
<p>(1) If <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable, then :</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{G}] &amp; =Y
\end{aligned}\]</span></p>
<p>(2) Taking out what is known. More generally, if <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G-}\)</span>measurable and <span class="math inline">\(X\)</span> is another integrable random variable (with <span class="math inline">\(XY\)</span> also integrable), then :</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[XY|\mathcal{G}] &amp; =Y\mathbf{E}[X|\mathcal{G}]
\end{aligned}\]</span></p>
<p>This makes sense, since <span class="math inline">\(Y\)</span> is determined by <span class="math inline">\(\mathcal{G}\)</span>, so we can take out what is known; it can be treated as a constant for the conditional expectation.</p>
<p>(3) Independence. If <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(\mathcal{G}\)</span>, that is, for any events <span class="math inline">\(\{Y\in(a,b]\}\)</span> and <span class="math inline">\(A\in\mathcal{G}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(\{Y\in I\}\cap A) &amp; =\mathbb{P}(\{Y\in I\})\cdot\mathbb{P}(A)
\end{aligned}\]</span></p>
<p>then</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{G}] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
<p>In other words, if you have no information on <span class="math inline">\(Y\)</span>, your best guess for its value is simply plain expectation.</p>
<p>(4) Linearity of conditional expectations. Let <span class="math inline">\(X\)</span> be another integrable random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Then,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[aX+bY|\mathcal{G}] &amp; =a\mathbf{E}[X|\mathcal{G}]+b\mathbf{E}[Y|\mathcal{G}],\quad\text{for any }a,b\in\mathbf{R}
\end{aligned}\]</span></p>
<p>The linearity justifies the cumbersom choice of notation <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> for the random variable.</p>
<p>(5) Tower Property : If <span class="math inline">\(\mathcal{H}\subseteq\mathcal{G}\)</span> is another sigma-field of <span class="math inline">\(\Omega\)</span>, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{H}] &amp; =\mathbf{E}[\mathbf{E}[Y|\mathcal{G}]|\mathcal{H}]
\end{aligned}\]</span></p>
<p>Think in terms of two successive projections: first on a plane, then on a line in the plane.</p>
<p>(6) Pythagoras Theorem. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y^{2}] &amp; =\mathbf{E}\left[\left(\mathbf{E}[Y|\mathcal{G}]\right)^{2}\right]+\mathbf{E}\left[\left(Y-\mathbf{E}[Y|\mathcal{G}]\right)^{2}\right]
\end{aligned}\]</span></p>
<p>In particular:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[\left(\mathbf{E}\left[Y|\mathcal{G}\right]\right)^{2}\right] &amp; \leq\mathbf{E}[Y^{2}]
\end{aligned}\]</span></p>
<p>In words, the <span class="math inline">\(L^{2}\)</span> norm of <span class="math inline">\(\mathbf{E}[X|\mathcal{G}]\)</span> is smaller than the one of <span class="math inline">\(X\)</span>, which is clear if you think in terms of orthogonal projection.</p>
<p>(7) Expectation of the conditional expectation.</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[\mathbf{E}[Y|\mathcal{G}]\right] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
</div>
<p><em>Proof.</em></p>
<p>The uniqueness property of conditional expectations in theorem (<a href="#th:existence-and-uniqueness-of-conditional-expectations-II" data-reference-type="ref" data-reference="th:existence-and-uniqueness-of-conditional-expectations-II">[th:existence-and-uniqueness-of-conditional-expectations-II]</a>) might appear to be an academic curiosity. On the contrary, it is very practical, since it ensures, that if we find a candidate for the conditional expectation that has the two properties in Definition (<a href="#def:conditional-expectation" data-reference-type="ref" data-reference="def:conditional-expectation">[def:conditional-expectation]</a>), then it must be <em>the</em> conditional expectation. To see this, let’s prove property (1).</p>
<div class="claim">
<p>If <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable, then <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]=Y\)</span>.</p>
<p>It suffices to show that <span class="math inline">\(Y\)</span> has the two defining properties of conditional expectation.</p>
<p>(1) We are given that, <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable. So, property (A) is satisfied.</p>
<p>(2) For any bounded random variable <span class="math inline">\(W\)</span> that is <span class="math inline">\(\mathcal{G}\)</span>-measurable, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[W(Y-Y)] &amp; =\mathbf{E}[0]=0
\end{aligned}\]</span></p>
<p>So, property (B) is also a triviality.</p>
</div>
<div class="claim">
<p>(Taking out what is known.) If <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable and <span class="math inline">\(X\)</span> is another integrable random variable, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[XY|\mathcal{G}] &amp; =Y\mathbf{E}[X|\mathcal{G}]
\end{aligned}\]</span></p>
<p>In a similar vein, it suffices to show that, <span class="math inline">\(Y\mathbf{E}[X|\mathcal{G}]\)</span> has the two defining properties of conditional expectation.</p>
<p>(1) We are given that <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable; from property (1), <span class="math inline">\(\mathbf{E}[X|\mathcal{G}]\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable. It follows that, <span class="math inline">\(Y\mathbf{E}[X|\mathcal{G}]\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable.</p>
<p>(2) From theorem (<a href="#th:existence-and-uniqueness-of-conditional-expectations-II" data-reference-type="ref" data-reference="th:existence-and-uniqueness-of-conditional-expectations-II">[th:existence-and-uniqueness-of-conditional-expectations-II]</a>), <span class="math inline">\(X-\mathbf{E}[X|\mathcal{G}]\)</span> is orthogonal to the random variables <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>. So, if <span class="math inline">\(W\)</span> is any bounded <span class="math inline">\(\mathcal{G}\)</span>-measurable random variable, it follows that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[WY(X-\mathbf{E}[X|\mathcal{G}])] &amp; =0\\
\implies\mathbf{E}[W\cdot XY] &amp; =\mathbf{E}[WY\mathbf{E}[X|\mathcal{G}]]
\end{aligned}\]</span></p>
<p>This closes the proof.</p>
</div>
<div class="claim">
<p>(Independence.) If <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(\mathcal{G}\)</span>, that is, for all events <span class="math inline">\(\{Y\in(a,b]\}\)</span> and <span class="math inline">\(A\in\mathcal{G}\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{\mathbb{P}}\{Y\in(a,b]\cap A\} &amp; =\mathbb{P}\{Y\in(a,b]\}\cdot\mathbb{P}(A)
\end{aligned}\]</span></p>
<p>then</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{G}] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
<p>Let us show that <span class="math inline">\(\mathbf{E}[Y]\)</span> has the two defining properties of conditional expectations.</p>
<p>(1) <span class="math inline">\(\mathbf{E}[Y]\)</span> is a constant and so it is <span class="math inline">\(\mathcal{F}_{0}\)</span> measurable. Hence, it is <span class="math inline">\(\mathcal{G}\)</span> measurable.</p>
<p>(2) If <span class="math inline">\(W\)</span> is another <span class="math inline">\(\mathcal{G}\)</span>-measurable random variable,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[WY] &amp; =\mathbf{E}[W]\cdot\mathbf{E}[Y]
\end{aligned}\]</span></p>
<p>since <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(\mathcal{G}\)</span> and therefore it is independent of <span class="math inline">\(Y\)</span>. Hence,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[W(Y-\mathbf{E}[Y])] &amp; =0
\end{aligned}\]</span></p>
<p>Consequently, <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]=\mathbf{E}[Y]\)</span>.</p>
</div>
<div class="claim">
<p>(Linearity of conditional expectations) Let <span class="math inline">\(X\)</span> be another integrable random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Then,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[aX+bY|\mathcal{G}] &amp; =a\mathbf{E}[X|\mathcal{G}]+b\mathbf{E}[Y|\mathcal{G}],\quad\text{for any }a,b\in\mathbf{R}
\end{aligned}\]</span></p>
</div>
<p>Since <span class="math inline">\(\mathbf{E}[X|\mathcal{G}]\)</span> and <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> are <span class="math inline">\(\mathcal{G}-\)</span>measurable, any linear combination of these two random variables is also <span class="math inline">\(\mathcal{G}\)</span>-measurable.</p>
<p>Also, if <span class="math inline">\(W\)</span> is any bounded <span class="math inline">\(\mathcal{G}-\)</span>measurable random variable, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[W(aX+bY-(a\mathbf{E}[X|\mathcal{G}]+b\mathbf{E}[Y|\mathcal{G}]))] &amp; =a\mathbf{E}[W(X-\mathbf{E}[X|\mathcal{G}])]\\
&amp; +b\mathbf{E}[W(Y-\mathbf{E}[Y|\mathcal{G}])]
\end{aligned}\]</span></p>
<p>By definition, <span class="math inline">\(X-\mathbf{E}(X|\mathcal{G})\)</span> is orthogonal t o the subspace <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span> and hence to all <span class="math inline">\(\mathcal{G}\)</span>-measurable random-variables. Hence, the two expectations on the right hand side of the above expression are <span class="math inline">\(0\)</span>. Since, conditional expectations are unique, we have the desired result.</p>
<div class="claim">
<p>If <span class="math inline">\(\mathcal{H}\subseteq\mathcal{G}\)</span> is another sigma-field of <span class="math inline">\(\Omega\)</span>, then</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y|\mathcal{H}] &amp; =\mathbf{E}[\mathbf{E}[Y|\mathcal{G}]|\mathcal{H}]
\end{aligned}\]</span></p>
<p>Define <span class="math inline">\(U:=\mathbf{E}[Y|\mathcal{G}]\)</span>. By definition, <span class="math inline">\(\mathbf{E}[U|\mathcal{H}]\)</span> is <span class="math inline">\(\mathcal{H}\)</span>-measurable.</p>
<p>Let <span class="math inline">\(W\)</span> be any bounded <span class="math inline">\(\mathcal{H}\)</span>-measurable random variable. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[W\{\mathbf{E}(Y|\mathcal{G})-\mathbf{E}(\mathbf{E}(Y|\mathcal{G})|\mathcal{H})\}] &amp; =\mathbf{E}[W(U-\mathbf{E}(U|\mathcal{H})]
\end{aligned}\]</span></p>
<p>But, by definition <span class="math inline">\(U-\mathbf{E}(U|\mathcal{H})\)</span> is always orthogonal to the subspace <span class="math inline">\(L^{2}(\Omega,\mathcal{H},\mathbb{P})\)</span> and hence, <span class="math inline">\(\mathbf{E}[W(U-\mathbf{\mathbf{E}}(U|\mathcal{H})]=0\)</span>. Since, conditional expectations are unique, we have the desired result.</p>
</div>
<div class="claim">
<p><strong>Pythagoras’s theorem</strong>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y^{2}] &amp; =\mathbf{E}[(\mathbf{E}[Y|\mathcal{G}])^{2}]+\mathbf{E}[(Y-\mathbf{E}(Y|\mathcal{G}))^{2}]
\end{aligned}\]</span></p>
<p>In particular,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(\mathbf{E}[Y|\mathcal{G}])^{2}] &amp; \leq\mathbf{E}[Y^{2}]
\end{aligned}\]</span></p>
<p>Consider the orthogonal decomposition:</p>
<p><span class="math display">\[\begin{aligned}
Y &amp; =\mathbf{E}[Y|\mathcal{G}]+(Y-\mathbf{E}[Y|\mathcal{G}])
\end{aligned}\]</span></p>
<p>Squaring on both sides and taking expectations, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[Y^{2}] &amp; =\mathbf{E}[(\mathbf{E}(Y|\mathcal{G}))^{2}]+\mathbf{E}[(Y-\mathbf{E}[Y|\mathcal{G}])^{2}]+2\mathbf{E}\left[\mathbf{E}[Y|\mathcal{G}](Y-\mathbf{E}[Y|\mathcal{G}])\right]
\end{aligned}\]</span></p>
<p>By definition of conditional expectation, <span class="math inline">\((Y-\mathbf{E}[Y|\mathcal{G}])\)</span> is orthogonal to the subspace <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>. By the properties of conditional expectation, <span class="math inline">\(\mathbf{E}[Y|\mathcal{G}]\)</span> is <span class="math inline">\(\mathcal{G}-\)</span>measurable, so it belongs to <span class="math inline">\(L^{2}(\Omega,\mathcal{G},\mathbb{P})\)</span>. Hence, the dot-product on the right-hand side is <span class="math inline">\(0\)</span>. Consequently, we have the desired result.</p>
<p>Moreover, since <span class="math inline">\((Y-\mathbf{E}[Y|\mathcal{G}])^{2}\)</span> is a non-negative random variable, <span class="math inline">\(\mathbf{E}[(Y-\mathbf{E}[Y|\mathcal{G}])^{2}]\geq0\)</span>. It follows that: <span class="math inline">\(\mathbf{E}[Y^{2}]\geq\mathbf{E}[(\mathbf{E}(Y|\mathcal{G}))^{2}]\)</span>.</p>
</div>
<div class="claim">
<p>Our claim is:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[\mathbf{E}[Y|\mathcal{G}]\right] &amp; =\mathbf{E}[Y]
\end{aligned}\]</span></p>
<p>We know that, if <span class="math inline">\(W\)</span> is any bounded <span class="math inline">\(\mathcal{G}\)</span>-measurable random variable:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[WY\right] &amp; =\mathbf{E}[W\mathbf{E}[Y|\mathcal{G}]]
\end{aligned}\]</span></p>
<p>Taking <span class="math inline">\(W=1\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}\left[Y\right] &amp; =\mathbf{E}[\mathbf{E}[Y|\mathcal{G}]]
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>(Brownian Conditioning II). We continue the example (<a href="#ex:brownian-conditioning-I" data-reference-type="ref" data-reference="ex:brownian-conditioning-I">[ex:brownian-conditioning-I]</a>). Let’s now compute the conditional expectations <span class="math inline">\(\mathbf{E}[e^{aB_{1}}|B_{1/2}]\)</span> and <span class="math inline">\(\mathbf{E}[e^{aB_{1/2}}|B_{1}]\)</span> for some parameter <span class="math inline">\(a\)</span>. We shall need the properties of conditional expectation in proposition (<a href="#prop:properties-of-conditional-expectation" data-reference-type="ref" data-reference="prop:properties-of-conditional-expectation">[prop:properties-of-conditional-expectation]</a>). For the first one we use the fact that <span class="math inline">\(B_{1/2}\)</span> is independent of <strong><span class="math inline">\(B_{1}-B_{1/2}\)</span></strong> to get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[e^{aB_{1}}|B_{1/2}] &amp; =\mathbf{E}[e^{a((B_{1}-B_{1/2})+B_{1/2})}|B_{1/2}]\\
&amp; =\mathbf{E}[e^{a(B_{1}-B_{2})}\cdot e^{aB_{1/2}}|B_{1/2}]\\
&amp; \quad\left\{ \text{Taking out what is known}\right\} \\
&amp; =e^{aB_{1/2}}\mathbf{E}[e^{a(B_{1}-B_{1/2})}|B_{1/2}]\\
&amp; =e^{aB_{1/2}}\cdot\mathbf{E}[e^{a(B_{1}-B_{1/2})}]\\
&amp; \quad\{\text{Independence}\}
\end{aligned}\]</span></p>
<p>We know that, <span class="math inline">\(a(B_{1}-B_{1/2})\)</span> is a gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(a^{2}/2\)</span>. We also know that, <span class="math inline">\(\mathbf{E}[e^{tZ}]=e^{t^{2}/2}\)</span>. So, <span class="math inline">\(\mathbf{E}[e^{a(B_{1}-B_{1/2})}]=e^{a^{2}/4}\)</span>. Consequently, <span class="math inline">\(\mathbf{E}[e^{aB_{1}}|B_{1/2}]=e^{aB_{1/2}+a^{2}/4}\)</span>.</p>
<p>The result itself has the form of the MGF of a Gaussian with mean <span class="math inline">\(B_{1/2}\)</span> and variance <span class="math inline">\(1/2\)</span>. (The MGF of <span class="math inline">\(X=\mu+\sigma Z\)</span>, <span class="math inline">\(Z=N(0,1)\)</span> is <span class="math inline">\(M_{X}(a)=\exp\left[\mu+\frac{1}{2}\sigma^{2}a^{2}\right]\)</span>.) In fact, this shows that the conditional distribution of <span class="math inline">\(B_{1}\)</span> given <span class="math inline">\(B_{1/2}\)</span> is Gaussian of mean <span class="math inline">\(B_{1/2}\)</span> and variance <span class="math inline">\(1/2\)</span>.</p>
<p>For the other expectation, note that <span class="math inline">\(B_{1/2}-\frac{1}{2}B_{1}\)</span> is independent of <span class="math inline">\(B_{1}\)</span>. We have: <span class="math display">\[\begin{aligned}
\mathbf{E}\left[\left(B_{1/2}-\frac{1}{2}B_{1}\right)B_{1}\right] &amp; =\mathbf{E}(B_{1/2}B_{1})-\frac{1}{2}\mathbf{E}[B_{1}^{2}]\\
&amp; =\frac{1}{2}-\frac{1}{2}\cdot1\\
&amp; =0
\end{aligned}\]</span></p>
<p>Therefore, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[e^{aB_{1/2}}|B_{1}] &amp; =\mathbf{E}[e^{a(B_{1/2}-\frac{1}{2}B_{1})+\frac{a}{2}B_{1}}|B_{1}]\\
&amp; =\mathbf{E}[e^{a(B_{1/2}-\frac{1}{2}B_{1})}\cdot e^{\frac{a}{2}B_{1}}|B_{1}]\\
&amp; =e^{\frac{a}{2}B_{1}}\mathbf{E}[e^{a(B_{1/2}-\frac{1}{2}B_{1})}|B_{1}]\\
&amp; \quad\{\text{Taking out what is known }\}\\
&amp; =e^{\frac{a}{2}B_{1}}\mathbf{E}[e^{a(B_{1/2}-\frac{1}{2}B_{1})}]\\
&amp; \quad\{\text{Independence}\}
\end{aligned}\]</span></p>
<p>Now, <span class="math inline">\(a(B_{1/2}-\frac{1}{2}B_{1})\)</span> is a random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(a^{2}(\frac{1}{2}-\frac{1}{4})=\frac{a^{2}}{4}\)</span>. Consequently, <span class="math inline">\(\mathbf{E}[e^{(a/2)Z}]=e^{\frac{a^{2}}{8}}\)</span>. Thus, <span class="math inline">\(\mathbf{E}[e^{aB_{1/2}}|B_{1}]=e^{\frac{a}{2}B_{1}+\frac{a^{2}}{8}}\)</span>.</p>
</div>
<div class="example">
<p>(Brownian bridge is conditioned Brownian motion). We know that the Brownian bridge <span class="math inline">\(M_{t}=B_{t}-tB_{1}\)</span>, <span class="math inline">\(t\in[0,1]\)</span> is independent of <span class="math inline">\(B_{1}\)</span>. We use this to show that the conditional distribution of the Brownian motion given the value at the end-point <span class="math inline">\(B_{1}\)</span> is the one of a Brownian bridge shifted by the straight line going from <span class="math inline">\(0\)</span> to <span class="math inline">\(B_{1}\)</span>. To see this, we compute the conditional MGF of <span class="math inline">\((B_{t_{1}},B_{t_{2}},\ldots,B_{t_{n}})\)</span> given <span class="math inline">\(B_{1}\)</span> for some arbitrary choices of <span class="math inline">\(t_{1},t_{2},\ldots,t_{n}\)</span> in <span class="math inline">\([0,1]\)</span>. We get the following by adding and subtracting <span class="math inline">\(t_{j}B_{1}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[e^{a_{1}B_{t_{1}}+\ldots+a_{n}B_{t_{n}}}|B_{1}] &amp; =\mathbf{E}[e^{a_{1}(B_{t_{1}}-t_{1}B_{1})+\ldots+a_{n}(B_{t_{n}}-t_{n}B_{1})}\cdot e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}|B_{1}]\\
&amp; =e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}\mathbf{E}[e^{a_{1}M_{t_{1}}+\ldots+a_{n}M_{t_{n}}}|B_{1}]\\
&amp; \quad\{\text{Taking out what is known}\}\\
&amp; =e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}\mathbf{E}[e^{a_{1}M_{t_{1}}+\ldots+a_{n}M_{t_{n}}}]\\
&amp; \quad\{\text{Independence}\}
\end{aligned}\]</span></p>
<p>The right side is exactly the MGF of the process <span class="math inline">\(M_{t}+tB_{1},t\in[0,1]\)</span> (for a fixed value <span class="math inline">\(B_{1})\)</span>, where <span class="math inline">\((M_{t},t\in[0,1])\)</span> is a Brownian bridge. This proves the claim.</p>
</div>
<div class="lem">
<p>(Conditional Jensen’s Inequality) If <span class="math inline">\(c\)</span> is a convex function on <span class="math inline">\(\mathbf{R}\)</span> and <span class="math inline">\(X\)</span> is a random variable on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[c(X)] &amp; \geq c(\mathbf{E}[X])
\end{aligned}\]</span></p>
<p>More generally, if <span class="math inline">\(\mathcal{G}\subseteq\mathcal{F}\)</span> is a sigma-field, then:</p>
<p><span class="math display">\[\mathbf{E}[c(X)|\mathcal{G}]\geq c(\mathbf{E}[X|\mathcal{G}])\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> We know that, if <span class="math inline">\(c(x)\)</span> is a convex function, the tangent to the curve <span class="math inline">\(c\)</span> at any point lies below the curve. The tangent to the cuve at this point, is a straight-line of the form:</p>
<p><span class="math display">\[\begin{aligned}
c(t)=y &amp; =mt+c
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(m(t)=c'(t)\)</span>. This holds for all <span class="math inline">\(t\in\mathbf{R}\)</span>. At an arbitrary point <span class="math inline">\(x\)</span> we have:</p>
<p><span class="math display">\[\begin{aligned}
c(x)\geq &amp; y=mx+c
\end{aligned}\]</span></p>
<p>Therefore, we have:</p>
<p><span class="math display">\[\begin{aligned}
c(x)-c(t) &amp; \geq m(t)(x-t)
\end{aligned}\]</span></p>
<p>for any <span class="math inline">\(x\)</span> and any point of tangency <span class="math inline">\(t\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
c(X)-c(Y) &amp; \geq m(Y)(X-Y)
\end{aligned}\]</span></p>
<p>Substituting <span class="math inline">\(Y=\mathbf{E}[X|\mathcal{G}]\)</span>, we get:</p>
<p><span class="math display">\[\begin{aligned}
c(X)-c(\mathbf{E}[X|\mathcal{G}]) &amp; \geq m(\mathbf{E}[X|\mathcal{G}])(X-\mathbf{E}[X|\mathcal{G}])
\end{aligned}\]</span></p>
<p>Taking expectations on both sides, we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(c(X)-c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}] &amp; \geq\mathbf{E}[m(\mathbf{E}[X|\mathcal{G}])(X-\mathbf{E}[X|\mathcal{G}])|\mathcal{G}]
\end{aligned}\]</span></p>
<p>The left-hand side simplifies as:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(c(X)-c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}] &amp; =\mathbf{E}[c(X)|\mathcal{G}]-\mathbf{E}[c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}]\\
&amp; \quad\{\text{Linearity}\}\\
&amp; =\mathbf{E}[c(X)|\mathcal{G}]-c(\mathbf{E}[X|\mathcal{G}])\\
&amp; \quad\{\text{c(\ensuremath{\mathbf{E}}[X|\ensuremath{\mathcal{G}}])) is \ensuremath{\mathcal{G}}-measurable}\}
\end{aligned}\]</span></p>
<p>On the right hand side, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[m(\mathbf{E}[X|\mathcal{G}])(X-\mathbf{E}[X|\mathcal{G}])|\mathcal{G}] &amp; =\mathbf{E}[m(\mathbf{E}[X|\mathcal{G}])\cdot X|\mathcal{G}]-\mathbf{E}[m(\mathbf{E}[X|\mathcal{G}])\cdot\mathbf{E}[X|\mathcal{G}]|\mathcal{G}]\\
&amp; =\mathbf{E}[X|\mathcal{G}]m(\mathbf{E}[X|\mathcal{G}])-m(\mathbf{E}[X|\mathcal{G}])\cdot\mathbf{E}[X|\mathcal{G}]\\
&amp; =0
\end{aligned}\]</span></p>
<p>Consequently, it follows that <span class="math inline">\(\mathbf{E}[c(X)|\mathcal{G}]\geq c(\mathbf{E}[X|\mathcal{G}])\)</span>. ◻</p>
</div>
<div class="example">
<p>(Embeddings of <span class="math inline">\(L^{p}\)</span> spaces) Square-integrable random variables are in fact integrable. In other words, there is always the inclusion <span class="math inline">\(L^{2}(\Omega,\mathcal{F},\mathbb{P})\subseteq L^{1}(\Omega,\mathcal{F},\mathbb{P})\)</span>. In particular, square integrable random variables always have a well-defined variance. This embedding is a simple consequence of Jensen’s inequality since:</p>
<p><span class="math display">\[\begin{aligned}
|\mathbf{E}[X]|^{2} &amp; \leq\mathbf{E}[|X|^{2}]
\end{aligned}\]</span></p>
<p>as <span class="math inline">\(f(x)=|x|^{2}\)</span> is convex. By taking the square root on both sides, we get:</p>
<p><span class="math display">\[\begin{aligned}
\left\Vert X\right\Vert _{1} &amp; \leq\left\Vert X\right\Vert _{2}
\end{aligned}\]</span></p>
<p>More generally, for any <span class="math inline">\(1&lt;p&lt;\infty\)</span>, we can define <span class="math inline">\(L^{p}(\Omega,\mathcal{F},\mathbb{P})\)</span> to be the linear space of random variables such that <span class="math inline">\(\mathbf{E}[|X|^{p}]&lt;\infty\)</span>. Then for <span class="math inline">\(p&lt;q\)</span>, since <span class="math inline">\(x^{q/p}\)</span> is convex, we get by Jensen’s inequality :</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[|X|^{q}] &amp; =\mathbf{E}[(|X|^{p})^{\frac{q}{p}}]\geq\left(\mathbf{E}[|X|^{p}]\right)^{\frac{q}{p}}
\end{aligned}\]</span></p>
<p>Taking the <span class="math inline">\(q\)</span>-th root on both sides:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[|X|^{p}]^{1/p} &amp; \leq\mathbf{E}[|X|^{q}]^{1/q}
\end{aligned}\]</span></p>
<p>So, if <span class="math inline">\(X\in L^{q}\)</span>, then it must also be in <span class="math inline">\(L^{p}\)</span>. Concretely, this means that any random variable with a finite <span class="math inline">\(q\)</span>-moment will also have a finite <span class="math inline">\(p\)</span>-moment, for <span class="math inline">\(q&gt;p\)</span>.</p>
</div>
</section>
</section>
<section id="martingales.-1" class="level2">
<h2 class="anchored" data-anchor-id="martingales.-1">Martingales.</h2>
<p>We now have all the tools to define martingales.</p>
<div class="defn">
<p><span id="def:Filtration" data-label="def:Filtration"></span>(Filtration). A <em>filtration</em> <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> of <span class="math inline">\(\Omega\)</span> is an increasing sequence of <span class="math inline">\(\sigma\)</span>-fields of <span class="math inline">\(\Omega\)</span>. That is,</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{F}_{s} &amp; \subseteq\mathcal{F}_{t},\quad\forall s\leq t
\end{aligned}\]</span></p>
<p>We will usually take <span class="math inline">\(\mathcal{F}_{0}=\{\emptyset,\Omega\}\)</span>. The canonical example of a filtration is the natural filtration of a given process <span class="math inline">\((M_{s}:s\geq0)\)</span>. This is the filtration given by <span class="math inline">\(\mathcal{F}_{t}=\sigma(M_{s},s\leq t)\)</span>. The inclusions of the <span class="math inline">\(\sigma\)</span>-fields are then clear. For a given Brownian motion <span class="math inline">\((B_{t},t\geq0)\)</span>, the filtration <span class="math inline">\(\mathcal{F}_{t}=\sigma(B_{s},s\leq t)\)</span> is sometimes called the <em>Brownian filtration</em>. We think of the filtration as the <em>flow of information of the process</em>.</p>
</div>
<div class="defn">
<p><span id="def:Adapted-process" data-label="def:Adapted-process"></span>A stochastic process <span class="math inline">\((X_{t}:t\geq0)\)</span> is said to be adapted to <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>, if for each <span class="math inline">\(t\)</span>, the random variable <span class="math inline">\(X_{t}\)</span> is <span class="math inline">\(\mathcal{F}_{t}-\)</span>measurable.</p>
</div>
<div class="defn">
<p>(Martingale). A process <span class="math inline">\((M_{t}:t\geq0)\)</span> is a martingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> if the following hold:</p>
<p>(1) The process is <em>adapted</em>, that is <span class="math inline">\(M_{t}\)</span> is <span class="math inline">\(\mathcal{F}_{t}-\)</span>measurable for all <span class="math inline">\(t\geq0\)</span>.</p>
<p>(2) <span class="math inline">\(\mathbf{E}[|M_{t}|]&lt;\infty\)</span> for all <span class="math inline">\(t\geq0\)</span>. (This ensures that the conditional expectation is well defined.)</p>
<p>(3) <em>Martingale property:</em></p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}|\mathcal{F}_{s}] &amp; =M_{s}\quad\forall s\leq t
\end{aligned}\]</span></p>
<p>Roughly, speaking this means that the best approximation of a process at a future time <span class="math inline">\(t\)</span> is its value at the present.</p>
</div>
<p>In particular, the martingale property implies that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}|\mathcal{F}_{0}] &amp; =M_{0}\nonumber \\
\mathbf{E}[\mathbf{E}[M_{t}|\mathcal{F}_{0}]] &amp; =\mathbf{E}[M_{0}]\nonumber \\
\mathbf{E}[M_{t}] &amp; =\mathbf{E}[M_{0}]\label{eq:expected-value-of-martingale-at-any-time-is-constant}\\
&amp; \quad\{\text{Tower Property}\}\nonumber
\end{aligned}\]</span></p>
<p>Usually, we take <span class="math inline">\(\mathcal{F}_{0}\)</span> to be the trivial sigma-field <span class="math inline">\(\{\emptyset,\Omega\}\)</span>. A random variable that is <span class="math inline">\(\mathcal{F}_{0}\)</span>-measurable must be a constant, so <span class="math inline">\(M_{0}\)</span> is a constant. In this case, <span class="math inline">\(\mathbf{E}[M_{t}]=M_{0}\)</span> for all <span class="math inline">\(t\)</span>. If properties (1) and (2) are satisfied, but the best approximation is larger, <span class="math inline">\(\mathbf{E}[M_{t}|\mathcal{F}_{s}]\geq M_{s}\)</span>, the process is called a <em>submartingale</em>. If it is smaller on average, <span class="math inline">\(\mathbf{E}[M_{t}|\mathcal{F}_{s}]\leq\mathbf{E}[M_{s}]\)</span>, we say it is a supermartingale.</p>
<p>We will be mostly interested in martingales that are continuous and square-integrable. Continuous martingales are martingales whose paths <span class="math inline">\(t\mapsto M_{t}(\omega)\)</span> are continuous almost surely. Square-integrable martingales are such that <span class="math inline">\(\mathbf{E}[|M_{t}|^{2}]&lt;\infty\)</span> for all <span class="math inline">\(t\)</span>’s. This condition is stronger than <span class="math inline">\(\mathbf{E}[|M_{t}|]&lt;\infty\)</span> due to Jensen’s inequality.</p>
<div class="rem*">
<p>(Martingales in Discrete-time). Martingales can be defined the same way if the index set of the process is discrete. For example, the filtration <span class="math inline">\((\mathcal{F}_{n}:n\in\mathbf{N})\)</span> is a countable set and the martingale property is then replaced by <span class="math inline">\(\mathbf{E}[M_{n+1}|\mathcal{F}_{n}]=M_{n}\)</span> as expected. The tower-property then yields the martingale property <span class="math inline">\(\mathbf{E}[M_{n+k}|\mathcal{F}_{n}]=M_{n}\)</span> for <span class="math inline">\(k\geq1\)</span>.</p>
</div>
<div class="rem*">
<p>(Continuous Filtrations). Filtrations with continuous time can be tricky to handle rigorously. For example, one has to make sense of what it means for <span class="math inline">\(\mathcal{F}_{s}\)</span> as <span class="math inline">\(s\)</span> approaches <span class="math inline">\(t\)</span> from the left. Is it equal to <span class="math inline">\(\mathcal{F}_{t}\)</span>? Or is there actually less information in <span class="math inline">\(\lim_{s\to t^{-}}\mathcal{F}_{s}\)</span> than in <span class="math inline">\(\mathcal{F}_{t}\)</span>? This is a bit of headache when dealing with processes with jumps, like the Poisson process. However, if the paths are continuous, the technical problems are not as heavy.</p>
<p>Let’s look at some of the important examples of martingales constructed from Brownian Motion.</p>
</div>
<div class="example">
<p>(Examples of Brownian Martingales)</p>
<p>(i) <em>Standard Brownian Motion.</em> Let <span class="math inline">\((B_{t}:t\geq0)\)</span> be a standard Brownian motion and let <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> be a <em>Brownian filtration</em>. Then <span class="math inline">\((B_{t}:t\geq0)\)</span> is a square integrable martingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>. Property (1) is obvious, because all the sets in <span class="math inline">\(\mathcal{F}_{t}\)</span> are resolved, upon observing the outcome of <span class="math inline">\(B_{t}\)</span>. Similarly, <span class="math inline">\(\mathbf{E}[|B_{t}|]=0\)</span>. As for the martingale property, note that, by the properties of conditional expectation in proposition (<a href="#prop:properties-of-conditional-expectation" data-reference-type="ref" data-reference="prop:properties-of-conditional-expectation">[prop:properties-of-conditional-expectation]</a>), we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[B_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}[B_{t}|B_{s}]\\
&amp; =\mathbf{E}[B_{t}-B_{s}+B_{s}|B_{s}]\\
&amp; =\mathbf{E}[B_{t}-B_{s}|B_{s}]+\mathbf{E}[B_{s}|B_{s}]\\
&amp; \quad\{\text{Linearity}\}\\
&amp; =\mathbf{E}[B_{t}-B_{s}]+B_{s}\\
&amp; \quad\{\text{Independence}\}\\
&amp; =B_{s}
\end{aligned}\]</span></p>
<p>(ii) <em>Geometric Brownian Motion.</em> Let <span class="math inline">\((B_{t},t\ge0)\)</span> be a standard brownian motion, and <span class="math inline">\(\mathcal{F}_{t}=\sigma(B_{s},s\leq t)\)</span>. A <em>geometric brownian motion</em> is a process <span class="math inline">\((S_{t},t\geq0)\)</span> defined by:</p>
<p><span class="math display">\[\begin{aligned}
S_{t} &amp; =S_{0}\exp\left(\sigma B_{t}+\mu t\right)
\end{aligned}\]</span></p>
<p>for some parameter <span class="math inline">\(\sigma&gt;0\)</span> and <span class="math inline">\(\mu\in\mathbf{R}\)</span>. This is simply the exponential of the Brownian motion with drift. This is not a martingale for most choices of <span class="math inline">\(\mu\)</span>! In fact, one must take</p>
<p><span class="math display">\[\begin{aligned}
\mu &amp; =-\frac{1}{2}\sigma^{2}
\end{aligned}\]</span> for the process to be a martingale for the Brownian filtration. Let’s verify this. Property (1) is obvious since <span class="math inline">\(S_{t}\)</span> is a function of <span class="math inline">\(B_{t}\)</span> for each <span class="math inline">\(t\)</span>. So, it is <span class="math inline">\(\mathcal{F}_{t}\)</span> measurable. Moreover, property (2) is clear: <span class="math inline">\(\mathbf{E}[\exp(\sigma B_{t}+\mu t)]=\mathbf{E}[\exp(\sigma\sqrt{t}Z+\mu t)]=\exp(\mu t+\frac{1}{2}\sigma^{2}t)\)</span>. So, its a finite quantity. As for the martingale property, note that by the properties of conditional expectation, and the MGF of Gaussians, we have for <span class="math inline">\(s\leq t\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[S_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}\left[S_{0}\exp\left(\sigma B_{t}-\frac{1}{2}\sigma^{2}t\right)|\mathcal{F}_{s}\right]\\
&amp; =S_{0}\exp(-\frac{1}{2}\sigma^{2}t)\mathbf{E}[\exp(\sigma(B_{t}-B_{s}+B_{s}))|\mathcal{F}_{s}]\\
&amp; =S_{0}\exp(-\frac{1}{2}\sigma^{2}t)\exp(\sigma B_{s})\mathbf{E}[\exp(\sigma(B_{t}-B_{s}))|\mathcal{F}_{s}]\\
&amp; \quad\{\text{Taking out what is known}\}\\
&amp; =S_{0}\exp\left(\sigma B_{s}-\frac{1}{2}\sigma^{2}t\right)\mathbf{E}\left[\exp\left(\sigma(B_{t}-B_{s})\right)\right]\\
&amp; \quad\{\text{Independence}\}\\
&amp; =S_{0}\exp\left(\sigma B_{s}-\frac{1}{2}\sigma^{2}t+\frac{1}{2}\sigma^{2}(t-s)\right)\\
&amp; =S_{0}\exp(\sigma B_{s}-\frac{1}{2}\sigma^{2}s)\\
&amp; =S_{s}
\end{aligned}\]</span></p>
<p>We will sometimes abuse terminology and refer to the martingale case of geometric brownian motion simply as geometric Brownian Motion when the context is clear.</p>
<p>(iii) <em>The square of the Brownian motion, compensated</em>. It is easy to check <span class="math inline">\((B_{t}^{2},t\geq0)\)</span> is a submartingale by direct computation using increments or by Jensen’s inequality: <span class="math inline">\(\mathbf{E}[B_{t}^{2}|\mathcal{F}_{s}]&gt;(\mathbf{E}[B_{t}|\mathcal{F}_{s}])^{2}=B_{s}^{2}\)</span>, <span class="math inline">\(s&lt;t\)</span>. It is nevertheless possible to compensate to get a martingale:</p>
<p><span class="math display">\[\begin{aligned}
M_{t} &amp; =B_{t}^{2}-t
\end{aligned}\]</span></p>
<p>It is an easy exercise to verify that <span class="math inline">\((M_{t}:t\geq0)\)</span> is a martingale for the Brownian filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}[B_{t}^{2}-t|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[B_{t}^{2}|\mathcal{F}_{s}]-t\\
&amp; =\mathbf{E}[(B_{t}-B_{s}+B_{s})^{2}|\mathcal{F}_{s}]-t\\
&amp; =\mathbf{E}[(B_{t}-B_{s})^{2}|\mathcal{F}_{s}]+2\mathbf{E}[(B_{t}-B_{s})B_{s}|\mathcal{F}_{s}]+\mathbf{E}[B_{s}^{2}|\mathcal{F}_{s}]-t\\
&amp; =\mathbf{E}[(B_{t}-B_{s})^{2}]+2B_{s}\mathbf{E}[(B_{t}-B_{s})|\mathcal{F}_{s}]+B_{s}^{2}-t\\
&amp; =\mathbf{E}[(B_{t}-B_{s})^{2}]+2B_{s}\mathbf{E}[(B_{t}-B_{s})]+B_{s}^{2}-t\\
&amp; \left\{ \begin{array}{c}
\text{\ensuremath{(B_{t}-B_{s})} is independent of \ensuremath{\mathcal{F}_{s}}}\\
\text{Also, \ensuremath{B_{s}} is known at time \ensuremath{s}}
\end{array}\right\} \\
&amp; =(t-s)+2B_{s}\cdot0+B_{s}^{2}-t\\
&amp; =B_{s}^{2}-s\\
&amp; =M_{s}
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>(Other important martingales).</p>
<p>(1) <em>Symmetric random walks.</em> This is an example of a martingale in discrete time. Take <span class="math inline">\((X_{i}:i\in\mathbf{N})\)</span> to be IID random variables with <span class="math inline">\(\mathbf{E}[X_{i}]=0\)</span> and <span class="math inline">\(\mathbf{E}[|X_{i}|]&lt;\infty\)</span>. Take <span class="math inline">\(\mathcal{F}_{n}=\sigma(X_{i},i\leq n)\)</span> and</p>
<p><span class="math display">\[\begin{aligned}
S_{n} &amp; =X_{1}+X_{2}+\ldots+X_{n},\quad S_{0}=0
\end{aligned}\]</span></p>
<p>Firstly, the information learned by observing the outcomes of <span class="math inline">\(X_{1}\)</span>,<span class="math inline">\(\ldots\)</span>,<span class="math inline">\(X_{n}\)</span> is enough to completely determine <span class="math inline">\(S_{n}\)</span>. Hence, <span class="math inline">\(S_{n}\)</span> is <span class="math inline">\(\mathcal{F}_{n}-\)</span>measurable.</p>
<p>Next, <span class="math display">\[\begin{aligned}
|S_{n}| &amp; =\left|\sum_{i=1}^{n}X_{i}\right|\\
&amp; \leq\sum_{i=1}^{n}|X_{i}|
\end{aligned}\]</span></p>
<p>Consequently, by the montonocity of expectations, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[|S_{n}|] &amp; \leq\sum_{i=1}^{n}\mathbf{E}[|X_{i}|]&lt;\infty
\end{aligned}\]</span></p>
<p>The martingale property is also satisfied. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[S_{n+1}|\mathcal{F}_{n}] &amp; =\mathbf{E}[S_{n}+X_{n+1}|\mathcal{F}_{n}]\\
&amp; =\mathbf{E}[S_{n}|\mathcal{F}_{n}]+\mathbf{E}[X_{n+1}|\mathcal{F}_{n}]\\
&amp; =S_{n}+\mathbf{E}[X_{n+1}]\\
&amp; \left\{ \begin{array}{c}
\text{\ensuremath{S_{n}} is \ensuremath{\mathcal{F}_{n}}-measurable}\\
\text{\ensuremath{X_{n+1}} is independent of \ensuremath{\mathcal{F}_{n}}}
\end{array}\right\} \\
&amp; =S_{n}+0\\
&amp; =S_{n}
\end{aligned}\]</span></p>
<p>(2) <em>Compensated Poisson process</em>. Let <span class="math inline">\((N_{t}:t\geq0)\)</span> be a Poisson process with rate <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\mathcal{F}_{t}=\sigma(N_{s},s\leq t)\)</span>. Then, <span class="math inline">\(N_{t}\)</span> is a submartingale for its natural filtration. Again, properties (1) and (2) are easily checked. <span class="math inline">\(N_{t}\)</span> is <span class="math inline">\(\mathcal{F}_{t}\)</span> measurable. Moreover, <span class="math inline">\(\mathbf{E}[|N_{t}|]=\mathbf{E}[N_{t}]=\frac{1}{\lambda t}&lt;\infty\)</span>. The submartingale property follows by the independence of increments : for <span class="math inline">\(s\leq t\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[N_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}[N_{t}-N_{s}+N_{s}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[N_{t}-N_{s}|\mathcal{F}_{s}]+\mathbf{E}[N_{s}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[N_{t}-N_{s}]+N_{s}\\
&amp; =\lambda(t-s)+N_{s}\\
&amp; \left\{ \because\mathbf{E}[N_{t}]=\lambda t\right\}
\end{aligned}\]</span></p>
<p>More importantly, we get a martingale by slightly modifying the process. Indeed, if we subtract <span class="math inline">\(\lambda t\)</span>, we have that the process :</p>
<p><span class="math display">\[\begin{aligned}
M_{t} &amp; =N_{t}-\lambda t
\end{aligned}\]</span></p>
<p>is a martingale. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}|\mathcal{F}_{s}] &amp; =\mathbf{E}[N_{t}-\lambda t|\mathcal{F}_{s}]\\
&amp; =\lambda t-\lambda s+N_{s}-\lambda t\\
&amp; =N_{s}-\lambda s\\
&amp; =M_{s}
\end{aligned}\]</span></p>
<p>This is called the <em>compensated Poisson process</em>. Let us simulate <span class="math inline">\(10\)</span> paths of the compensated poisson process on <span class="math inline">\([0,10]\)</span>.</p>
</div>
<pre data-caption="Generating 10 paths of a compensated Poisson process"><code>import numpy as np
import matplotlib.pyplot as plt

# Generates a sample path of a compensated poisson process 
# with rate : `lambda_` per unit time
# on the interval [0,T], and subintervals of size `stepSize`.

def generateCompensatedPoissonPath(lambda_,T,stepSize):
    N = int(T/stepSize)   

    poissonParam = lambda_ * stepSize        

    x = np.random.poisson(lam=poissonParam,size=N)  
    x = np.concatenate([[0.0], x])
    N_t = np.cumsum(x)  
    t = np.linspace(start=0.0,stop=10.0,num=1001)

    M_t = np.subtract(N_t,lambda_ * t)  
    return M_t


t = np.linspace(0,10,1001)
plt.grid(True)

plt.xlabel(r'Time $t$')
plt.ylabel(r'Compensated poisson process $M(t)$')
plt.grid(True)
plt.title(r'$10$ paths of the compensated Poisson process on $[0,10]$')

for i in range(10):
    # Generate a poisson path with rate 1 /sec = 0.01 /millisec
    n_t = generateCompensatedPoissonPath(lambda_=1.0, T=10, stepSize=0.01)
    plt.plot(t, n_t)


plt.show()
plt.close()</code></pre>
<p>We saw in the two examples, that, even though a process is not itself a martingale, we can sometimes <em>compensate</em> to obtain a martingale! Ito Calculus will greatly extend this perspective. We will have systematic rules that show when a function of Brownian motion is a martingale and if not, how to modify it to get one.</p>
<p>For now, we observe that a convex function of a martingale is always a submartingale by Jensen’s inequality.</p>
<div class="cor">
<p><span id="corollary:the-convex-function-of-martingale-is-a-submartingale" data-label="corollary:the-convex-function-of-martingale-is-a-submartingale"></span>If <span class="math inline">\(c\)</span> is a convex function on <span class="math inline">\(\mathbf{R}\)</span> and <span class="math inline">\((M_{t}:t\geq0)\)</span> is a martingale for <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>, then the process <span class="math inline">\((c(M_{t}):t\geq0)\)</span> is a submartingale for the same filtration, granted that <span class="math inline">\(\mathbf{E}[|c(M_{t})|]&lt;\infty\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> The fact that <span class="math inline">\(c(M_{t})\)</span> is adapted to the filtration is clear since it is an explicit function of <span class="math inline">\(M_{t}\)</span>. The integrability is by assumption. The submartingale property is checked as follows:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[c(M_{t})|\mathcal{F}_{s}] &amp; \geq c(\mathbf{E}[M_{t}|\mathcal{F}_{s}])=c(M_{s})
\end{aligned}\]</span> ◻</p>
</div>
<div class="rem*">
<p>(The Doob-Meyer Decomposition Theorem). Let <span class="math inline">\((X_{n}:n\in\mathbf{N})\)</span> be a submartingale with respect to a filtration <span class="math inline">\((\mathcal{F}_{n}:n\in\mathbf{N})\)</span>. Define a sequence of random variables <span class="math inline">\((A_{n}:n\in\mathbf{N})\)</span> by <span class="math inline">\(A_{0}=0\)</span> and</p>
<p><span class="math display">\[\begin{aligned}
A_{n} &amp; =\sum_{i=1}^{n}(\mathbf{E}[X_{i}|\mathcal{F}_{i-1}]-X_{i-1}),\quad n\geq1
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(A_{n}\)</span> is <span class="math inline">\(\mathcal{F}_{n-1}\)</span>-measurable. Moreover, since <span class="math inline">\((X_{n}:n\in\mathbf{N})\)</span> is a submartingale, we have <span class="math inline">\(\mathbf{E}[X_{i}|\mathcal{F}_{i-1}]-X_{i-1}\geq0\)</span> almost surely. Hence, <span class="math inline">\((A_{n}:n\in\mathbf{N})\)</span> is an increasing sequence almost surely. Let <span class="math inline">\(M_{n}=X_{n}-A_{n}\)</span>.</p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{n}|\mathcal{F}_{n-1}] &amp; =\mathbf{E}[X_{n}-A_{n}|\mathcal{F}_{n-1}]\\
&amp; =\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-\mathbf{E}[A_{n}|\mathcal{F}_{n-1}]\\
&amp; =\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-\mathbf{E}\left[\left.\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-X_{n-1}+A_{n-1}\right|\mathcal{F}_{n-1}\right]\\
&amp; =\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]+\mathbf{E}[X_{n-1}|\mathcal{F}_{n-1}]-\mathbf{E}[A_{n-1}|\mathcal{F}_{n-1}]\\
&amp; =\cancel{\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]}-\cancel{\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]}+X_{n-1}-A_{n-1}\\
&amp; =M_{n-1}
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\((M_{n}:n\in\mathbf{N})\)</span> is a martingale. Thus, we have obtained the Doob decomposition:</p>
<p><span class="math display">\[\begin{aligned}
X_{n} &amp; =M_{n}+A_{n}\label{eq:doob-decomposition}
\end{aligned}\]</span></p>
<p>This decomposition of a submartingale as a sum of a martingale and an adapted increasing sequence is unique, if we require that <span class="math inline">\(A_{0}=0\)</span> and that <span class="math inline">\(A_{n}\)</span> is <span class="math inline">\(\mathcal{F}_{n-1}\)</span>-measurable.</p>
<p>For the continuous-time case, the situation is much more complicated. The analogue of equation (<a href="#eq:doob-decomposition" data-reference-type="ref" data-reference="eq:doob-decomposition">[eq:doob-decomposition]</a>) is called the <em>Doob-Meyer decomposition</em>. We briefly describe this decomposition and avoid the technical details. All stochastic processes <span class="math inline">\(X(t)\)</span> are assumed to be right-continuous with left-hand limits <span class="math inline">\(X(t-)\)</span>.</p>
<p>Let <span class="math inline">\(X(t)\)</span>, <span class="math inline">\(a\leq t\leq b\)</span> be a submartingale with respect to a right-continuous filtration <span class="math inline">\((\mathcal{F}_{t}:a\leq t\leq b)\)</span>. If <span class="math inline">\(X(t)\)</span> satisfies certain conditions, then it can be uniquely decomposed as:</p>
<p><span class="math display">\[\begin{aligned}
X(t) &amp; =M(t)+C(t),\quad a\leq t\leq b
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(M(t)\)</span>, <span class="math inline">\(a\leq t\leq b\)</span> is a martingale with respect to <span class="math inline">\((\mathcal{F}_{t};a\leq t\leq b)\)</span>, <span class="math inline">\(C(t)\)</span> is right-continuous and increasing almost surely with <span class="math inline">\(\mathbf{E}[C(t)]&lt;\infty\)</span>.</p>
</div>
<div class="example">
<p>(Square of a Poisson Process). Let <span class="math inline">\((N_{t}:t\geq0)\)</span> be a Poisson process with rate <span class="math inline">\(\lambda\)</span>. We consider the compensated process <span class="math inline">\(M_{t}=N_{t}-\lambda t\)</span>. By (<a href="#corollary:the-convex-function-of-martingale-is-a-submartingale" data-reference-type="ref" data-reference="corollary:the-convex-function-of-martingale-is-a-submartingale">[corollary:the-convex-function-of-martingale-is-a-submartingale]</a>), the process <span class="math inline">\((M_{t}^{2}:t\geq0)\)</span> is a submartingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> of the Poisson process. How should we compensated <span class="math inline">\(M_{t}^{2}\)</span> to get a martingale? A direct computation using the properties of conditional expectation yields:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}^{2}|\mathcal{F}_{s}] &amp; =\mathbf{E}[(M_{t}-M_{s}+M_{s})^{2}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[(M_{t}-M_{s})^{2}+2(M_{t}-M_{s})M_{s}+M_{s}^{2}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[(M_{t}-M_{s})^{2}|\mathcal{F}_{s}]+2\mathbf{E}[(M_{t}-M_{s})M_{s}|\mathcal{F}_{s}]+\mathbf{E}[M_{s}^{2}|\mathcal{F}_{s}]\\
&amp; =\mathbf{E}[(M_{t}-M_{s})^{2}]+2M_{s}\underbrace{\mathbf{E}[M_{t}-M_{s}]}_{\text{equals \ensuremath{0}}}+M_{s}^{2}\\
&amp; =\mathbf{E}[(M_{t}-M_{s})^{2}]+M_{s}^{2}
\end{aligned}\]</span></p>
<p>Now, if <span class="math inline">\(X\sim\text{Poisson\ensuremath{(\lambda t)}}\)</span>, then <span class="math inline">\(\mathbf{E}[X]=\lambda t\)</span> and <span class="math inline">\(\mathbf{E}\ensuremath{[X^{2}]}=\lambda t(\lambda t+1)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[(M_{t}-M_{s})^{2}] &amp; =\mathbf{E}\left[\left\{ (N_{t}-N_{s})-\lambda(t-s)\right\} ^{2}\right]\\
&amp; =\mathbf{E}\left[(N_{t}-N_{s})^{2}\right]-2\lambda(t-s)\mathbf{E}\left[(N_{t}-N_{s})\right]+\lambda^{2}(t-s)^{2}\\
&amp; =\lambda^{2}(t-s)^{2}+\lambda(t-s)-2\lambda(t-s)\cdot\lambda(t-s)+\lambda^{2}(t-s)^{2}\\
&amp; =\lambda(t-s)
\end{aligned}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t}^{2}-\lambda t|\mathcal{F}_{s}] &amp; =M_{s}^{2}-\lambda s
\end{aligned}\]</span></p>
<p>We conclude that the process <span class="math inline">\((M_{t}^{2}-\lambda t:t\geq0)\)</span> is a martingale. The Doob-Meyer decomposition of the submartingale <span class="math inline">\(M_{t}^{2}\)</span> is then:</p>
<p><span class="math display">\[\begin{aligned}
M_{t}^{2} &amp; =(M_{t}^{2}-\lambda t)+\lambda t
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>Consider a Brownian motion <span class="math inline">\(B(t)\)</span>. The quadratic variation of the process <span class="math inline">\((B(t):t\geq0)\)</span> over the interval <span class="math inline">\([0,t]\)</span> is given by <span class="math inline">\([B]_{t}=t\)</span>. On the other hand, we saw, that the square of Brownian motion compensated, <span class="math inline">\((B_{t}^{2}-t:t\geq0)\)</span> is a martingale. Hence, the Doob-Meyer decomposition of <span class="math inline">\(B(t)^{2}\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
B(t)^{2} &amp; =(B(t)^{2}-t)+t
\end{aligned}\]</span></p>
</div>
</section>
<section id="computations-with-martingales." class="level2">
<h2 class="anchored" data-anchor-id="computations-with-martingales.">Computations with Martingales.</h2>
<p>Martingales are not only conceptually interesting, they are also formidable tools to compute probabilities and expectations of processes. For example, in this section, we will solve the <em>gambler’s ruin</em> problem for Brownian motion. For convenience, we introduce the notion of <em>stopping time</em> before doing so.</p>
<div class="defn">
<p>A random variable <span class="math inline">\(\tau:\Omega\to\mathbf{N}\cup\{+\infty\}\)</span> is said to be a <em>stopping time</em> for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> if and only if:</p>
<p><span class="math display">\[\begin{aligned}
\{\omega:\tau(\omega)\leq t\} &amp; \in\mathcal{F}_{t},\quad\forall t\geq0
\end{aligned}\]</span> Note that since <span class="math inline">\(\mathcal{F}_{t}\)</span> is a sigma-field, if <span class="math inline">\(\tau\)</span> is a stopping time, then we must also have that <span class="math inline">\(\{\omega:\tau(\omega)&gt;t\}\in\mathcal{F}_{t}\)</span>.</p>
<p>In other words, <span class="math inline">\(\tau\)</span> is a stopping time, if we can decide if the events <span class="math inline">\(\{\tau\leq t\}\)</span> occurred or not based on the information available at time <span class="math inline">\(t\)</span>.</p>
<p>The term <em>stopping time</em> comes from gambling: a gambler can decide to stop playing at a random time (depending for example on previous gains or losses), but when he or she decides to stop, his/her decision is based solely upon the knowledge of what happened before, and does not depend on future outcomes. In other words, the stopping policy/strategy can only depend on past outcomes. Otherwise, it would mean that he/she has a crystall ball.</p>
</div>
<div class="example">
<p>(Examples of stopping times).</p>
<p>(i) <em>First passage time</em>. This is the first time when a process reaches a certain value. To be precise, let <span class="math inline">\(X=(X_{t}:t\geq0)\)</span> be a process and <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> be its natural filtration. For <span class="math inline">\(a&gt;0\)</span>, we define the first passage time at <span class="math inline">\(a\)</span> to be:</p>
<p><span class="math display">\[\begin{aligned}
\tau(\omega) &amp; =\inf\{s\geq0:X_{s}(\omega)\geq a\}
\end{aligned}\]</span></p>
<p>If the path <span class="math inline">\(\omega\)</span> never reaches <span class="math inline">\(a\)</span>, we set <span class="math inline">\(\tau(\omega)=\infty\)</span>. Now, for <span class="math inline">\(t\)</span> fixed and for a given path <span class="math inline">\(X(\omega)\)</span>, it is possible to know if <span class="math inline">\(\{\tau(\omega)\leq t\}\)</span> (the path has reached <span class="math inline">\(a\)</span> before time <span class="math inline">\(t\)</span>) or <span class="math inline">\(\{\tau(\omega)&gt;t\}\)</span> (the path has not reached <span class="math inline">\(a\)</span> before time <span class="math inline">\(t\)</span>) with the information available at time <span class="math inline">\(t\)</span>, since we are looking at the first time the process reaches <span class="math inline">\(a\)</span>. Hence, we conclude that <span class="math inline">\(\tau\)</span> is a stopping time.</p>
<p>(ii) <em>Hitting time</em>. More generally, we can consider the first time (if ever) that the path of a process <span class="math inline">\((X_{t}:t\geq0)\)</span> enters or hits a subset <span class="math inline">\(B\)</span> of <span class="math inline">\(\mathbf{R}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\tau(\omega) &amp; =\min\{s\geq0:X_{s}(\omega)\in B\}
\end{aligned}\]</span></p>
<p>The first passage time is the particular case in which <span class="math inline">\(B=[a,\infty)\)</span>.</p>
<p>(iii) <em>Minimum of two stopping times.</em> If <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\tau'\)</span> are two stopping times for the same filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>, then so is the minimum <span class="math inline">\(\tau\land\tau'\)</span> between the two, where</p>
<p><span class="math display">\[\begin{aligned}
(\tau\land\tau')(\omega) &amp; =\min\{\tau(\omega),\tau'(\omega)\}
\end{aligned}\]</span></p>
<p>This is because for any <span class="math inline">\(t\geq0\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\{\omega: &amp; (\tau\land\tau')(\omega)\leq t\}=\{\omega:\tau(\omega)\leq t\}\cup\{\omega:\tau'(\omega)\leq t\}
\end{aligned}\]</span></p>
<p>Since the right hand side is the union of two events in <span class="math inline">\(\mathcal{F}_{t}\)</span>, it must also be in <span class="math inline">\(\mathcal{F}_{t}\)</span> by the properties of a sigma-field. We conclude that <span class="math inline">\(\tau\land\tau'\)</span> is a stopping time. Is it also the case that the maximum <span class="math inline">\(\tau\lor\tau'\)</span> is a stopping time?</p>
<p>For any fixed <span class="math inline">\(t\geq0\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\{\omega:(\tau\lor\tau')(\omega)\leq t\} &amp; =\{\omega:\tau(\omega)\leq t\}\cap\{\omega:\tau'(\omega)\leq t\}
\end{aligned}\]</span></p>
<p>Since the right hand side is the intersection of two events in <span class="math inline">\(\mathcal{F}_{t}\)</span>, it must also be in <span class="math inline">\(\mathcal{F}_{t}\)</span> by the properties of a sigma-field. We conclude that <span class="math inline">\(\tau\lor\tau'\)</span> is a stopping time.</p>
</div>
<div class="example">
<p>(Last passage time is not a stopping time). What if we look at the last time the process reaches <span class="math inline">\(a\)</span>, that is:</p>
<p><span class="math display">\[\begin{aligned}
\rho(\omega) &amp; =\sup\{t\geq0:X_{t}(\omega)\geq a\}
\end{aligned}\]</span></p>
<p>This is a well-defined random variable, but it is not a stopping time. Based on the information available at time <span class="math inline">\(t\)</span>, we are not able to decide whether or not <span class="math inline">\(\{\rho(\omega)\leq t\}\)</span> occurred or not, as the path can always reach <span class="math inline">\(a\)</span> one more time after <span class="math inline">\(t\)</span>.</p>
</div>
<p>It turns out that a martingale that is stopped when the stopping time is attained remains a martingale.</p>
<div class="prop">
<p>(Stopped Martingale). If <span class="math inline">\((M_{t}:t\geq0)\)</span> is a continuous martingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> and <span class="math inline">\(\tau\)</span> is a stopping time for the same filtration, then the stopped process defined by <span class="math display">\[\begin{aligned}
M_{t\land\tau} &amp; =\begin{cases}
M_{t} &amp; t\leq\tau\\
M_{\tau} &amp; t&gt;\tau
\end{cases}
\end{aligned}\]</span></p>
<p>is also a continuous martingale for the same filtration.</p>
</div>
<div class="thm">
<p>[]{#th:doob’s-optional-sampling-theorem label=“th:doob’s-optional-sampling-theorem”}(Doob’s Optional sampling theorem). If <span class="math inline">\((M_{t}:t\geq0)\)</span> is a continuous martingale for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span> and <span class="math inline">\(\tau\)</span> is a stopping time such that <span class="math inline">\(\tau&lt;\infty\)</span> and the stopped process <span class="math inline">\((M_{t\land\tau}:t\geq0)\)</span> is bounded, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{\tau}] &amp; =M_{0}
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Since <span class="math inline">\((M_{\tau\land t}:t\geq0)\)</span> is a martingale, we always have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{\tau\land t}] &amp; =M_{0}
\end{aligned}\]</span></p>
<p>Now, since <span class="math inline">\(\tau(\omega)&lt;\infty\)</span>, we must</p>
<p>have that <span class="math inline">\(\lim_{t\to\infty}M_{\tau\land t}=M_{\tau}\)</span> almost surely. In particular, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{\tau}] &amp; =\mathbf{E}\left[\lim_{t\to\infty}M_{\tau\land t}\right]=\lim_{t\to\infty}\mathbf{E}[M_{\tau\land t}]=\lim_{t\to\infty}M_{0}
\end{aligned}\]</span></p>
<p>where we passed to the limit, using the dominated convergence theorem (<a href="#th:dominated-convergence-theorem" data-reference-type="ref" data-reference="th:dominated-convergence-theorem">[th:dominated-convergence-theorem]</a>). ◻</p>
</div>
<div class="example">
<p><span id="example:probability-of-hitting-times" data-label="example:probability-of-hitting-times"></span>(Gambler’s ruin with Brownian motion). The <em>gambler’s ruin problem</em> is known in different forms. Roughly speaking, it refers to the problem of computing the probability of a gambler making a series of bets reaching a certain amount before going broke. In terms of Brownian motion (and stochastic processes in general), it translates to the following questions: Let <span class="math inline">\((B_{t}:t\geq0)\)</span> be a standard brownian motion starting at <span class="math inline">\(B_{0}=0\)</span> and <span class="math inline">\(a,b&gt;0\)</span>.</p>
<p>(1) What is the probability that a Brownian path reaches <span class="math inline">\(a\)</span> before <span class="math inline">\(-b\)</span>?</p>
<p>(2) What is the expected waiting time for the path to reach <span class="math inline">\(a\)</span> or <span class="math inline">\(-b\)</span>?</p>
<p>For the first question, it is a simple computation using stopping time and martingale properties. Define the hitting time:</p>
<p><span class="math display">\[\begin{aligned}
\tau(\omega) &amp; =\inf\{t\geq0:B_{t}(\omega)\geq a\text{ or }B_{t}(\omega)\leq-b\}
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(\tau\)</span> is the minimum between the first passage time at <span class="math inline">\(a\)</span> and the one at <span class="math inline">\(-b\)</span>.</p>
<p>We first show that <span class="math inline">\(\tau&lt;\infty\)</span> almost surely. In other words, all Brownian paths reach <span class="math inline">\(a\)</span> or <span class="math inline">\(-b\)</span> eventually. To see this, consider the event <span class="math inline">\(E_{n}\)</span> that the <span class="math inline">\(n\)</span>-th increment exceeds <span class="math inline">\(a+b\)</span></p>
<p><span class="math display">\[\begin{aligned}
E_{n} &amp; :=\left\{ |B_{n}-B_{n-1}|&gt;a+b\right\}
\end{aligned}\]</span></p>
<p>Note that, if <span class="math inline">\(E_{n}\)</span> occurs, then we must have that the Brownian motion path exits the interval <span class="math inline">\([-b,a].\)</span> Moreover, we have <span class="math inline">\(\mathbb{P}(E_{n})=\mathbb{P}(E_{1})\)</span> for all <span class="math inline">\(n\)</span>. Call this probability <span class="math inline">\(p\)</span>.</p>
<p>Since the events <span class="math inline">\(E_{n}\)</span> are independent, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}) &amp; =(1-p)^{n}
\end{aligned}\]</span></p>
<p>As <span class="math inline">\(n\to\infty\)</span> we have:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\mathbb{P}(E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}) &amp; =0
\end{aligned}\]</span></p>
<p>The sequence of events <span class="math inline">\((F_{n})\)</span> where <span class="math inline">\(F_{n}=E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}\)</span> is a decreasing sequence of events. By the continuity of probability measure lemma (<a href="#th:continuity-property-of-lebesgue-measure" data-reference-type="ref" data-reference="th:continuity-property-of-lebesgue-measure">[th:continuity-property-of-lebesgue-measure]</a>), we conclude that:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\mathbb{P}\left(F_{n}\right) &amp; =\mathbb{P}\left(\bigcap_{n=1}^{\infty}F_{n}\right)=0
\end{aligned}\]</span></p>
<p>Therefore, it must be the case <span class="math inline">\(\mathbb{P}(\cup_{n=1}^{\infty}E_{n})=1\)</span>. So, <span class="math inline">\(E_{n}\)</span> must occur for some <span class="math inline">\(n\)</span>, so all brownian motion paths reach <span class="math inline">\(a\)</span> or <span class="math inline">\(-b\)</span> almost surely.</p>
<p>Since <span class="math inline">\(\tau&lt;\infty\)</span> with probability one, the random variable <span class="math inline">\(B_{\tau}\)</span> is well-defined : <span class="math inline">\(B_{\tau}(\omega)=B_{t}(\omega)\)</span> if <span class="math inline">\(\tau(\omega)=t\)</span>. It can only take two values: <span class="math inline">\(a\)</span> or <span class="math inline">\(-b\)</span>. Question (1) above translates into computing <span class="math inline">\(\mathbb{P}(B_{\tau}=a)\)</span>. On one hand, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[B_{\tau}] &amp; =a\mathbb{P}(B_{\tau}=a)+(-b)(1-\mathbb{P}(B_{\tau}=a))
\end{aligned}\]</span></p>
<p>On the other hand, by corollary (<a href="#th:doob's-optional-sampling-theorem" data-reference-type="ref" data-reference="th:doob's-optional-sampling-theorem">[th:doob's-optional-sampling-theorem]</a>), we have <span class="math inline">\(\mathbf{E}[B_{\tau}]=\mathbf{E}[B_{0}]=0\)</span>. (Note that the stopped process <span class="math inline">\((B_{t\land\tau}:t\geq0)\)</span> is bounded above by <span class="math inline">\(a\)</span> and by <span class="math inline">\(-b\)</span> below). Putting these two observations together, we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(B_{\tau}=a) &amp; =\frac{b}{a+b}
\end{aligned}\]</span></p>
<p>A very simple and elegant answer!</p>
<p>We will revisit this problem again and again. In particular, we will answer the question above for Brownian motion with a drift at length further ahead.</p>
</div>
<div class="example">
<p><span id="ex:expected-waiting-times" data-label="ex:expected-waiting-times"></span>(Expected Waiting Time). Let <span class="math inline">\(\tau\)</span> be as in the last example. We now answer question (2) of the gambler’s ruin problem:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[\tau] &amp; =ab
\end{aligned}\]</span></p>
<p>Note that the expected waiting time is consistent with the rough heuristic that Brownian motion travels a distance <span class="math inline">\(\sqrt{t}\)</span> by time <span class="math inline">\(t\)</span>. We now use the martingale <span class="math inline">\(M_{t}=B_{t}^{2}-t\)</span>. On the one hand, if we apply optional stopping in corollary (<a href="#th:doob's-optional-sampling-theorem" data-reference-type="ref" data-reference="th:doob's-optional-sampling-theorem">[th:doob's-optional-sampling-theorem]</a>), we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{\tau}] &amp; =M_{0}=0
\end{aligned}\]</span></p>
<p>Moreover, we know the distribution of <span class="math inline">\(B_{\tau}\)</span>, thanks to the probability calculated in the last example. We can therefore compute <span class="math inline">\(\mathbf{E}[M_{\tau}]\)</span> directly:</p>
<p><span class="math display">\[\begin{aligned}
0 &amp; =\mathbf{E}[M_{\tau}]\\
&amp; =\mathbf{E}[B_{\tau}^{2}-\tau]\\
&amp; =\mathbf{E}[B_{\tau}^{2}]-\mathbf{E}[\tau]\\
&amp; =a^{2}\cdot\frac{b}{a+b}+b^{2}\cdot\frac{a}{a+b}-\mathbf{E}[\tau]\\
\mathbf{E}[\tau] &amp; =\frac{a^{2}b+b^{2}a}{a+b}\\
&amp; =\frac{ab\cancel{(a+b)}}{\cancel{(a+b)}}=ab
\end{aligned}\]</span></p>
<p>Why can we apply optional stopping here? The random variable <span class="math inline">\(\tau\)</span> is finite with probability <span class="math inline">\(1\)</span> as before. However, the stopped martingale is not necessarily bounded as before: <span class="math inline">\(B_{\tau\land t}\)</span> is bounded but <span class="math inline">\(\tau\)</span> is not. However, the conclusion of optional stopping still holds. Indeed, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{t\land\tau}] &amp; =\mathbf{E}[B_{t\land\tau}^{2}]-\mathbf{E}[t\land\tau]
\end{aligned}\]</span></p>
<p>By the bounded convergence theorem, we get <span class="math inline">\(\lim_{t\to\infty}\mathbf{E}[B_{t\land\tau}^{2}]=\mathbf{E}[\lim_{t\to\infty}B_{t\land\tau}^{2}]=\mathbf{E}[B_{\tau}^{2}]\)</span>. Since <span class="math inline">\(\tau\land t\)</span> is a non-decreasing sequence and as <span class="math inline">\(t\to\infty\)</span>, <span class="math inline">\(t\land\tau\to\tau\)</span> almost surely, as <span class="math inline">\(\tau&lt;\infty\)</span>, by the monotone convergence theorem, <span class="math inline">\(\lim_{t\to\infty}\mathbf{E}[t\land\tau]=\mathbf{E}[\tau]\)</span>.</p>
</div>
<div class="example">
<p>(First passage time of Brownian Motion.) We can use the previous two examples to get some very interesting information on the first passage time:</p>
<p><span class="math display">\[\begin{aligned}
\tau_{a} &amp; =\inf\{t\geq0:B_{t}\geq a\}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(\tau=\tau_{a}\land\tau_{-b}\)</span> be as in the previous examples with <span class="math inline">\(\tau_{-b}=\inf\{t\geq0:B_{t}\leq-b\}\)</span>. Note that <span class="math inline">\((\tau_{-b},b\in\mathbf{R}_{+})\)</span> is a sequence of random variables that is increasing in <span class="math inline">\(b\)</span>. A brownian motion path must cross through <span class="math inline">\(-1\)</span> before it hits <span class="math inline">\(-2\)</span> for the first time and in general <span class="math inline">\(\tau_{-n}(\omega)\leq\tau_{-(n+1)}(\omega)\)</span>. Moreover, we have <span class="math inline">\(\tau_{-b}\to\infty\)</span> almost surely as <span class="math inline">\(b\to\infty\)</span>. That’s because, <span class="math inline">\(\mathbb{P}\{\tau&lt;\infty\}=1\)</span>. Moreover, the event <span class="math inline">\(\{B_{\tau}=a\}\)</span> is the same as <span class="math inline">\(\{\tau_{a}&lt;\tau_{-b}\}\)</span>. Now, the events <span class="math inline">\(\{\tau_{a}&lt;\tau_{-b}\}\)</span> are increasing in <span class="math inline">\(b\)</span>, since if a path reaches <span class="math inline">\(a\)</span> before <span class="math inline">\(-b\)</span>, it will do so as well for a more negative value of <span class="math inline">\(-b\)</span>. On one hand, this means by the continuity of probability measure lemma (<a href="#th:continuity-property-of-lebesgue-measure" data-reference-type="ref" data-reference="th:continuity-property-of-lebesgue-measure">[th:continuity-property-of-lebesgue-measure]</a>) that:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{b\to\infty}\mathbb{P}\left\{ \tau_{a}&lt;\tau_{-b}\right\}  &amp; =\mathbb{P}\{\lim_{b\to\infty}\tau_{a}&lt;\tau_{-b}\}\\
&amp; =\mathbb{P}\{\tau_{a}&lt;\infty\}
\end{aligned}\]</span></p>
<p>On the other hand, we have by example (<a href="#example:probability-of-hitting-times" data-reference-type="ref" data-reference="example:probability-of-hitting-times">[example:probability-of-hitting-times]</a>)</p>
<p><span class="math display">\[\begin{aligned}
\lim_{b\to\infty}\mathbb{P}\left\{ \tau_{a}&lt;\tau_{-b}\right\}  &amp; =\lim_{b\to\infty}\mathbb{P}\{B_{\tau}=a\}\\
&amp; =\lim_{b\to\infty}\frac{b}{b+a}\\
&amp; =1
\end{aligned}\]</span></p>
<p>We just showed that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left\{ \tau_{a}&lt;\infty\right\}  &amp; =1\label{eq:first-passage-time-to-a-is-finite-almost-surely}
\end{aligned}\]</span></p>
<p>In other words, every Brownian path will reach <span class="math inline">\(a\)</span>, no matter how large <span class="math inline">\(a\)</span> is!</p>
<p>How long will it take to reach <span class="math inline">\(a\)</span> on average? Well, we know from example (<a href="#ex:expected-waiting-times" data-reference-type="ref" data-reference="ex:expected-waiting-times">[ex:expected-waiting-times]</a>) that <span class="math inline">\(\mathbf{E}[\tau_{a}\land\tau_{-b}]=ab\)</span>. On one hand this means,</p>
<p><span class="math display">\[\begin{aligned}
\lim_{b\to\infty}\mathbf{E}[\tau_{a}\land\tau_{-b}] &amp; =\lim_{b\to\infty}ab=\infty
\end{aligned}\]</span></p>
<p>On the other hand, since the random variables <span class="math inline">\(\tau_{-b}\)</span> are increasing,</p>
<p><span class="math display">\[\begin{aligned}
\lim_{b\to\infty}\mathbf{E}[\tau_{a}\land\tau_{-b}] &amp; =\mathbf{E}\left[\lim_{b\to\infty}\tau_{a}\land\tau_{-b}\right]=\mathbf{E}[\tau_{a}]
\end{aligned}\]</span></p>
<p>by the monotone convergence theorem (<a href="#th:monotone-convergence-theorem" data-reference-type="ref" data-reference="th:monotone-convergence-theorem">[th:monotone-convergence-theorem]</a>). We just proved that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[\tau_{a}] &amp; =\infty
\end{aligned}\]</span></p>
<p>In other words, any Brownian motion path will reach <span class="math inline">\(a\)</span>, but the expected waiting time for this to occur is infinite, no matter, how small <span class="math inline">\(a\)</span> is! What is happening here? No matter, how small <span class="math inline">\(a\)</span> is, there is always paths that reach very large negative values before hitting <span class="math inline">\(a\)</span>. These paths might be unlikely. However, the first passage time for these paths is so large that they affect the value of the expectation substantially. In other words, <span class="math inline">\(\tau_{a}\)</span> is a <em>heavy-tailed random variable</em>. We look at the distribution of <span class="math inline">\(\tau_{a}\)</span> in more detail in the next section.</p>
</div>
<div class="example">
<p>(When option stopping fails). Consider <span class="math inline">\(\tau_{a}\)</span>, the first passage time at <span class="math inline">\(a&gt;0\)</span>. The random variable <span class="math inline">\(B_{\tau_{a}}\)</span> is well-defined since <span class="math inline">\(\tau_{a}&lt;\infty\)</span>. In fact, we have <span class="math inline">\(B_{\tau_{a}}=a\)</span> with probability one. Therefore, the following must hold:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[B_{\tau_{a}}] &amp; =a\neq B_{0}
\end{aligned}\]</span></p>
<p>Optional stopping theorem corollary (<a href="#th:doob's-optional-sampling-theorem" data-reference-type="ref" data-reference="th:doob's-optional-sampling-theorem">[th:doob's-optional-sampling-theorem]</a>) does not apply here, since the stopped process <span class="math inline">\((B_{t\land\tau_{a}}:t\geq0)\)</span> is not bounded. <span class="math inline">\(B_{t\land\tau_{a}}\)</span> can become infinitely negative before hitting <span class="math inline">\(a\)</span>.</p>
</div>
</section>
<section id="reflection-principle-for-brownian-motion." class="level2">
<h2 class="anchored" data-anchor-id="reflection-principle-for-brownian-motion.">Reflection principle for Brownian motion.</h2>
<div class="prop">
<p><span id="prop:bacheliers-formula" data-label="prop:bacheliers-formula"></span>(Bachelier’s formula). Let <span class="math inline">\((B_{t}:t\leq T)\)</span> be a standard brownian motion on <span class="math inline">\([0,T].\)</span> Then, the CDF of the random variable <span class="math inline">\(\sup_{0\leq t\leq T}B_{t}\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\sup_{0\leq t\leq T}B_{t}\leq a\right) &amp; =\mathbb{P}\left(|B_{T}|\leq a\right)
\end{aligned}\]</span></p>
<p>In particular, its PDF is:</p>
<p><span class="math display">\[\begin{aligned}
f_{\max}(a) &amp; =\frac{2}{\sqrt{2\pi T}}e^{-\frac{a^{2}}{2T}}
\end{aligned}\]</span></p>
</div>
<div class="rem*">
<p>We can verify these results empirically. Note that the paths of the random variables <span class="math inline">\(\max_{0\leq s\leq t}B_{s}\)</span> and <span class="math inline">\(|B_{t}|\)</span> are very different as <span class="math inline">\(t\)</span> varies for a given <span class="math inline">\(\omega\)</span>. One is increasing and the other is not. The equality holds in distribution for a fixed <span class="math inline">\(t\)</span>. As a bonus corollary, we get the distribution of the first passage time at <span class="math inline">\(a\)</span>.</p>
</div>
<div class="cor">
<p>Let <span class="math inline">\(a\geq0\)</span> and <span class="math inline">\(\tau_{a}=\inf\{t\geq0:B_{t}\geq a\}\)</span>. Then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\tau_{a}\leq T\right) &amp; =\mathbb{P}\left(\max_{0\leq t\leq T}B_{t}\geq a\right)=\int_{a}^{\infty}\frac{2}{\sqrt{2\pi T}}e^{-\frac{x^{2}}{2T}}dx
\end{aligned}\]</span></p>
<p>In particular, the random variable <span class="math inline">\(\tau_{a}\)</span> has the PDF:</p>
<p><span class="math display">\[\begin{aligned}
f_{\tau_{a}}(t) &amp; =\frac{a}{\sqrt{2\pi}}\frac{e^{-\frac{a^{2}}{2t}}}{t^{3/2}},\quad t&gt;0
\end{aligned}\]</span></p>
<p>This implies that it is heavy-tailed with <span class="math inline">\(\mathbf{E}[\tau_{a}]=\infty\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> The maximum on <span class="math inline">\([0,T]\)</span> is larger than or equal to <span class="math inline">\(a\)</span> if and only if <span class="math inline">\(\tau_{a}\leq T\)</span>. Therefore, the events <span class="math inline">\(\{\max_{0\leq t\leq T}B_{t}\geq a\}\)</span> and <span class="math inline">\(\{\tau_{a}\leq T\}\)</span> are the same. So, the CDF <span class="math inline">\(\mathbb{P}(\tau_{a}\leq t)\)</span> of <span class="math inline">\(\tau_{a}\)</span>, by proposition (<a href="#prop:bacheliers-formula" data-reference-type="ref" data-reference="prop:bacheliers-formula">[prop:bacheliers-formula]</a>) <span class="math inline">\(\int_{a}^{\infty}f_{\max}(x)dx=\int_{a}^{\infty}\frac{2}{\sqrt{2\pi T}}e^{-\frac{x^{2}}{2T}}dx\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
f_{\tau_{a}}(t) &amp; =-2\phi(a/\sqrt{t})\cdot a\cdot\left(-\frac{1}{2t^{3/2}}\right)\\
&amp; =\frac{a}{t^{3/2}}\phi\left(\frac{a}{\sqrt{t}}\right)\\
&amp; =\frac{a}{t^{3/2}}\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{a^{2}}{2t}}
\end{aligned}\]</span></p>
<p>To estimate the expectation, it suffices to realize that for <span class="math inline">\(t\geq1\)</span>, <span class="math inline">\(e^{-\frac{a^{2}}{2t}}\)</span> is larger than <span class="math inline">\(e^{-\frac{a^{2}}{2}}\)</span>. Therefore, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[\tau_{a}] &amp; =\int_{0}^{\infty}t\frac{a}{\sqrt{2\pi}}\frac{e^{-a^{2}/2t}}{t^{3/2}}dt\geq\frac{ae^{-a^{2}/2}}{\sqrt{2\pi}}\int_{1}^{\infty}t^{-1/2}dt
\end{aligned}\]</span></p>
<p>This is an improper integral and it diverges like <span class="math inline">\(\sqrt{t}\)</span> and is infinite as claimed. ◻</p>
</div>
<p>To prove proposition (<a href="#prop:bacheliers-formula" data-reference-type="ref" data-reference="prop:bacheliers-formula">[prop:bacheliers-formula]</a>), we will need an important property of Brownian motion called the <em>reflection principle</em>. To motivate it, recall the reflection symmetry of Brownian motion at time <span class="math inline">\(s\)</span> in proposition (<a href="#prop:brownian-motion-symmetry-of-reflection-at-time-s" data-reference-type="ref" data-reference="prop:brownian-motion-symmetry-of-reflection-at-time-s">[prop:brownian-motion-symmetry-of-reflection-at-time-s]</a>). It turns out that this reflection property also holds if <span class="math inline">\(s\)</span> is replaced by a stopping time.</p>
<div class="lem">
<p><span id="lemma:BM-reflection-principle" data-label="lemma:BM-reflection-principle"></span>(Reflection principle). Let <span class="math inline">\((B_{t}:t\geq0)\)</span> be a standard Brownian motion and let <span class="math inline">\(\tau\)</span> be a stopping time for its filtration. Then, the process <span class="math inline">\((\tilde{B}_{t}:t\geq0)\)</span> defined by the reflection at time <span class="math inline">\(\tau\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{B}_{t} &amp; =\begin{cases}
B_{t} &amp; \text{if \ensuremath{t\leq\tau}}\\
B_{\tau}-(B_{t}-B_{\tau}) &amp; \text{if \ensuremath{t&gt;\tau}}
\end{cases}
\end{aligned}\]</span></p>
<p>is also a standard brownian motion.</p>
</div>
<div class="rem*">
<p>We defer the proof of the reflection property of Brownian motion to a further section. It is intuitive and instructive to quickly picture this in the discrete-time setting. I adopt the approach as in Shreve-I.</p>
<p>We repeatedly toss a fair coin (<span class="math inline">\(p\)</span>, the probability of <span class="math inline">\(H\)</span> on each toss, and <span class="math inline">\(q=1-p\)</span>, the probability of <span class="math inline">\(T\)</span> on each toss, are both equal to <span class="math inline">\(\frac{1}{2}\)</span>). We denote the successive outcomes of the tosses by <span class="math inline">\(\omega_{1}\omega_{2}\omega_{3}\ldots\)</span>. Let</p>
<p><span class="math display">\[\begin{aligned}
X_{j} &amp; =\begin{cases}
-1 &amp; \text{if \ensuremath{\omega_{j}=H}}\\
+1 &amp; \text{if \ensuremath{\omega_{j}=T}}
\end{cases}
\end{aligned}\]</span></p>
<p>and define <span class="math inline">\(M_{0}=0\)</span>, <span class="math inline">\(M_{n}=\sum_{j=1}^{n}X_{n}\)</span>. The process <span class="math inline">\((M_{n}:n\in\mathbf{N})\)</span> is a symmetric random walk.</p>
<p>Suppose we toss a coin an odd number <span class="math inline">\((2j-1)\)</span> of times. Some of the paths will reach level <span class="math inline">\(1\)</span> in the first <span class="math inline">\(2j-1\)</span> steps and other will not reach. In the case of <span class="math inline">\(3\)</span> tosses, there are <span class="math inline">\(2^{3}=8\)</span> possible paths and <span class="math inline">\(5\)</span> of these reach level <span class="math inline">\(1\)</span> at some time <span class="math inline">\(\tau_{1}\leq2j-1\)</span>. From that moment on, we can create a reflected path, which steps up each time the original path steps down and steps down each time the original path steps up. If the original path ends above <span class="math inline">\(1\)</span> at the final time <span class="math inline">\(2j-1\)</span>, the reflected path ends below <span class="math inline">\(1\)</span> and vice versa. If the original path ends at <span class="math inline">\(1\)</span>, the reflected path does also. In fact, the reflection at the first hitting time has the same distribution as the original random walk.</p>
<p>The key here is, out of the <span class="math inline">\(5\)</span> paths that reach level <span class="math inline">\(1\)</span> at some time, there are as many reflected paths that exceed <span class="math inline">\(1\)</span> at time <span class="math inline">\((2j-1)\)</span> as there are original paths that exceed <span class="math inline">\(1\)</span> at time <span class="math inline">\((2j-1)\)</span>. So, to count the total number of paths that reach level <span class="math inline">\(1\)</span> by time <span class="math inline">\((2j-1)\)</span>, we can count the paths that are at <span class="math inline">\(1\)</span> at time <span class="math inline">\((2j-1)\)</span> and then add on <em>twice</em> the number of paths that exceed <span class="math inline">\(1\)</span> at time <span class="math inline">\((2j-1)\)</span>.</p>
</div>
<p>With this new tool, we can now prove proposition (<a href="#prop:bacheliers-formula" data-reference-type="ref" data-reference="prop:bacheliers-formula">[prop:bacheliers-formula]</a>).</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Consider <span class="math inline">\(\mathbb{P}(\max_{t\leq T}B_{t}\geq a)\)</span>. By splitting this probability over the event of the endpoint, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}&gt;a\right)+\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right)
\end{aligned}\]</span></p>
<p>Note also, that <span class="math inline">\(\mathbb{P}(B_{T}=a)=0\)</span>. Hence, the first probability equals <span class="math inline">\(\mathbb{P}(B_{T}\geq a)\)</span>. As for the second, consider the time <span class="math inline">\(\tau_{a}\)</span>. On the event considered, we have <span class="math inline">\(\tau_{a}\leq T\)</span> and using lemma (<a href="#lemma:BM-reflection-principle" data-reference-type="ref" data-reference="lemma:BM-reflection-principle">[lemma:BM-reflection-principle]</a>) at that time, we get</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,\tilde{B}_{T}\geq a\right)
\end{aligned}\]</span></p>
<p>Observe that the event <span class="math inline">\(\{\max_{t\leq T}B_{t}\geq a\}\)</span> is the same as <span class="math inline">\(\{\max_{t\leq T}\tilde{B}_{T}\geq a\}\)</span>. (A rough picture might help here.) Thereforem the above probability is</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}\tilde{B}_{t}\geq a,\tilde{B}_{T}\geq a\right)=\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\geq a\right)
\end{aligned}\]</span></p>
<p>where the last equality follows from the reflection principle (<span class="math inline">\(\tilde{B}_{t}\)</span> is also a standard brownian motion, and <span class="math inline">\(B_{T}\)</span> and <span class="math inline">\(\tilde{B}_{T}\)</span> have the same distribution.) But, as above, the last probability is equal to <span class="math inline">\(\mathbb{P}(B_{T}\geq a)\)</span>. We conclude that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a\right) &amp; =2\mathbb{P}(B_{T}\geq a)=\frac{2}{\sqrt{2\pi T}}\int_{a}^{\infty}e^{-\frac{x^{2}}{2T}}dx=\mathbb{P}(|B_{T}|\geq a)
\end{aligned}\]</span></p>
<p>This implies in particular that <span class="math inline">\(\mathbb{P}\left(\max_{t\leq T}B_{t}=a\right)=0\)</span>. Thus, we also have <span class="math inline">\(\mathbb{P}(\max_{t\leq T}B_{t}\leq a)=\mathbb{P}(|B_{T}|\leq a)\)</span> as claimed. ◻</p>
</div>
<div class="example">
<p>(Simulating Martingales) Sample <span class="math inline">\(10\)</span> paths of the following process with a step-size of <span class="math inline">\(0.01\)</span>:</p>
<p>(a) <span class="math inline">\(B_{t}^{2}-t\)</span>, <span class="math inline">\(t\in[0,1]\)</span></p>
<p>(b) Geometric Brownian motion : <span class="math inline">\(S_{t}=\exp(B_{t}-t/2)\)</span>, <span class="math inline">\(t\in[0,1]\)</span>.</p>
<p>Let’s write a simple <span class="math inline">\(\texttt{BrownianMotion}\)</span> class, that we shall use to generate sample paths.</p>
</div>
<pre data-caption="10 paths of $B_t^2 - t$"><code>import numpy as np
import matplotlib.pyplot as plt

import attrs
from attrs import define, field

@define
class BrownianMotion:
    _step_size = field(validator=attrs.validators.and_(attrs.validators.instance_of(float),
                                                       attrs.validators.ge(0.0)))
    # Time T
    _T = field(validator=attrs.validators.and_(attrs.validators.instance_of(float),
                                               attrs.validators.ge(0.0)))
    # number of paths
    _N = field(validator=attrs.validators.and_(attrs.validators.instance_of(int),
                                               attrs.validators.gt(0)))

    _num_steps = field(init=False)

    def __attrs_post_init__(self):
        self._num_steps = int(self._T/self._step_size)

    def covariance_matrix(self):
        C = np.zeros((self._num_steps,self._num_steps))

        for i in range(self._num_steps):
            for j in range(self._num_steps):
                s = (i+1) * self._step_size
                t = (j+1) * self._step_size
                C[i,j] = min(s,t)
        return C

    # Each column vector represents a sample path
    def generate_paths(self):
        C = self.covariance_matrix()
        A = np.linalg.cholesky(C)
        Z = np.random.standard_normal((self._num_steps, self._N))
        X = np.matmul(A,Z)
        X = np.concatenate((np.zeros((1,self._N)),X),axis=0)
        return X.transpose()</code></pre>
<p>Now, the process <span class="math inline">\(B_{t}^{2}-t\)</span> can be sampled as follows:</p>
<pre data-caption="10 paths of $B_t^2 - t$"><code>
def generateSquareOfBMCompensated(numOfPaths,stepSize,T):
    N = int(T/stepSize)

    X = []
    brownianMotion = BrownianMotion(stepSize,T)
    for n in range(numOfPaths):

        B_t = brownianMotion.samplePath()

        B_t_sq = np.square(B_t)

        t = np.linspace(start=0.0,stop=1.0,num=N+1)
        M_t = np.subtract(B_t_sq,t)
        X.append(M_t)

    return X</code></pre>
<p>The gBM process can be sampled similarly, with <span class="math inline">\(\texttt{\ensuremath{M_{t}} = np.exp(np.subtract(\ensuremath{B_{t}},t/2))}\)</span>.</p>
<div class="example">
<p><strong>(Maximum of Brownian Motion.)</strong> Consider the maximum of Brownian motion on <span class="math inline">\([0,1]\)</span>: <span class="math inline">\(\max_{s\leq1}B_{s}\)</span>.</p>
<p>(a) Draw the histogram of the random variable <span class="math inline">\(\max_{s\leq1}B_{s}\)</span>using <span class="math inline">\(10,0000\)</span> sampled Brownian paths with a step size of <span class="math inline">\(0.01\)</span>.</p>
<p>(b) Compare this to the PDF of the random variable <span class="math inline">\(|B_{1}|\)</span>.</p>
</div>
<p><em>Solution.</em></p>
<p>I use the <span class="math inline">\(\texttt{itertools}\)</span> python library to compute the running maximum of a brownian motion path.</p>
<pre data-caption="The process $\sup_{s\leq 1}B_s$"><code>
brownianMotion = BrownianMotion(stepSize=0.01,T=1)
data = []

for i in range(10000):
    B_t = brownianMotion.samplePath()
    max_B_t = list(itertools.accumulate(B_t,max))
    data.append(max_B_t[100])</code></pre>
<p>Analytically, we know that <span class="math inline">\(B_{1}\)</span> is a gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(|B_{1}|\leq z) &amp; =\mathbb{P}(|Z|\leq z)\\
&amp; =\mathbb{P}(-z\leq Z\leq z)\\
&amp; =\mathbb{P}(Z\leq z)-\mathbb{P}(Z\leq-z)\\
&amp; =\mathbb{P}(Z\leq z)-(1-\mathbb{P}(Z\leq z))\\
F_{|B_{1}|}(z) &amp; =2\Phi(z)-1
\end{aligned}\]</span></p>
<p>Differentiating on both sides, we get:</p>
<p><span class="math display">\[\begin{aligned}
f_{|B_{1}|}(z) &amp; =2\phi(z)=\frac{2}{\sqrt{2\pi}}e^{-\frac{z^{2}}{2}},\quad z\in[0,\infty)
\end{aligned}\]</span></p>
<div class="example">
<p>(First passage time.) Let <span class="math inline">\((B_{t}:t\geq0)\)</span> be a standard brownian motion. Consider the random variable:</p>
<p><span class="math display">\[\begin{aligned}
\tau &amp; =\min\{t\geq0:B_{t}\geq1\}
\end{aligned}\]</span></p>
<p>This is the first time that <span class="math inline">\(B_{t}\)</span> reaches <span class="math inline">\(1\)</span>.</p>
<p>(a) Draw a histogram for the distribution of <span class="math inline">\(\tau\land10\)</span> on the time-interval <span class="math inline">\([0,10]\)</span> using <span class="math inline">\(10,000\)</span> brownian motion paths on <span class="math inline">\([0,10]\)</span> with discretization <span class="math inline">\(0.01\)</span>.</p>
<p><em>The notation <span class="math inline">\(\tau\land10\)</span> means that if the path does not reach <span class="math inline">\(1\)</span> on <span class="math inline">\([0,10]\)</span>, then give the value <span class="math inline">\(10\)</span> to the stopping time.</em></p>
<p>(b) Estimate <span class="math inline">\(\mathbf{E}[\tau\land10]\)</span>.</p>
<p>(c) What proportion of paths never reach <span class="math inline">\(1\)</span> in the time interval <span class="math inline">\([0,10]\)</span>?</p>
</div>
<p><em>Solution.</em></p>
<p>To compute the expectation, we classify the hitting times of all paths into <span class="math inline">\(50\)</span> bins. I simply did</p>
<p><span class="math inline">\(\texttt{frequency, bins = np.histogram(firstPassageTimes,bins=50,range=(0,10))}\)</span></p>
<p>and then computed</p>
<p><span class="math inline">\(\texttt{expectation=np.dot(frequency,bins[1:])/10000}\)</span>.</p>
<p>This expectation estimate on my machine is <span class="math inline">\(\mathbf{E}[\tau\land10]=4.34\)</span> secs. There were approximately <span class="math inline">\(2600\)</span> paths out of <span class="math inline">\(10,000\)</span> that did not reach <span class="math inline">\(1\)</span>.</p>
<div class="example">
<p><strong>Gambler’s ruin at the French Roulette.</strong> Consider the scenario in which you are gambling <span class="math inline">\(\$1\)</span> at the French roulette on the reds: You gain <span class="math inline">\(\$1\)</span> with probability <span class="math inline">\(18/38\)</span> and you lose a dollar with probability <span class="math inline">\(20/38\)</span>. We estimate the probability of your fortune reaching <span class="math inline">\(\$200\)</span> before it reaches <span class="math inline">\(0\)</span>.</p>
<p>(a) Write a function that samples the simple random walk path from time <span class="math inline">\(0\)</span> to time <span class="math inline">\(5,000\)</span> with a given starting point.</p>
<p>(b) Use the above to estimate the probability of reaching <span class="math inline">\(\$200\)</span> before <span class="math inline">\(\$0\)</span> on a sample of <span class="math inline">\(100\)</span> paths if you start with <span class="math inline">\(\$100\)</span>.</p>
</div>
<div class="example">
<p><strong>[]{#ex:doob’s-maximal-inequality label=“ex:doob’s-maximal-inequality”}Doob’s maximal inequalities</strong>. We prove the following: Let <span class="math inline">\((M_{k}:k\geq1)\)</span> be positive submartingale for the filtration <span class="math inline">\((\mathcal{F}_{k}:k\in\mathbf{N})\)</span>. Then, for any <span class="math inline">\(1\leq p&lt;\infty\)</span> and <span class="math inline">\(a&gt;0\)</span></p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{k\leq n}M_{k}&gt;a\right) &amp; \leq\frac{1}{a^{p}}\mathbf{E}[M_{n}^{p}]
\end{aligned}\]</span></p>
<p>(a) Use Jensen’s inequality to show that if <span class="math inline">\((M_{k}:k\geq1)\)</span> is a positive submartingale, then so is <span class="math inline">\((M_{k}^{p}:k\geq1)\)</span> for <span class="math inline">\(1\leq p&lt;\infty\)</span>. Conclude that it suffices to prove the statement for <span class="math inline">\(p=1\)</span>.</p>
</div>
<p><em>Solution.</em></p>
<p>The function <span class="math inline">\(f(x)=x^{p}\)</span> is convex. By conditional Jensen’s inequality,</p>
<p><span class="math display">\[\begin{aligned}
\left(\mathbf{E}[M_{k+1}|\mathcal{F}_{k}]\right)^{p} &amp; \leq\mathbf{E}[M_{k}^{p}|\mathcal{F}_{k}]
\end{aligned}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{k+1}^{p}|\mathcal{F}_{k}] &amp; \geq\left(\mathbf{E}[M_{k+1}|\mathcal{F}_{k}]\right)^{p}\geq M_{k}^{p}
\end{aligned}\]</span></p>
<p>where the last inequality follows from the fact that <span class="math inline">\((M_{k}:k\geq1)\)</span> is a positive submartingale, so <span class="math inline">\(\mathbf{E}[M_{k+1}|\mathcal{F}_{k}]\geq M_{k}\)</span>. Consequently, <span class="math inline">\((M_{k}^{p}:k\geq1)\)</span> is also a positive submartingale.</p>
<p>(b) Consider the events</p>
<p><span class="math display">\[\begin{aligned}
B_{k} &amp; =\bigcap_{j&lt;k}\{\omega:M_{j}(\omega)\leq a\}\cap\{\omega:M_{k}(\omega)&gt;a\}
\end{aligned}\]</span></p>
<p>Argue that the <span class="math inline">\(B_{k}\)</span>’s are disjoint and that <span class="math inline">\(\bigcup_{k\leq n}B_{k}=\{\max_{k\leq n}M_{k}&gt;a\}=B\)</span>.</p>
<p><em>Solution.</em></p>
<p>Clearly, <span class="math inline">\(B_{k}\)</span> is the event that the first time to cross <span class="math inline">\(a\)</span> is <span class="math inline">\(k\)</span>. If <span class="math inline">\(B_{k}\)</span> occurs, <span class="math inline">\(B_{k+1},B_{k+2},\ldots\)</span> fail to occur. Hence, all <span class="math inline">\(B_{k}'s\)</span> are pairwise disjoint. The event <span class="math inline">\(\bigcup_{k\leq n}B_{k}\)</span> is the event that the random walk crosses <span class="math inline">\(a\)</span> at any time <span class="math inline">\(k\leq n\)</span>. Thus, the running maximum of the Brownian motion at time <span class="math inline">\(n\)</span> exceeds <span class="math inline">\(a\)</span>.</p>
<p>(c) Show that</p>
<p><span class="math display">\[\begin{aligned}
\mathbf{E}[M_{n}]\geq\mathbf{E}[M_{n}\mathbf{1}_{B}] &amp; \geq a\sum_{k\leq n}\mathbb{P}(B_{k})=a\mathbb{P}(B)
\end{aligned}\]</span></p>
<p>by decomposing <span class="math inline">\(B\)</span> in <span class="math inline">\(B_{k}\)</span>’s and by using the properties of expectations, as well as the submartingale property.</p>
<p><em>Solution.</em></p>
<p>Clearly, <span class="math inline">\(M_{n}\geq M_{n}\mathbf{1}_{B}\geq a\mathbf{1}_{B}\)</span>. And <span class="math inline">\(M_{n}\)</span> is a positive random variable. By monotonicity of expectations, <span class="math inline">\(\mathbf{E}[M_{n}]\geq\mathbf{E}[M_{n}\mathbf{1}_{B}]\geq a\mathbf{E}[\mathbf{1}_{B}]=a\mathbb{P}(B)=a\sum_{k\leq n}\mathbb{P}(B_{k})\)</span>, where the last equality holds because the <span class="math inline">\(B_{k}\)</span>’s are disjoint.</p>
<p>(d) Argue that the inequality holds for continuous paths by discretizing time and using convergence theorems : If <span class="math inline">\((M_{t}:t\geq0)\)</span> is a positive submartingale with continuous paths for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>, then for any <span class="math inline">\(1\leq p&lt;\infty\)</span> and <span class="math inline">\(a&gt;0\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{s\leq t}M_{s}&gt;a\right) &amp; \leq\frac{1}{a^{p}}\mathbf{E}[M_{t}^{p}]
\end{aligned}\]</span></p>
<p><em>Solution.</em></p>
<p>Let <span class="math inline">\((M_{t}:t\geq0)\)</span> be a positive submartingale with continuous paths for the filtration <span class="math inline">\((\mathcal{F}_{t}:t\geq0)\)</span>. Consider a sequence of partitions of the interval <span class="math inline">\([0,t]\)</span> into <span class="math inline">\(2^{r}\)</span> subintervals :</p>
<p><span class="math display">\[\begin{aligned}
D_{r} &amp; =\left\{ \frac{kt}{2^{r}}:k=0,1,2,\ldots,2^{n}\right\}
\end{aligned}\]</span></p>
<p>And consider a sequence of discrete positive sub-martingales:</p>
<p><span class="math display">\[\begin{aligned}
M_{kt/2^{r}}^{(r)} &amp; =M_{kt/2^{r}},\quad k\in\mathbf{N},0\leq k\leq2^{r}
\end{aligned}\]</span></p>
<p>Next, we define for <span class="math inline">\(r=1,2,3,\ldots\)</span></p>
<p><span class="math display">\[\begin{aligned}
A_{r} &amp; =\left\{ \sup_{s\in D_{r}}|M_{s}^{(r)}|&gt;a\right\}
\end{aligned}\]</span></p>
<p>By using the maximal inequality in discrete time, gives us:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(A_{r})=\mathbb{P}\left\{ \sup_{s\in D_{r}}|M_{s}^{(r)}|&gt;a\right\}  &amp; \leq\frac{1}{a^{p}}\mathbf{E}\left[\left(M_{s}^{(r)}\right)^{p}\right]=\frac{1}{a^{p}}\mathbf{E}\left[M_{t}^{p}\right]
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\max_{s\leq t}M_{s}&gt;a\right) &amp; =\mathbb{P}\left(\bigcup_{r=1}^{\infty}A_{r}\right)\\
&amp; =\lim_{r\to\infty}\mathbb{P}\left(A_{r}\right)\\
&amp; \left\{ \text{Continuity of probability measure}\right\} \\
&amp; \leq\lim_{r\to\infty}\frac{1}{a^{p}}\mathbf{E}\left[M_{t}^{p}\right]
\end{aligned}\]</span></p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb5" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">---</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="an">title:</span><span class="co"> "Martingales"</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="an">author:</span><span class="co"> "Quasar"</span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="an">date:</span><span class="co"> "2024-07-12"</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="an">categories:</span><span class="co"> [Stochastic Calculus]      </span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="an">image:</span><span class="co"> "image.jpg"</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co">---</span></span>
<span id="cb5-10"><a href="#cb5-10"></a></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="fu"># Martingales.</span></span>
<span id="cb5-12"><a href="#cb5-12"></a></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="fu">## Elementary conditional expectation.</span></span>
<span id="cb5-14"><a href="#cb5-14"></a></span>
<span id="cb5-15"><a href="#cb5-15"></a>In elementary probability, the conditional expectation of a variable $Y$ given another random variable $X$ refers to the expectation of $Y$ given the conditional distribution $f_{Y|X}(y|x)$ of $Y$ given $X$. To illustrate this, let's go through a simple example. Consider $\mathcal{B}_{1}$, $\mathcal{B}_{2}$ to be two independent Bernoulli-distributed random variables with $p=1/2$. Then, construct:</span>
<span id="cb5-16"><a href="#cb5-16"></a></span>
<span id="cb5-17"><a href="#cb5-17"></a>$$\begin{aligned}</span>
<span id="cb5-18"><a href="#cb5-18"></a>X=\mathcal{B}_{1}, &amp; \quad Y=\mathcal{B}_{1}+\mathcal{B}_{2}</span>
<span id="cb5-19"><a href="#cb5-19"></a>\end{aligned}$$</span>
<span id="cb5-20"><a href="#cb5-20"></a></span>
<span id="cb5-21"><a href="#cb5-21"></a>It is easy to compute $\mathbb{E}<span class="co">[</span><span class="ot">Y|X=0</span><span class="co">]</span>$ and $\mathbb{E}<span class="co">[</span><span class="ot">Y|X=1</span><span class="co">]</span>$. By definition, it is given by:</span>
<span id="cb5-22"><a href="#cb5-22"></a></span>
<span id="cb5-23"><a href="#cb5-23"></a>$$\begin{aligned}</span>
<span id="cb5-24"><a href="#cb5-24"></a>\mathbb{E}<span class="co">[</span><span class="ot">Y|X=0</span><span class="co">]</span> &amp; =\sum_{j=0}^{2}j\mathbb{P}(Y=j|X=0)<span class="sc">\\</span></span>
<span id="cb5-25"><a href="#cb5-25"></a> &amp; =\sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(Y=j,X=0)}{P(X=0)}<span class="sc">\\</span></span>
<span id="cb5-26"><a href="#cb5-26"></a> &amp; =0+1\cdot\frac{(1/4)}{(1/2)}+2\cdot\frac{0}{(1/2)}<span class="sc">\\</span></span>
<span id="cb5-27"><a href="#cb5-27"></a> &amp; =\frac{1}{2}</span>
<span id="cb5-28"><a href="#cb5-28"></a>\end{aligned}$$</span>
<span id="cb5-29"><a href="#cb5-29"></a></span>
<span id="cb5-30"><a href="#cb5-30"></a>and</span>
<span id="cb5-31"><a href="#cb5-31"></a></span>
<span id="cb5-32"><a href="#cb5-32"></a>$$\begin{aligned}</span>
<span id="cb5-33"><a href="#cb5-33"></a>\mathbb{E}<span class="co">[</span><span class="ot">Y|X=1</span><span class="co">]</span> &amp; =\sum_{j=0}^{2}j\mathbb{P}(Y=j|X=1)<span class="sc">\\</span></span>
<span id="cb5-34"><a href="#cb5-34"></a> &amp; =\sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(Y=j,X=1)}{P(X=1)}<span class="sc">\\</span></span>
<span id="cb5-35"><a href="#cb5-35"></a> &amp; =0+1\cdot\frac{(1/4)}{(1/2)}+2\cdot\frac{(1/4)}{(1/2)}<span class="sc">\\</span></span>
<span id="cb5-36"><a href="#cb5-36"></a> &amp; =\frac{3}{2}</span>
<span id="cb5-37"><a href="#cb5-37"></a>\end{aligned}$$</span>
<span id="cb5-38"><a href="#cb5-38"></a></span>
<span id="cb5-39"><a href="#cb5-39"></a>With this point of view, the conditional expectation is computed given</span>
<span id="cb5-40"><a href="#cb5-40"></a>the information that the event $<span class="sc">\{</span>X=0<span class="sc">\}</span>$ occurred or the event $<span class="sc">\{</span>X=1<span class="sc">\}</span>$</span>
<span id="cb5-41"><a href="#cb5-41"></a>occurred. It is possible to regroup both conditional expectations in a</span>
<span id="cb5-42"><a href="#cb5-42"></a>single object, if we think of the conditional expectation as a random</span>
<span id="cb5-43"><a href="#cb5-43"></a>variable and denote it by $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$. Namely, we take:</span>
<span id="cb5-44"><a href="#cb5-44"></a></span>
<span id="cb5-45"><a href="#cb5-45"></a>$$\begin{aligned}</span>
<span id="cb5-46"><a href="#cb5-46"></a>\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">](\omega)</span> &amp; =\begin{cases}</span>
<span id="cb5-47"><a href="#cb5-47"></a>\frac{1}{2} &amp; \text{if }X(\omega)=0<span class="sc">\\</span></span>
<span id="cb5-48"><a href="#cb5-48"></a>\frac{3}{2} &amp; \text{if }X(\omega)=1</span>
<span id="cb5-49"><a href="#cb5-49"></a>\end{cases}\label{eq:elementary-conditional-expectation-example}</span>
<span id="cb5-50"><a href="#cb5-50"></a>\end{aligned}$$</span>
<span id="cb5-51"><a href="#cb5-51"></a></span>
<span id="cb5-52"><a href="#cb5-52"></a>This random variable is called the *conditional expectation* of $Y$</span>
<span id="cb5-53"><a href="#cb5-53"></a>given $X$. We make two important observations:</span>
<span id="cb5-54"><a href="#cb5-54"></a></span>
<span id="cb5-55"><a href="#cb5-55"></a><span class="sc">\(</span>i<span class="sc">\)</span> If the value of $X$ is known, then the value of $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$</span>
<span id="cb5-56"><a href="#cb5-56"></a>is determined.</span>
<span id="cb5-57"><a href="#cb5-57"></a></span>
<span id="cb5-58"><a href="#cb5-58"></a><span class="sc">\(</span>ii<span class="sc">\)</span> If we have another random variable $g(X)$ constructed from $X$,</span>
<span id="cb5-59"><a href="#cb5-59"></a>then we have:</span>
<span id="cb5-60"><a href="#cb5-60"></a></span>
<span id="cb5-61"><a href="#cb5-61"></a>$$\begin{aligned}</span>
<span id="cb5-62"><a href="#cb5-62"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(X)Y</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">g(X)\mathbb{E}[Y|X]</span><span class="co">]</span></span>
<span id="cb5-63"><a href="#cb5-63"></a>\end{aligned}$$</span>
<span id="cb5-64"><a href="#cb5-64"></a></span>
<span id="cb5-65"><a href="#cb5-65"></a>In other words, as far as $X$ is concerned, the conditional expectation</span>
<span id="cb5-66"><a href="#cb5-66"></a>$\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ is a proxy for $Y$ in the expectation. We sometimes</span>
<span id="cb5-67"><a href="#cb5-67"></a>say that $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ is the best estimate of $Y$ given the</span>
<span id="cb5-68"><a href="#cb5-68"></a>information of $X$.</span>
<span id="cb5-69"><a href="#cb5-69"></a></span>
<span id="cb5-70"><a href="#cb5-70"></a>The last observation is easy to verify since:</span>
<span id="cb5-71"><a href="#cb5-71"></a></span>
<span id="cb5-72"><a href="#cb5-72"></a>$$\begin{aligned}</span>
<span id="cb5-73"><a href="#cb5-73"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(X)Y</span><span class="co">]</span> &amp; =\sum_{i=0}^{1}\sum_{j=0}^{2}g(i)\cdot j\cdot\mathbb{P}(X=i,Y=j)<span class="sc">\\</span></span>
<span id="cb5-74"><a href="#cb5-74"></a> &amp; =\sum_{i=0}^{1}\mathbb{P}(X=i)g(i)\left<span class="sc">\{</span> \sum_{j=0}^{2}j\cdot\frac{\mathbb{P}(X=i,Y=j)}{\mathbb{P}(X=i)}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb5-75"><a href="#cb5-75"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">g(X)\mathbb{E}[Y|X]</span><span class="co">]</span></span>
<span id="cb5-76"><a href="#cb5-76"></a>\end{aligned}$$</span>
<span id="cb5-77"><a href="#cb5-77"></a></span>
<span id="cb5-78"><a href="#cb5-78"></a>::: example</span>
<span id="cb5-79"><a href="#cb5-79"></a>[]{#ex:elementary-definitions-of-conditional-expectation</span>
<span id="cb5-80"><a href="#cb5-80"></a>label="ex:elementary-definitions-of-conditional-expectation"}(Elementary</span>
<span id="cb5-81"><a href="#cb5-81"></a>Definitions of Conditional Expectation).</span>
<span id="cb5-82"><a href="#cb5-82"></a></span>
<span id="cb5-83"><a href="#cb5-83"></a><span class="sc">\(</span>1<span class="sc">\)</span> $(X,Y)$ discrete. The treatment is similar to the above. If a</span>
<span id="cb5-84"><a href="#cb5-84"></a>random variable $X$ takes values $(x_{i},i\geq1)$ and $Y$ takes values</span>
<span id="cb5-85"><a href="#cb5-85"></a>$(y_{j},j\geq1)$, we have by definition that the conditional expectation</span>
<span id="cb5-86"><a href="#cb5-86"></a>as a random variable is:</span>
<span id="cb5-87"><a href="#cb5-87"></a></span>
<span id="cb5-88"><a href="#cb5-88"></a>$$\begin{aligned}</span>
<span id="cb5-89"><a href="#cb5-89"></a>\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">](\omega)</span> &amp; =\sum_{j\geq1}y_{j}\mathbb{P}(Y=y_{j}|X=x_{i})\quad\text{for }\omega\text{ such that }X(\omega)=x_{i}</span>
<span id="cb5-90"><a href="#cb5-90"></a>\end{aligned}$$ (2) $(X,Y)$ continuous with joint PDF $f_{X,Y}(x,y)$: In</span>
<span id="cb5-91"><a href="#cb5-91"></a>this case, the conditional expectation is the random variable given by</span>
<span id="cb5-92"><a href="#cb5-92"></a></span>
<span id="cb5-93"><a href="#cb5-93"></a>$$\begin{aligned}</span>
<span id="cb5-94"><a href="#cb5-94"></a>\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span> &amp; =h(X)</span>
<span id="cb5-95"><a href="#cb5-95"></a>\end{aligned}$$</span>
<span id="cb5-96"><a href="#cb5-96"></a></span>
<span id="cb5-97"><a href="#cb5-97"></a>where</span>
<span id="cb5-98"><a href="#cb5-98"></a></span>
<span id="cb5-99"><a href="#cb5-99"></a>$$\begin{aligned}</span>
<span id="cb5-100"><a href="#cb5-100"></a>h(x) &amp; =\int_{\mathbf{R}}yf_{Y|X}(y|x)dy=\int_{\mathbf{R}}y\frac{f_{X,Y}(x,y)}{f_{X}(x)}dy=\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}</span>
<span id="cb5-101"><a href="#cb5-101"></a>\end{aligned}$$</span>
<span id="cb5-102"><a href="#cb5-102"></a>:::</span>
<span id="cb5-103"><a href="#cb5-103"></a></span>
<span id="cb5-104"><a href="#cb5-104"></a>In the two examples above, the expectation of the random variable</span>
<span id="cb5-105"><a href="#cb5-105"></a>$\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ is equal to $\mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$. Indeed in the discrete</span>
<span id="cb5-106"><a href="#cb5-106"></a>case, we have:</span>
<span id="cb5-107"><a href="#cb5-107"></a></span>
<span id="cb5-108"><a href="#cb5-108"></a>$$\begin{aligned}</span>
<span id="cb5-109"><a href="#cb5-109"></a>\mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}[Y|X]</span><span class="co">]</span> &amp; =\sum_{i=0}^{1}P(X=x_{i})\cdot\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j}|X=x_{i})<span class="sc">\\</span></span>
<span id="cb5-110"><a href="#cb5-110"></a> &amp; =\sum_{i=0}^{1}\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j},X=x_{i})<span class="sc">\\</span></span>
<span id="cb5-111"><a href="#cb5-111"></a> &amp; =\sum_{j=0}^{2}y_{j}\mathbb{P}(Y=y_{j})<span class="sc">\\</span></span>
<span id="cb5-112"><a href="#cb5-112"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span></span>
<span id="cb5-113"><a href="#cb5-113"></a>\end{aligned}$$</span>
<span id="cb5-114"><a href="#cb5-114"></a></span>
<span id="cb5-115"><a href="#cb5-115"></a>::: example</span>
<span id="cb5-116"><a href="#cb5-116"></a>(Conditional Probability vs Conditional expectation). The conditional</span>
<span id="cb5-117"><a href="#cb5-117"></a>probability of the event $A$ given $B$ can be recast in terms of</span>
<span id="cb5-118"><a href="#cb5-118"></a>conditional expectation using indicator functions. If</span>
<span id="cb5-119"><a href="#cb5-119"></a>$0&lt;\mathbb{P}(B)&lt;1$, it is not hard to check that:</span>
<span id="cb5-120"><a href="#cb5-120"></a>$\mathbb{P}(A|B)=\mathbb{E}<span class="co">[</span><span class="ot">\mathbf{1}_{A}|\mathbf{1}_{B}=1</span><span class="co">]</span>$ and</span>
<span id="cb5-121"><a href="#cb5-121"></a>$\mathbb{P}(A|B^{C})=\mathbb{E}<span class="co">[</span><span class="ot">\mathbf{1}_{A}|1_{B}=0</span><span class="co">]</span>$. Indeed the</span>
<span id="cb5-122"><a href="#cb5-122"></a>random variables $\mathbf{1}_{A}$ and $\mathbf{1}_{B}$ are discrete. If</span>
<span id="cb5-123"><a href="#cb5-123"></a>we proceed as in the discrete case above, we have:</span>
<span id="cb5-124"><a href="#cb5-124"></a></span>
<span id="cb5-125"><a href="#cb5-125"></a>$$\begin{aligned}</span>
<span id="cb5-126"><a href="#cb5-126"></a>\mathbb{E}<span class="co">[</span><span class="ot">\mathbf{1}_{A}|\mathbf{1}_{B}=1</span><span class="co">]</span> &amp; =1\cdot\mathbb{P}(\mathbf{1}_{A}=1|\mathbf{1}_{B}=1)<span class="sc">\\</span></span>
<span id="cb5-127"><a href="#cb5-127"></a> &amp; =\frac{\mathbb{P}(\mathbf{1}_{A}=1,\mathbf{1}_{B}=1)}{\mathbb{P}(\mathbf{1}_{B}=1)}<span class="sc">\\</span></span>
<span id="cb5-128"><a href="#cb5-128"></a> &amp; =\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}<span class="sc">\\</span></span>
<span id="cb5-129"><a href="#cb5-129"></a> &amp; =\mathbb{P}(A|B)</span>
<span id="cb5-130"><a href="#cb5-130"></a>\end{aligned}$$</span>
<span id="cb5-131"><a href="#cb5-131"></a></span>
<span id="cb5-132"><a href="#cb5-132"></a>A similar calculation gives $\mathbb{P}(A|B^{C})$. In particular, the</span>
<span id="cb5-133"><a href="#cb5-133"></a>formula for total probability for $A$ is a rewriting of the expectation</span>
<span id="cb5-134"><a href="#cb5-134"></a>of the random variable $\mathbb{E}<span class="co">[</span><span class="ot">\mathbf{1}_{A}|\mathbf{1}_{B}</span><span class="co">]</span>$:</span>
<span id="cb5-135"><a href="#cb5-135"></a></span>
<span id="cb5-136"><a href="#cb5-136"></a>$$\begin{aligned}</span>
<span id="cb5-137"><a href="#cb5-137"></a>\mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}]</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">\mathbf{1}_{A}|\mathbf{1}_{B}=1</span><span class="co">]</span>\mathbb{P}(\mathbf{1}_{B}=1)+\mathbb{E}[\mathbf{1}_{A}|\mathbf{1}_{B}=0]\mathbb{P}(\mathbf{1}_{B}=0)<span class="sc">\\</span></span>
<span id="cb5-138"><a href="#cb5-138"></a> &amp; =\mathbb{P}(A|B)\cdot\mathbb{P}(B)+\mathbb{P}(A|B^{C})\cdot\mathbb{P}(B^{C})<span class="sc">\\</span></span>
<span id="cb5-139"><a href="#cb5-139"></a> &amp; =\mathbb{P}(A)</span>
<span id="cb5-140"><a href="#cb5-140"></a>\end{aligned}$$</span>
<span id="cb5-141"><a href="#cb5-141"></a>:::</span>
<span id="cb5-142"><a href="#cb5-142"></a></span>
<span id="cb5-143"><a href="#cb5-143"></a><span class="fu">## Conditional Expectation as a projection.</span></span>
<span id="cb5-144"><a href="#cb5-144"></a></span>
<span id="cb5-145"><a href="#cb5-145"></a><span class="fu">#### Conditioning on one variable. </span></span>
<span id="cb5-146"><a href="#cb5-146"></a></span>
<span id="cb5-147"><a href="#cb5-147"></a>We start by giving the definition of conditional expectation given a</span>
<span id="cb5-148"><a href="#cb5-148"></a>single variable. This relates to the two observations (A) and (B) made</span>
<span id="cb5-149"><a href="#cb5-149"></a>previously. We assume that the random variable is integrable for the</span>
<span id="cb5-150"><a href="#cb5-150"></a>expectations to be well-defined.</span>
<span id="cb5-151"><a href="#cb5-151"></a></span>
<span id="cb5-152"><a href="#cb5-152"></a>::: defn</span>
<span id="cb5-153"><a href="#cb5-153"></a>[]{#def:conditional-expectation label="def:conditional-expectation"}Let</span>
<span id="cb5-154"><a href="#cb5-154"></a>$X$ and $Y$ be integrable random variables on</span>
<span id="cb5-155"><a href="#cb5-155"></a>$(\Omega,\mathcal{F},\mathbb{P})$. The conditional expectation of $Y$</span>
<span id="cb5-156"><a href="#cb5-156"></a>given $X$ is the random variable denoted by $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ with the</span>
<span id="cb5-157"><a href="#cb5-157"></a>following two properties:</span>
<span id="cb5-158"><a href="#cb5-158"></a></span>
<span id="cb5-159"><a href="#cb5-159"></a><span class="sc">\(</span>A<span class="sc">\)</span> There exists a function $h:\mathbf{R}\to\mathbf{R}$ such that</span>
<span id="cb5-160"><a href="#cb5-160"></a>$\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>=h(X)$.</span>
<span id="cb5-161"><a href="#cb5-161"></a></span>
<span id="cb5-162"><a href="#cb5-162"></a><span class="sc">\(</span>B<span class="sc">\)</span> For any bounded random variable of the form $g(X)$ for some</span>
<span id="cb5-163"><a href="#cb5-163"></a>function $g$,</span>
<span id="cb5-164"><a href="#cb5-164"></a></span>
<span id="cb5-165"><a href="#cb5-165"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">g(X)Y</span><span class="co">]</span>=\mathbb{E}<span class="co">[</span><span class="ot">g(X)\mathbb{E}[Y|X]</span><span class="co">]</span>\label{eq:definition-conditional-expectation}$$</span>
<span id="cb5-166"><a href="#cb5-166"></a></span>
<span id="cb5-167"><a href="#cb5-167"></a>We can intepret the second property as follows. The conditional</span>
<span id="cb5-168"><a href="#cb5-168"></a>expectation $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ serves as a proxy for $Y$ as far as $X$ is</span>
<span id="cb5-169"><a href="#cb5-169"></a>concerned. Note that in equation</span>
<span id="cb5-170"><a href="#cb5-170"></a>(<span class="co">[</span><span class="ot">\[eq:definition-conditional-expectation\]</span><span class="co">](#eq:definition-conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-171"><a href="#cb5-171"></a>reference="eq:definition-conditional-expectation"}), the expectation on</span>
<span id="cb5-172"><a href="#cb5-172"></a>the left can be seen as an average over the joint values of $(X,Y)$,</span>
<span id="cb5-173"><a href="#cb5-173"></a>whereas the one on the right is an average over the values of $X$ only!</span>
<span id="cb5-174"><a href="#cb5-174"></a>Another way to see this property is to write is as:</span>
<span id="cb5-175"><a href="#cb5-175"></a></span>
<span id="cb5-176"><a href="#cb5-176"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">g(X)(Y-\mathbb{E}[Y|X])</span><span class="co">]</span>=0$$</span>
<span id="cb5-177"><a href="#cb5-177"></a></span>
<span id="cb5-178"><a href="#cb5-178"></a>In other words, the *random variable $Y-\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ is orthogonal</span>
<span id="cb5-179"><a href="#cb5-179"></a>to any random variable constructed from $X$.*</span>
<span id="cb5-180"><a href="#cb5-180"></a></span>
<span id="cb5-181"><a href="#cb5-181"></a>Finally, it is important to notice that if we take $g(X)=1$, then the</span>
<span id="cb5-182"><a href="#cb5-182"></a>second property implies :</span>
<span id="cb5-183"><a href="#cb5-183"></a></span>
<span id="cb5-184"><a href="#cb5-184"></a>$$\begin{aligned}</span>
<span id="cb5-185"><a href="#cb5-185"></a>\mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">\mathbb{E}[Y|X]</span><span class="co">]</span></span>
<span id="cb5-186"><a href="#cb5-186"></a>\end{aligned}$$</span>
<span id="cb5-187"><a href="#cb5-187"></a></span>
<span id="cb5-188"><a href="#cb5-188"></a>In other words, the expectation of the conditional expectation of $Y$ is</span>
<span id="cb5-189"><a href="#cb5-189"></a>simply the expectation of $Y$.</span>
<span id="cb5-190"><a href="#cb5-190"></a></span>
<span id="cb5-191"><a href="#cb5-191"></a>The existence of the conditional expectation $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ is not</span>
<span id="cb5-192"><a href="#cb5-192"></a>obvious. We know, it exists in particular cases given in example</span>
<span id="cb5-193"><a href="#cb5-193"></a>(<span class="co">[</span><span class="ot">\[ex:elementary-definitions-of-conditional-expectation\]</span><span class="co">](#ex:elementary-definitions-of-conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-194"><a href="#cb5-194"></a>reference="ex:elementary-definitions-of-conditional-expectation"}). We</span>
<span id="cb5-195"><a href="#cb5-195"></a>will show more generally, that it exists, it is unique whenever $Y$ is</span>
<span id="cb5-196"><a href="#cb5-196"></a>in $L^{2}(\Omega,\mathcal{F},\mathbb{P})$ (In fact, it can be shown to</span>
<span id="cb5-197"><a href="#cb5-197"></a>exist whenever $Y$ is integrable). Before doing so, let's warm up by</span>
<span id="cb5-198"><a href="#cb5-198"></a>looking at the case of Gaussian vectors.</span>
<span id="cb5-199"><a href="#cb5-199"></a>:::</span>
<span id="cb5-200"><a href="#cb5-200"></a></span>
<span id="cb5-201"><a href="#cb5-201"></a>::: example</span>
<span id="cb5-202"><a href="#cb5-202"></a>[]{#ex:conditional-expectation-of-gaussian-vectors</span>
<span id="cb5-203"><a href="#cb5-203"></a>label="ex:conditional-expectation-of-gaussian-vectors"}(Conditional</span>
<span id="cb5-204"><a href="#cb5-204"></a>expectation of Gaussian vectors - I). Let $(X,Y)$ be a Gaussian vector</span>
<span id="cb5-205"><a href="#cb5-205"></a>of mean $0$. Then:</span>
<span id="cb5-206"><a href="#cb5-206"></a></span>
<span id="cb5-207"><a href="#cb5-207"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>=\frac{\mathbb{E}<span class="co">[</span><span class="ot">XY</span><span class="co">]</span>}{\mathbb{E}<span class="co">[</span><span class="ot">X^{2}</span><span class="co">]</span>}X\label{eq:conditional-expectation-of-gaussian-vector}$$</span>
<span id="cb5-208"><a href="#cb5-208"></a></span>
<span id="cb5-209"><a href="#cb5-209"></a>This candidate satisfies the two defining properties of conditional</span>
<span id="cb5-210"><a href="#cb5-210"></a>expectation : (A) It is clearly a function of $X$; in fact it is a</span>
<span id="cb5-211"><a href="#cb5-211"></a>simple multiple of $X$. (B) We have that the random variable</span>
<span id="cb5-212"><a href="#cb5-212"></a>$\left(Y-\frac{\mathbb{E}<span class="co">[</span><span class="ot">XY</span><span class="co">]</span>}{\mathbb{E}<span class="co">[</span><span class="ot">X^{2}</span><span class="co">]</span>}X\right)$ is orthogonal</span>
<span id="cb5-213"><a href="#cb5-213"></a>and thus independent to $X$. This is a consequence of the proposition</span>
<span id="cb5-214"><a href="#cb5-214"></a>(<span class="co">[</span><span class="ot">\[prop:diagonal-cov-matrix-implies-independence-of-gaussians\]</span><span class="co">](#prop:diagonal-cov-matrix-implies-independence-of-gaussians)</span>{reference-type="ref"</span>
<span id="cb5-215"><a href="#cb5-215"></a>reference="prop:diagonal-cov-matrix-implies-independence-of-gaussians"}),</span>
<span id="cb5-216"><a href="#cb5-216"></a>since:</span>
<span id="cb5-217"><a href="#cb5-217"></a></span>
<span id="cb5-218"><a href="#cb5-218"></a>$$\begin{aligned}</span>
<span id="cb5-219"><a href="#cb5-219"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">X\left(Y-\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X\right)\right</span><span class="co">]</span> &amp; =\mathbb{E}XY-\frac{\mathbb{E}<span class="co">[</span><span class="ot">XY</span><span class="co">]</span>}{\mathbb{E}<span class="co">[</span><span class="ot">X^{2}</span><span class="co">]</span>}\mathbb{E}X^{2}<span class="sc">\\</span></span>
<span id="cb5-220"><a href="#cb5-220"></a> &amp; =\mathbb{E}XY-\frac{\mathbb{E}<span class="co">[</span><span class="ot">XY</span><span class="co">]</span>}{\cancel{\mathbb{E}<span class="co">[</span><span class="ot">X^{2}</span><span class="co">]</span>}}\cancel{\mathbb{E}X^{2}}<span class="sc">\\</span></span>
<span id="cb5-221"><a href="#cb5-221"></a> &amp; =0</span>
<span id="cb5-222"><a href="#cb5-222"></a>\end{aligned}$$</span>
<span id="cb5-223"><a href="#cb5-223"></a></span>
<span id="cb5-224"><a href="#cb5-224"></a>Therefore, we have for any bounded function $g(X)$ of $X$:</span>
<span id="cb5-225"><a href="#cb5-225"></a></span>
<span id="cb5-226"><a href="#cb5-226"></a>$$\begin{aligned}</span>
<span id="cb5-227"><a href="#cb5-227"></a>\mathbb{E}<span class="co">[</span><span class="ot">g(X)(Y-\mathbb{E}(Y|X))</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">g(X)</span><span class="co">]</span>\mathbb{E}<span class="co">[</span><span class="ot">Y-\mathbb{E}[Y|X]</span><span class="co">]</span>=0</span>
<span id="cb5-228"><a href="#cb5-228"></a>\end{aligned}$$</span>
<span id="cb5-229"><a href="#cb5-229"></a>:::</span>
<span id="cb5-230"><a href="#cb5-230"></a></span>
<span id="cb5-231"><a href="#cb5-231"></a>::: example</span>
<span id="cb5-232"><a href="#cb5-232"></a>[]{#ex:brownian-conditioning-I</span>
<span id="cb5-233"><a href="#cb5-233"></a>label="ex:brownian-conditioning-I"}(Brownian conditioning-I) Let</span>
<span id="cb5-234"><a href="#cb5-234"></a>$(B_{t},t\geq0)$ be a standard Brownian motion. Consider the Gaussian</span>
<span id="cb5-235"><a href="#cb5-235"></a>vector $(B_{1/2},B_{1})$. Its covariance matrix is:</span>
<span id="cb5-236"><a href="#cb5-236"></a></span>
<span id="cb5-237"><a href="#cb5-237"></a>$$\begin{aligned}</span>
<span id="cb5-238"><a href="#cb5-238"></a>C &amp; =\left[\begin{array}{cc}</span>
<span id="cb5-239"><a href="#cb5-239"></a>1/2 &amp; 1/2<span class="sc">\\</span></span>
<span id="cb5-240"><a href="#cb5-240"></a>1/2 &amp; 1</span>
<span id="cb5-241"><a href="#cb5-241"></a>\end{array}\right]</span>
<span id="cb5-242"><a href="#cb5-242"></a>\end{aligned}$$</span>
<span id="cb5-243"><a href="#cb5-243"></a></span>
<span id="cb5-244"><a href="#cb5-244"></a>Let's compute $\mathbb{E}<span class="co">[</span><span class="ot">B_{1}|B_{1/2}</span><span class="co">]</span>$ and</span>
<span id="cb5-245"><a href="#cb5-245"></a>$\mathbb{E}<span class="co">[</span><span class="ot">B_{1/2}|B_{1}</span><span class="co">]</span>$. This is easy using the equation</span>
<span id="cb5-246"><a href="#cb5-246"></a>(<span class="co">[</span><span class="ot">\[eq:conditional-expectation-of-gaussian-vector\]</span><span class="co">](#eq:conditional-expectation-of-gaussian-vector)</span>{reference-type="ref"</span>
<span id="cb5-247"><a href="#cb5-247"></a>reference="eq:conditional-expectation-of-gaussian-vector"}). We have:</span>
<span id="cb5-248"><a href="#cb5-248"></a></span>
<span id="cb5-249"><a href="#cb5-249"></a>$$\begin{aligned}</span>
<span id="cb5-250"><a href="#cb5-250"></a>\mathbb{E}<span class="co">[</span><span class="ot">B_{1}|B_{1/2}</span><span class="co">]</span> &amp; =\frac{\mathbb{E}<span class="co">[</span><span class="ot">B_{1}B_{1/2}</span><span class="co">]</span>}{\mathbb{E}<span class="co">[</span><span class="ot">B_{1/2}^{2}</span><span class="co">]</span>}B_{1/2}<span class="sc">\\</span></span>
<span id="cb5-251"><a href="#cb5-251"></a> &amp; =\frac{(1/2)}{(1/2)}B_{1/2}<span class="sc">\\</span></span>
<span id="cb5-252"><a href="#cb5-252"></a> &amp; =B_{1/2}</span>
<span id="cb5-253"><a href="#cb5-253"></a>\end{aligned}$$</span>
<span id="cb5-254"><a href="#cb5-254"></a></span>
<span id="cb5-255"><a href="#cb5-255"></a>In other words, the best approximation of $B_{1}$ given the information</span>
<span id="cb5-256"><a href="#cb5-256"></a>of $B_{1/2}$ is $B_{1/2}$. There is no problem in computing</span>
<span id="cb5-257"><a href="#cb5-257"></a>$\mathbb{E}<span class="co">[</span><span class="ot">B_{1/2}|B_{1}</span><span class="co">]</span>$, even though we are conditioning on a future</span>
<span id="cb5-258"><a href="#cb5-258"></a>position. Indeed the same formula gives</span>
<span id="cb5-259"><a href="#cb5-259"></a></span>
<span id="cb5-260"><a href="#cb5-260"></a>$$\begin{aligned}</span>
<span id="cb5-261"><a href="#cb5-261"></a>\mathbb{E}<span class="co">[</span><span class="ot">B_{1/2}|B_{1}</span><span class="co">]</span> &amp; =\frac{\mathbb{E}<span class="co">[</span><span class="ot">B_{1}B_{1/2}</span><span class="co">]</span>}{\mathbb{E}<span class="co">[</span><span class="ot">B_{1}^{2}</span><span class="co">]</span>}B_{1}=\frac{1}{2}B_{1}</span>
<span id="cb5-262"><a href="#cb5-262"></a>\end{aligned}$$</span>
<span id="cb5-263"><a href="#cb5-263"></a></span>
<span id="cb5-264"><a href="#cb5-264"></a>This means that the best approximation of $B_{1/2}$ given the position</span>
<span id="cb5-265"><a href="#cb5-265"></a>at time $1$, is $\frac{1}{2}B_{1}$ which makes a whole lot of sense!</span>
<span id="cb5-266"><a href="#cb5-266"></a>:::</span>
<span id="cb5-267"><a href="#cb5-267"></a></span>
<span id="cb5-268"><a href="#cb5-268"></a>In example</span>
<span id="cb5-269"><a href="#cb5-269"></a>(<span class="co">[</span><span class="ot">\[eq:conditional-expectation-of-gaussian-vector\]</span><span class="co">](#eq:conditional-expectation-of-gaussian-vector)</span>{reference-type="ref"</span>
<span id="cb5-270"><a href="#cb5-270"></a>reference="eq:conditional-expectation-of-gaussian-vector"}) for the</span>
<span id="cb5-271"><a href="#cb5-271"></a>Gaussian vector $(X,Y)$, the conditional expectation was equal to the</span>
<span id="cb5-272"><a href="#cb5-272"></a>*orthogonal projection* of $Y$ onto $X$ in $L^{2}$. In particular, the</span>
<span id="cb5-273"><a href="#cb5-273"></a>conditional expectation was a multiple of $X$. Is this always the case?</span>
<span id="cb5-274"><a href="#cb5-274"></a>Unfortunately, it is not. For example, in the equation</span>
<span id="cb5-275"><a href="#cb5-275"></a>(<span class="co">[</span><span class="ot">\[eq:elementary-conditional-expectation-example\]</span><span class="co">](#eq:elementary-conditional-expectation-example)</span>{reference-type="ref"</span>
<span id="cb5-276"><a href="#cb5-276"></a>reference="eq:elementary-conditional-expectation-example"}), the</span>
<span id="cb5-277"><a href="#cb5-277"></a>conditional expectation is clearly not a multiple of the random variable</span>
<span id="cb5-278"><a href="#cb5-278"></a>$X$. However, it is a function of $X$, as is always the case by</span>
<span id="cb5-279"><a href="#cb5-279"></a>definition</span>
<span id="cb5-280"><a href="#cb5-280"></a>(<span class="co">[</span><span class="ot">\[def:conditional-expectation\]</span><span class="co">](#def:conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-281"><a href="#cb5-281"></a>reference="def:conditional-expectation"}).</span>
<span id="cb5-282"><a href="#cb5-282"></a></span>
<span id="cb5-283"><a href="#cb5-283"></a>The idea to construct the conditional expectation $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ in</span>
<span id="cb5-284"><a href="#cb5-284"></a>general is to *project $Y$ on the space of all random variables that can</span>
<span id="cb5-285"><a href="#cb5-285"></a>be constructed from $X$. To make this precise, consider the following</span>
<span id="cb5-286"><a href="#cb5-286"></a>subspace of $L^{2}(\Omega,\mathcal{F},\mathbb{P})$ :*</span>
<span id="cb5-287"><a href="#cb5-287"></a></span>
<span id="cb5-288"><a href="#cb5-288"></a>::: defn</span>
<span id="cb5-289"><a href="#cb5-289"></a>Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $X$ a</span>
<span id="cb5-290"><a href="#cb5-290"></a>random variable defined on it. The space</span>
<span id="cb5-291"><a href="#cb5-291"></a>$L^{2}(\Omega,\sigma(X),\mathbb{P})$ is the linear subspace of</span>
<span id="cb5-292"><a href="#cb5-292"></a>$L^{2}(\Omega,\mathcal{F},\mathbb{P})$ consisting of the</span>
<span id="cb5-293"><a href="#cb5-293"></a>square-integrable random variables of the form $g(X)$ for some function</span>
<span id="cb5-294"><a href="#cb5-294"></a>$g:\mathbf{R}\to\mathbf{R}$.</span>
<span id="cb5-295"><a href="#cb5-295"></a>:::</span>
<span id="cb5-296"><a href="#cb5-296"></a></span>
<span id="cb5-297"><a href="#cb5-297"></a>This is a linear subspace of $L^{2}(\Omega,\mathcal{F},\mathbb{P})$: It</span>
<span id="cb5-298"><a href="#cb5-298"></a>contains the random variable $0$, and any linear combination of random</span>
<span id="cb5-299"><a href="#cb5-299"></a>variables of this kind is also a function of $X$ and must have a finite</span>
<span id="cb5-300"><a href="#cb5-300"></a>second moment. We note the following:</span>
<span id="cb5-301"><a href="#cb5-301"></a></span>
<span id="cb5-302"><a href="#cb5-302"></a>::: rem*</span>
<span id="cb5-303"><a href="#cb5-303"></a>$L^{2}(\Omega,\sigma(X),\mathbb{P})$ is a subspace of</span>
<span id="cb5-304"><a href="#cb5-304"></a>$L^{2}(\Omega,\mathcal{F},\mathbb{P})$, very much how a plane or line</span>
<span id="cb5-305"><a href="#cb5-305"></a>(going through the origin) is a subspace of $\mathbf{R}^{3}$.</span>
<span id="cb5-306"><a href="#cb5-306"></a></span>
<span id="cb5-307"><a href="#cb5-307"></a>In particular, as in the case of a line or a plane, we can project an</span>
<span id="cb5-308"><a href="#cb5-308"></a>element of $Y$ of $L^{2}(\Omega,\mathcal{F},\mathbb{P})$ onto</span>
<span id="cb5-309"><a href="#cb5-309"></a>$L^{2}(\Omega,\sigma(X),\mathbb{P})$. The resulting projection is an</span>
<span id="cb5-310"><a href="#cb5-310"></a>element of $L^{2}(\Omega,\sigma(X),\mathbb{P})$, a square-integrable</span>
<span id="cb5-311"><a href="#cb5-311"></a>random-variable that is a function of $X$. For a subspace $\mathcal{S}$</span>
<span id="cb5-312"><a href="#cb5-312"></a>of $\mathbf{R}^{3}$ (e.g. a line or a plane), the projection of the</span>
<span id="cb5-313"><a href="#cb5-313"></a>vector $\mathbf{v}\in\mathbf{R}^{3}$ onto the subspace $\mathcal{S}$,</span>
<span id="cb5-314"><a href="#cb5-314"></a>denoted $\text{Proj}_{\mathcal{S}}(\mathbf{v})$ is the closest point to</span>
<span id="cb5-315"><a href="#cb5-315"></a>$\mathbf{v}$ lying in the subspace $\mathcal{S}$. Moreover,</span>
<span id="cb5-316"><a href="#cb5-316"></a>$\mathbf{v}-\text{Proj}_{\mathcal{S}}(\mathbf{v})$ is orthogonal to the</span>
<span id="cb5-317"><a href="#cb5-317"></a>subspace. This picture of orthogonal projection also holds in $L^{2}$.</span>
<span id="cb5-318"><a href="#cb5-318"></a>Let $Y$ be a random variable in $L^{2}(\Omega,\mathcal{F},\mathbb{P})$</span>
<span id="cb5-319"><a href="#cb5-319"></a>and let $L^{2}(\Omega,\sigma(X),\mathbb{P})$ be the subspace of those</span>
<span id="cb5-320"><a href="#cb5-320"></a>random variables that are functions of $X$. We write $Y^{\star}$ for the</span>
<span id="cb5-321"><a href="#cb5-321"></a>random variable in $L^{2}(\Omega,\sigma(X),\mathbb{P})$ that is</span>
<span id="cb5-322"><a href="#cb5-322"></a>*closest* to $Y$. In other words, we have (using the definition of the</span>
<span id="cb5-323"><a href="#cb5-323"></a>$L^{2}$-distance square):</span>
<span id="cb5-324"><a href="#cb5-324"></a></span>
<span id="cb5-325"><a href="#cb5-325"></a>$$\inf_{Z\in L^{2}(\Omega,\sigma(X),\mathbb{P})}\mathbb{E}<span class="co">[</span><span class="ot">(Y-Z)^{2}</span><span class="co">]</span>=\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y^{\star})^{2}</span><span class="co">]</span>\label{eq:Y-star-is-the-closest-to-Y-in-L2-sense}$$</span>
<span id="cb5-326"><a href="#cb5-326"></a>:::</span>
<span id="cb5-327"><a href="#cb5-327"></a></span>
<span id="cb5-328"><a href="#cb5-328"></a>It turns out that $Y^{\star}$ is the right candidate for the conditional</span>
<span id="cb5-329"><a href="#cb5-329"></a>expectation.</span>
<span id="cb5-330"><a href="#cb5-330"></a></span>
<span id="cb5-331"><a href="#cb5-331"></a>::: center</span>
<span id="cb5-332"><a href="#cb5-332"></a>Figure. An illustration of the conditional expectation $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$</span>
<span id="cb5-333"><a href="#cb5-333"></a>as an orthogonal projection of $Y$ onto the subspace</span>
<span id="cb5-334"><a href="#cb5-334"></a>$L^2(\Omega,\sigma(X),\mathbb{P})$.</span>
<span id="cb5-335"><a href="#cb5-335"></a>:::</span>
<span id="cb5-336"><a href="#cb5-336"></a></span>
<span id="cb5-337"><a href="#cb5-337"></a>::: thm</span>
<span id="cb5-338"><a href="#cb5-338"></a>[]{#th:existence-and-uniqueness-of-the-conditional-expectation</span>
<span id="cb5-339"><a href="#cb5-339"></a>label="th:existence-and-uniqueness-of-the-conditional-expectation"}(Existence</span>
<span id="cb5-340"><a href="#cb5-340"></a>and uniqueness of the conditional expectation) Let $X$ be a random</span>
<span id="cb5-341"><a href="#cb5-341"></a>variable on $(\Omega,\mathcal{F},\mathbb{P})$. Let $Y$ be a random</span>
<span id="cb5-342"><a href="#cb5-342"></a>variable in $L^{2}(\Omega,\mathcal{F},\mathbb{P})$. Then the conditional</span>
<span id="cb5-343"><a href="#cb5-343"></a>expectation $\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ is the random variable $Y^{\star}$ given</span>
<span id="cb5-344"><a href="#cb5-344"></a>in the equation</span>
<span id="cb5-345"><a href="#cb5-345"></a>(<span class="co">[</span><span class="ot">\[eq:Y-star-is-the-closest-to-Y-in-L2-sense\]</span><span class="co">](#eq:Y-star-is-the-closest-to-Y-in-L2-sense)</span>{reference-type="ref"</span>
<span id="cb5-346"><a href="#cb5-346"></a>reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense"}). Namely, it is</span>
<span id="cb5-347"><a href="#cb5-347"></a>the random variable in $L^{2}(\Omega,\sigma(X),\mathbb{P})$ that is</span>
<span id="cb5-348"><a href="#cb5-348"></a>closest to $Y$ in the $L^{2}$-distance.</span>
<span id="cb5-349"><a href="#cb5-349"></a></span>
<span id="cb5-350"><a href="#cb5-350"></a>In particular we have the following:</span>
<span id="cb5-351"><a href="#cb5-351"></a></span>
<span id="cb5-352"><a href="#cb5-352"></a>1<span class="sc">\)</span> It is the orthogonal projection of $Y$ onto</span>
<span id="cb5-353"><a href="#cb5-353"></a>$L^{2}(\Omega,\sigma(X),\mathbb{P})$, that is $Y-Y^{\star}$ is</span>
<span id="cb5-354"><a href="#cb5-354"></a>orthogonal to any random variables in the subspace</span>
<span id="cb5-355"><a href="#cb5-355"></a>$L^{2}(\Omega,\sigma(X),\mathbb{P})$.</span>
<span id="cb5-356"><a href="#cb5-356"></a></span>
<span id="cb5-357"><a href="#cb5-357"></a>2<span class="sc">\)</span> It is unique.</span>
<span id="cb5-358"><a href="#cb5-358"></a>:::</span>
<span id="cb5-359"><a href="#cb5-359"></a></span>
<span id="cb5-360"><a href="#cb5-360"></a>::: rem*</span>
<span id="cb5-361"><a href="#cb5-361"></a>This result reinforces the meaning of the conditional expectation</span>
<span id="cb5-362"><a href="#cb5-362"></a>$\mathbb{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ as the best estimation of $Y$ given the information of</span>
<span id="cb5-363"><a href="#cb5-363"></a>$X$: it is the closest random variable to $Y$ among all the functions of</span>
<span id="cb5-364"><a href="#cb5-364"></a>$X$ in the sense of $L^{2}$.</span>
<span id="cb5-365"><a href="#cb5-365"></a>:::</span>
<span id="cb5-366"><a href="#cb5-366"></a></span>
<span id="cb5-367"><a href="#cb5-367"></a>::: proof</span>
<span id="cb5-368"><a href="#cb5-368"></a>*Proof.* We write for short $L^{2}(X)$ for the subspace</span>
<span id="cb5-369"><a href="#cb5-369"></a>$L^{2}(\Omega,\sigma(X),\mathbb{P})$. Let $Y^{\star}$ be as in equation</span>
<span id="cb5-370"><a href="#cb5-370"></a>(<span class="co">[</span><span class="ot">\[eq:Y-star-is-the-closest-to-Y-in-L2-sense\]</span><span class="co">](#eq:Y-star-is-the-closest-to-Y-in-L2-sense)</span>{reference-type="ref"</span>
<span id="cb5-371"><a href="#cb5-371"></a>reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense"}). We show</span>
<span id="cb5-372"><a href="#cb5-372"></a>successively that (1) $Y-Y^{\star}$ is orthogonal to any element of</span>
<span id="cb5-373"><a href="#cb5-373"></a>$L^{2}(X)$, so it is the orthogonal projection (2) $Y^{\star}$ has the</span>
<span id="cb5-374"><a href="#cb5-374"></a>properties of conditional expectation in definition</span>
<span id="cb5-375"><a href="#cb5-375"></a>(<span class="co">[</span><span class="ot">\[eq:definition-conditional-expectation\]</span><span class="co">](#eq:definition-conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-376"><a href="#cb5-376"></a>reference="eq:definition-conditional-expectation"}) (3) $Y^{\star}$ is</span>
<span id="cb5-377"><a href="#cb5-377"></a>unique.</span>
<span id="cb5-378"><a href="#cb5-378"></a></span>
<span id="cb5-379"><a href="#cb5-379"></a><span class="sc">\(</span>1<span class="sc">\)</span> Let $W=g(X)$ be a random variable in $L^{2}(X)$. We show that $W$</span>
<span id="cb5-380"><a href="#cb5-380"></a>is orthogonal to $Y-Y^{\star}$; that is $\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y^{\star})W</span><span class="co">]</span>=0$.</span>
<span id="cb5-381"><a href="#cb5-381"></a>This should be intuitively clear from figure above. On the one hand, we</span>
<span id="cb5-382"><a href="#cb5-382"></a>have by developing the square:</span>
<span id="cb5-383"><a href="#cb5-383"></a></span>
<span id="cb5-384"><a href="#cb5-384"></a>$$\begin{aligned}</span>
<span id="cb5-385"><a href="#cb5-385"></a>\mathbb{E}<span class="co">[</span><span class="ot">(W-(Y-Y^{\star}))^{2}</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">W^{2}-2W(Y-Y^{\star})+(Y-Y^{\star})^{2}</span><span class="co">]</span>\nonumber <span class="sc">\\</span></span>
<span id="cb5-386"><a href="#cb5-386"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">W^{2}</span><span class="co">]</span>-2\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>+\mathbb{E}(Y-Y^{\star})^{2}]\label{eq:developing-the-square}</span>
<span id="cb5-387"><a href="#cb5-387"></a>\end{aligned}$$</span>
<span id="cb5-388"><a href="#cb5-388"></a></span>
<span id="cb5-389"><a href="#cb5-389"></a>On the other hand, $Y^{\star}+W$ is an arbitrary vector in $L^{2}(X)$(it</span>
<span id="cb5-390"><a href="#cb5-390"></a>is a linear combination of the elements in $L^{2}(X)$), we must have</span>
<span id="cb5-391"><a href="#cb5-391"></a>from equation</span>
<span id="cb5-392"><a href="#cb5-392"></a>(<span class="co">[</span><span class="ot">\[eq:Y-star-is-the-closest-to-Y-in-L2-sense\]</span><span class="co">](#eq:Y-star-is-the-closest-to-Y-in-L2-sense)</span>{reference-type="ref"</span>
<span id="cb5-393"><a href="#cb5-393"></a>reference="eq:Y-star-is-the-closest-to-Y-in-L2-sense"}):</span>
<span id="cb5-394"><a href="#cb5-394"></a></span>
<span id="cb5-395"><a href="#cb5-395"></a>$$\begin{aligned}</span>
<span id="cb5-396"><a href="#cb5-396"></a>\mathbb{E}<span class="co">[</span><span class="ot">(W-(Y-Y^{\star}))^{2}</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">(Y-(Y^{\star}+W))^{2}</span><span class="co">]</span>\nonumber <span class="sc">\\</span></span>
<span id="cb5-397"><a href="#cb5-397"></a> &amp; \geq\inf_{Z\in L^{2}(X)}\mathbb{E}<span class="co">[</span><span class="ot">(Y-Z)^{2}</span><span class="co">]</span>\nonumber <span class="sc">\\</span></span>
<span id="cb5-398"><a href="#cb5-398"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y^{\star})^{2}</span><span class="co">]</span>\label{eq:lower-bound}</span>
<span id="cb5-399"><a href="#cb5-399"></a>\end{aligned}$$</span>
<span id="cb5-400"><a href="#cb5-400"></a></span>
<span id="cb5-401"><a href="#cb5-401"></a>Putting the last two equations</span>
<span id="cb5-402"><a href="#cb5-402"></a>(<span class="co">[</span><span class="ot">\[eq:developing-the-square\]</span><span class="co">](#eq:developing-the-square)</span>{reference-type="ref"</span>
<span id="cb5-403"><a href="#cb5-403"></a>reference="eq:developing-the-square"}),</span>
<span id="cb5-404"><a href="#cb5-404"></a>(<span class="co">[</span><span class="ot">\[eq:lower-bound\]</span><span class="co">](#eq:lower-bound)</span>{reference-type="ref"</span>
<span id="cb5-405"><a href="#cb5-405"></a>reference="eq:lower-bound"}) together, we get that for any</span>
<span id="cb5-406"><a href="#cb5-406"></a>$W\in L^{2}(X)$:</span>
<span id="cb5-407"><a href="#cb5-407"></a></span>
<span id="cb5-408"><a href="#cb5-408"></a>$$\begin{aligned}</span>
<span id="cb5-409"><a href="#cb5-409"></a>\mathbb{E}<span class="co">[</span><span class="ot">W^{2}</span><span class="co">]</span>-2\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span> &amp; \geq0</span>
<span id="cb5-410"><a href="#cb5-410"></a>\end{aligned}$$</span>
<span id="cb5-411"><a href="#cb5-411"></a></span>
<span id="cb5-412"><a href="#cb5-412"></a>In particular, this also holds for $aW$, in which case we get:</span>
<span id="cb5-413"><a href="#cb5-413"></a></span>
<span id="cb5-414"><a href="#cb5-414"></a>$$\begin{aligned}</span>
<span id="cb5-415"><a href="#cb5-415"></a>a^{2}\mathbb{E}<span class="co">[</span><span class="ot">W^{2}</span><span class="co">]</span>-2a\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span> &amp; \geq0<span class="sc">\\</span></span>
<span id="cb5-416"><a href="#cb5-416"></a>\implies a\left<span class="sc">\{</span> a\mathbb{E}<span class="co">[</span><span class="ot">W^{2}</span><span class="co">]</span>-2\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>\right<span class="sc">\}</span>  &amp; \geq0</span>
<span id="cb5-417"><a href="#cb5-417"></a>\end{aligned}$$</span>
<span id="cb5-418"><a href="#cb5-418"></a></span>
<span id="cb5-419"><a href="#cb5-419"></a>If $a&gt;0$, then:</span>
<span id="cb5-420"><a href="#cb5-420"></a></span>
<span id="cb5-421"><a href="#cb5-421"></a>$$a\mathbb{E}<span class="co">[</span><span class="ot">W^{2}</span><span class="co">]</span>-2\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>\geq0\label{eq:case-when-a-gt-zero}$$</span>
<span id="cb5-422"><a href="#cb5-422"></a></span>
<span id="cb5-423"><a href="#cb5-423"></a>whereas if $a&lt;0$, then the sign changes upon dividing throughout by $a$,</span>
<span id="cb5-424"><a href="#cb5-424"></a>and we have:</span>
<span id="cb5-425"><a href="#cb5-425"></a></span>
<span id="cb5-426"><a href="#cb5-426"></a>$$a\mathbb{E}<span class="co">[</span><span class="ot">W^{2}</span><span class="co">]</span>-2\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>\leq0\label{eq:case-when-a-lt-zero}$$</span>
<span id="cb5-427"><a href="#cb5-427"></a></span>
<span id="cb5-428"><a href="#cb5-428"></a>Rearranging</span>
<span id="cb5-429"><a href="#cb5-429"></a>(<span class="co">[</span><span class="ot">\[eq:case-when-a-gt-zero\]</span><span class="co">](#eq:case-when-a-gt-zero)</span>{reference-type="ref"</span>
<span id="cb5-430"><a href="#cb5-430"></a>reference="eq:case-when-a-gt-zero"}) yields:</span>
<span id="cb5-431"><a href="#cb5-431"></a></span>
<span id="cb5-432"><a href="#cb5-432"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>\leq a\mathbb{E}<span class="co">[</span><span class="ot">W^{2}</span><span class="co">]</span>/2\label{eq:case-when-a-gt-zero-rearranged}$$</span>
<span id="cb5-433"><a href="#cb5-433"></a></span>
<span id="cb5-434"><a href="#cb5-434"></a>Rearranging</span>
<span id="cb5-435"><a href="#cb5-435"></a>(<span class="co">[</span><span class="ot">\[eq:case-when-a-lt-zero\]</span><span class="co">](#eq:case-when-a-lt-zero)</span>{reference-type="ref"</span>
<span id="cb5-436"><a href="#cb5-436"></a>reference="eq:case-when-a-lt-zero"}) yields:</span>
<span id="cb5-437"><a href="#cb5-437"></a></span>
<span id="cb5-438"><a href="#cb5-438"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>\geq a\mathbb{E}<span class="co">[</span><span class="ot">W^{2}</span><span class="co">]</span>/2\label{eq:case-when-a-lt-zero-rearranged}$$</span>
<span id="cb5-439"><a href="#cb5-439"></a></span>
<span id="cb5-440"><a href="#cb5-440"></a>Since</span>
<span id="cb5-441"><a href="#cb5-441"></a>(<span class="co">[</span><span class="ot">\[eq:case-when-a-gt-zero-rearranged\]</span><span class="co">](#eq:case-when-a-gt-zero-rearranged)</span>{reference-type="ref"</span>
<span id="cb5-442"><a href="#cb5-442"></a>reference="eq:case-when-a-gt-zero-rearranged"}) holds for all $a&gt;0$, the</span>
<span id="cb5-443"><a href="#cb5-443"></a>stronger inequality, $\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>\leq0$ must hold. Since,</span>
<span id="cb5-444"><a href="#cb5-444"></a>(<span class="co">[</span><span class="ot">\[eq:case-when-a-lt-zero-rearranged\]</span><span class="co">](#eq:case-when-a-lt-zero-rearranged)</span>{reference-type="ref"</span>
<span id="cb5-445"><a href="#cb5-445"></a>reference="eq:case-when-a-lt-zero-rearranged"}) holds for all $a&lt;0$, the</span>
<span id="cb5-446"><a href="#cb5-446"></a>stronger inequality $\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>\geq0$ must hold.</span>
<span id="cb5-447"><a href="#cb5-447"></a>Consequently,</span>
<span id="cb5-448"><a href="#cb5-448"></a></span>
<span id="cb5-449"><a href="#cb5-449"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span>=0$$</span>
<span id="cb5-450"><a href="#cb5-450"></a></span>
<span id="cb5-451"><a href="#cb5-451"></a><span class="sc">\(</span>2<span class="sc">\)</span> It is clear that $Y^{\star}$ is a function of $X$ by construction,</span>
<span id="cb5-452"><a href="#cb5-452"></a>since it is in $L^{2}(X)$. Moreover, for any $W\in L^{2}(X)$, we have</span>
<span id="cb5-453"><a href="#cb5-453"></a>from (1) that:</span>
<span id="cb5-454"><a href="#cb5-454"></a></span>
<span id="cb5-455"><a href="#cb5-455"></a>$$\begin{aligned}</span>
<span id="cb5-456"><a href="#cb5-456"></a>\mathbb{E}<span class="co">[</span><span class="ot">W(Y-Y^{\star})</span><span class="co">]</span> &amp; =0</span>
<span id="cb5-457"><a href="#cb5-457"></a>\end{aligned}$$</span>
<span id="cb5-458"><a href="#cb5-458"></a></span>
<span id="cb5-459"><a href="#cb5-459"></a>which is the second defining property of conditional expectations.</span>
<span id="cb5-460"><a href="#cb5-460"></a></span>
<span id="cb5-461"><a href="#cb5-461"></a><span class="sc">\(</span>3<span class="sc">\)</span> Lastly, suppose there is another element $Y'$ that is in</span>
<span id="cb5-462"><a href="#cb5-462"></a>$L^{2}(X)$ that minimizes the distance to $Y$. Then we would get:</span>
<span id="cb5-463"><a href="#cb5-463"></a></span>
<span id="cb5-464"><a href="#cb5-464"></a>$$\begin{aligned}</span>
<span id="cb5-465"><a href="#cb5-465"></a>\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y')^{2}</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y^{\star}+Y^{\star}-Y')^{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-466"><a href="#cb5-466"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y^{\star})^{2}</span><span class="co">]</span>+2\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y^{\star})(Y^{\star}-Y')</span><span class="co">]</span>+\mathbb{E}<span class="co">[</span><span class="ot">(Y^{\star}-Y')^{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-467"><a href="#cb5-467"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y^{\star})^{2}</span><span class="co">]</span>+0+\mathbb{E}<span class="co">[</span><span class="ot">(Y^{\star}-Y')^{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-468"><a href="#cb5-468"></a> &amp; \quad\left<span class="sc">\{</span> (Y^{\star}-Y')\in L^{2}(X)\perp(Y-Y^{\star})\right<span class="sc">\}</span> </span>
<span id="cb5-469"><a href="#cb5-469"></a>\end{aligned}$$</span>
<span id="cb5-470"><a href="#cb5-470"></a></span>
<span id="cb5-471"><a href="#cb5-471"></a>where we used the fact, that $Y^{\star}-Y'$ is a vector in $L^{2}(X)$</span>
<span id="cb5-472"><a href="#cb5-472"></a>and the orthogonality of $Y-Y^{\star}$ with $L^{2}(X)$ as in (1). But,</span>
<span id="cb5-473"><a href="#cb5-473"></a>this implies that:</span>
<span id="cb5-474"><a href="#cb5-474"></a></span>
<span id="cb5-475"><a href="#cb5-475"></a>$$\begin{aligned}</span>
<span id="cb5-476"><a href="#cb5-476"></a>\cancel{\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y')^{2}</span><span class="co">]</span>} &amp; =\cancel{\mathbb{E}<span class="co">[</span><span class="ot">(Y-Y^{\star})^{2}</span><span class="co">]</span>}+\mathbb{E}<span class="co">[</span><span class="ot">(Y^{\star}-Y')^{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-477"><a href="#cb5-477"></a>\mathbb{E}<span class="co">[</span><span class="ot">(Y^{\star}-Y')^{2}</span><span class="co">]</span> &amp; =0</span>
<span id="cb5-478"><a href="#cb5-478"></a>\end{aligned}$$</span>
<span id="cb5-479"><a href="#cb5-479"></a></span>
<span id="cb5-480"><a href="#cb5-480"></a>So, $Y^{\star}=Y'$ almost surely. ◻</span>
<span id="cb5-481"><a href="#cb5-481"></a>:::</span>
<span id="cb5-482"><a href="#cb5-482"></a></span>
<span id="cb5-483"><a href="#cb5-483"></a>::: example</span>
<span id="cb5-484"><a href="#cb5-484"></a></span>
<span id="cb5-485"><a href="#cb5-485"></a>**Conditional Expectation of continuous random variables.** Let $(X,Y)$</span>
<span id="cb5-486"><a href="#cb5-486"></a>be two random variables with joint density $f_{X,Y}(x,y)$ on</span>
<span id="cb5-487"><a href="#cb5-487"></a>$\mathbf{R}^{2}$. Suppose for simplicity, that</span>
<span id="cb5-488"><a href="#cb5-488"></a>$\int_{\mathbf{R}}f(x,y)dx&gt;0$ for every $y$ belonging to $\mathbf{R}$.</span>
<span id="cb5-489"><a href="#cb5-489"></a>Show that the conditional expectation $\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ equals $h(X)$</span>
<span id="cb5-490"><a href="#cb5-490"></a>where $h$ is the function:</span>
<span id="cb5-491"><a href="#cb5-491"></a></span>
<span id="cb5-492"><a href="#cb5-492"></a>$$\begin{aligned}</span>
<span id="cb5-493"><a href="#cb5-493"></a>h(x) &amp; =\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\label{eq:conditional-expectation-of-continuous-random-variables}</span>
<span id="cb5-494"><a href="#cb5-494"></a>\end{aligned}$$</span>
<span id="cb5-495"><a href="#cb5-495"></a></span>
<span id="cb5-496"><a href="#cb5-496"></a>In particular, verify that $\mathbf{E}<span class="co">[</span><span class="ot">\mathbf{E}[Y|X]</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$.</span>
<span id="cb5-497"><a href="#cb5-497"></a></span>
<span id="cb5-498"><a href="#cb5-498"></a>*Hint*: To prove this, verify that the above formula satisfies both the</span>
<span id="cb5-499"><a href="#cb5-499"></a>properties of conditional expectations; then invoke uniqueness to finish</span>
<span id="cb5-500"><a href="#cb5-500"></a>it off.</span>
<span id="cb5-501"><a href="#cb5-501"></a>:::</span>
<span id="cb5-502"><a href="#cb5-502"></a></span>
<span id="cb5-503"><a href="#cb5-503"></a>::: sol*</span>
<span id="cb5-504"><a href="#cb5-504"></a><span class="sc">\(</span>i<span class="sc">\)</span> The density function $f_{X,Y}(x,y)$ is a map</span>
<span id="cb5-505"><a href="#cb5-505"></a>$f:\mathbf{R}^{2}\to\mathbf{R}$. The integral</span>
<span id="cb5-506"><a href="#cb5-506"></a>$\int_{y=-\infty}^{y=+\infty}yf_{X,Y}(x_{0},y)dy$ is the area under the</span>
<span id="cb5-507"><a href="#cb5-507"></a>curve $yf(x,y)$ at the point $x=x_{0}$. Let's call it $A(x_{0})$. If</span>
<span id="cb5-508"><a href="#cb5-508"></a>instead, we have an arbitrary $x$,</span>
<span id="cb5-509"><a href="#cb5-509"></a>$\int_{y=-\infty}^{y=+\infty}yf_{X,Y}(x,y)dy$ represents the area $A(x)$</span>
<span id="cb5-510"><a href="#cb5-510"></a>of an arbitrary slice of the surface $yf_{X,Y}$ at the point $x$. Hence,</span>
<span id="cb5-511"><a href="#cb5-511"></a>it is a function of $x$. The denominator</span>
<span id="cb5-512"><a href="#cb5-512"></a>$\int_{\mathbf{R}}f_{X,Y}(x,y)dy=f_{X}(x)$, the density of $X$, which is</span>
<span id="cb5-513"><a href="#cb5-513"></a>a function of $x$. Hence, the ratio is a function of $x$.</span>
<span id="cb5-514"><a href="#cb5-514"></a></span>
<span id="cb5-515"><a href="#cb5-515"></a><span class="sc">\(</span>ii<span class="sc">\)</span> Let $g(X)$ is a bounded random variable. We have:</span>
<span id="cb5-516"><a href="#cb5-516"></a></span>
<span id="cb5-517"><a href="#cb5-517"></a>$$\begin{aligned}</span>
<span id="cb5-518"><a href="#cb5-518"></a>\mathbf{E}<span class="co">[</span><span class="ot">g(X)(Y-h(X))</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">Yg(X)</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">g(X)h(X)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-519"><a href="#cb5-519"></a> &amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx-\int_{\mathbf{R}}g(x)h(x)f(x)dx<span class="sc">\\</span></span>
<span id="cb5-520"><a href="#cb5-520"></a> &amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx<span class="sc">\\</span></span>
<span id="cb5-521"><a href="#cb5-521"></a> &amp; -\int_{\mathbf{R}}\begin{array}{c}</span>
<span id="cb5-522"><a href="#cb5-522"></a>g(x)\cdot\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\end{array}\cdot\int_{\mathbf{R}}f_{X,Y}(x,y)dy\ dx<span class="sc">\\</span></span>
<span id="cb5-523"><a href="#cb5-523"></a> &amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx<span class="sc">\\</span></span>
<span id="cb5-524"><a href="#cb5-524"></a> &amp; -\int_{\mathbf{R}}\begin{array}{c}</span>
<span id="cb5-525"><a href="#cb5-525"></a>g(x)\cdot\frac{\int_{\mathbf{R}}yf_{X,Y}(x,y)dy}{\cancel{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}}\end{array}\cdot\cancel{\int_{\mathbf{R}}f_{X,Y}(x,y)dy}\ dx<span class="sc">\\</span></span>
<span id="cb5-526"><a href="#cb5-526"></a> &amp; =\int\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)dydx-\int_{\mathbf{R}^{2}}yg(x)f_{X,Y}(x,y)\cdot dx\cdot dy<span class="sc">\\</span></span>
<span id="cb5-527"><a href="#cb5-527"></a> &amp; =0</span>
<span id="cb5-528"><a href="#cb5-528"></a>\end{aligned}$$</span>
<span id="cb5-529"><a href="#cb5-529"></a>:::</span>
<span id="cb5-530"><a href="#cb5-530"></a></span>
<span id="cb5-531"><a href="#cb5-531"></a>Thus, $h(X)$ is a valid candidate for the conditional expectation</span>
<span id="cb5-532"><a href="#cb5-532"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$. Moreover, by the existence and uniqueness theorem</span>
<span id="cb5-533"><a href="#cb5-533"></a>(<span class="co">[</span><span class="ot">\[th:existence-and-uniqueness-of-the-conditional-expectation\]</span><span class="co">](#th:existence-and-uniqueness-of-the-conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-534"><a href="#cb5-534"></a>reference="th:existence-and-uniqueness-of-the-conditional-expectation"}),</span>
<span id="cb5-535"><a href="#cb5-535"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ is unique and equals $h(X)$.</span>
<span id="cb5-536"><a href="#cb5-536"></a></span>
<span id="cb5-537"><a href="#cb5-537"></a><span class="fu">#### Conditioning on several random variables. {#conditioning-on-several-random-variables. .unnumbered}</span></span>
<span id="cb5-538"><a href="#cb5-538"></a></span>
<span id="cb5-539"><a href="#cb5-539"></a>We would like to generalize the conditional expectation to the case when</span>
<span id="cb5-540"><a href="#cb5-540"></a>we condition on the information of more than one random variable. Taking</span>
<span id="cb5-541"><a href="#cb5-541"></a>the $L^{2}$ point of view, we should expect that the conditional</span>
<span id="cb5-542"><a href="#cb5-542"></a>expectation is the orthogonal projection of the given random variable on</span>
<span id="cb5-543"><a href="#cb5-543"></a>the subspace generated by square integrable functions of all the</span>
<span id="cb5-544"><a href="#cb5-544"></a>variables on which we condition.</span>
<span id="cb5-545"><a href="#cb5-545"></a></span>
<span id="cb5-546"><a href="#cb5-546"></a>It is now useful to study sigma-fields, an object that was defined in</span>
<span id="cb5-547"><a href="#cb5-547"></a>chapter 1.</span>
<span id="cb5-548"><a href="#cb5-548"></a></span>
<span id="cb5-549"><a href="#cb5-549"></a>::: defn</span>
<span id="cb5-550"><a href="#cb5-550"></a>[]{#def:sigma-field label="def:sigma-field"}(Sigma-Field) A sigma-field</span>
<span id="cb5-551"><a href="#cb5-551"></a>or sigma-algebra $\mathcal{F}$ of a sample space $\Omega$ is a</span>
<span id="cb5-552"><a href="#cb5-552"></a>collection of all measurable events with the following properties:</span>
<span id="cb5-553"><a href="#cb5-553"></a></span>
<span id="cb5-554"><a href="#cb5-554"></a><span class="sc">\(</span>1<span class="sc">\)</span> $\Omega$ is in $\mathcal{F}$.</span>
<span id="cb5-555"><a href="#cb5-555"></a></span>
<span id="cb5-556"><a href="#cb5-556"></a><span class="sc">\(</span>2<span class="sc">\)</span> Closure under complement. If $A\in\mathcal{F}$, then</span>
<span id="cb5-557"><a href="#cb5-557"></a>$A^{C}\in\mathcal{F}$.</span>
<span id="cb5-558"><a href="#cb5-558"></a></span>
<span id="cb5-559"><a href="#cb5-559"></a><span class="sc">\(</span>3<span class="sc">\)</span> Closure under countable unions. If</span>
<span id="cb5-560"><a href="#cb5-560"></a>$A_{1},A_{2},\ldots,\in\mathcal{F}$, then</span>
<span id="cb5-561"><a href="#cb5-561"></a>$\bigcup_{n=1}^{\infty}A_{n}\in\mathcal{F}$.</span>
<span id="cb5-562"><a href="#cb5-562"></a>:::</span>
<span id="cb5-563"><a href="#cb5-563"></a></span>
<span id="cb5-564"><a href="#cb5-564"></a>Such objects play a fundamental role in the rigorous study of</span>
<span id="cb5-565"><a href="#cb5-565"></a>probability and real analysis in general. We will focus on the intuition</span>
<span id="cb5-566"><a href="#cb5-566"></a>behind them. First let's mention some examples of sigma-fields of a</span>
<span id="cb5-567"><a href="#cb5-567"></a>given sample space $\Omega$ to get acquainted with the concept.</span>
<span id="cb5-568"><a href="#cb5-568"></a></span>
<span id="cb5-569"><a href="#cb5-569"></a>::: example</span>
<span id="cb5-570"><a href="#cb5-570"></a>(Examples of sigma-fields).</span>
<span id="cb5-571"><a href="#cb5-571"></a></span>
<span id="cb5-572"><a href="#cb5-572"></a><span class="sc">\(</span>1<span class="sc">\)</span> *The trivial sigma-field*. Note that the collection of events</span>
<span id="cb5-573"><a href="#cb5-573"></a>$<span class="sc">\{</span>\emptyset,\Omega<span class="sc">\}</span>$ is a sigma-field of $\Omega$. We generally denote</span>
<span id="cb5-574"><a href="#cb5-574"></a>it by $\mathcal{F}_{0}$.</span>
<span id="cb5-575"><a href="#cb5-575"></a></span>
<span id="cb5-576"><a href="#cb5-576"></a><span class="sc">\(</span>2<span class="sc">\)</span> *The $\sigma$-field generated by an event $A$.* Let $A$ be an</span>
<span id="cb5-577"><a href="#cb5-577"></a>event that is not $\emptyset$ and not the entire $\Omega$. Then the</span>
<span id="cb5-578"><a href="#cb5-578"></a>smallest sigma-field containing $A$ ought to be:</span>
<span id="cb5-579"><a href="#cb5-579"></a></span>
<span id="cb5-580"><a href="#cb5-580"></a>$$\begin{aligned}</span>
<span id="cb5-581"><a href="#cb5-581"></a>\mathcal{F}_{1} &amp; =<span class="sc">\{</span>\emptyset,A,A^{C},\Omega<span class="sc">\}</span></span>
<span id="cb5-582"><a href="#cb5-582"></a>\end{aligned}$$</span>
<span id="cb5-583"><a href="#cb5-583"></a></span>
<span id="cb5-584"><a href="#cb5-584"></a>This sigma-field is denoted by $\sigma(A)$.</span>
<span id="cb5-585"><a href="#cb5-585"></a></span>
<span id="cb5-586"><a href="#cb5-586"></a><span class="sc">\(</span>3<span class="sc">\)</span> The *sigma-field generated by a random variable $X$.*</span>
<span id="cb5-587"><a href="#cb5-587"></a></span>
<span id="cb5-588"><a href="#cb5-588"></a>We now define the $\mathcal{F}_{X}$ as follows:</span>
<span id="cb5-589"><a href="#cb5-589"></a></span>
<span id="cb5-590"><a href="#cb5-590"></a>$$\begin{aligned}</span>
<span id="cb5-591"><a href="#cb5-591"></a>\mathcal{F}_{X} &amp; =X^{-1}(\mathcal{B}):=<span class="sc">\{</span>\omega:X(\omega)\in B<span class="sc">\}</span>,\forall B\in\mathcal{B}(\mathbf{R})</span>
<span id="cb5-592"><a href="#cb5-592"></a>\end{aligned}$$</span>
<span id="cb5-593"><a href="#cb5-593"></a></span>
<span id="cb5-594"><a href="#cb5-594"></a>where $\mathcal{B}$ is the Borel $\sigma$-algebra on $\mathbf{R}$.</span>
<span id="cb5-595"><a href="#cb5-595"></a>$\mathcal{F}_{X}$ is sometimes denoted as $\sigma(X)$.</span>
<span id="cb5-596"><a href="#cb5-596"></a>$\mathcal{F}_{X}$is the set of all events pertaining to $X$. It is a</span>
<span id="cb5-597"><a href="#cb5-597"></a>sigma-algebra because:</span>
<span id="cb5-598"><a href="#cb5-598"></a></span>
<span id="cb5-599"><a href="#cb5-599"></a><span class="sc">\(</span>i<span class="sc">\)</span> $\Omega\in\sigma(X)$ because</span>
<span id="cb5-600"><a href="#cb5-600"></a>$\Omega=<span class="sc">\{</span>\omega:X(\omega)\in\mathbf{R}<span class="sc">\}</span>$ and</span>
<span id="cb5-601"><a href="#cb5-601"></a>$\mathbf{R}\in\mathcal{B}(\mathbf{R})$.</span>
<span id="cb5-602"><a href="#cb5-602"></a></span>
<span id="cb5-603"><a href="#cb5-603"></a><span class="sc">\(</span>ii<span class="sc">\)</span> Let any event $C\in\sigma(X)$. We need to show that</span>
<span id="cb5-604"><a href="#cb5-604"></a>$\Omega\setminus C\in\sigma(X)$.</span>
<span id="cb5-605"><a href="#cb5-605"></a></span>
<span id="cb5-606"><a href="#cb5-606"></a>Since $C\in\sigma(X)$, there exists $A\in\mathcal{B}(\mathbf{R})$, such</span>
<span id="cb5-607"><a href="#cb5-607"></a>that:</span>
<span id="cb5-608"><a href="#cb5-608"></a></span>
<span id="cb5-609"><a href="#cb5-609"></a>$$\begin{aligned}</span>
<span id="cb5-610"><a href="#cb5-610"></a>C &amp; =<span class="sc">\{</span>\omega\in\Omega:X(\omega)\in A<span class="sc">\}</span></span>
<span id="cb5-611"><a href="#cb5-611"></a>\end{aligned}$$</span>
<span id="cb5-612"><a href="#cb5-612"></a></span>
<span id="cb5-613"><a href="#cb5-613"></a>Now, we calculate:</span>
<span id="cb5-614"><a href="#cb5-614"></a></span>
<span id="cb5-615"><a href="#cb5-615"></a>$$\begin{aligned}</span>
<span id="cb5-616"><a href="#cb5-616"></a>\Omega\setminus C &amp; =<span class="sc">\{</span>\omega\in\Omega:X(\omega)\in\mathbf{R}\setminus A<span class="sc">\}</span></span>
<span id="cb5-617"><a href="#cb5-617"></a>\end{aligned}$$</span>
<span id="cb5-618"><a href="#cb5-618"></a></span>
<span id="cb5-619"><a href="#cb5-619"></a>Since $\mathcal{B}(\mathbf{R})$ is a sigma-algebra, it is closed under</span>
<span id="cb5-620"><a href="#cb5-620"></a>complementation. Hence, if $A\in\mathcal{B}(\mathbf{R})$, it implies</span>
<span id="cb5-621"><a href="#cb5-621"></a>that $\mathbf{R}\setminus A\in\mathcal{B}(\mathbf{R})$. So,</span>
<span id="cb5-622"><a href="#cb5-622"></a>$\Omega\setminus C\in\sigma(X)$.</span>
<span id="cb5-623"><a href="#cb5-623"></a></span>
<span id="cb5-624"><a href="#cb5-624"></a><span class="sc">\(</span>iii<span class="sc">\)</span> Consider a sequence of events</span>
<span id="cb5-625"><a href="#cb5-625"></a>$C_{1},C_{2},\ldots,C_{n},\ldots\in\sigma(X)$. We need to prove that</span>
<span id="cb5-626"><a href="#cb5-626"></a>$\bigcup_{n=1}^{\infty}C_{n}\in\sigma(X)$.</span>
<span id="cb5-627"><a href="#cb5-627"></a></span>
<span id="cb5-628"><a href="#cb5-628"></a>Since $C_{n}\in\sigma(X)$, there exists</span>
<span id="cb5-629"><a href="#cb5-629"></a>$A_{n}\in\mathcal{B}(\mathbf{R})$ such that:</span>
<span id="cb5-630"><a href="#cb5-630"></a></span>
<span id="cb5-631"><a href="#cb5-631"></a>$$\begin{aligned}</span>
<span id="cb5-632"><a href="#cb5-632"></a>C_{n} &amp; =<span class="sc">\{</span>\omega\in\Omega:X(\omega)\in A_{n}<span class="sc">\}</span></span>
<span id="cb5-633"><a href="#cb5-633"></a>\end{aligned}$$</span>
<span id="cb5-634"><a href="#cb5-634"></a></span>
<span id="cb5-635"><a href="#cb5-635"></a>Now, we calculuate:</span>
<span id="cb5-636"><a href="#cb5-636"></a></span>
<span id="cb5-637"><a href="#cb5-637"></a>$$\begin{aligned}</span>
<span id="cb5-638"><a href="#cb5-638"></a>\bigcup_{n=1}C_{n} &amp; =<span class="sc">\{</span>\omega\in\Omega:X(\omega)\in\bigcup_{n=1}^{\infty}A_{n}<span class="sc">\}</span></span>
<span id="cb5-639"><a href="#cb5-639"></a>\end{aligned}$$</span>
<span id="cb5-640"><a href="#cb5-640"></a></span>
<span id="cb5-641"><a href="#cb5-641"></a>But, $\bigcup_{n=1}^{\infty}A_{n}\in\mathcal{B}(\mathbf{R})$. So,</span>
<span id="cb5-642"><a href="#cb5-642"></a>$\bigcup_{n=1}^{\infty}C_{n}\in\sigma(X)$.</span>
<span id="cb5-643"><a href="#cb5-643"></a></span>
<span id="cb5-644"><a href="#cb5-644"></a>Consequently, $\sigma(X)$ is indeed a $\sigma$-algebra.</span>
<span id="cb5-645"><a href="#cb5-645"></a></span>
<span id="cb5-646"><a href="#cb5-646"></a>Intuitively, we think of $\sigma(X)$ as containing all information about</span>
<span id="cb5-647"><a href="#cb5-647"></a>$X$.</span>
<span id="cb5-648"><a href="#cb5-648"></a></span>
<span id="cb5-649"><a href="#cb5-649"></a><span class="sc">\(</span>4<span class="sc">\)</span> *The sigma-field generated by a stochastic process</span>
<span id="cb5-650"><a href="#cb5-650"></a>$(X_{s},s\leq t)$.* Let $(X_{s},s\geq0)$ be a stochastic process.</span>
<span id="cb5-651"><a href="#cb5-651"></a>Consider the process restricted to $<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>$, $(X_{s},s\leq t)$. We</span>
<span id="cb5-652"><a href="#cb5-652"></a>consider the smallest sigma-field containing all events pertaining to</span>
<span id="cb5-653"><a href="#cb5-653"></a>the random variables $X_{s},s\leq t$. We denote it by</span>
<span id="cb5-654"><a href="#cb5-654"></a>$\sigma(X_{s},s\leq t)$ or $\mathcal{F}_{t}$.</span>
<span id="cb5-655"><a href="#cb5-655"></a>:::</span>
<span id="cb5-656"><a href="#cb5-656"></a></span>
<span id="cb5-657"><a href="#cb5-657"></a>The sigma-fields on $\Omega$ have a natural (partial) ordering: two</span>
<span id="cb5-658"><a href="#cb5-658"></a>sigma-fields $\mathcal{G}$ and $\mathcal{F}$ of $\Omega$ are such that</span>
<span id="cb5-659"><a href="#cb5-659"></a>$\mathcal{G}\subseteq\mathcal{F}$ if all the events in $\mathcal{G}$ are</span>
<span id="cb5-660"><a href="#cb5-660"></a>in $\mathcal{F}$. For example, the trivial $\sigma$-field</span>
<span id="cb5-661"><a href="#cb5-661"></a>$\mathcal{F}_{0}=<span class="sc">\{</span>\emptyset,\Omega<span class="sc">\}</span>$ is contained in all the</span>
<span id="cb5-662"><a href="#cb5-662"></a>$\sigma$-fields of $\Omega$. Clearly, the $\sigma$-field</span>
<span id="cb5-663"><a href="#cb5-663"></a>$\mathcal{F}_{t}=\sigma(X_{s},s\leq t)$ is contained in</span>
<span id="cb5-664"><a href="#cb5-664"></a>$\mathcal{F}_{t'}$ if $t\leq t'$.</span>
<span id="cb5-665"><a href="#cb5-665"></a></span>
<span id="cb5-666"><a href="#cb5-666"></a>If all the events pertaining to a random variable $X$ are in the</span>
<span id="cb5-667"><a href="#cb5-667"></a>$\sigma$-field $\mathcal{G}$ (and thus we can compute</span>
<span id="cb5-668"><a href="#cb5-668"></a>$\mu(X^{-1}((a,b]))$), we will say that $X$ is $\mathcal{G}$-measurable.</span>
<span id="cb5-669"><a href="#cb5-669"></a>This means that all information about $X$ is contained in $\mathcal{G}$.</span>
<span id="cb5-670"><a href="#cb5-670"></a></span>
<span id="cb5-671"><a href="#cb5-671"></a>::: defn</span>
<span id="cb5-672"><a href="#cb5-672"></a>Let $X$ be a random variable defined on</span>
<span id="cb5-673"><a href="#cb5-673"></a>$(\Omega,\mathcal{F},\mathbb{P})$. Consider another</span>
<span id="cb5-674"><a href="#cb5-674"></a>$\mathcal{G}\subseteq\mathcal{F}$. Then $X$ is said to be</span>
<span id="cb5-675"><a href="#cb5-675"></a>$\mathcal{G}$-measurable, if and only if:</span>
<span id="cb5-676"><a href="#cb5-676"></a></span>
<span id="cb5-677"><a href="#cb5-677"></a>$$\begin{aligned}</span>
<span id="cb5-678"><a href="#cb5-678"></a><span class="sc">\{</span>\omega:X(\omega)\in(a,b]<span class="sc">\}</span> &amp; \in\mathcal{G}\text{ for all intervals }(a,b]\in\mathbf{R}</span>
<span id="cb5-679"><a href="#cb5-679"></a>\end{aligned}$$</span>
<span id="cb5-680"><a href="#cb5-680"></a>:::</span>
<span id="cb5-681"><a href="#cb5-681"></a></span>
<span id="cb5-682"><a href="#cb5-682"></a>::: example</span>
<span id="cb5-683"><a href="#cb5-683"></a>($\mathcal{F}_{0}$-measurable random variables). Consider the trivial</span>
<span id="cb5-684"><a href="#cb5-684"></a>sigma-field $\mathcal{F}_{0}=<span class="sc">\{</span>\emptyset,\Omega<span class="sc">\}</span>$. A random variable</span>
<span id="cb5-685"><a href="#cb5-685"></a>that is $\mathcal{F}_{0}$-measurable must be a constant. Indeed, we have</span>
<span id="cb5-686"><a href="#cb5-686"></a>that for any interval $(a,b]$, $<span class="sc">\{</span>\omega:X(\omega)\in(a,b]<span class="sc">\}</span>=\emptyset$</span>
<span id="cb5-687"><a href="#cb5-687"></a>or $<span class="sc">\{</span>\omega:X(\omega)\in(a,b]<span class="sc">\}</span>=\Omega$. This can only hold if $X$</span>
<span id="cb5-688"><a href="#cb5-688"></a>takes a single value.</span>
<span id="cb5-689"><a href="#cb5-689"></a>:::</span>
<span id="cb5-690"><a href="#cb5-690"></a></span>
<span id="cb5-691"><a href="#cb5-691"></a>::: example</span>
<span id="cb5-692"><a href="#cb5-692"></a>[]{#ex:sigma(X)-measurable-random-variables-example</span>
<span id="cb5-693"><a href="#cb5-693"></a>label="ex:sigma(X)-measurable-random-variables-example"}($\sigma(X)$-measurable</span>
<span id="cb5-694"><a href="#cb5-694"></a>random variables). Let $X$ be a given random variable on</span>
<span id="cb5-695"><a href="#cb5-695"></a>$(\Omega,\mathcal{F},\mathbb{P})$. Roughly speaking, a</span>
<span id="cb5-696"><a href="#cb5-696"></a>$\sigma(X)$-measurable random variable is determined by the information</span>
<span id="cb5-697"><a href="#cb5-697"></a>of $X$ only. Here is the simplest example of a $\sigma(X)$-measurable</span>
<span id="cb5-698"><a href="#cb5-698"></a>random variable. Take the indicator function $Y=\mathbf{1}_{<span class="sc">\{</span>X\in B<span class="sc">\}</span>}$</span>
<span id="cb5-699"><a href="#cb5-699"></a>for some event $<span class="sc">\{</span>X\in B<span class="sc">\}</span>$ pertaining to $X$. Then the pre-images</span>
<span id="cb5-700"><a href="#cb5-700"></a>$<span class="sc">\{</span>\omega:Y(\omega)\in(a,b]<span class="sc">\}</span>$ are either $\emptyset$, $<span class="sc">\{</span>X\in B<span class="sc">\}</span>$,</span>
<span id="cb5-701"><a href="#cb5-701"></a>$<span class="sc">\{</span>X\in B^{C}<span class="sc">\}</span>$ or $\Omega$ depending on whether $0,1$ are in $(a,b]$</span>
<span id="cb5-702"><a href="#cb5-702"></a>or not. All of these events are in $\sigma(X)$. More generally, one can</span>
<span id="cb5-703"><a href="#cb5-703"></a>construct a $\sigma(X)$-measurable random variable by taking linear</span>
<span id="cb5-704"><a href="#cb5-704"></a>combinations of indicator functions of events of the form $<span class="sc">\{</span>X\in B<span class="sc">\}</span>$.</span>
<span id="cb5-705"><a href="#cb5-705"></a></span>
<span id="cb5-706"><a href="#cb5-706"></a>It turns out that any (Borel measurable) function of $X$ can be</span>
<span id="cb5-707"><a href="#cb5-707"></a>approximated by taking limits of such simple functions.</span>
<span id="cb5-708"><a href="#cb5-708"></a></span>
<span id="cb5-709"><a href="#cb5-709"></a>Concretely, this translates to the following statement:</span>
<span id="cb5-710"><a href="#cb5-710"></a></span>
<span id="cb5-711"><a href="#cb5-711"></a>$$\text{If }Y\text{ is \ensuremath{\sigma}(X)-measurable, then Y=g(X) for some function g}$$</span>
<span id="cb5-712"><a href="#cb5-712"></a></span>
<span id="cb5-713"><a href="#cb5-713"></a>In the same way, if $Z$ is $\sigma(X,Y)$-measurable, then $Z=h(X,Y)$ for</span>
<span id="cb5-714"><a href="#cb5-714"></a>some $h$. These facts can be proved rigorously using measure theory.</span>
<span id="cb5-715"><a href="#cb5-715"></a>:::</span>
<span id="cb5-716"><a href="#cb5-716"></a></span>
<span id="cb5-717"><a href="#cb5-717"></a>We are ready to give the general definition of conditional expectation.</span>
<span id="cb5-718"><a href="#cb5-718"></a></span>
<span id="cb5-719"><a href="#cb5-719"></a>::: example</span>
<span id="cb5-720"><a href="#cb5-720"></a>(Coin-Tossing Space). Suppose a coin is tossed infinitely many times.</span>
<span id="cb5-721"><a href="#cb5-721"></a>Let $\Omega$ be the set of all infinite sequences of $H$s and $T$s. A</span>
<span id="cb5-722"><a href="#cb5-722"></a>generic element of $\Omega$ is denoted by $\omega_{1}\omega_{2}\ldots$,</span>
<span id="cb5-723"><a href="#cb5-723"></a>where $\omega_{n}$ indicates the result of the $n$th coin toss. $\Omega$</span>
<span id="cb5-724"><a href="#cb5-724"></a>is an uncountable sample space. The trivial sigma-field</span>
<span id="cb5-725"><a href="#cb5-725"></a>$\mathcal{F}_{0}=<span class="sc">\{</span>\emptyset,\Omega<span class="sc">\}</span>$. Assume that we don't know</span>
<span id="cb5-726"><a href="#cb5-726"></a>anything about the outcome of the experiement. Even without any</span>
<span id="cb5-727"><a href="#cb5-727"></a>information, we know that the true $\omega$ belongs to $\Omega$ and does</span>
<span id="cb5-728"><a href="#cb5-728"></a>not belong to $\emptyset$. It is the information learned at time $0$.</span>
<span id="cb5-729"><a href="#cb5-729"></a></span>
<span id="cb5-730"><a href="#cb5-730"></a>Next, assume that we know the outcome of the first coin toss. Define</span>
<span id="cb5-731"><a href="#cb5-731"></a>$A_{H}=<span class="sc">\{</span>\omega:\omega_{1}=H<span class="sc">\}</span>$=set of all sequences beginning with $H$</span>
<span id="cb5-732"><a href="#cb5-732"></a>and $A_{T}=<span class="sc">\{</span>\omega:\omega_{1}=T<span class="sc">\}</span>$=set of all sequences beginning with</span>
<span id="cb5-733"><a href="#cb5-733"></a>$T$. The four sets resolved by the first coin-toss form the the</span>
<span id="cb5-734"><a href="#cb5-734"></a>$\sigma$-field $\mathcal{F}_{1}=\{\emptyset,A_{H},A_{T},\Omega<span class="sc">\}</span>$. We</span>
<span id="cb5-735"><a href="#cb5-735"></a>shall think of this $\sigma$-field as containing the information learned</span>
<span id="cb5-736"><a href="#cb5-736"></a>by knowing the outcome of the first coin toss. More precisely, if</span>
<span id="cb5-737"><a href="#cb5-737"></a>instead of being told about the first coin toss, we are told for each</span>
<span id="cb5-738"><a href="#cb5-738"></a>set in $\mathcal{F}_{1}$, whether or not the true $\omega$ belongs to</span>
<span id="cb5-739"><a href="#cb5-739"></a>that set, then we know the outcome of the first coin toss and nothing</span>
<span id="cb5-740"><a href="#cb5-740"></a>more.</span>
<span id="cb5-741"><a href="#cb5-741"></a></span>
<span id="cb5-742"><a href="#cb5-742"></a>If we are told the first two coin tosses, we obtain a finer resolution.</span>
<span id="cb5-743"><a href="#cb5-743"></a>In particular, the four sets:</span>
<span id="cb5-744"><a href="#cb5-744"></a></span>
<span id="cb5-745"><a href="#cb5-745"></a>$$\begin{aligned}</span>
<span id="cb5-746"><a href="#cb5-746"></a>A_{HH} &amp; =<span class="sc">\{</span>\omega:\omega_{1}=H,\omega_{2}=H<span class="sc">\}\\</span></span>
<span id="cb5-747"><a href="#cb5-747"></a>A_{HT} &amp; =<span class="sc">\{</span>\omega:\omega_{1}=H,\omega_{2}=T<span class="sc">\}\\</span></span>
<span id="cb5-748"><a href="#cb5-748"></a>A_{TH} &amp; =<span class="sc">\{</span>\omega:\omega_{1}=T,\omega_{2}=H<span class="sc">\}\\</span></span>
<span id="cb5-749"><a href="#cb5-749"></a>A_{TT} &amp; =<span class="sc">\{</span>\omega:\omega_{1}=T,\omega_{2}=T<span class="sc">\}</span></span>
<span id="cb5-750"><a href="#cb5-750"></a>\end{aligned}$$</span>
<span id="cb5-751"><a href="#cb5-751"></a></span>
<span id="cb5-752"><a href="#cb5-752"></a>are resolved. Of course, the sets in $\mathcal{F}_{1}$ are resolved.</span>
<span id="cb5-753"><a href="#cb5-753"></a>Whenever a set is resolved, so is its complement, which means that</span>
<span id="cb5-754"><a href="#cb5-754"></a>$A_{HH}^{C}$, $A_{HT}^{C}$, $A_{TH}^{C}$ and $A_{TT}^{C}$ are resolved,</span>
<span id="cb5-755"><a href="#cb5-755"></a>so is their union which means that $A_{HH}\cup A_{TH}$,</span>
<span id="cb5-756"><a href="#cb5-756"></a>$A_{HH}\cup A_{TT}$, $A_{HT}\cup A_{TH}$ and $A_{HT}\cup A_{TT}$ are</span>
<span id="cb5-757"><a href="#cb5-757"></a>resolved. The other two pair-wise unions $A_{HH}\cup A_{HT}=A_{H}$ and</span>
<span id="cb5-758"><a href="#cb5-758"></a>$A_{TH}\cup A_{TT}=A_{T}$ are already resolved. Finally, the triple</span>
<span id="cb5-759"><a href="#cb5-759"></a>unions are also resolved, because</span>
<span id="cb5-760"><a href="#cb5-760"></a>$A_{HH}\cup A_{HT}\cup A_{TH}=A_{TT}^{C}$ and so forth. Hence, the</span>
<span id="cb5-761"><a href="#cb5-761"></a>information pertaining to the second coin-toss is contained in:</span>
<span id="cb5-762"><a href="#cb5-762"></a></span>
<span id="cb5-763"><a href="#cb5-763"></a>$$\begin{aligned}</span>
<span id="cb5-764"><a href="#cb5-764"></a>\mathcal{F}_{2} &amp; =<span class="sc">\{</span>\emptyset,\Omega,<span class="sc">\\</span></span>
<span id="cb5-765"><a href="#cb5-765"></a> &amp; A_{H},A_{T},<span class="sc">\\</span></span>
<span id="cb5-766"><a href="#cb5-766"></a> &amp; A_{HH},A_{HT},A_{TH},A_{TT},<span class="sc">\\</span></span>
<span id="cb5-767"><a href="#cb5-767"></a> &amp; A_{HH}^{C},A_{HT}^{C},A_{TH}^{C},A_{TT}^{C},<span class="sc">\\</span></span>
<span id="cb5-768"><a href="#cb5-768"></a> &amp; A_{HH}\cup A_{TH},A_{HH}\cup A_{TT},A_{HT}\cup A_{TH},A_{HT}\cup A_{TT}<span class="sc">\}</span></span>
<span id="cb5-769"><a href="#cb5-769"></a>\end{aligned}$$</span>
<span id="cb5-770"><a href="#cb5-770"></a></span>
<span id="cb5-771"><a href="#cb5-771"></a>Hence, if the outcome of the first two coin tosses is known, all of the</span>
<span id="cb5-772"><a href="#cb5-772"></a>events in $\mathcal{F}_{2}$ are resolved - we exactly know, if each</span>
<span id="cb5-773"><a href="#cb5-773"></a>event has ocurred or not. $\mathcal{F}_{2}$ is the information learned</span>
<span id="cb5-774"><a href="#cb5-774"></a>by observing the first two coin tosses.</span>
<span id="cb5-775"><a href="#cb5-775"></a>:::</span>
<span id="cb5-776"><a href="#cb5-776"></a></span>
<span id="cb5-777"><a href="#cb5-777"></a>::: xca</span>
<span id="cb5-778"><a href="#cb5-778"></a>(**Exercises on sigma-fields**).</span>
<span id="cb5-779"><a href="#cb5-779"></a></span>
<span id="cb5-780"><a href="#cb5-780"></a><span class="sc">\(</span>a<span class="sc">\)</span> Let $A$, $B$ be two proper subsets of $\Omega$ such that</span>
<span id="cb5-781"><a href="#cb5-781"></a>$A\cap B\neq\emptyset$ and $A\cup B\neq\Omega$. Write down</span>
<span id="cb5-782"><a href="#cb5-782"></a>$\sigma(<span class="sc">\{</span>A,B<span class="sc">\}</span>)$, the smallest sigma-field containing $A$ and $B$</span>
<span id="cb5-783"><a href="#cb5-783"></a>explicitly. What if $A\cap B=\emptyset$?</span>
<span id="cb5-784"><a href="#cb5-784"></a></span>
<span id="cb5-785"><a href="#cb5-785"></a><span class="sc">\(</span>b<span class="sc">\)</span> The Borel sigma-field is the smallest sigma-field containing</span>
<span id="cb5-786"><a href="#cb5-786"></a>intervals of the form $(a,b]$ in $\mathbf{R}$. Show that all singletons</span>
<span id="cb5-787"><a href="#cb5-787"></a>$<span class="sc">\{</span>b<span class="sc">\}</span>$ are in $\mathcal{B}(\mathbf{R})$ by writing $<span class="sc">\{</span>b<span class="sc">\}</span>$ as a</span>
<span id="cb5-788"><a href="#cb5-788"></a>countable intersection of intervals $(a,b]$. Conclude that all open</span>
<span id="cb5-789"><a href="#cb5-789"></a>intervals $(a,b)$ and all closed intervals $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$ are in</span>
<span id="cb5-790"><a href="#cb5-790"></a>$\mathcal{B}(\mathbf{R})$. Is the subset $\mathbf{Q}$ of rational</span>
<span id="cb5-791"><a href="#cb5-791"></a>numbers a Borel set?</span>
<span id="cb5-792"><a href="#cb5-792"></a>:::</span>
<span id="cb5-793"><a href="#cb5-793"></a></span>
<span id="cb5-794"><a href="#cb5-794"></a>::: proof</span>
<span id="cb5-795"><a href="#cb5-795"></a>*Proof.* (a) The sigma-field generated by the two events $A$, $B$ is</span>
<span id="cb5-796"><a href="#cb5-796"></a>given by:</span>
<span id="cb5-797"><a href="#cb5-797"></a></span>
<span id="cb5-798"><a href="#cb5-798"></a>$$\begin{aligned}</span>
<span id="cb5-799"><a href="#cb5-799"></a>\sigma(<span class="sc">\{</span>A,B<span class="sc">\}</span>) &amp; =<span class="sc">\{</span>\emptyset,\Omega,<span class="sc">\\</span></span>
<span id="cb5-800"><a href="#cb5-800"></a> &amp; A,B,A^{C},B^{C},<span class="sc">\\</span></span>
<span id="cb5-801"><a href="#cb5-801"></a> &amp; A\cup B,A\cap B,<span class="sc">\\</span></span>
<span id="cb5-802"><a href="#cb5-802"></a> &amp; A\cup B^{C},A^{C}\cup B,A^{C}\cup B^{C},<span class="sc">\\</span></span>
<span id="cb5-803"><a href="#cb5-803"></a> &amp; A\cap B^{C},A^{C}\cap B,A^{C}\cap B^{C},<span class="sc">\\</span></span>
<span id="cb5-804"><a href="#cb5-804"></a> &amp; (A\cup B)\cap(A\cap B)^{C},<span class="sc">\\</span></span>
<span id="cb5-805"><a href="#cb5-805"></a> &amp; (A\cup B)^{C}\cup(A\cap B)<span class="sc">\}</span></span>
<span id="cb5-806"><a href="#cb5-806"></a>\end{aligned}$$</span>
<span id="cb5-807"><a href="#cb5-807"></a></span>
<span id="cb5-808"><a href="#cb5-808"></a><span class="sc">\(</span>b<span class="sc">\)</span> Firstly, recall that:</span>
<span id="cb5-809"><a href="#cb5-809"></a></span>
<span id="cb5-810"><a href="#cb5-810"></a>$$\begin{aligned}</span>
<span id="cb5-811"><a href="#cb5-811"></a>\mathcal{B}(\mathbf{R}) &amp; =\bigcap_{\alpha\in\Lambda}\mathcal{F}_{\alpha}=\bigcap\sigma(<span class="sc">\{</span>I:I\text{ is an interval }(a,b]\subseteq\mathbf{R}<span class="sc">\}</span>)</span>
<span id="cb5-812"><a href="#cb5-812"></a>\end{aligned}$$</span>
<span id="cb5-813"><a href="#cb5-813"></a></span>
<span id="cb5-814"><a href="#cb5-814"></a>We can write:</span>
<span id="cb5-815"><a href="#cb5-815"></a></span>
<span id="cb5-816"><a href="#cb5-816"></a>$$\begin{aligned}</span>
<span id="cb5-817"><a href="#cb5-817"></a><span class="sc">\{</span>b<span class="sc">\}</span> &amp; =\bigcap_{n=1}^{\infty}\left(b-\frac{1}{n},b\right]</span>
<span id="cb5-818"><a href="#cb5-818"></a>\end{aligned}$$</span>
<span id="cb5-819"><a href="#cb5-819"></a></span>
<span id="cb5-820"><a href="#cb5-820"></a>As $\mathcal{B}(\mathbf{R})$ is a sigma-field, it is closed under</span>
<span id="cb5-821"><a href="#cb5-821"></a>countable intersections. Hence, the singleton set $<span class="sc">\{</span>b<span class="sc">\}</span>$is a Borel set.</span>
<span id="cb5-822"><a href="#cb5-822"></a></span>
<span id="cb5-823"><a href="#cb5-823"></a>Similarly, we can write, any open interval as the countable union:</span>
<span id="cb5-824"><a href="#cb5-824"></a></span>
<span id="cb5-825"><a href="#cb5-825"></a>$$\begin{aligned}</span>
<span id="cb5-826"><a href="#cb5-826"></a>(a,b) &amp; =\bigcup_{n=1}^{\infty}\left(a,b-\frac{1}{n}\right]</span>
<span id="cb5-827"><a href="#cb5-827"></a>\end{aligned}$$</span>
<span id="cb5-828"><a href="#cb5-828"></a></span>
<span id="cb5-829"><a href="#cb5-829"></a>We can convince ourselves, that equality indeed holds. Let $x\in(a,b)$</span>
<span id="cb5-830"><a href="#cb5-830"></a>and choose $N$, such that $\frac{1}{N}&lt;|b-x|$. Then, for all $n\geq N$,</span>
<span id="cb5-831"><a href="#cb5-831"></a>$x\in(a,b-1/n]$. Thus, it belongs to the RHS. In the reverse direction,</span>
<span id="cb5-832"><a href="#cb5-832"></a>let $x$ belong to $\bigcup_{n=1}^{\infty}\left(a,b-\frac{1}{n}\right]$.</span>
<span id="cb5-833"><a href="#cb5-833"></a>So, $x$ belongs to atleast one of these sets. Therefore, $x\in(a,b)$ is</span>
<span id="cb5-834"><a href="#cb5-834"></a>trivially true. So, the two sets are equal.</span>
<span id="cb5-835"><a href="#cb5-835"></a></span>
<span id="cb5-836"><a href="#cb5-836"></a>Hence, open intervals are Borel sets.</span>
<span id="cb5-837"><a href="#cb5-837"></a></span>
<span id="cb5-838"><a href="#cb5-838"></a>Similarly, we may write:</span>
<span id="cb5-839"><a href="#cb5-839"></a></span>
<span id="cb5-840"><a href="#cb5-840"></a>$$\begin{aligned}</span>
<span id="cb5-841"><a href="#cb5-841"></a><span class="co">[</span><span class="ot">a,b</span><span class="co">]</span> &amp; =\bigcap_{n=1}^{\infty}\left(a-\frac{1}{n},b+\frac{1}{n}\right)</span>
<span id="cb5-842"><a href="#cb5-842"></a>\end{aligned}$$</span>
<span id="cb5-843"><a href="#cb5-843"></a></span>
<span id="cb5-844"><a href="#cb5-844"></a>Consequently, closed intervals are Borel sets. Since $\mathbf{Q}$ is</span>
<span id="cb5-845"><a href="#cb5-845"></a>countable, it is a Borel set. Moreover, the empty set $\emptyset$ and</span>
<span id="cb5-846"><a href="#cb5-846"></a>$\mathbf{R}$ are Borel sets. So, $\mathbf{R}\backslash\mathbf{Q}$ is</span>
<span id="cb5-847"><a href="#cb5-847"></a>also a Borel set. ◻</span>
<span id="cb5-848"><a href="#cb5-848"></a>:::</span>
<span id="cb5-849"><a href="#cb5-849"></a></span>
<span id="cb5-850"><a href="#cb5-850"></a>::: xca</span>
<span id="cb5-851"><a href="#cb5-851"></a>Let $(X,Y)$ be a Gaussian vector with mean $0$ and</span>
<span id="cb5-852"><a href="#cb5-852"></a>covariance matrix</span>
<span id="cb5-853"><a href="#cb5-853"></a></span>
<span id="cb5-854"><a href="#cb5-854"></a>$$\begin{aligned}</span>
<span id="cb5-855"><a href="#cb5-855"></a>C &amp; =\left[\begin{array}{cc}</span>
<span id="cb5-856"><a href="#cb5-856"></a>1 &amp; \rho<span class="sc">\\</span></span>
<span id="cb5-857"><a href="#cb5-857"></a>\rho &amp; 1</span>
<span id="cb5-858"><a href="#cb5-858"></a>\end{array}\right]</span>
<span id="cb5-859"><a href="#cb5-859"></a>\end{aligned}$$</span>
<span id="cb5-860"><a href="#cb5-860"></a></span>
<span id="cb5-861"><a href="#cb5-861"></a>for $\rho\in(-1,1)$. We verify that the example</span>
<span id="cb5-862"><a href="#cb5-862"></a>(<span class="co">[</span><span class="ot">\[ex:conditional-expectation-of-gaussian-vectors\]</span><span class="co">](#ex:conditional-expectation-of-gaussian-vectors)</span>{reference-type="ref"</span>
<span id="cb5-863"><a href="#cb5-863"></a>reference="ex:conditional-expectation-of-gaussian-vectors"}) and</span>
<span id="cb5-864"><a href="#cb5-864"></a>exercise</span>
<span id="cb5-865"><a href="#cb5-865"></a>(<span class="co">[</span><span class="ot">\[ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables\]</span><span class="co">](#ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables)</span>{reference-type="ref"</span>
<span id="cb5-866"><a href="#cb5-866"></a>reference="ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables"})</span>
<span id="cb5-867"><a href="#cb5-867"></a>yield the same conditional expectation.</span>
<span id="cb5-868"><a href="#cb5-868"></a></span>
<span id="cb5-869"><a href="#cb5-869"></a><span class="sc">\(</span>a<span class="sc">\)</span> Use equation</span>
<span id="cb5-870"><a href="#cb5-870"></a>(<span class="co">[</span><span class="ot">\[eq:conditional-expectation-of-gaussian-vector\]</span><span class="co">](#eq:conditional-expectation-of-gaussian-vector)</span>{reference-type="ref"</span>
<span id="cb5-871"><a href="#cb5-871"></a>reference="eq:conditional-expectation-of-gaussian-vector"}) to show that</span>
<span id="cb5-872"><a href="#cb5-872"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>=\rho X$.</span>
<span id="cb5-873"><a href="#cb5-873"></a></span>
<span id="cb5-874"><a href="#cb5-874"></a><span class="sc">\(</span>b<span class="sc">\)</span> Write down the joint PDF $f(x,y)$ of $(X,Y)$.</span>
<span id="cb5-875"><a href="#cb5-875"></a></span>
<span id="cb5-876"><a href="#cb5-876"></a><span class="sc">\(</span>c<span class="sc">\)</span> Show that $\int_{\mathbf{R}}yf(x,y)dy=\rho x$ and that</span>
<span id="cb5-877"><a href="#cb5-877"></a>$\int_{\mathbf{R}}f(x,y)dy=1$.</span>
<span id="cb5-878"><a href="#cb5-878"></a></span>
<span id="cb5-879"><a href="#cb5-879"></a><span class="sc">\(</span>d<span class="sc">\)</span> Deduce that $\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>=\rho X$ using the equation</span>
<span id="cb5-880"><a href="#cb5-880"></a>(<span class="co">[</span><span class="ot">\[eq:conditional-expectation-of-continuous-random-variables\]</span><span class="co">](#eq:conditional-expectation-of-continuous-random-variables)</span>{reference-type="ref"</span>
<span id="cb5-881"><a href="#cb5-881"></a>reference="eq:conditional-expectation-of-continuous-random-variables"}).</span>
<span id="cb5-882"><a href="#cb5-882"></a>:::</span>
<span id="cb5-883"><a href="#cb5-883"></a></span>
<span id="cb5-884"><a href="#cb5-884"></a>::: proof</span>
<span id="cb5-885"><a href="#cb5-885"></a>*Proof.* (a) Since $(X,Y)$ have mean $0$ and variance $1$, it follows</span>
<span id="cb5-886"><a href="#cb5-886"></a>that:</span>
<span id="cb5-887"><a href="#cb5-887"></a></span>
<span id="cb5-888"><a href="#cb5-888"></a>$$\begin{aligned}</span>
<span id="cb5-889"><a href="#cb5-889"></a>\mathbf{E}<span class="co">[</span><span class="ot">(X-EX)(Y-EY)</span><span class="co">]</span> &amp; =\mathbf{E}(XY)<span class="sc">\\</span></span>
<span id="cb5-890"><a href="#cb5-890"></a>\sqrt{(\mathbf{E}<span class="co">[</span><span class="ot">X^{2}</span><span class="co">]</span>-(\mathbf{E}X)^{2})}\cdot\sqrt{(\mathbf{E}<span class="co">[</span><span class="ot">Y^{2}</span><span class="co">]</span>-(\mathbf{E}Y)^{2})} &amp; =\sqrt{(1-0)(1-0)}<span class="sc">\\</span></span>
<span id="cb5-891"><a href="#cb5-891"></a> &amp; =1</span>
<span id="cb5-892"><a href="#cb5-892"></a>\end{aligned}$$</span>
<span id="cb5-893"><a href="#cb5-893"></a></span>
<span id="cb5-894"><a href="#cb5-894"></a>and therefore,</span>
<span id="cb5-895"><a href="#cb5-895"></a></span>
<span id="cb5-896"><a href="#cb5-896"></a>$$\begin{aligned}</span>
<span id="cb5-897"><a href="#cb5-897"></a>\rho &amp; =\frac{\mathbf{E}(XY)}{1}=\frac{\mathbf{E}<span class="co">[</span><span class="ot">XY</span><span class="co">]</span>}{\mathbf{E}<span class="co">[</span><span class="ot">X^{2}</span><span class="co">]</span>}</span>
<span id="cb5-898"><a href="#cb5-898"></a>\end{aligned}$$</span>
<span id="cb5-899"><a href="#cb5-899"></a></span>
<span id="cb5-900"><a href="#cb5-900"></a>Since $(X,Y)$ is a Gaussian vector, using</span>
<span id="cb5-901"><a href="#cb5-901"></a>(<span class="co">[</span><span class="ot">\[eq:conditional-expectation-of-gaussian-vector\]</span><span class="co">](#eq:conditional-expectation-of-gaussian-vector)</span>{reference-type="ref"</span>
<span id="cb5-902"><a href="#cb5-902"></a>reference="eq:conditional-expectation-of-gaussian-vector"}), we have:</span>
<span id="cb5-903"><a href="#cb5-903"></a></span>
<span id="cb5-904"><a href="#cb5-904"></a>$$\begin{aligned}</span>
<span id="cb5-905"><a href="#cb5-905"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span> &amp; =\frac{\mathbf{E}<span class="co">[</span><span class="ot">XY</span><span class="co">]</span>}{\mathbf{E}<span class="co">[</span><span class="ot">X^{2}</span><span class="co">]</span>}X=\rho X</span>
<span id="cb5-906"><a href="#cb5-906"></a>\end{aligned}$$</span>
<span id="cb5-907"><a href="#cb5-907"></a></span>
<span id="cb5-908"><a href="#cb5-908"></a><span class="sc">\(</span>b<span class="sc">\)</span> Consider the augmented matrix $<span class="co">[</span><span class="ot">C|I</span><span class="co">]</span>$. We have:</span>
<span id="cb5-909"><a href="#cb5-909"></a></span>
<span id="cb5-910"><a href="#cb5-910"></a>$$\begin{aligned}</span>
<span id="cb5-911"><a href="#cb5-911"></a><span class="co">[</span><span class="ot">C|I</span><span class="co">]</span> &amp; =\left[\left.\begin{array}{cc}</span>
<span id="cb5-912"><a href="#cb5-912"></a>1 &amp; \rho<span class="sc">\\</span></span>
<span id="cb5-913"><a href="#cb5-913"></a>\rho &amp; 1</span>
<span id="cb5-914"><a href="#cb5-914"></a>\end{array}\right|\begin{array}{cc}</span>
<span id="cb5-915"><a href="#cb5-915"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb5-916"><a href="#cb5-916"></a>0 &amp; 1</span>
<span id="cb5-917"><a href="#cb5-917"></a>\end{array}\right]</span>
<span id="cb5-918"><a href="#cb5-918"></a>\end{aligned}$$</span>
<span id="cb5-919"><a href="#cb5-919"></a></span>
<span id="cb5-920"><a href="#cb5-920"></a>Performing $R_{2}=R_{2}-\rho R_{1}$, the above system is row-equivalent</span>
<span id="cb5-921"><a href="#cb5-921"></a>to:</span>
<span id="cb5-922"><a href="#cb5-922"></a></span>
<span id="cb5-923"><a href="#cb5-923"></a>$$\left[\left.\begin{array}{cc}</span>
<span id="cb5-924"><a href="#cb5-924"></a>1 &amp; \rho<span class="sc">\\</span></span>
<span id="cb5-925"><a href="#cb5-925"></a>0 &amp; 1-\rho^{2}</span>
<span id="cb5-926"><a href="#cb5-926"></a>\end{array}\right|\begin{array}{cc}</span>
<span id="cb5-927"><a href="#cb5-927"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb5-928"><a href="#cb5-928"></a>-\rho &amp; 1</span>
<span id="cb5-929"><a href="#cb5-929"></a>\end{array}\right]$$</span>
<span id="cb5-930"><a href="#cb5-930"></a></span>
<span id="cb5-931"><a href="#cb5-931"></a>Performing $R_{2}=\frac{1}{1-\rho^{2}}R_{2}$, the above system is</span>
<span id="cb5-932"><a href="#cb5-932"></a>row-equivalent to:</span>
<span id="cb5-933"><a href="#cb5-933"></a></span>
<span id="cb5-934"><a href="#cb5-934"></a>$$\left[\begin{array}{cc}</span>
<span id="cb5-935"><a href="#cb5-935"></a>1 &amp; \rho<span class="sc">\\</span></span>
<span id="cb5-936"><a href="#cb5-936"></a>0 &amp; 1</span>
<span id="cb5-937"><a href="#cb5-937"></a>\end{array}\left|\begin{array}{cc}</span>
<span id="cb5-938"><a href="#cb5-938"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb5-939"><a href="#cb5-939"></a>\frac{-\rho}{1-\rho^{2}} &amp; \frac{1}{1-\rho^{2}}</span>
<span id="cb5-940"><a href="#cb5-940"></a>\end{array}\right.\right]$$</span>
<span id="cb5-941"><a href="#cb5-941"></a></span>
<span id="cb5-942"><a href="#cb5-942"></a>Performing $R_{1}=R_{1}-\rho R_{2}$, we have:</span>
<span id="cb5-943"><a href="#cb5-943"></a></span>
<span id="cb5-944"><a href="#cb5-944"></a>$$\left[\begin{array}{cc}</span>
<span id="cb5-945"><a href="#cb5-945"></a>1 &amp; 0<span class="sc">\\</span></span>
<span id="cb5-946"><a href="#cb5-946"></a>0 &amp; 1</span>
<span id="cb5-947"><a href="#cb5-947"></a>\end{array}\left|\begin{array}{cc}</span>
<span id="cb5-948"><a href="#cb5-948"></a>\frac{1}{1-\rho^{2}} &amp; -\frac{\rho}{1-\rho^{2}}<span class="sc">\\</span></span>
<span id="cb5-949"><a href="#cb5-949"></a>\frac{-\rho}{1-\rho^{2}} &amp; \frac{1}{1-\rho^{2}}</span>
<span id="cb5-950"><a href="#cb5-950"></a>\end{array}\right.\right]$$</span>
<span id="cb5-951"><a href="#cb5-951"></a></span>
<span id="cb5-952"><a href="#cb5-952"></a>Thus, $$\begin{aligned}</span>
<span id="cb5-953"><a href="#cb5-953"></a>C^{-1} &amp; =\frac{1}{1-\rho^{2}}\left[\begin{array}{cc}</span>
<span id="cb5-954"><a href="#cb5-954"></a>1 &amp; -\rho<span class="sc">\\</span></span>
<span id="cb5-955"><a href="#cb5-955"></a>-\rho &amp; 1</span>
<span id="cb5-956"><a href="#cb5-956"></a>\end{array}\right]</span>
<span id="cb5-957"><a href="#cb5-957"></a>\end{aligned}$$</span>
<span id="cb5-958"><a href="#cb5-958"></a></span>
<span id="cb5-959"><a href="#cb5-959"></a>Moreover, $\det C=1-\rho^{2}.$</span>
<span id="cb5-960"><a href="#cb5-960"></a></span>
<span id="cb5-961"><a href="#cb5-961"></a>Therefore, the joint density of $(X,Y)$ is given by:</span>
<span id="cb5-962"><a href="#cb5-962"></a></span>
<span id="cb5-963"><a href="#cb5-963"></a>$$\begin{aligned}</span>
<span id="cb5-964"><a href="#cb5-964"></a>f(x,y) &amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}\left[\begin{array}{cc}</span>
<span id="cb5-965"><a href="#cb5-965"></a>x &amp; y\end{array}\right]\left[\begin{array}{cc}</span>
<span id="cb5-966"><a href="#cb5-966"></a>1 &amp; -\rho<span class="sc">\\</span></span>
<span id="cb5-967"><a href="#cb5-967"></a>-\rho &amp; 1</span>
<span id="cb5-968"><a href="#cb5-968"></a>\end{array}\right]\left[\begin{array}{c}</span>
<span id="cb5-969"><a href="#cb5-969"></a>x<span class="sc">\\</span></span>
<span id="cb5-970"><a href="#cb5-970"></a>y</span>
<span id="cb5-971"><a href="#cb5-971"></a>\end{array}\right]\right]<span class="sc">\\</span></span>
<span id="cb5-972"><a href="#cb5-972"></a> &amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left[-\frac{1}{2(1-\rho^{2})}\left[\begin{array}{cc}</span>
<span id="cb5-973"><a href="#cb5-973"></a>x-\rho y &amp; -\rho x+y\end{array}\right]\left[\begin{array}{c}</span>
<span id="cb5-974"><a href="#cb5-974"></a>x<span class="sc">\\</span></span>
<span id="cb5-975"><a href="#cb5-975"></a>y</span>
<span id="cb5-976"><a href="#cb5-976"></a>\end{array}\right]\right]<span class="sc">\\</span></span>
<span id="cb5-977"><a href="#cb5-977"></a> &amp; \frac{1}{2\pi\sqrt{1-\rho^{2}}}\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2(1-\rho^{2})}(x^{2}-2\rho xy+y^{2})\right</span><span class="co">]</span></span>
<span id="cb5-978"><a href="#cb5-978"></a>\end{aligned}$$</span>
<span id="cb5-979"><a href="#cb5-979"></a></span>
<span id="cb5-980"><a href="#cb5-980"></a><span class="sc">\(</span>c<span class="sc">\)</span> Claim I. $\int_{\mathbf{R}}yf(x,y)dy=\rho x$.</span>
<span id="cb5-981"><a href="#cb5-981"></a></span>
<span id="cb5-982"><a href="#cb5-982"></a>Completing the square, we have:</span>
<span id="cb5-983"><a href="#cb5-983"></a></span>
<span id="cb5-984"><a href="#cb5-984"></a>$$\begin{aligned}</span>
<span id="cb5-985"><a href="#cb5-985"></a>(x^{2}-2\rho xy+y^{2}) &amp; =(y-\rho x)^{2}+x^{2}(1-\rho^{2})</span>
<span id="cb5-986"><a href="#cb5-986"></a>\end{aligned}$$</span>
<span id="cb5-987"><a href="#cb5-987"></a></span>
<span id="cb5-988"><a href="#cb5-988"></a>Thus, we can write:</span>
<span id="cb5-989"><a href="#cb5-989"></a></span>
<span id="cb5-990"><a href="#cb5-990"></a>$$\begin{aligned}</span>
<span id="cb5-991"><a href="#cb5-991"></a>\int_{\mathbf{R}}yf(x,y)dy &amp; =\frac{1}{2\pi\sqrt{1-\rho^{2}}}e^{-\frac{1}{2}x^{2}}\int_{\mathbf{R}}ye^{-\frac{1}{2}\frac{(y-\rho x)^{2}}{(1-\rho^{2})}}dy</span>
<span id="cb5-992"><a href="#cb5-992"></a>\end{aligned}$$</span>
<span id="cb5-993"><a href="#cb5-993"></a></span>
<span id="cb5-994"><a href="#cb5-994"></a>Let's substitute</span>
<span id="cb5-995"><a href="#cb5-995"></a></span>
<span id="cb5-996"><a href="#cb5-996"></a>$$\begin{aligned}</span>
<span id="cb5-997"><a href="#cb5-997"></a>z &amp; =\frac{(y-\rho x)}{\sqrt{1-\rho^{2}}}<span class="sc">\\</span></span>
<span id="cb5-998"><a href="#cb5-998"></a>dz &amp; =\frac{dy}{\sqrt{1-\rho^{2}}}</span>
<span id="cb5-999"><a href="#cb5-999"></a>\end{aligned}$$</span>
<span id="cb5-1000"><a href="#cb5-1000"></a></span>
<span id="cb5-1001"><a href="#cb5-1001"></a>Therefore,</span>
<span id="cb5-1002"><a href="#cb5-1002"></a></span>
<span id="cb5-1003"><a href="#cb5-1003"></a>$$\begin{aligned}</span>
<span id="cb5-1004"><a href="#cb5-1004"></a>\int_{\mathbf{R}}ye^{-\frac{1}{2}\frac{(y-\rho x)^{2}}{(1-\rho^{2})}}dy &amp; =\sqrt{1-\rho^{2}}\int_{\mathbf{R}}(\rho x+\sqrt{1-\rho^{2}}z)e^{-\frac{z^{2}}{2}}dz<span class="sc">\\</span></span>
<span id="cb5-1005"><a href="#cb5-1005"></a> &amp; =\rho x\cdot\sqrt{1-\rho^{2}}\int_{\mathbf{R}}e^{-\frac{z^{2}}{2}}dz+(1-\rho^{2})\int_{\mathbf{R}}ze^{-\frac{z^{2}}{2}}dz<span class="sc">\\</span></span>
<span id="cb5-1006"><a href="#cb5-1006"></a> &amp; =\rho x\cdot\sqrt{1-\rho^{2}}\cdot\sqrt{2\pi}+(1-\rho^{2})\cdot0<span class="sc">\\</span></span>
<span id="cb5-1007"><a href="#cb5-1007"></a> &amp; =\rho x\cdot\sqrt{1-\rho^{2}}\cdot\sqrt{2\pi}</span>
<span id="cb5-1008"><a href="#cb5-1008"></a>\end{aligned}$$</span>
<span id="cb5-1009"><a href="#cb5-1009"></a></span>
<span id="cb5-1010"><a href="#cb5-1010"></a>Consequently,</span>
<span id="cb5-1011"><a href="#cb5-1011"></a></span>
<span id="cb5-1012"><a href="#cb5-1012"></a>$$\begin{aligned}</span>
<span id="cb5-1013"><a href="#cb5-1013"></a>\int_{\mathbf{R}}yf(x,y)dy &amp; =\frac{1}{2\pi\cancel{\sqrt{1-\rho^{2}}}}e^{-\frac{1}{2}x^{2}}\rho x\cdot\cancel{\sqrt{1-\rho^{2}}}\cdot\sqrt{2\pi}<span class="sc">\\</span></span>
<span id="cb5-1014"><a href="#cb5-1014"></a> &amp; =\rho x\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}}<span class="sc">\\</span></span>
<span id="cb5-1015"><a href="#cb5-1015"></a> &amp; =\rho x\cdot f_{X}(x)<span class="sc">\\</span></span>
<span id="cb5-1016"><a href="#cb5-1016"></a>\frac{\int_{\mathbf{R}}yf(x,y)dy}{f_{X}(x)} &amp; =\frac{\int_{\mathbf{R}}yf(x,y)dy}{\int_{\mathbf{R}}f(x,y)}=\rho x</span>
<span id="cb5-1017"><a href="#cb5-1017"></a>\end{aligned}$$</span>
<span id="cb5-1018"><a href="#cb5-1018"></a></span>
<span id="cb5-1019"><a href="#cb5-1019"></a><span class="sc">\(</span>d<span class="sc">\)</span> For a Gaussian vector $(X,Y),$ the conditional expectation</span>
<span id="cb5-1020"><a href="#cb5-1020"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>=h(X)$. Hence, $\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>=\rho X$. ◻</span>
<span id="cb5-1021"><a href="#cb5-1021"></a>:::</span>
<span id="cb5-1022"><a href="#cb5-1022"></a></span>
<span id="cb5-1023"><a href="#cb5-1023"></a>::: defn</span>
<span id="cb5-1024"><a href="#cb5-1024"></a>(Conditional Expectation) Let $Y$ be an integrable random variable on</span>
<span id="cb5-1025"><a href="#cb5-1025"></a>$(\Omega,\mathcal{F},\mathbb{P})$ and let</span>
<span id="cb5-1026"><a href="#cb5-1026"></a>$\mathcal{G}\subseteq\mathcal{F}$ be a sigma-field of $\Omega$. The</span>
<span id="cb5-1027"><a href="#cb5-1027"></a>conditional expectation of $Y$ given $\mathcal{G}$ is the random</span>
<span id="cb5-1028"><a href="#cb5-1028"></a>variable denoted by $\mathbb{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ such that the following</span>
<span id="cb5-1029"><a href="#cb5-1029"></a>hold:</span>
<span id="cb5-1030"><a href="#cb5-1030"></a></span>
<span id="cb5-1031"><a href="#cb5-1031"></a><span class="sc">\(</span>a<span class="sc">\)</span> $\mathbb{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ is $\mathcal{G}$-measurable.</span>
<span id="cb5-1032"><a href="#cb5-1032"></a></span>
<span id="cb5-1033"><a href="#cb5-1033"></a>In other words, all events pertaining to the random variable</span>
<span id="cb5-1034"><a href="#cb5-1034"></a>$\mathbb{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ are in $\mathcal{G}$.</span>
<span id="cb5-1035"><a href="#cb5-1035"></a></span>
<span id="cb5-1036"><a href="#cb5-1036"></a><span class="sc">\(</span>b<span class="sc">\)</span> For any (bounded) random variable $W$, that is</span>
<span id="cb5-1037"><a href="#cb5-1037"></a>$\mathcal{G}$-measurable,</span>
<span id="cb5-1038"><a href="#cb5-1038"></a></span>
<span id="cb5-1039"><a href="#cb5-1039"></a>$$\begin{aligned}</span>
<span id="cb5-1040"><a href="#cb5-1040"></a>\mathbb{E}<span class="co">[</span><span class="ot">WY</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">W\mathbb{E}[Y|\mathcal{G}]</span><span class="co">]</span></span>
<span id="cb5-1041"><a href="#cb5-1041"></a>\end{aligned}$$</span>
<span id="cb5-1042"><a href="#cb5-1042"></a></span>
<span id="cb5-1043"><a href="#cb5-1043"></a>In other words, $\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ is a proxy for $Y$ as far as</span>
<span id="cb5-1044"><a href="#cb5-1044"></a>the events in $\mathcal{G}$ are concerned.</span>
<span id="cb5-1045"><a href="#cb5-1045"></a></span>
<span id="cb5-1046"><a href="#cb5-1046"></a>Note that, by taking $W=1$ in the property (B), we recover:</span>
<span id="cb5-1047"><a href="#cb5-1047"></a></span>
<span id="cb5-1048"><a href="#cb5-1048"></a>$$\begin{aligned}</span>
<span id="cb5-1049"><a href="#cb5-1049"></a>\mathbf{E}<span class="co">[</span><span class="ot">\mathbf{E}[Y|\mathcal{G}]</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span></span>
<span id="cb5-1050"><a href="#cb5-1050"></a>\end{aligned}$$</span>
<span id="cb5-1051"><a href="#cb5-1051"></a>:::</span>
<span id="cb5-1052"><a href="#cb5-1052"></a></span>
<span id="cb5-1053"><a href="#cb5-1053"></a>::: rem*</span>
<span id="cb5-1054"><a href="#cb5-1054"></a>Beware of the notation! If $\mathcal{G}=\sigma(X)$, then the conditional</span>
<span id="cb5-1055"><a href="#cb5-1055"></a>expectation $\mathbf{E}<span class="co">[</span><span class="ot">Y|\sigma(X)</span><span class="co">]</span>$ is usually denoted by</span>
<span id="cb5-1056"><a href="#cb5-1056"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ for short. However, one should always keep in mind</span>
<span id="cb5-1057"><a href="#cb5-1057"></a>that conditioning on $X$ is in fact projecting on the linear subspace</span>
<span id="cb5-1058"><a href="#cb5-1058"></a>*generated by all variables constructed from $X$* and not on the linear</span>
<span id="cb5-1059"><a href="#cb5-1059"></a>space generated by generated by $X$ alone. In the same way, the</span>
<span id="cb5-1060"><a href="#cb5-1060"></a>conditional expectation $\mathbf{E}<span class="co">[</span><span class="ot">Z|\sigma(X,Y)</span><span class="co">]</span>$ is often written</span>
<span id="cb5-1061"><a href="#cb5-1061"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Z|X,Y</span><span class="co">]</span>$ for short.</span>
<span id="cb5-1062"><a href="#cb5-1062"></a></span>
<span id="cb5-1063"><a href="#cb5-1063"></a>As expected, if $Y$ is in $L^{2}(\Omega,\mathcal{F},\mathbb{P})$, then</span>
<span id="cb5-1064"><a href="#cb5-1064"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ is given by the orthogonal projection of $Y$</span>
<span id="cb5-1065"><a href="#cb5-1065"></a>onto the subspace $L^{2}(\Omega,\mathcal{G},\mathbb{P})$, the subspace</span>
<span id="cb5-1066"><a href="#cb5-1066"></a>of square integrable random variables that are $\mathcal{G}$-measurable.</span>
<span id="cb5-1067"><a href="#cb5-1067"></a>We write $Y^{\star}$ for the random variable in</span>
<span id="cb5-1068"><a href="#cb5-1068"></a>$L^{2}(\Omega,\mathcal{G},\mathbb{P})$ that is closest to $Y$ that is:</span>
<span id="cb5-1069"><a href="#cb5-1069"></a></span>
<span id="cb5-1070"><a href="#cb5-1070"></a>$$\begin{aligned}</span>
<span id="cb5-1071"><a href="#cb5-1071"></a>\min_{Z\in L^{2}(\Omega,\mathcal{G},\mathbb{P})}\mathbf{E}<span class="co">[</span><span class="ot">(Y-Z)^{2}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(Y-Y^{\star})^{2}</span><span class="co">]</span>\label{eq:conditional-expectation}</span>
<span id="cb5-1072"><a href="#cb5-1072"></a>\end{aligned}$$</span>
<span id="cb5-1073"><a href="#cb5-1073"></a>:::</span>
<span id="cb5-1074"><a href="#cb5-1074"></a></span>
<span id="cb5-1075"><a href="#cb5-1075"></a>::: thm</span>
<span id="cb5-1076"><a href="#cb5-1076"></a>[]{#th:existence-and-uniqueness-of-conditional-expectations-II</span>
<span id="cb5-1077"><a href="#cb5-1077"></a>label="th:existence-and-uniqueness-of-conditional-expectations-II"}(Existence</span>
<span id="cb5-1078"><a href="#cb5-1078"></a>and Uniqueness of Conditional Expectations) Let</span>
<span id="cb5-1079"><a href="#cb5-1079"></a>$\mathcal{G}\subset\mathcal{F}$ be a sigma-field of $\Omega$. Let $Y$ be</span>
<span id="cb5-1080"><a href="#cb5-1080"></a>a random variable in $L^{2}(\Omega,\mathcal{F},\mathbb{P})$. Then, the</span>
<span id="cb5-1081"><a href="#cb5-1081"></a>conditional expectation $\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ is the random</span>
<span id="cb5-1082"><a href="#cb5-1082"></a>variable $Y^{\star}$ given in the equation</span>
<span id="cb5-1083"><a href="#cb5-1083"></a>(<span class="co">[</span><span class="ot">\[eq:conditional-expectation\]</span><span class="co">](#eq:conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-1084"><a href="#cb5-1084"></a>reference="eq:conditional-expectation"}). Namely, it is the random</span>
<span id="cb5-1085"><a href="#cb5-1085"></a>variable in $L^{2}(\Omega,\mathcal{G},\mathbb{P})$ that is closest to</span>
<span id="cb5-1086"><a href="#cb5-1086"></a>$Y$ in the $L^{2}$-distance. In particular we have the following:</span>
<span id="cb5-1087"><a href="#cb5-1087"></a>:::</span>
<span id="cb5-1088"><a href="#cb5-1088"></a></span>
<span id="cb5-1089"><a href="#cb5-1089"></a><span class="ss">-   </span>It is the orthogonal projection of $Y$ onto</span>
<span id="cb5-1090"><a href="#cb5-1090"></a>    $L^{2}(\Omega,\mathcal{G},\mathbb{P})$, that is, $Y-Y^{\star}$ is</span>
<span id="cb5-1091"><a href="#cb5-1091"></a>    orthogonal to the random variables in</span>
<span id="cb5-1092"><a href="#cb5-1092"></a>    $L^{2}(\Omega,\mathcal{G},\mathbb{P})$.</span>
<span id="cb5-1093"><a href="#cb5-1093"></a></span>
<span id="cb5-1094"><a href="#cb5-1094"></a><span class="ss">-   </span>It is unique.</span>
<span id="cb5-1095"><a href="#cb5-1095"></a></span>
<span id="cb5-1096"><a href="#cb5-1096"></a>Again, the result should be interpreted as follows: The conditional</span>
<span id="cb5-1097"><a href="#cb5-1097"></a>expectation $\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ is the best approximation of $Y$</span>
<span id="cb5-1098"><a href="#cb5-1098"></a>given the information included in $\mathcal{G}$.</span>
<span id="cb5-1099"><a href="#cb5-1099"></a></span>
<span id="cb5-1100"><a href="#cb5-1100"></a>::: rem*</span>
<span id="cb5-1101"><a href="#cb5-1101"></a>The conditional expectation in fact exists and is unique for any</span>
<span id="cb5-1102"><a href="#cb5-1102"></a>integrable random variable $Y$(i.e.</span>
<span id="cb5-1103"><a href="#cb5-1103"></a>$Y\in L^{1}(\Omega,\mathcal{F},\mathbb{P})$ as the definition suggests.</span>
<span id="cb5-1104"><a href="#cb5-1104"></a>However, there is no orthogonal projection in $L^{1}$, so the intuitive</span>
<span id="cb5-1105"><a href="#cb5-1105"></a>geometric picture is lost.</span>
<span id="cb5-1106"><a href="#cb5-1106"></a>:::</span>
<span id="cb5-1107"><a href="#cb5-1107"></a></span>
<span id="cb5-1108"><a href="#cb5-1108"></a>::: center</span>
<span id="cb5-1109"><a href="#cb5-1109"></a>Figure. An illustration of the conditional expectation</span>
<span id="cb5-1110"><a href="#cb5-1110"></a>$\mathbb{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ as an orthogonal projection of $Y$ onto the</span>
<span id="cb5-1111"><a href="#cb5-1111"></a>subspace $L^2(\Omega,\mathcal{G},\mathbb{P})$.</span>
<span id="cb5-1112"><a href="#cb5-1112"></a>:::</span>
<span id="cb5-1113"><a href="#cb5-1113"></a></span>
<span id="cb5-1114"><a href="#cb5-1114"></a>::: example</span>
<span id="cb5-1115"><a href="#cb5-1115"></a>(Conditional Expectation for Gaussian Vectors. II.) Consider the</span>
<span id="cb5-1116"><a href="#cb5-1116"></a>Gaussian vector $(X_{1},\ldots,X_{n})$. Without loss of generality,</span>
<span id="cb5-1117"><a href="#cb5-1117"></a>suppose it has mean $0$ and is non-degenerate. What is the best</span>
<span id="cb5-1118"><a href="#cb5-1118"></a>approximation of $X_{n}$ given the information $X_{1},\ldots,X_{n-1}$?</span>
<span id="cb5-1119"><a href="#cb5-1119"></a>In other words, what is:</span>
<span id="cb5-1120"><a href="#cb5-1120"></a></span>
<span id="cb5-1121"><a href="#cb5-1121"></a>$$\mathbf{E}[X_{n}|\sigma(X_{1},\ldots,X_{n-1})$$</span>
<span id="cb5-1122"><a href="#cb5-1122"></a></span>
<span id="cb5-1123"><a href="#cb5-1123"></a>With example</span>
<span id="cb5-1124"><a href="#cb5-1124"></a>(<span class="co">[</span><span class="ot">\[ex:sigma(X)-measurable-random-variables-example\]</span><span class="co">]</span>(#ex:sigma(X)-measurable-random-variables-example){reference-type="ref"</span>
<span id="cb5-1125"><a href="#cb5-1125"></a>reference="ex:sigma(X)-measurable-random-variables-example"}) in mind,</span>
<span id="cb5-1126"><a href="#cb5-1126"></a>let's write $\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|X_{1}\ldots X_{n-1}</span><span class="co">]</span>$ for short. From</span>
<span id="cb5-1127"><a href="#cb5-1127"></a>example</span>
<span id="cb5-1128"><a href="#cb5-1128"></a>(<span class="co">[</span><span class="ot">\[ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables\]</span><span class="co">](#ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables)</span>{reference-type="ref"</span>
<span id="cb5-1129"><a href="#cb5-1129"></a>reference="ex:=00005BArguin-4.1=00005D-Conditional-Expectation-of-continuous-random-variables"}),</span>
<span id="cb5-1130"><a href="#cb5-1130"></a>we know that if $(X,Y)$ is a Gaussian vector with mean $0$, then</span>
<span id="cb5-1131"><a href="#cb5-1131"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ is a multiple of $X$. Thus, we expect, that</span>
<span id="cb5-1132"><a href="#cb5-1132"></a>$\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|X_{1}X_{2}\ldots X_{n-1}</span><span class="co">]</span>$ is a linear combination of</span>
<span id="cb5-1133"><a href="#cb5-1133"></a>$X_{1},X_{2},\ldots,X_{n-1}$. That is, there exists</span>
<span id="cb5-1134"><a href="#cb5-1134"></a>$a_{1},\ldots,a_{n-1}$ such that:</span>
<span id="cb5-1135"><a href="#cb5-1135"></a></span>
<span id="cb5-1136"><a href="#cb5-1136"></a>$$\begin{aligned}</span>
<span id="cb5-1137"><a href="#cb5-1137"></a>\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|X_{1}X_{2}\ldots X_{n-1}</span><span class="co">]</span> &amp; =a_{1}X_{1}+a_{2}X_{2}+\ldots+a_{n-1}X_{n-1}</span>
<span id="cb5-1138"><a href="#cb5-1138"></a>\end{aligned}$$ In particular, since the conditional expectation is a</span>
<span id="cb5-1139"><a href="#cb5-1139"></a>linear combination of the $X$'s, it is itself a Gaussian random</span>
<span id="cb5-1140"><a href="#cb5-1140"></a>variable. The best way to find the coefficient $a$'s is to go back to</span>
<span id="cb5-1141"><a href="#cb5-1141"></a>IID decomposition of Gaussian vectors.</span>
<span id="cb5-1142"><a href="#cb5-1142"></a></span>
<span id="cb5-1143"><a href="#cb5-1143"></a>Let $(Z_{1},Z_{2},\ldots,Z_{n-1})$ be IID standard Gaussians constructed</span>
<span id="cb5-1144"><a href="#cb5-1144"></a>from the linear combination of $(X_{1},X_{2},\ldots,X_{n-1})$. Then, we</span>
<span id="cb5-1145"><a href="#cb5-1145"></a>have:</span>
<span id="cb5-1146"><a href="#cb5-1146"></a></span>
<span id="cb5-1147"><a href="#cb5-1147"></a>$$\begin{aligned}</span>
<span id="cb5-1148"><a href="#cb5-1148"></a>\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|X_{1}X_{2}\ldots X_{n-1}</span><span class="co">]</span> &amp; =b_{1}Z_{1}+\ldots+b_{n-1}Z_{n-1}</span>
<span id="cb5-1149"><a href="#cb5-1149"></a>\end{aligned}$$</span>
<span id="cb5-1150"><a href="#cb5-1150"></a></span>
<span id="cb5-1151"><a href="#cb5-1151"></a>Now, recall, that we construct the random variables $Z_{1}$, $Z_{2}$,</span>
<span id="cb5-1152"><a href="#cb5-1152"></a>$\ldots$, $Z_{n}$ using Gram-Schmidt orthogonalization:</span>
<span id="cb5-1153"><a href="#cb5-1153"></a></span>
<span id="cb5-1154"><a href="#cb5-1154"></a>$$\begin{aligned}</span>
<span id="cb5-1155"><a href="#cb5-1155"></a>\tilde{Z_{1}} &amp; =X_{1}, &amp; Z_{1} &amp; =\frac{\tilde{Z_{1}}}{\mathbf{E}(\tilde{Z}_{1}^{2})}<span class="sc">\\</span></span>
<span id="cb5-1156"><a href="#cb5-1156"></a>\tilde{Z_{2}} &amp; =X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1} &amp; Z_{2} &amp; =\frac{\tilde{Z}_{2}}{\mathbf{E}(\tilde{Z}_{2}^{2})}<span class="sc">\\</span></span>
<span id="cb5-1157"><a href="#cb5-1157"></a>\tilde{Z_{3}} &amp; =X_{3}-\sum_{i=1}^{2}\mathbf{E}(X_{3}Z_{i})Z_{i} &amp; Z_{3} &amp; =\frac{\tilde{Z}_{3}}{\mathbf{E}(\tilde{Z}_{3}^{2})}<span class="sc">\\</span></span>
<span id="cb5-1158"><a href="#cb5-1158"></a> &amp; \vdots</span>
<span id="cb5-1159"><a href="#cb5-1159"></a>\end{aligned}$$</span>
<span id="cb5-1160"><a href="#cb5-1160"></a>:::</span>
<span id="cb5-1161"><a href="#cb5-1161"></a></span>
<span id="cb5-1162"><a href="#cb5-1162"></a>**The simple case for $n=2$ random variables.**</span>
<span id="cb5-1163"><a href="#cb5-1163"></a></span>
<span id="cb5-1164"><a href="#cb5-1164"></a>We have already seen before:</span>
<span id="cb5-1165"><a href="#cb5-1165"></a></span>
<span id="cb5-1166"><a href="#cb5-1166"></a>$$\begin{aligned}</span>
<span id="cb5-1167"><a href="#cb5-1167"></a>\mathbf{E}<span class="co">[</span><span class="ot">X_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1168"><a href="#cb5-1168"></a> &amp; =\frac{\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>}{\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>}\times\mathbf{E}\left<span class="co">[</span><span class="ot">\tilde{Z}_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1169"><a href="#cb5-1169"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>\times\mathbf{E}\left<span class="co">[</span><span class="ot">\frac{\tilde{Z}_{1}}{\mathbf{E}[\tilde{Z}_{1}^{2}]}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1170"><a href="#cb5-1170"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>\times\mathbf{E}<span class="co">[</span><span class="ot">Z_{1}(X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1})</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1171"><a href="#cb5-1171"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>\times\left(\mathbf{E}<span class="co">[</span><span class="ot">Z_{1}X_{2}</span><span class="co">]</span>-\mathbf{E}(X_{2}Z_{1})\mathbf{E}<span class="co">[</span><span class="ot">Z_{1}^{2}</span><span class="co">]</span>\right)<span class="sc">\\</span></span>
<span id="cb5-1172"><a href="#cb5-1172"></a> &amp; =0</span>
<span id="cb5-1173"><a href="#cb5-1173"></a>\end{aligned}$$</span>
<span id="cb5-1174"><a href="#cb5-1174"></a></span>
<span id="cb5-1175"><a href="#cb5-1175"></a>So,$X_{2}-\mathbf{E}(X_{2}Z_{1})Z_{1}$ is orthogonal to $X_{1}$.</span>
<span id="cb5-1176"><a href="#cb5-1176"></a></span>
<span id="cb5-1177"><a href="#cb5-1177"></a>Moreover, $\mathbf{E}(X_{2}Z_{1})Z_{1}$ is a function of $X_{1}$. Thus,</span>
<span id="cb5-1178"><a href="#cb5-1178"></a>both the properties of conditional expectation are satisfied. Since</span>
<span id="cb5-1179"><a href="#cb5-1179"></a>conditional expectations are unique, we must have,</span>
<span id="cb5-1180"><a href="#cb5-1180"></a>$\mathbf{E}<span class="co">[</span><span class="ot">X_{2}|X_{1}</span><span class="co">]</span>=\mathbf{E}(X_{2}Z_{1})Z_{1}$.</span>
<span id="cb5-1181"><a href="#cb5-1181"></a></span>
<span id="cb5-1182"><a href="#cb5-1182"></a>**The case for $n=3$ random variables.**</span>
<span id="cb5-1183"><a href="#cb5-1183"></a></span>
<span id="cb5-1184"><a href="#cb5-1184"></a>We have seen that:</span>
<span id="cb5-1185"><a href="#cb5-1185"></a></span>
<span id="cb5-1186"><a href="#cb5-1186"></a>$$\begin{aligned}</span>
<span id="cb5-1187"><a href="#cb5-1187"></a>\mathbf{E}<span class="co">[</span><span class="ot">X_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})</span><span class="co">]</span> &amp; =\frac{\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>}{\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>}\times\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1188"><a href="#cb5-1188"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>\times\mathbf{E}\left<span class="sc">\{</span> \frac{\tilde{Z}_{1}}{\mathbf{E}[\tilde{Z}_{1}^{2}]}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb5-1189"><a href="#cb5-1189"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>\times\mathbf{E}\left<span class="sc">\{</span> Z_{1}(X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2})\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb5-1190"><a href="#cb5-1190"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\tilde{Z}_{1}^{2}</span><span class="co">]</span>\times\mathbf{E}<span class="co">[</span><span class="ot">X_{3}Z_{1}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">X_{3}Z_{1}</span><span class="co">]</span>\mathbf{E}<span class="co">[</span><span class="ot">Z_{1}^{2}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">X_{3}Z_{2}</span><span class="co">]</span>\mathbf{E}<span class="co">[</span><span class="ot">Z_{1}Z_{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1191"><a href="#cb5-1191"></a> &amp; =0</span>
<span id="cb5-1192"><a href="#cb5-1192"></a>\end{aligned}$$</span>
<span id="cb5-1193"><a href="#cb5-1193"></a></span>
<span id="cb5-1194"><a href="#cb5-1194"></a>It is an easy exercise to show that it is orthogonal to $X_{2}$.</span>
<span id="cb5-1195"><a href="#cb5-1195"></a></span>
<span id="cb5-1196"><a href="#cb5-1196"></a>Hence, $X_{3}-\mathbf{E}(X_{3}Z_{1})Z_{1}-\mathbf{E}(X_{3}Z_{2})Z_{2}$</span>
<span id="cb5-1197"><a href="#cb5-1197"></a>is orthogonal to $X_{1}$ and $X_{2}$. Moreover,</span>
<span id="cb5-1198"><a href="#cb5-1198"></a>$\mathbf{E}(X_{3}Z_{1})Z_{1}+\mathbf{E}(X_{3}Z_{2})Z_{2}$ is a function</span>
<span id="cb5-1199"><a href="#cb5-1199"></a>of $X_{1}$, $X_{2}$. Thus, we must have:</span>
<span id="cb5-1200"><a href="#cb5-1200"></a></span>
<span id="cb5-1201"><a href="#cb5-1201"></a>$$\begin{aligned}</span>
<span id="cb5-1202"><a href="#cb5-1202"></a>\mathbf{E}<span class="co">[</span><span class="ot">X_{3}|X_{1}X_{2}</span><span class="co">]</span> &amp; =\mathbf{E}(X_{3}Z_{1})Z_{1}+\mathbf{E}(X_{3}Z_{2})Z_{2}</span>
<span id="cb5-1203"><a href="#cb5-1203"></a>\end{aligned}$$</span>
<span id="cb5-1204"><a href="#cb5-1204"></a></span>
<span id="cb5-1205"><a href="#cb5-1205"></a>In general, $X_{n}-\sum_{i=1}^{n-1}\mathbf{E}(X_{n}Z_{i})Z_{i}$ is</span>
<span id="cb5-1206"><a href="#cb5-1206"></a>orthogonal to $X_{1}$, $X_{2}$, $\ldots$, $X_{n-1}$. Hence,</span>
<span id="cb5-1207"><a href="#cb5-1207"></a></span>
<span id="cb5-1208"><a href="#cb5-1208"></a>$$\begin{aligned}</span>
<span id="cb5-1209"><a href="#cb5-1209"></a>\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|X_{1}X_{2}\ldots X_{n-1}</span><span class="co">]</span> &amp; =\sum_{i=1}^{n-1}\mathbf{E}(X_{n}Z_{i})Z_{i}</span>
<span id="cb5-1210"><a href="#cb5-1210"></a>\end{aligned}$$</span>
<span id="cb5-1211"><a href="#cb5-1211"></a></span>
<span id="cb5-1212"><a href="#cb5-1212"></a><span class="fu">### Properties of Conditional Expectation.</span></span>
<span id="cb5-1213"><a href="#cb5-1213"></a></span>
<span id="cb5-1214"><a href="#cb5-1214"></a>We now list the properties of conditional expectation that follow from</span>
<span id="cb5-1215"><a href="#cb5-1215"></a>the two defining properties (A), (B) in the definition. They are</span>
<span id="cb5-1216"><a href="#cb5-1216"></a>extremely useful, when doing explicit computations on martingales. A</span>
<span id="cb5-1217"><a href="#cb5-1217"></a>good way to remember them is to understand how they relate to the</span>
<span id="cb5-1218"><a href="#cb5-1218"></a>interpretation of conditional expectation as an orthogonal projection</span>
<span id="cb5-1219"><a href="#cb5-1219"></a>onto a subspace or, equivalently, as the best approximation of the</span>
<span id="cb5-1220"><a href="#cb5-1220"></a>variable given the information available.</span>
<span id="cb5-1221"><a href="#cb5-1221"></a></span>
<span id="cb5-1222"><a href="#cb5-1222"></a>::: prop</span>
<span id="cb5-1223"><a href="#cb5-1223"></a>[]{#prop:properties-of-conditional-expectation</span>
<span id="cb5-1224"><a href="#cb5-1224"></a>label="prop:properties-of-conditional-expectation"}Let $Y$ be an</span>
<span id="cb5-1225"><a href="#cb5-1225"></a>integrable random variable on $(\Omega,\mathcal{F},\mathbb{P})$. Let</span>
<span id="cb5-1226"><a href="#cb5-1226"></a>$\mathcal{G}\subseteq\mathcal{F}$ be another sigma-field of $\Omega$.</span>
<span id="cb5-1227"><a href="#cb5-1227"></a>Then, the conditional expectation $\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ has the</span>
<span id="cb5-1228"><a href="#cb5-1228"></a>following properties:</span>
<span id="cb5-1229"><a href="#cb5-1229"></a></span>
<span id="cb5-1230"><a href="#cb5-1230"></a><span class="sc">\(</span>1<span class="sc">\)</span> If $Y$ is $\mathcal{G}$-measurable, then :</span>
<span id="cb5-1231"><a href="#cb5-1231"></a></span>
<span id="cb5-1232"><a href="#cb5-1232"></a>$$\begin{aligned}</span>
<span id="cb5-1233"><a href="#cb5-1233"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span> &amp; =Y</span>
<span id="cb5-1234"><a href="#cb5-1234"></a>\end{aligned}$$</span>
<span id="cb5-1235"><a href="#cb5-1235"></a></span>
<span id="cb5-1236"><a href="#cb5-1236"></a><span class="sc">\(</span>2<span class="sc">\)</span> Taking out what is known. More generally, if $Y$ is</span>
<span id="cb5-1237"><a href="#cb5-1237"></a>$\mathcal{G-}$measurable and $X$ is another integrable random variable</span>
<span id="cb5-1238"><a href="#cb5-1238"></a>(with $XY$ also integrable), then :</span>
<span id="cb5-1239"><a href="#cb5-1239"></a></span>
<span id="cb5-1240"><a href="#cb5-1240"></a>$$\begin{aligned}</span>
<span id="cb5-1241"><a href="#cb5-1241"></a>\mathbf{E}<span class="co">[</span><span class="ot">XY|\mathcal{G}</span><span class="co">]</span> &amp; =Y\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span></span>
<span id="cb5-1242"><a href="#cb5-1242"></a>\end{aligned}$$</span>
<span id="cb5-1243"><a href="#cb5-1243"></a></span>
<span id="cb5-1244"><a href="#cb5-1244"></a>This makes sense, since $Y$ is determined by $\mathcal{G}$, so we can</span>
<span id="cb5-1245"><a href="#cb5-1245"></a>take out what is known; it can be treated as a constant for the</span>
<span id="cb5-1246"><a href="#cb5-1246"></a>conditional expectation.</span>
<span id="cb5-1247"><a href="#cb5-1247"></a></span>
<span id="cb5-1248"><a href="#cb5-1248"></a><span class="sc">\(</span>3<span class="sc">\)</span> Independence. If $Y$ is independent of $\mathcal{G}$, that is, for</span>
<span id="cb5-1249"><a href="#cb5-1249"></a>any events $<span class="sc">\{</span>Y\in(a,b]<span class="sc">\}</span>$ and $A\in\mathcal{G}$:</span>
<span id="cb5-1250"><a href="#cb5-1250"></a></span>
<span id="cb5-1251"><a href="#cb5-1251"></a>$$\begin{aligned}</span>
<span id="cb5-1252"><a href="#cb5-1252"></a>\mathbb{P}(<span class="sc">\{</span>Y\in I<span class="sc">\}</span>\cap A) &amp; =\mathbb{P}(<span class="sc">\{</span>Y\in I<span class="sc">\}</span>)\cdot\mathbb{P}(A)</span>
<span id="cb5-1253"><a href="#cb5-1253"></a>\end{aligned}$$</span>
<span id="cb5-1254"><a href="#cb5-1254"></a></span>
<span id="cb5-1255"><a href="#cb5-1255"></a>then</span>
<span id="cb5-1256"><a href="#cb5-1256"></a></span>
<span id="cb5-1257"><a href="#cb5-1257"></a>$$\begin{aligned}</span>
<span id="cb5-1258"><a href="#cb5-1258"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span></span>
<span id="cb5-1259"><a href="#cb5-1259"></a>\end{aligned}$$</span>
<span id="cb5-1260"><a href="#cb5-1260"></a></span>
<span id="cb5-1261"><a href="#cb5-1261"></a>In other words, if you have no information on $Y$, your best guess for</span>
<span id="cb5-1262"><a href="#cb5-1262"></a>its value is simply plain expectation.</span>
<span id="cb5-1263"><a href="#cb5-1263"></a></span>
<span id="cb5-1264"><a href="#cb5-1264"></a><span class="sc">\(</span>4<span class="sc">\)</span> Linearity of conditional expectations. Let $X$ be another</span>
<span id="cb5-1265"><a href="#cb5-1265"></a>integrable random variable on $(\Omega,\mathcal{F},\mathbb{P})$. Then,</span>
<span id="cb5-1266"><a href="#cb5-1266"></a></span>
<span id="cb5-1267"><a href="#cb5-1267"></a>$$\begin{aligned}</span>
<span id="cb5-1268"><a href="#cb5-1268"></a>\mathbf{E}<span class="co">[</span><span class="ot">aX+bY|\mathcal{G}</span><span class="co">]</span> &amp; =a\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>+b\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>,\quad\text{for any }a,b\in\mathbf{R}</span>
<span id="cb5-1269"><a href="#cb5-1269"></a>\end{aligned}$$</span>
<span id="cb5-1270"><a href="#cb5-1270"></a></span>
<span id="cb5-1271"><a href="#cb5-1271"></a>The linearity justifies the cumbersom choice of notation</span>
<span id="cb5-1272"><a href="#cb5-1272"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ for the random variable.</span>
<span id="cb5-1273"><a href="#cb5-1273"></a></span>
<span id="cb5-1274"><a href="#cb5-1274"></a><span class="sc">\(</span>5<span class="sc">\)</span> Tower Property : If $\mathcal{H}\subseteq\mathcal{G}$ is another</span>
<span id="cb5-1275"><a href="#cb5-1275"></a>sigma-field of $\Omega$, then:</span>
<span id="cb5-1276"><a href="#cb5-1276"></a></span>
<span id="cb5-1277"><a href="#cb5-1277"></a>$$\begin{aligned}</span>
<span id="cb5-1278"><a href="#cb5-1278"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{H}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\mathbf{E}[Y|\mathcal{G}]|\mathcal{H}</span><span class="co">]</span></span>
<span id="cb5-1279"><a href="#cb5-1279"></a>\end{aligned}$$</span>
<span id="cb5-1280"><a href="#cb5-1280"></a></span>
<span id="cb5-1281"><a href="#cb5-1281"></a>Think in terms of two successive projections: first on a plane, then on</span>
<span id="cb5-1282"><a href="#cb5-1282"></a>a line in the plane.</span>
<span id="cb5-1283"><a href="#cb5-1283"></a></span>
<span id="cb5-1284"><a href="#cb5-1284"></a><span class="sc">\(</span>6<span class="sc">\)</span> Pythagoras Theorem. We have:</span>
<span id="cb5-1285"><a href="#cb5-1285"></a></span>
<span id="cb5-1286"><a href="#cb5-1286"></a>$$\begin{aligned}</span>
<span id="cb5-1287"><a href="#cb5-1287"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y^{2}</span><span class="co">]</span> &amp; =\mathbf{E}\left<span class="co">[</span><span class="ot">\left(\mathbf{E}[Y|\mathcal{G}]\right)^{2}\right</span><span class="co">]</span>+\mathbf{E}\left<span class="co">[</span><span class="ot">\left(Y-\mathbf{E}[Y|\mathcal{G}]\right)^{2}\right</span><span class="co">]</span></span>
<span id="cb5-1288"><a href="#cb5-1288"></a>\end{aligned}$$</span>
<span id="cb5-1289"><a href="#cb5-1289"></a></span>
<span id="cb5-1290"><a href="#cb5-1290"></a>In particular:</span>
<span id="cb5-1291"><a href="#cb5-1291"></a></span>
<span id="cb5-1292"><a href="#cb5-1292"></a>$$\begin{aligned}</span>
<span id="cb5-1293"><a href="#cb5-1293"></a>\mathbf{E}\left<span class="co">[</span><span class="ot">\left(\mathbf{E}\left[Y|\mathcal{G}\right]\right)^{2}\right</span><span class="co">]</span> &amp; \leq\mathbf{E}<span class="co">[</span><span class="ot">Y^{2}</span><span class="co">]</span></span>
<span id="cb5-1294"><a href="#cb5-1294"></a>\end{aligned}$$</span>
<span id="cb5-1295"><a href="#cb5-1295"></a></span>
<span id="cb5-1296"><a href="#cb5-1296"></a>In words, the $L^{2}$ norm of $\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>$ is smaller</span>
<span id="cb5-1297"><a href="#cb5-1297"></a>than the one of $X$, which is clear if you think in terms of orthogonal</span>
<span id="cb5-1298"><a href="#cb5-1298"></a>projection.</span>
<span id="cb5-1299"><a href="#cb5-1299"></a></span>
<span id="cb5-1300"><a href="#cb5-1300"></a><span class="sc">\(</span>7<span class="sc">\)</span> Expectation of the conditional expectation.</span>
<span id="cb5-1301"><a href="#cb5-1301"></a></span>
<span id="cb5-1302"><a href="#cb5-1302"></a>$$\begin{aligned}</span>
<span id="cb5-1303"><a href="#cb5-1303"></a>\mathbf{E}\left<span class="co">[</span><span class="ot">\mathbf{E}[Y|\mathcal{G}]\right</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span></span>
<span id="cb5-1304"><a href="#cb5-1304"></a>\end{aligned}$$</span>
<span id="cb5-1305"><a href="#cb5-1305"></a>:::</span>
<span id="cb5-1306"><a href="#cb5-1306"></a></span>
<span id="cb5-1307"><a href="#cb5-1307"></a>*Proof.*</span>
<span id="cb5-1308"><a href="#cb5-1308"></a></span>
<span id="cb5-1309"><a href="#cb5-1309"></a>The uniqueness property of conditional expectations in theorem</span>
<span id="cb5-1310"><a href="#cb5-1310"></a>(<span class="co">[</span><span class="ot">\[th:existence-and-uniqueness-of-conditional-expectations-II\]</span><span class="co">](#th:existence-and-uniqueness-of-conditional-expectations-II)</span>{reference-type="ref"</span>
<span id="cb5-1311"><a href="#cb5-1311"></a>reference="th:existence-and-uniqueness-of-conditional-expectations-II"})</span>
<span id="cb5-1312"><a href="#cb5-1312"></a>might appear to be an academic curiosity. On the contrary, it is very</span>
<span id="cb5-1313"><a href="#cb5-1313"></a>practical, since it ensures, that if we find a candidate for the</span>
<span id="cb5-1314"><a href="#cb5-1314"></a>conditional expectation that has the two properties in Definition</span>
<span id="cb5-1315"><a href="#cb5-1315"></a>(<span class="co">[</span><span class="ot">\[def:conditional-expectation\]</span><span class="co">](#def:conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-1316"><a href="#cb5-1316"></a>reference="def:conditional-expectation"}), then it must be *the*</span>
<span id="cb5-1317"><a href="#cb5-1317"></a>conditional expectation. To see this, let's prove property (1).</span>
<span id="cb5-1318"><a href="#cb5-1318"></a></span>
<span id="cb5-1319"><a href="#cb5-1319"></a>::: claim</span>
<span id="cb5-1320"><a href="#cb5-1320"></a>If $Y$ is $\mathcal{G}$-measurable, then $\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>=Y$.</span>
<span id="cb5-1321"><a href="#cb5-1321"></a></span>
<span id="cb5-1322"><a href="#cb5-1322"></a>It suffices to show that $Y$ has the two defining properties of</span>
<span id="cb5-1323"><a href="#cb5-1323"></a>conditional expectation.</span>
<span id="cb5-1324"><a href="#cb5-1324"></a></span>
<span id="cb5-1325"><a href="#cb5-1325"></a><span class="sc">\(</span>1<span class="sc">\)</span> We are given that, $Y$ is $\mathcal{G}$-measurable. So, property</span>
<span id="cb5-1326"><a href="#cb5-1326"></a>(A) is satisfied.</span>
<span id="cb5-1327"><a href="#cb5-1327"></a></span>
<span id="cb5-1328"><a href="#cb5-1328"></a><span class="sc">\(</span>2<span class="sc">\)</span> For any bounded random variable $W$ that is</span>
<span id="cb5-1329"><a href="#cb5-1329"></a>$\mathcal{G}$-measurable, we have:</span>
<span id="cb5-1330"><a href="#cb5-1330"></a></span>
<span id="cb5-1331"><a href="#cb5-1331"></a>$$\begin{aligned}</span>
<span id="cb5-1332"><a href="#cb5-1332"></a>\mathbf{E}<span class="co">[</span><span class="ot">W(Y-Y)</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">0</span><span class="co">]</span>=0</span>
<span id="cb5-1333"><a href="#cb5-1333"></a>\end{aligned}$$</span>
<span id="cb5-1334"><a href="#cb5-1334"></a></span>
<span id="cb5-1335"><a href="#cb5-1335"></a>So, property (B) is also a triviality.</span>
<span id="cb5-1336"><a href="#cb5-1336"></a>:::</span>
<span id="cb5-1337"><a href="#cb5-1337"></a></span>
<span id="cb5-1338"><a href="#cb5-1338"></a>::: claim</span>
<span id="cb5-1339"><a href="#cb5-1339"></a>(Taking out what is known.) If $Y$ is $\mathcal{G}$-measurable and $X$</span>
<span id="cb5-1340"><a href="#cb5-1340"></a>is another integrable random variable, then:</span>
<span id="cb5-1341"><a href="#cb5-1341"></a></span>
<span id="cb5-1342"><a href="#cb5-1342"></a>$$\begin{aligned}</span>
<span id="cb5-1343"><a href="#cb5-1343"></a>\mathbf{E}<span class="co">[</span><span class="ot">XY|\mathcal{G}</span><span class="co">]</span> &amp; =Y\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span></span>
<span id="cb5-1344"><a href="#cb5-1344"></a>\end{aligned}$$</span>
<span id="cb5-1345"><a href="#cb5-1345"></a></span>
<span id="cb5-1346"><a href="#cb5-1346"></a>In a similar vein, it suffices to show that,</span>
<span id="cb5-1347"><a href="#cb5-1347"></a>$Y\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>$ has the two defining properties of</span>
<span id="cb5-1348"><a href="#cb5-1348"></a>conditional expectation.</span>
<span id="cb5-1349"><a href="#cb5-1349"></a></span>
<span id="cb5-1350"><a href="#cb5-1350"></a><span class="sc">\(</span>1<span class="sc">\)</span> We are given that $Y$ is $\mathcal{G}$-measurable; from property</span>
<span id="cb5-1351"><a href="#cb5-1351"></a>(1), $\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>$ is $\mathcal{G}$-measurable. It follows</span>
<span id="cb5-1352"><a href="#cb5-1352"></a>that, $Y\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>$ is $\mathcal{G}$-measurable.</span>
<span id="cb5-1353"><a href="#cb5-1353"></a></span>
<span id="cb5-1354"><a href="#cb5-1354"></a><span class="sc">\(</span>2<span class="sc">\)</span> From theorem</span>
<span id="cb5-1355"><a href="#cb5-1355"></a>(<span class="co">[</span><span class="ot">\[th:existence-and-uniqueness-of-conditional-expectations-II\]</span><span class="co">](#th:existence-and-uniqueness-of-conditional-expectations-II)</span>{reference-type="ref"</span>
<span id="cb5-1356"><a href="#cb5-1356"></a>reference="th:existence-and-uniqueness-of-conditional-expectations-II"}),</span>
<span id="cb5-1357"><a href="#cb5-1357"></a>$X-\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>$ is orthogonal to the random variables</span>
<span id="cb5-1358"><a href="#cb5-1358"></a>$L^{2}(\Omega,\mathcal{G},\mathbb{P})$. So, if $W$ is any bounded</span>
<span id="cb5-1359"><a href="#cb5-1359"></a>$\mathcal{G}$-measurable random variable, it follows that:</span>
<span id="cb5-1360"><a href="#cb5-1360"></a></span>
<span id="cb5-1361"><a href="#cb5-1361"></a>$$\begin{aligned}</span>
<span id="cb5-1362"><a href="#cb5-1362"></a>\mathbf{E}<span class="co">[</span><span class="ot">WY(X-\mathbf{E}[X|\mathcal{G}])</span><span class="co">]</span> &amp; =0<span class="sc">\\</span></span>
<span id="cb5-1363"><a href="#cb5-1363"></a>\implies\mathbf{E}<span class="co">[</span><span class="ot">W\cdot XY</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">WY\mathbf{E}[X|\mathcal{G}]</span><span class="co">]</span></span>
<span id="cb5-1364"><a href="#cb5-1364"></a>\end{aligned}$$</span>
<span id="cb5-1365"><a href="#cb5-1365"></a></span>
<span id="cb5-1366"><a href="#cb5-1366"></a>This closes the proof.</span>
<span id="cb5-1367"><a href="#cb5-1367"></a>:::</span>
<span id="cb5-1368"><a href="#cb5-1368"></a></span>
<span id="cb5-1369"><a href="#cb5-1369"></a>::: claim</span>
<span id="cb5-1370"><a href="#cb5-1370"></a>(Independence.) If $Y$ is independent of $\mathcal{G}$, that is, for all</span>
<span id="cb5-1371"><a href="#cb5-1371"></a>events $<span class="sc">\{</span>Y\in(a,b]<span class="sc">\}</span>$ and $A\in\mathcal{G}$,</span>
<span id="cb5-1372"><a href="#cb5-1372"></a></span>
<span id="cb5-1373"><a href="#cb5-1373"></a>$$\begin{aligned}</span>
<span id="cb5-1374"><a href="#cb5-1374"></a>\mathbf{\mathbb{P}}<span class="sc">\{</span>Y\in(a,b]\cap A<span class="sc">\}</span> &amp; =\mathbb{P}<span class="sc">\{</span>Y\in(a,b]<span class="sc">\}</span>\cdot\mathbb{P}(A)</span>
<span id="cb5-1375"><a href="#cb5-1375"></a>\end{aligned}$$</span>
<span id="cb5-1376"><a href="#cb5-1376"></a></span>
<span id="cb5-1377"><a href="#cb5-1377"></a>then</span>
<span id="cb5-1378"><a href="#cb5-1378"></a></span>
<span id="cb5-1379"><a href="#cb5-1379"></a>$$\begin{aligned}</span>
<span id="cb5-1380"><a href="#cb5-1380"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span></span>
<span id="cb5-1381"><a href="#cb5-1381"></a>\end{aligned}$$</span>
<span id="cb5-1382"><a href="#cb5-1382"></a></span>
<span id="cb5-1383"><a href="#cb5-1383"></a>Let us show that $\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$ has the two defining properties of</span>
<span id="cb5-1384"><a href="#cb5-1384"></a>conditional expectations.</span>
<span id="cb5-1385"><a href="#cb5-1385"></a></span>
<span id="cb5-1386"><a href="#cb5-1386"></a><span class="sc">\(</span>1<span class="sc">\)</span> $\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$ is a constant and so it is $\mathcal{F}_{0}$</span>
<span id="cb5-1387"><a href="#cb5-1387"></a>measurable. Hence, it is $\mathcal{G}$ measurable.</span>
<span id="cb5-1388"><a href="#cb5-1388"></a></span>
<span id="cb5-1389"><a href="#cb5-1389"></a><span class="sc">\(</span>2<span class="sc">\)</span> If $W$ is another $\mathcal{G}$-measurable random variable,</span>
<span id="cb5-1390"><a href="#cb5-1390"></a></span>
<span id="cb5-1391"><a href="#cb5-1391"></a>$$\begin{aligned}</span>
<span id="cb5-1392"><a href="#cb5-1392"></a>\mathbf{E}<span class="co">[</span><span class="ot">WY</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">W</span><span class="co">]</span>\cdot\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span></span>
<span id="cb5-1393"><a href="#cb5-1393"></a>\end{aligned}$$</span>
<span id="cb5-1394"><a href="#cb5-1394"></a></span>
<span id="cb5-1395"><a href="#cb5-1395"></a>since $Y$ is independent of $\mathcal{G}$ and therefore it is</span>
<span id="cb5-1396"><a href="#cb5-1396"></a>independent of $Y$. Hence,</span>
<span id="cb5-1397"><a href="#cb5-1397"></a></span>
<span id="cb5-1398"><a href="#cb5-1398"></a>$$\begin{aligned}</span>
<span id="cb5-1399"><a href="#cb5-1399"></a>\mathbf{E}<span class="co">[</span><span class="ot">W(Y-\mathbf{E}[Y])</span><span class="co">]</span> &amp; =0</span>
<span id="cb5-1400"><a href="#cb5-1400"></a>\end{aligned}$$</span>
<span id="cb5-1401"><a href="#cb5-1401"></a></span>
<span id="cb5-1402"><a href="#cb5-1402"></a>Consequently, $\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$.</span>
<span id="cb5-1403"><a href="#cb5-1403"></a>:::</span>
<span id="cb5-1404"><a href="#cb5-1404"></a></span>
<span id="cb5-1405"><a href="#cb5-1405"></a>::: claim</span>
<span id="cb5-1406"><a href="#cb5-1406"></a>(Linearity of conditional expectations) Let $X$ be another integrable</span>
<span id="cb5-1407"><a href="#cb5-1407"></a>random variable on $(\Omega,\mathcal{F},\mathbb{P})$. Then,</span>
<span id="cb5-1408"><a href="#cb5-1408"></a></span>
<span id="cb5-1409"><a href="#cb5-1409"></a>$$\begin{aligned}</span>
<span id="cb5-1410"><a href="#cb5-1410"></a>\mathbf{E}<span class="co">[</span><span class="ot">aX+bY|\mathcal{G}</span><span class="co">]</span> &amp; =a\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>+b\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>,\quad\text{for any }a,b\in\mathbf{R}</span>
<span id="cb5-1411"><a href="#cb5-1411"></a>\end{aligned}$$</span>
<span id="cb5-1412"><a href="#cb5-1412"></a>:::</span>
<span id="cb5-1413"><a href="#cb5-1413"></a></span>
<span id="cb5-1414"><a href="#cb5-1414"></a>Since $\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>$ and $\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ are</span>
<span id="cb5-1415"><a href="#cb5-1415"></a>$\mathcal{G}-$measurable, any linear combination of these two random</span>
<span id="cb5-1416"><a href="#cb5-1416"></a>variables is also $\mathcal{G}$-measurable.</span>
<span id="cb5-1417"><a href="#cb5-1417"></a></span>
<span id="cb5-1418"><a href="#cb5-1418"></a>Also, if $W$ is any bounded $\mathcal{G}-$measurable random variable, we</span>
<span id="cb5-1419"><a href="#cb5-1419"></a>have:</span>
<span id="cb5-1420"><a href="#cb5-1420"></a></span>
<span id="cb5-1421"><a href="#cb5-1421"></a>$$\begin{aligned}</span>
<span id="cb5-1422"><a href="#cb5-1422"></a>\mathbf{E}<span class="co">[</span><span class="ot">W(aX+bY-(a\mathbf{E}[X|\mathcal{G}]+b\mathbf{E}[Y|\mathcal{G}]))</span><span class="co">]</span> &amp; =a\mathbf{E}<span class="co">[</span><span class="ot">W(X-\mathbf{E}[X|\mathcal{G}])</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1423"><a href="#cb5-1423"></a> &amp; +b\mathbf{E}<span class="co">[</span><span class="ot">W(Y-\mathbf{E}[Y|\mathcal{G}])</span><span class="co">]</span></span>
<span id="cb5-1424"><a href="#cb5-1424"></a>\end{aligned}$$</span>
<span id="cb5-1425"><a href="#cb5-1425"></a></span>
<span id="cb5-1426"><a href="#cb5-1426"></a>By definition, $X-\mathbf{E}(X|\mathcal{G})$ is orthogonal t o the</span>
<span id="cb5-1427"><a href="#cb5-1427"></a>subspace $L^{2}(\Omega,\mathcal{G},\mathbb{P})$ and hence to all</span>
<span id="cb5-1428"><a href="#cb5-1428"></a>$\mathcal{G}$-measurable random-variables. Hence, the two expectations</span>
<span id="cb5-1429"><a href="#cb5-1429"></a>on the right hand side of the above expression are $0$. Since,</span>
<span id="cb5-1430"><a href="#cb5-1430"></a>conditional expectations are unique, we have the desired result.</span>
<span id="cb5-1431"><a href="#cb5-1431"></a></span>
<span id="cb5-1432"><a href="#cb5-1432"></a>::: claim</span>
<span id="cb5-1433"><a href="#cb5-1433"></a>If $\mathcal{H}\subseteq\mathcal{G}$ is another sigma-field of $\Omega$,</span>
<span id="cb5-1434"><a href="#cb5-1434"></a>then</span>
<span id="cb5-1435"><a href="#cb5-1435"></a></span>
<span id="cb5-1436"><a href="#cb5-1436"></a>$$\begin{aligned}</span>
<span id="cb5-1437"><a href="#cb5-1437"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{H}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\mathbf{E}[Y|\mathcal{G}]|\mathcal{H}</span><span class="co">]</span></span>
<span id="cb5-1438"><a href="#cb5-1438"></a>\end{aligned}$$</span>
<span id="cb5-1439"><a href="#cb5-1439"></a></span>
<span id="cb5-1440"><a href="#cb5-1440"></a>Define $U:=\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$. By definition,</span>
<span id="cb5-1441"><a href="#cb5-1441"></a>$\mathbf{E}<span class="co">[</span><span class="ot">U|\mathcal{H}</span><span class="co">]</span>$ is $\mathcal{H}$-measurable.</span>
<span id="cb5-1442"><a href="#cb5-1442"></a></span>
<span id="cb5-1443"><a href="#cb5-1443"></a>Let $W$ be any bounded $\mathcal{H}$-measurable random variable. We</span>
<span id="cb5-1444"><a href="#cb5-1444"></a>have:</span>
<span id="cb5-1445"><a href="#cb5-1445"></a></span>
<span id="cb5-1446"><a href="#cb5-1446"></a>$$\begin{aligned}</span>
<span id="cb5-1447"><a href="#cb5-1447"></a>\mathbf{E}<span class="co">[</span><span class="ot">W\{\mathbf{E}(Y|\mathcal{G})-\mathbf{E}(\mathbf{E}(Y|\mathcal{G})|\mathcal{H})\}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">W(U-\mathbf{E}(U|\mathcal{H})</span><span class="co">]</span></span>
<span id="cb5-1448"><a href="#cb5-1448"></a>\end{aligned}$$</span>
<span id="cb5-1449"><a href="#cb5-1449"></a></span>
<span id="cb5-1450"><a href="#cb5-1450"></a>But, by definition $U-\mathbf{E}(U|\mathcal{H})$ is always orthogonal to</span>
<span id="cb5-1451"><a href="#cb5-1451"></a>the subspace $L^{2}(\Omega,\mathcal{H},\mathbb{P})$ and hence,</span>
<span id="cb5-1452"><a href="#cb5-1452"></a>$\mathbf{E}<span class="co">[</span><span class="ot">W(U-\mathbf{\mathbf{E}}(U|\mathcal{H})</span><span class="co">]</span>=0$. Since,</span>
<span id="cb5-1453"><a href="#cb5-1453"></a>conditional expectations are unique, we have the desired result.</span>
<span id="cb5-1454"><a href="#cb5-1454"></a>:::</span>
<span id="cb5-1455"><a href="#cb5-1455"></a></span>
<span id="cb5-1456"><a href="#cb5-1456"></a>::: claim</span>
<span id="cb5-1457"><a href="#cb5-1457"></a>**Pythagoras's theorem**. We have:</span>
<span id="cb5-1458"><a href="#cb5-1458"></a></span>
<span id="cb5-1459"><a href="#cb5-1459"></a>$$\begin{aligned}</span>
<span id="cb5-1460"><a href="#cb5-1460"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y^{2}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(\mathbf{E}[Y|\mathcal{G}])^{2}</span><span class="co">]</span>+\mathbf{E}<span class="co">[</span><span class="ot">(Y-\mathbf{E}(Y|\mathcal{G}))^{2}</span><span class="co">]</span></span>
<span id="cb5-1461"><a href="#cb5-1461"></a>\end{aligned}$$</span>
<span id="cb5-1462"><a href="#cb5-1462"></a></span>
<span id="cb5-1463"><a href="#cb5-1463"></a>In particular,</span>
<span id="cb5-1464"><a href="#cb5-1464"></a></span>
<span id="cb5-1465"><a href="#cb5-1465"></a>$$\begin{aligned}</span>
<span id="cb5-1466"><a href="#cb5-1466"></a>\mathbf{E}<span class="co">[</span><span class="ot">(\mathbf{E}[Y|\mathcal{G}])^{2}</span><span class="co">]</span> &amp; \leq\mathbf{E}<span class="co">[</span><span class="ot">Y^{2}</span><span class="co">]</span></span>
<span id="cb5-1467"><a href="#cb5-1467"></a>\end{aligned}$$</span>
<span id="cb5-1468"><a href="#cb5-1468"></a></span>
<span id="cb5-1469"><a href="#cb5-1469"></a>Consider the orthogonal decomposition:</span>
<span id="cb5-1470"><a href="#cb5-1470"></a></span>
<span id="cb5-1471"><a href="#cb5-1471"></a>$$\begin{aligned}</span>
<span id="cb5-1472"><a href="#cb5-1472"></a>Y &amp; =\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>+(Y-\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>)</span>
<span id="cb5-1473"><a href="#cb5-1473"></a>\end{aligned}$$</span>
<span id="cb5-1474"><a href="#cb5-1474"></a></span>
<span id="cb5-1475"><a href="#cb5-1475"></a>Squaring on both sides and taking expectations, we have:</span>
<span id="cb5-1476"><a href="#cb5-1476"></a></span>
<span id="cb5-1477"><a href="#cb5-1477"></a>$$\begin{aligned}</span>
<span id="cb5-1478"><a href="#cb5-1478"></a>\mathbf{E}<span class="co">[</span><span class="ot">Y^{2}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(\mathbf{E}(Y|\mathcal{G}))^{2}</span><span class="co">]</span>+\mathbf{E}<span class="co">[</span><span class="ot">(Y-\mathbf{E}[Y|\mathcal{G}])^{2}</span><span class="co">]</span>+2\mathbf{E}\left<span class="co">[</span><span class="ot">\mathbf{E}[Y|\mathcal{G}](Y-\mathbf{E}[Y|\mathcal{G}])\right</span><span class="co">]</span></span>
<span id="cb5-1479"><a href="#cb5-1479"></a>\end{aligned}$$</span>
<span id="cb5-1480"><a href="#cb5-1480"></a></span>
<span id="cb5-1481"><a href="#cb5-1481"></a>By definition of conditional expectation,</span>
<span id="cb5-1482"><a href="#cb5-1482"></a>$(Y-\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>)$ is orthogonal to the subspace</span>
<span id="cb5-1483"><a href="#cb5-1483"></a>$L^{2}(\Omega,\mathcal{G},\mathbb{P})$. By the properties of conditional</span>
<span id="cb5-1484"><a href="#cb5-1484"></a>expectation, $\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>$ is $\mathcal{G}-$measurable, so</span>
<span id="cb5-1485"><a href="#cb5-1485"></a>it belongs to $L^{2}(\Omega,\mathcal{G},\mathbb{P})$. Hence, the</span>
<span id="cb5-1486"><a href="#cb5-1486"></a>dot-product on the right-hand side is $0$. Consequently, we have the</span>
<span id="cb5-1487"><a href="#cb5-1487"></a>desired result.</span>
<span id="cb5-1488"><a href="#cb5-1488"></a></span>
<span id="cb5-1489"><a href="#cb5-1489"></a>Moreover, since $(Y-\mathbf{E}<span class="co">[</span><span class="ot">Y|\mathcal{G}</span><span class="co">]</span>)^{2}$ is a non-negative</span>
<span id="cb5-1490"><a href="#cb5-1490"></a>random variable, $\mathbf{E}<span class="co">[</span><span class="ot">(Y-\mathbf{E}[Y|\mathcal{G}])^{2}</span><span class="co">]</span>\geq0$.</span>
<span id="cb5-1491"><a href="#cb5-1491"></a>It follows that:</span>
<span id="cb5-1492"><a href="#cb5-1492"></a>$\mathbf{E}<span class="co">[</span><span class="ot">Y^{2}</span><span class="co">]</span>\geq\mathbf{E}<span class="co">[</span><span class="ot">(\mathbf{E}(Y|\mathcal{G}))^{2}</span><span class="co">]</span>$.</span>
<span id="cb5-1493"><a href="#cb5-1493"></a>:::</span>
<span id="cb5-1494"><a href="#cb5-1494"></a></span>
<span id="cb5-1495"><a href="#cb5-1495"></a>::: claim</span>
<span id="cb5-1496"><a href="#cb5-1496"></a>Our claim is:</span>
<span id="cb5-1497"><a href="#cb5-1497"></a></span>
<span id="cb5-1498"><a href="#cb5-1498"></a>$$\begin{aligned}</span>
<span id="cb5-1499"><a href="#cb5-1499"></a>\mathbf{E}\left<span class="co">[</span><span class="ot">\mathbf{E}[Y|\mathcal{G}]\right</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span></span>
<span id="cb5-1500"><a href="#cb5-1500"></a>\end{aligned}$$</span>
<span id="cb5-1501"><a href="#cb5-1501"></a></span>
<span id="cb5-1502"><a href="#cb5-1502"></a>We know that, if $W$ is any bounded $\mathcal{G}$-measurable random</span>
<span id="cb5-1503"><a href="#cb5-1503"></a>variable:</span>
<span id="cb5-1504"><a href="#cb5-1504"></a></span>
<span id="cb5-1505"><a href="#cb5-1505"></a>$$\begin{aligned}</span>
<span id="cb5-1506"><a href="#cb5-1506"></a>\mathbf{E}\left<span class="co">[</span><span class="ot">WY\right</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">W\mathbf{E}[Y|\mathcal{G}]</span><span class="co">]</span></span>
<span id="cb5-1507"><a href="#cb5-1507"></a>\end{aligned}$$</span>
<span id="cb5-1508"><a href="#cb5-1508"></a></span>
<span id="cb5-1509"><a href="#cb5-1509"></a>Taking $W=1$, we have:</span>
<span id="cb5-1510"><a href="#cb5-1510"></a></span>
<span id="cb5-1511"><a href="#cb5-1511"></a>$$\begin{aligned}</span>
<span id="cb5-1512"><a href="#cb5-1512"></a>\mathbf{E}\left<span class="co">[</span><span class="ot">Y\right</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">\mathbf{E}[Y|\mathcal{G}]</span><span class="co">]</span></span>
<span id="cb5-1513"><a href="#cb5-1513"></a>\end{aligned}$$</span>
<span id="cb5-1514"><a href="#cb5-1514"></a>:::</span>
<span id="cb5-1515"><a href="#cb5-1515"></a></span>
<span id="cb5-1516"><a href="#cb5-1516"></a>::: example</span>
<span id="cb5-1517"><a href="#cb5-1517"></a>(Brownian Conditioning II). We continue the example</span>
<span id="cb5-1518"><a href="#cb5-1518"></a>(<span class="co">[</span><span class="ot">\[ex:brownian-conditioning-I\]</span><span class="co">](#ex:brownian-conditioning-I)</span>{reference-type="ref"</span>
<span id="cb5-1519"><a href="#cb5-1519"></a>reference="ex:brownian-conditioning-I"}). Let's now compute the</span>
<span id="cb5-1520"><a href="#cb5-1520"></a>conditional expectations $\mathbf{E}<span class="co">[</span><span class="ot">e^{aB_{1}}|B_{1/2}</span><span class="co">]</span>$ and</span>
<span id="cb5-1521"><a href="#cb5-1521"></a>$\mathbf{E}<span class="co">[</span><span class="ot">e^{aB_{1/2}}|B_{1}</span><span class="co">]</span>$ for some parameter $a$. We shall need</span>
<span id="cb5-1522"><a href="#cb5-1522"></a>the properties of conditional expectation in proposition</span>
<span id="cb5-1523"><a href="#cb5-1523"></a>(<span class="co">[</span><span class="ot">\[prop:properties-of-conditional-expectation\]</span><span class="co">](#prop:properties-of-conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-1524"><a href="#cb5-1524"></a>reference="prop:properties-of-conditional-expectation"}). For the first</span>
<span id="cb5-1525"><a href="#cb5-1525"></a>one we use the fact that $B_{1/2}$ is independent of **$B_{1}-B_{1/2}$**</span>
<span id="cb5-1526"><a href="#cb5-1526"></a>to get:</span>
<span id="cb5-1527"><a href="#cb5-1527"></a></span>
<span id="cb5-1528"><a href="#cb5-1528"></a>$$\begin{aligned}</span>
<span id="cb5-1529"><a href="#cb5-1529"></a>\mathbf{E}<span class="co">[</span><span class="ot">e^{aB_{1}}|B_{1/2}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">e^{a((B_{1}-B_{1/2})+B_{1/2})}|B_{1/2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1530"><a href="#cb5-1530"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">e^{a(B_{1}-B_{2})}\cdot e^{aB_{1/2}}|B_{1/2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1531"><a href="#cb5-1531"></a> &amp; \quad\left<span class="sc">\{</span> \text{Taking out what is known}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb5-1532"><a href="#cb5-1532"></a> &amp; =e^{aB_{1/2}}\mathbf{E}<span class="co">[</span><span class="ot">e^{a(B_{1}-B_{1/2})}|B_{1/2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1533"><a href="#cb5-1533"></a> &amp; =e^{aB_{1/2}}\cdot\mathbf{E}<span class="co">[</span><span class="ot">e^{a(B_{1}-B_{1/2})}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1534"><a href="#cb5-1534"></a> &amp; \quad<span class="sc">\{</span>\text{Independence}<span class="sc">\}</span></span>
<span id="cb5-1535"><a href="#cb5-1535"></a>\end{aligned}$$</span>
<span id="cb5-1536"><a href="#cb5-1536"></a></span>
<span id="cb5-1537"><a href="#cb5-1537"></a>We know that, $a(B_{1}-B_{1/2})$ is a gaussian random variable with mean</span>
<span id="cb5-1538"><a href="#cb5-1538"></a>$0$ and variance $a^{2}/2$. We also know that,</span>
<span id="cb5-1539"><a href="#cb5-1539"></a>$\mathbf{E}<span class="co">[</span><span class="ot">e^{tZ}</span><span class="co">]</span>=e^{t^{2}/2}$. So,</span>
<span id="cb5-1540"><a href="#cb5-1540"></a>$\mathbf{E}<span class="co">[</span><span class="ot">e^{a(B_{1}-B_{1/2})}</span><span class="co">]</span>=e^{a^{2}/4}$. Consequently,</span>
<span id="cb5-1541"><a href="#cb5-1541"></a>$\mathbf{E}<span class="co">[</span><span class="ot">e^{aB_{1}}|B_{1/2}</span><span class="co">]</span>=e^{aB_{1/2}+a^{2}/4}$.</span>
<span id="cb5-1542"><a href="#cb5-1542"></a></span>
<span id="cb5-1543"><a href="#cb5-1543"></a>The result itself has the form of the MGF of a Gaussian with mean</span>
<span id="cb5-1544"><a href="#cb5-1544"></a>$B_{1/2}$ and variance $1/2$. (The MGF of $X=\mu+\sigma Z$, $Z=N(0,1)$</span>
<span id="cb5-1545"><a href="#cb5-1545"></a>is $M_{X}(a)=\exp\left<span class="co">[</span><span class="ot">\mu+\frac{1}{2}\sigma^{2}a^{2}\right</span><span class="co">]</span>$.) In fact,</span>
<span id="cb5-1546"><a href="#cb5-1546"></a>this shows that the conditional distribution of $B_{1}$ given $B_{1/2}$</span>
<span id="cb5-1547"><a href="#cb5-1547"></a>is Gaussian of mean $B_{1/2}$ and variance $1/2$.</span>
<span id="cb5-1548"><a href="#cb5-1548"></a></span>
<span id="cb5-1549"><a href="#cb5-1549"></a>For the other expectation, note that $B_{1/2}-\frac{1}{2}B_{1}$ is</span>
<span id="cb5-1550"><a href="#cb5-1550"></a>independent of $B_{1}$. We have: $$\begin{aligned}</span>
<span id="cb5-1551"><a href="#cb5-1551"></a>\mathbf{E}\left<span class="co">[</span><span class="ot">\left(B_{1/2}-\frac{1}{2}B_{1}\right)B_{1}\right</span><span class="co">]</span> &amp; =\mathbf{E}(B_{1/2}B_{1})-\frac{1}{2}\mathbf{E}<span class="co">[</span><span class="ot">B_{1}^{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1552"><a href="#cb5-1552"></a> &amp; =\frac{1}{2}-\frac{1}{2}\cdot1<span class="sc">\\</span></span>
<span id="cb5-1553"><a href="#cb5-1553"></a> &amp; =0</span>
<span id="cb5-1554"><a href="#cb5-1554"></a>\end{aligned}$$</span>
<span id="cb5-1555"><a href="#cb5-1555"></a></span>
<span id="cb5-1556"><a href="#cb5-1556"></a>Therefore, we have:</span>
<span id="cb5-1557"><a href="#cb5-1557"></a></span>
<span id="cb5-1558"><a href="#cb5-1558"></a>$$\begin{aligned}</span>
<span id="cb5-1559"><a href="#cb5-1559"></a>\mathbf{E}<span class="co">[</span><span class="ot">e^{aB_{1/2}}|B_{1}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">e^{a(B_{1/2}-\frac{1}{2}B_{1})+\frac{a}{2}B_{1}}|B_{1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1560"><a href="#cb5-1560"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">e^{a(B_{1/2}-\frac{1}{2}B_{1})}\cdot e^{\frac{a}{2}B_{1}}|B_{1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1561"><a href="#cb5-1561"></a> &amp; =e^{\frac{a}{2}B_{1}}\mathbf{E}<span class="co">[</span><span class="ot">e^{a(B_{1/2}-\frac{1}{2}B_{1})}|B_{1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1562"><a href="#cb5-1562"></a> &amp; \quad<span class="sc">\{</span>\text{Taking out what is known }<span class="sc">\}\\</span></span>
<span id="cb5-1563"><a href="#cb5-1563"></a> &amp; =e^{\frac{a}{2}B_{1}}\mathbf{E}<span class="co">[</span><span class="ot">e^{a(B_{1/2}-\frac{1}{2}B_{1})}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1564"><a href="#cb5-1564"></a> &amp; \quad<span class="sc">\{</span>\text{Independence}<span class="sc">\}</span></span>
<span id="cb5-1565"><a href="#cb5-1565"></a>\end{aligned}$$</span>
<span id="cb5-1566"><a href="#cb5-1566"></a></span>
<span id="cb5-1567"><a href="#cb5-1567"></a>Now, $a(B_{1/2}-\frac{1}{2}B_{1})$ is a random variable with mean $0$</span>
<span id="cb5-1568"><a href="#cb5-1568"></a>and variance $a^{2}(\frac{1}{2}-\frac{1}{4})=\frac{a^{2}}{4}$.</span>
<span id="cb5-1569"><a href="#cb5-1569"></a>Consequently, $\mathbf{E}<span class="co">[</span><span class="ot">e^{(a/2)Z}</span><span class="co">]</span>=e^{\frac{a^{2}}{8}}$. Thus,</span>
<span id="cb5-1570"><a href="#cb5-1570"></a>$\mathbf{E}<span class="co">[</span><span class="ot">e^{aB_{1/2}}|B_{1}</span><span class="co">]</span>=e^{\frac{a}{2}B_{1}+\frac{a^{2}}{8}}$.</span>
<span id="cb5-1571"><a href="#cb5-1571"></a>:::</span>
<span id="cb5-1572"><a href="#cb5-1572"></a></span>
<span id="cb5-1573"><a href="#cb5-1573"></a>::: example</span>
<span id="cb5-1574"><a href="#cb5-1574"></a>(Brownian bridge is conditioned Brownian motion). We know that the</span>
<span id="cb5-1575"><a href="#cb5-1575"></a>Brownian bridge $M_{t}=B_{t}-tB_{1}$, $t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ is independent of</span>
<span id="cb5-1576"><a href="#cb5-1576"></a>$B_{1}$. We use this to show that the conditional distribution of the</span>
<span id="cb5-1577"><a href="#cb5-1577"></a>Brownian motion given the value at the end-point $B_{1}$ is the one of a</span>
<span id="cb5-1578"><a href="#cb5-1578"></a>Brownian bridge shifted by the straight line going from $0$ to $B_{1}$.</span>
<span id="cb5-1579"><a href="#cb5-1579"></a>To see this, we compute the conditional MGF of</span>
<span id="cb5-1580"><a href="#cb5-1580"></a>$(B_{t_{1}},B_{t_{2}},\ldots,B_{t_{n}})$ given $B_{1}$ for some</span>
<span id="cb5-1581"><a href="#cb5-1581"></a>arbitrary choices of $t_{1},t_{2},\ldots,t_{n}$ in $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. We get the</span>
<span id="cb5-1582"><a href="#cb5-1582"></a>following by adding and subtracting $t_{j}B_{1}$:</span>
<span id="cb5-1583"><a href="#cb5-1583"></a></span>
<span id="cb5-1584"><a href="#cb5-1584"></a>$$\begin{aligned}</span>
<span id="cb5-1585"><a href="#cb5-1585"></a>\mathbf{E}<span class="co">[</span><span class="ot">e^{a_{1}B_{t_{1}}+\ldots+a_{n}B_{t_{n}}}|B_{1}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">e^{a_{1}(B_{t_{1}}-t_{1}B_{1})+\ldots+a_{n}(B_{t_{n}}-t_{n}B_{1})}\cdot e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}|B_{1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1586"><a href="#cb5-1586"></a> &amp; =e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}\mathbf{E}<span class="co">[</span><span class="ot">e^{a_{1}M_{t_{1}}+\ldots+a_{n}M_{t_{n}}}|B_{1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1587"><a href="#cb5-1587"></a> &amp; \quad<span class="sc">\{</span>\text{Taking out what is known}<span class="sc">\}\\</span></span>
<span id="cb5-1588"><a href="#cb5-1588"></a> &amp; =e^{(a_{1}t_{1}B_{1}+\ldots+a_{n}t_{n}B_{1})}\mathbf{E}<span class="co">[</span><span class="ot">e^{a_{1}M_{t_{1}}+\ldots+a_{n}M_{t_{n}}}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1589"><a href="#cb5-1589"></a> &amp; \quad<span class="sc">\{</span>\text{Independence}<span class="sc">\}</span></span>
<span id="cb5-1590"><a href="#cb5-1590"></a>\end{aligned}$$</span>
<span id="cb5-1591"><a href="#cb5-1591"></a></span>
<span id="cb5-1592"><a href="#cb5-1592"></a>The right side is exactly the MGF of the process</span>
<span id="cb5-1593"><a href="#cb5-1593"></a>$M_{t}+tB_{1},t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ (for a fixed value $B_{1})$, where</span>
<span id="cb5-1594"><a href="#cb5-1594"></a>$(M_{t},t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>)$ is a Brownian bridge. This proves the claim.</span>
<span id="cb5-1595"><a href="#cb5-1595"></a>:::</span>
<span id="cb5-1596"><a href="#cb5-1596"></a></span>
<span id="cb5-1597"><a href="#cb5-1597"></a>::: lem</span>
<span id="cb5-1598"><a href="#cb5-1598"></a>(Conditional Jensen's Inequality) If $c$ is a convex function on</span>
<span id="cb5-1599"><a href="#cb5-1599"></a>$\mathbf{R}$ and $X$ is a random variable on</span>
<span id="cb5-1600"><a href="#cb5-1600"></a>$(\Omega,\mathcal{F},\mathbb{P})$, then:</span>
<span id="cb5-1601"><a href="#cb5-1601"></a></span>
<span id="cb5-1602"><a href="#cb5-1602"></a>$$\begin{aligned}</span>
<span id="cb5-1603"><a href="#cb5-1603"></a>\mathbf{E}<span class="co">[</span><span class="ot">c(X)</span><span class="co">]</span> &amp; \geq c(\mathbf{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>)</span>
<span id="cb5-1604"><a href="#cb5-1604"></a>\end{aligned}$$</span>
<span id="cb5-1605"><a href="#cb5-1605"></a></span>
<span id="cb5-1606"><a href="#cb5-1606"></a>More generally, if $\mathcal{G}\subseteq\mathcal{F}$ is a sigma-field,</span>
<span id="cb5-1607"><a href="#cb5-1607"></a>then:</span>
<span id="cb5-1608"><a href="#cb5-1608"></a></span>
<span id="cb5-1609"><a href="#cb5-1609"></a>$$\mathbf{E}<span class="co">[</span><span class="ot">c(X)|\mathcal{G}</span><span class="co">]</span>\geq c(\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>)$$</span>
<span id="cb5-1610"><a href="#cb5-1610"></a>:::</span>
<span id="cb5-1611"><a href="#cb5-1611"></a></span>
<span id="cb5-1612"><a href="#cb5-1612"></a>::: proof</span>
<span id="cb5-1613"><a href="#cb5-1613"></a>*Proof.* We know that, if $c(x)$ is a convex function, the tangent to</span>
<span id="cb5-1614"><a href="#cb5-1614"></a>the curve $c$ at any point lies below the curve. The tangent to the cuve</span>
<span id="cb5-1615"><a href="#cb5-1615"></a>at this point, is a straight-line of the form:</span>
<span id="cb5-1616"><a href="#cb5-1616"></a></span>
<span id="cb5-1617"><a href="#cb5-1617"></a>$$\begin{aligned}</span>
<span id="cb5-1618"><a href="#cb5-1618"></a>c(t)=y &amp; =mt+c</span>
<span id="cb5-1619"><a href="#cb5-1619"></a>\end{aligned}$$</span>
<span id="cb5-1620"><a href="#cb5-1620"></a></span>
<span id="cb5-1621"><a href="#cb5-1621"></a>where $m(t)=c'(t)$. This holds for all $t\in\mathbf{R}$. At an arbitrary</span>
<span id="cb5-1622"><a href="#cb5-1622"></a>point $x$ we have:</span>
<span id="cb5-1623"><a href="#cb5-1623"></a></span>
<span id="cb5-1624"><a href="#cb5-1624"></a>$$\begin{aligned}</span>
<span id="cb5-1625"><a href="#cb5-1625"></a>c(x)\geq &amp; y=mx+c</span>
<span id="cb5-1626"><a href="#cb5-1626"></a>\end{aligned}$$</span>
<span id="cb5-1627"><a href="#cb5-1627"></a></span>
<span id="cb5-1628"><a href="#cb5-1628"></a>Therefore, we have:</span>
<span id="cb5-1629"><a href="#cb5-1629"></a></span>
<span id="cb5-1630"><a href="#cb5-1630"></a>$$\begin{aligned}</span>
<span id="cb5-1631"><a href="#cb5-1631"></a>c(x)-c(t) &amp; \geq m(t)(x-t)</span>
<span id="cb5-1632"><a href="#cb5-1632"></a>\end{aligned}$$</span>
<span id="cb5-1633"><a href="#cb5-1633"></a></span>
<span id="cb5-1634"><a href="#cb5-1634"></a>for any $x$ and any point of tangency $t$.</span>
<span id="cb5-1635"><a href="#cb5-1635"></a></span>
<span id="cb5-1636"><a href="#cb5-1636"></a>$$\begin{aligned}</span>
<span id="cb5-1637"><a href="#cb5-1637"></a>c(X)-c(Y) &amp; \geq m(Y)(X-Y)</span>
<span id="cb5-1638"><a href="#cb5-1638"></a>\end{aligned}$$</span>
<span id="cb5-1639"><a href="#cb5-1639"></a></span>
<span id="cb5-1640"><a href="#cb5-1640"></a>Substituting $Y=\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>$, we get:</span>
<span id="cb5-1641"><a href="#cb5-1641"></a></span>
<span id="cb5-1642"><a href="#cb5-1642"></a>$$\begin{aligned}</span>
<span id="cb5-1643"><a href="#cb5-1643"></a>c(X)-c(\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>) &amp; \geq m(\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>)(X-\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>)</span>
<span id="cb5-1644"><a href="#cb5-1644"></a>\end{aligned}$$</span>
<span id="cb5-1645"><a href="#cb5-1645"></a></span>
<span id="cb5-1646"><a href="#cb5-1646"></a>Taking expectations on both sides, we get:</span>
<span id="cb5-1647"><a href="#cb5-1647"></a></span>
<span id="cb5-1648"><a href="#cb5-1648"></a>$$\begin{aligned}</span>
<span id="cb5-1649"><a href="#cb5-1649"></a>\mathbf{E}<span class="co">[</span><span class="ot">(c(X)-c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}</span><span class="co">]</span> &amp; \geq\mathbf{E}<span class="co">[</span><span class="ot">m(\mathbf{E}[X|\mathcal{G}])(X-\mathbf{E}[X|\mathcal{G}])|\mathcal{G}</span><span class="co">]</span></span>
<span id="cb5-1650"><a href="#cb5-1650"></a>\end{aligned}$$</span>
<span id="cb5-1651"><a href="#cb5-1651"></a></span>
<span id="cb5-1652"><a href="#cb5-1652"></a>The left-hand side simplifies as:</span>
<span id="cb5-1653"><a href="#cb5-1653"></a></span>
<span id="cb5-1654"><a href="#cb5-1654"></a>$$\begin{aligned}</span>
<span id="cb5-1655"><a href="#cb5-1655"></a>\mathbf{E}<span class="co">[</span><span class="ot">(c(X)-c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">c(X)|\mathcal{G}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">c(\mathbf{E}[X|\mathcal{G}]))|\mathcal{G}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1656"><a href="#cb5-1656"></a> &amp; \quad<span class="sc">\{</span>\text{Linearity}<span class="sc">\}\\</span></span>
<span id="cb5-1657"><a href="#cb5-1657"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">c(X)|\mathcal{G}</span><span class="co">]</span>-c(\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>)<span class="sc">\\</span></span>
<span id="cb5-1658"><a href="#cb5-1658"></a> &amp; \quad<span class="sc">\{</span>\text{c(\ensuremath{\mathbf{E}}<span class="co">[</span><span class="ot">X|\ensuremath{\mathcal{G}}</span><span class="co">]</span>)) is \ensuremath{\mathcal{G}}-measurable}<span class="sc">\}</span></span>
<span id="cb5-1659"><a href="#cb5-1659"></a>\end{aligned}$$</span>
<span id="cb5-1660"><a href="#cb5-1660"></a></span>
<span id="cb5-1661"><a href="#cb5-1661"></a>On the right hand side, we have:</span>
<span id="cb5-1662"><a href="#cb5-1662"></a></span>
<span id="cb5-1663"><a href="#cb5-1663"></a>$$\begin{aligned}</span>
<span id="cb5-1664"><a href="#cb5-1664"></a>\mathbf{E}<span class="co">[</span><span class="ot">m(\mathbf{E}[X|\mathcal{G}])(X-\mathbf{E}[X|\mathcal{G}])|\mathcal{G}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">m(\mathbf{E}[X|\mathcal{G}])\cdot X|\mathcal{G}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">m(\mathbf{E}[X|\mathcal{G}])\cdot\mathbf{E}[X|\mathcal{G}]|\mathcal{G}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1665"><a href="#cb5-1665"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>m(\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>)-m(\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>)\cdot\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1666"><a href="#cb5-1666"></a> &amp; =0</span>
<span id="cb5-1667"><a href="#cb5-1667"></a>\end{aligned}$$</span>
<span id="cb5-1668"><a href="#cb5-1668"></a></span>
<span id="cb5-1669"><a href="#cb5-1669"></a>Consequently, it follows that</span>
<span id="cb5-1670"><a href="#cb5-1670"></a>$\mathbf{E}<span class="co">[</span><span class="ot">c(X)|\mathcal{G}</span><span class="co">]</span>\geq c(\mathbf{E}<span class="co">[</span><span class="ot">X|\mathcal{G}</span><span class="co">]</span>)$. ◻</span>
<span id="cb5-1671"><a href="#cb5-1671"></a>:::</span>
<span id="cb5-1672"><a href="#cb5-1672"></a></span>
<span id="cb5-1673"><a href="#cb5-1673"></a>::: example</span>
<span id="cb5-1674"><a href="#cb5-1674"></a>(Embeddings of $L^{p}$ spaces) Square-integrable random variables are in</span>
<span id="cb5-1675"><a href="#cb5-1675"></a>fact integrable. In other words, there is always the inclusion</span>
<span id="cb5-1676"><a href="#cb5-1676"></a>$L^{2}(\Omega,\mathcal{F},\mathbb{P})\subseteq L^{1}(\Omega,\mathcal{F},\mathbb{P})$.</span>
<span id="cb5-1677"><a href="#cb5-1677"></a>In particular, square integrable random variables always have a</span>
<span id="cb5-1678"><a href="#cb5-1678"></a>well-defined variance. This embedding is a simple consequence of</span>
<span id="cb5-1679"><a href="#cb5-1679"></a>Jensen's inequality since:</span>
<span id="cb5-1680"><a href="#cb5-1680"></a></span>
<span id="cb5-1681"><a href="#cb5-1681"></a>$$\begin{aligned}</span>
<span id="cb5-1682"><a href="#cb5-1682"></a>|\mathbf{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>|^{2} &amp; \leq\mathbf{E}<span class="co">[</span><span class="ot">|X|^{2}</span><span class="co">]</span></span>
<span id="cb5-1683"><a href="#cb5-1683"></a>\end{aligned}$$</span>
<span id="cb5-1684"><a href="#cb5-1684"></a></span>
<span id="cb5-1685"><a href="#cb5-1685"></a>as $f(x)=|x|^{2}$ is convex. By taking the square root on both sides, we</span>
<span id="cb5-1686"><a href="#cb5-1686"></a>get:</span>
<span id="cb5-1687"><a href="#cb5-1687"></a></span>
<span id="cb5-1688"><a href="#cb5-1688"></a>$$\begin{aligned}</span>
<span id="cb5-1689"><a href="#cb5-1689"></a>\left\Vert X\right\Vert _{1} &amp; \leq\left\Vert X\right\Vert _{2}</span>
<span id="cb5-1690"><a href="#cb5-1690"></a>\end{aligned}$$</span>
<span id="cb5-1691"><a href="#cb5-1691"></a></span>
<span id="cb5-1692"><a href="#cb5-1692"></a>More generally, for any $1&lt;p&lt;\infty$, we can define</span>
<span id="cb5-1693"><a href="#cb5-1693"></a>$L^{p}(\Omega,\mathcal{F},\mathbb{P})$ to be the linear space of random</span>
<span id="cb5-1694"><a href="#cb5-1694"></a>variables such that $\mathbf{E}<span class="co">[</span><span class="ot">|X|^{p}</span><span class="co">]</span>&lt;\infty$. Then for $p&lt;q$, since</span>
<span id="cb5-1695"><a href="#cb5-1695"></a>$x^{q/p}$ is convex, we get by Jensen's inequality :</span>
<span id="cb5-1696"><a href="#cb5-1696"></a></span>
<span id="cb5-1697"><a href="#cb5-1697"></a>$$\begin{aligned}</span>
<span id="cb5-1698"><a href="#cb5-1698"></a>\mathbf{E}<span class="co">[</span><span class="ot">|X|^{q}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(|X|^{p})^{\frac{q}{p}}</span><span class="co">]</span>\geq\left(\mathbf{E}<span class="co">[</span><span class="ot">|X|^{p}</span><span class="co">]</span>\right)^{\frac{q}{p}}</span>
<span id="cb5-1699"><a href="#cb5-1699"></a>\end{aligned}$$</span>
<span id="cb5-1700"><a href="#cb5-1700"></a></span>
<span id="cb5-1701"><a href="#cb5-1701"></a>Taking the $q$-th root on both sides:</span>
<span id="cb5-1702"><a href="#cb5-1702"></a></span>
<span id="cb5-1703"><a href="#cb5-1703"></a>$$\begin{aligned}</span>
<span id="cb5-1704"><a href="#cb5-1704"></a>\mathbf{E}<span class="co">[</span><span class="ot">|X|^{p}</span><span class="co">]</span>^{1/p} &amp; \leq\mathbf{E}<span class="co">[</span><span class="ot">|X|^{q}</span><span class="co">]</span>^{1/q}</span>
<span id="cb5-1705"><a href="#cb5-1705"></a>\end{aligned}$$</span>
<span id="cb5-1706"><a href="#cb5-1706"></a></span>
<span id="cb5-1707"><a href="#cb5-1707"></a>So, if $X\in L^{q}$, then it must also be in $L^{p}$. Concretely, this</span>
<span id="cb5-1708"><a href="#cb5-1708"></a>means that any random variable with a finite $q$-moment will also have a</span>
<span id="cb5-1709"><a href="#cb5-1709"></a>finite $p$-moment, for $q&gt;p$.</span>
<span id="cb5-1710"><a href="#cb5-1710"></a>:::</span>
<span id="cb5-1711"><a href="#cb5-1711"></a></span>
<span id="cb5-1712"><a href="#cb5-1712"></a><span class="fu">## Martingales.</span></span>
<span id="cb5-1713"><a href="#cb5-1713"></a></span>
<span id="cb5-1714"><a href="#cb5-1714"></a>We now have all the tools to define martingales.</span>
<span id="cb5-1715"><a href="#cb5-1715"></a></span>
<span id="cb5-1716"><a href="#cb5-1716"></a>::: defn</span>
<span id="cb5-1717"><a href="#cb5-1717"></a>[]{#def:Filtration label="def:Filtration"}(Filtration). A *filtration*</span>
<span id="cb5-1718"><a href="#cb5-1718"></a>$(\mathcal{F}_{t}:t\geq0)$ of $\Omega$ is an increasing sequence of</span>
<span id="cb5-1719"><a href="#cb5-1719"></a>$\sigma$-fields of $\Omega$. That is,</span>
<span id="cb5-1720"><a href="#cb5-1720"></a></span>
<span id="cb5-1721"><a href="#cb5-1721"></a>$$\begin{aligned}</span>
<span id="cb5-1722"><a href="#cb5-1722"></a>\mathcal{F}_{s} &amp; \subseteq\mathcal{F}_{t},\quad\forall s\leq t</span>
<span id="cb5-1723"><a href="#cb5-1723"></a>\end{aligned}$$</span>
<span id="cb5-1724"><a href="#cb5-1724"></a></span>
<span id="cb5-1725"><a href="#cb5-1725"></a>We will usually take $\mathcal{F}_{0}=<span class="sc">\{</span>\emptyset,\Omega<span class="sc">\}</span>$. The</span>
<span id="cb5-1726"><a href="#cb5-1726"></a>canonical example of a filtration is the natural filtration of a given</span>
<span id="cb5-1727"><a href="#cb5-1727"></a>process $(M_{s}:s\geq0)$. This is the filtration given by</span>
<span id="cb5-1728"><a href="#cb5-1728"></a>$\mathcal{F}_{t}=\sigma(M_{s},s\leq t)$. The inclusions of the</span>
<span id="cb5-1729"><a href="#cb5-1729"></a>$\sigma$-fields are then clear. For a given Brownian motion</span>
<span id="cb5-1730"><a href="#cb5-1730"></a>$(B_{t},t\geq0)$, the filtration $\mathcal{F}_{t}=\sigma(B_{s},s\leq t)$</span>
<span id="cb5-1731"><a href="#cb5-1731"></a>is sometimes called the *Brownian filtration*. We think of the</span>
<span id="cb5-1732"><a href="#cb5-1732"></a>filtration as the *flow of information of the process*.</span>
<span id="cb5-1733"><a href="#cb5-1733"></a>:::</span>
<span id="cb5-1734"><a href="#cb5-1734"></a></span>
<span id="cb5-1735"><a href="#cb5-1735"></a>::: defn</span>
<span id="cb5-1736"><a href="#cb5-1736"></a>[]{#def:Adapted-process label="def:Adapted-process"}A stochastic process</span>
<span id="cb5-1737"><a href="#cb5-1737"></a>$(X_{t}:t\geq0)$ is said to be adapted to $(\mathcal{F}_{t}:t\geq0)$, if</span>
<span id="cb5-1738"><a href="#cb5-1738"></a>for each $t$, the random variable $X_{t}$ is</span>
<span id="cb5-1739"><a href="#cb5-1739"></a>$\mathcal{F}_{t}-$measurable.</span>
<span id="cb5-1740"><a href="#cb5-1740"></a>:::</span>
<span id="cb5-1741"><a href="#cb5-1741"></a></span>
<span id="cb5-1742"><a href="#cb5-1742"></a>::: defn</span>
<span id="cb5-1743"><a href="#cb5-1743"></a>(Martingale). A process $(M_{t}:t\geq0)$ is a martingale for the</span>
<span id="cb5-1744"><a href="#cb5-1744"></a>filtration $(\mathcal{F}_{t}:t\geq0)$ if the following hold:</span>
<span id="cb5-1745"><a href="#cb5-1745"></a></span>
<span id="cb5-1746"><a href="#cb5-1746"></a><span class="sc">\(</span>1<span class="sc">\)</span> The process is *adapted*, that is $M_{t}$ is</span>
<span id="cb5-1747"><a href="#cb5-1747"></a>$\mathcal{F}_{t}-$measurable for all $t\geq0$.</span>
<span id="cb5-1748"><a href="#cb5-1748"></a></span>
<span id="cb5-1749"><a href="#cb5-1749"></a><span class="sc">\(</span>2<span class="sc">\)</span> $\mathbf{E}<span class="co">[</span><span class="ot">|M_{t}|</span><span class="co">]</span>&lt;\infty$ for all $t\geq0$. (This ensures that</span>
<span id="cb5-1750"><a href="#cb5-1750"></a>the conditional expectation is well defined.)</span>
<span id="cb5-1751"><a href="#cb5-1751"></a></span>
<span id="cb5-1752"><a href="#cb5-1752"></a><span class="sc">\(</span>3<span class="sc">\)</span> *Martingale property:*</span>
<span id="cb5-1753"><a href="#cb5-1753"></a></span>
<span id="cb5-1754"><a href="#cb5-1754"></a>$$\begin{aligned}</span>
<span id="cb5-1755"><a href="#cb5-1755"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{t}|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =M_{s}\quad\forall s\leq t</span>
<span id="cb5-1756"><a href="#cb5-1756"></a>\end{aligned}$$</span>
<span id="cb5-1757"><a href="#cb5-1757"></a></span>
<span id="cb5-1758"><a href="#cb5-1758"></a>Roughly, speaking this means that the best approximation of a process at</span>
<span id="cb5-1759"><a href="#cb5-1759"></a>a future time $t$ is its value at the present.</span>
<span id="cb5-1760"><a href="#cb5-1760"></a>:::</span>
<span id="cb5-1761"><a href="#cb5-1761"></a></span>
<span id="cb5-1762"><a href="#cb5-1762"></a>In particular, the martingale property implies that:</span>
<span id="cb5-1763"><a href="#cb5-1763"></a></span>
<span id="cb5-1764"><a href="#cb5-1764"></a>$$\begin{aligned}</span>
<span id="cb5-1765"><a href="#cb5-1765"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{t}|\mathcal{F}_{0}</span><span class="co">]</span> &amp; =M_{0}\nonumber <span class="sc">\\</span></span>
<span id="cb5-1766"><a href="#cb5-1766"></a>\mathbf{E}<span class="co">[</span><span class="ot">\mathbf{E}[M_{t}|\mathcal{F}_{0}]</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">M_{0}</span><span class="co">]</span>\nonumber <span class="sc">\\</span></span>
<span id="cb5-1767"><a href="#cb5-1767"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{t}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">M_{0}</span><span class="co">]</span>\label{eq:expected-value-of-martingale-at-any-time-is-constant}<span class="sc">\\</span></span>
<span id="cb5-1768"><a href="#cb5-1768"></a> &amp; \quad<span class="sc">\{</span>\text{Tower Property}<span class="sc">\}</span>\nonumber </span>
<span id="cb5-1769"><a href="#cb5-1769"></a>\end{aligned}$$</span>
<span id="cb5-1770"><a href="#cb5-1770"></a></span>
<span id="cb5-1771"><a href="#cb5-1771"></a>Usually, we take $\mathcal{F}_{0}$ to be the trivial sigma-field</span>
<span id="cb5-1772"><a href="#cb5-1772"></a>$<span class="sc">\{</span>\emptyset,\Omega<span class="sc">\}</span>$. A random variable that is</span>
<span id="cb5-1773"><a href="#cb5-1773"></a>$\mathcal{F}_{0}$-measurable must be a constant, so $M_{0}$ is a</span>
<span id="cb5-1774"><a href="#cb5-1774"></a>constant. In this case, $\mathbf{E}<span class="co">[</span><span class="ot">M_{t}</span><span class="co">]</span>=M_{0}$ for all $t$. If</span>
<span id="cb5-1775"><a href="#cb5-1775"></a>properties (1) and (2) are satisfied, but the best approximation is</span>
<span id="cb5-1776"><a href="#cb5-1776"></a>larger, $\mathbf{E}<span class="co">[</span><span class="ot">M_{t}|\mathcal{F}_{s}</span><span class="co">]</span>\geq M_{s}$, the process is</span>
<span id="cb5-1777"><a href="#cb5-1777"></a>called a *submartingale*. If it is smaller on average,</span>
<span id="cb5-1778"><a href="#cb5-1778"></a>$\mathbf{E}<span class="co">[</span><span class="ot">M_{t}|\mathcal{F}_{s}</span><span class="co">]</span>\leq\mathbf{E}<span class="co">[</span><span class="ot">M_{s}</span><span class="co">]</span>$, we say it is a</span>
<span id="cb5-1779"><a href="#cb5-1779"></a>supermartingale.</span>
<span id="cb5-1780"><a href="#cb5-1780"></a></span>
<span id="cb5-1781"><a href="#cb5-1781"></a>We will be mostly interested in martingales that are continuous and</span>
<span id="cb5-1782"><a href="#cb5-1782"></a>square-integrable. Continuous martingales are martingales whose paths</span>
<span id="cb5-1783"><a href="#cb5-1783"></a>$t\mapsto M_{t}(\omega)$ are continuous almost surely. Square-integrable</span>
<span id="cb5-1784"><a href="#cb5-1784"></a>martingales are such that $\mathbf{E}<span class="co">[</span><span class="ot">|M_{t}|^{2}</span><span class="co">]</span>&lt;\infty$ for all</span>
<span id="cb5-1785"><a href="#cb5-1785"></a>$t$'s. This condition is stronger than $\mathbf{E}<span class="co">[</span><span class="ot">|M_{t}|</span><span class="co">]</span>&lt;\infty$ due</span>
<span id="cb5-1786"><a href="#cb5-1786"></a>to Jensen's inequality.</span>
<span id="cb5-1787"><a href="#cb5-1787"></a></span>
<span id="cb5-1788"><a href="#cb5-1788"></a>::: rem*</span>
<span id="cb5-1789"><a href="#cb5-1789"></a>(Martingales in Discrete-time). Martingales can be defined the same way</span>
<span id="cb5-1790"><a href="#cb5-1790"></a>if the index set of the process is discrete. For example, the filtration</span>
<span id="cb5-1791"><a href="#cb5-1791"></a>$(\mathcal{F}_{n}:n\in\mathbf{N})$ is a countable set and the martingale</span>
<span id="cb5-1792"><a href="#cb5-1792"></a>property is then replaced by $\mathbf{E}<span class="co">[</span><span class="ot">M_{n+1}|\mathcal{F}_{n}</span><span class="co">]</span>=M_{n}$</span>
<span id="cb5-1793"><a href="#cb5-1793"></a>as expected. The tower-property then yields the martingale property</span>
<span id="cb5-1794"><a href="#cb5-1794"></a>$\mathbf{E}<span class="co">[</span><span class="ot">M_{n+k}|\mathcal{F}_{n}</span><span class="co">]</span>=M_{n}$ for $k\geq1$.</span>
<span id="cb5-1795"><a href="#cb5-1795"></a>:::</span>
<span id="cb5-1796"><a href="#cb5-1796"></a></span>
<span id="cb5-1797"><a href="#cb5-1797"></a>::: rem*</span>
<span id="cb5-1798"><a href="#cb5-1798"></a>(Continuous Filtrations). Filtrations with continuous time can be tricky</span>
<span id="cb5-1799"><a href="#cb5-1799"></a>to handle rigorously. For example, one has to make sense of what it</span>
<span id="cb5-1800"><a href="#cb5-1800"></a>means for $\mathcal{F}_{s}$ as $s$ approaches $t$ from the left. Is it</span>
<span id="cb5-1801"><a href="#cb5-1801"></a>equal to $\mathcal{F}_{t}$? Or is there actually less information in</span>
<span id="cb5-1802"><a href="#cb5-1802"></a>$\lim_{s\to t^{-}}\mathcal{F}_{s}$ than in $\mathcal{F}_{t}$? This is a</span>
<span id="cb5-1803"><a href="#cb5-1803"></a>bit of headache when dealing with processes with jumps, like the Poisson</span>
<span id="cb5-1804"><a href="#cb5-1804"></a>process. However, if the paths are continuous, the technical problems</span>
<span id="cb5-1805"><a href="#cb5-1805"></a>are not as heavy.</span>
<span id="cb5-1806"><a href="#cb5-1806"></a></span>
<span id="cb5-1807"><a href="#cb5-1807"></a>Let's look at some of the important examples of martingales constructed</span>
<span id="cb5-1808"><a href="#cb5-1808"></a>from Brownian Motion.</span>
<span id="cb5-1809"><a href="#cb5-1809"></a>:::</span>
<span id="cb5-1810"><a href="#cb5-1810"></a></span>
<span id="cb5-1811"><a href="#cb5-1811"></a>::: example</span>
<span id="cb5-1812"><a href="#cb5-1812"></a>(Examples of Brownian Martingales)</span>
<span id="cb5-1813"><a href="#cb5-1813"></a></span>
<span id="cb5-1814"><a href="#cb5-1814"></a><span class="sc">\(</span>i<span class="sc">\)</span> *Standard Brownian Motion.* Let $(B_{t}:t\geq0)$ be a standard</span>
<span id="cb5-1815"><a href="#cb5-1815"></a>Brownian motion and let $(\mathcal{F}_{t}:t\geq0)$ be a *Brownian</span>
<span id="cb5-1816"><a href="#cb5-1816"></a>filtration*. Then $(B_{t}:t\geq0)$ is a square integrable martingale for</span>
<span id="cb5-1817"><a href="#cb5-1817"></a>the filtration $(\mathcal{F}_{t}:t\geq0)$. Property (1) is obvious,</span>
<span id="cb5-1818"><a href="#cb5-1818"></a>because all the sets in $\mathcal{F}_{t}$ are resolved, upon observing</span>
<span id="cb5-1819"><a href="#cb5-1819"></a>the outcome of $B_{t}$. Similarly, $\mathbf{E}<span class="co">[</span><span class="ot">|B_{t}|</span><span class="co">]</span>=0$. As for the</span>
<span id="cb5-1820"><a href="#cb5-1820"></a>martingale property, note that, by the properties of conditional</span>
<span id="cb5-1821"><a href="#cb5-1821"></a>expectation in proposition</span>
<span id="cb5-1822"><a href="#cb5-1822"></a>(<span class="co">[</span><span class="ot">\[prop:properties-of-conditional-expectation\]</span><span class="co">](#prop:properties-of-conditional-expectation)</span>{reference-type="ref"</span>
<span id="cb5-1823"><a href="#cb5-1823"></a>reference="prop:properties-of-conditional-expectation"}), we have:</span>
<span id="cb5-1824"><a href="#cb5-1824"></a></span>
<span id="cb5-1825"><a href="#cb5-1825"></a>$$\begin{aligned}</span>
<span id="cb5-1826"><a href="#cb5-1826"></a>\mathbf{E}<span class="co">[</span><span class="ot">B_{t}|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{t}|B_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1827"><a href="#cb5-1827"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{t}-B_{s}+B_{s}|B_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1828"><a href="#cb5-1828"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{t}-B_{s}|B_{s}</span><span class="co">]</span>+\mathbf{E}<span class="co">[</span><span class="ot">B_{s}|B_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1829"><a href="#cb5-1829"></a> &amp; \quad<span class="sc">\{</span>\text{Linearity}<span class="sc">\}\\</span></span>
<span id="cb5-1830"><a href="#cb5-1830"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{t}-B_{s}</span><span class="co">]</span>+B_{s}<span class="sc">\\</span></span>
<span id="cb5-1831"><a href="#cb5-1831"></a> &amp; \quad<span class="sc">\{</span>\text{Independence}<span class="sc">\}\\</span></span>
<span id="cb5-1832"><a href="#cb5-1832"></a> &amp; =B_{s}</span>
<span id="cb5-1833"><a href="#cb5-1833"></a>\end{aligned}$$</span>
<span id="cb5-1834"><a href="#cb5-1834"></a></span>
<span id="cb5-1835"><a href="#cb5-1835"></a><span class="sc">\(</span>ii<span class="sc">\)</span> *Geometric Brownian Motion.* Let $(B_{t},t\ge0)$ be a standard</span>
<span id="cb5-1836"><a href="#cb5-1836"></a>brownian motion, and $\mathcal{F}_{t}=\sigma(B_{s},s\leq t)$. A</span>
<span id="cb5-1837"><a href="#cb5-1837"></a>*geometric brownian motion* is a process $(S_{t},t\geq0)$ defined by:</span>
<span id="cb5-1838"><a href="#cb5-1838"></a></span>
<span id="cb5-1839"><a href="#cb5-1839"></a>$$\begin{aligned}</span>
<span id="cb5-1840"><a href="#cb5-1840"></a>S_{t} &amp; =S_{0}\exp\left(\sigma B_{t}+\mu t\right)</span>
<span id="cb5-1841"><a href="#cb5-1841"></a>\end{aligned}$$</span>
<span id="cb5-1842"><a href="#cb5-1842"></a></span>
<span id="cb5-1843"><a href="#cb5-1843"></a>for some parameter $\sigma&gt;0$ and $\mu\in\mathbf{R}$. This is simply the</span>
<span id="cb5-1844"><a href="#cb5-1844"></a>exponential of the Brownian motion with drift. This is not a martingale</span>
<span id="cb5-1845"><a href="#cb5-1845"></a>for most choices of $\mu$! In fact, one must take</span>
<span id="cb5-1846"><a href="#cb5-1846"></a></span>
<span id="cb5-1847"><a href="#cb5-1847"></a>$$\begin{aligned}</span>
<span id="cb5-1848"><a href="#cb5-1848"></a>\mu &amp; =-\frac{1}{2}\sigma^{2}</span>
<span id="cb5-1849"><a href="#cb5-1849"></a>\end{aligned}$$ for the process to be a martingale for the Brownian</span>
<span id="cb5-1850"><a href="#cb5-1850"></a>filtration. Let's verify this. Property (1) is obvious since $S_{t}$ is</span>
<span id="cb5-1851"><a href="#cb5-1851"></a>a function of $B_{t}$ for each $t$. So, it is $\mathcal{F}_{t}$</span>
<span id="cb5-1852"><a href="#cb5-1852"></a>measurable. Moreover, property (2) is clear:</span>
<span id="cb5-1853"><a href="#cb5-1853"></a>$\mathbf{E}<span class="co">[</span><span class="ot">\exp(\sigma B_{t}+\mu t)</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">\exp(\sigma\sqrt{t}Z+\mu t)</span><span class="co">]</span>=\exp(\mu t+\frac{1}{2}\sigma^{2}t)$.</span>
<span id="cb5-1854"><a href="#cb5-1854"></a>So, its a finite quantity. As for the martingale property, note that by</span>
<span id="cb5-1855"><a href="#cb5-1855"></a>the properties of conditional expectation, and the MGF of Gaussians, we</span>
<span id="cb5-1856"><a href="#cb5-1856"></a>have for $s\leq t$:</span>
<span id="cb5-1857"><a href="#cb5-1857"></a></span>
<span id="cb5-1858"><a href="#cb5-1858"></a>$$\begin{aligned}</span>
<span id="cb5-1859"><a href="#cb5-1859"></a>\mathbf{E}<span class="co">[</span><span class="ot">S_{t}|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =\mathbf{E}\left<span class="co">[</span><span class="ot">S_{0}\exp\left(\sigma B_{t}-\frac{1}{2}\sigma^{2}t\right)|\mathcal{F}_{s}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1860"><a href="#cb5-1860"></a> &amp; =S_{0}\exp(-\frac{1}{2}\sigma^{2}t)\mathbf{E}<span class="co">[</span><span class="ot">\exp(\sigma(B_{t}-B_{s}+B_{s}))|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1861"><a href="#cb5-1861"></a> &amp; =S_{0}\exp(-\frac{1}{2}\sigma^{2}t)\exp(\sigma B_{s})\mathbf{E}<span class="co">[</span><span class="ot">\exp(\sigma(B_{t}-B_{s}))|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1862"><a href="#cb5-1862"></a> &amp; \quad<span class="sc">\{</span>\text{Taking out what is known}<span class="sc">\}\\</span></span>
<span id="cb5-1863"><a href="#cb5-1863"></a> &amp; =S_{0}\exp\left(\sigma B_{s}-\frac{1}{2}\sigma^{2}t\right)\mathbf{E}\left<span class="co">[</span><span class="ot">\exp\left(\sigma(B_{t}-B_{s})\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1864"><a href="#cb5-1864"></a> &amp; \quad<span class="sc">\{</span>\text{Independence}<span class="sc">\}\\</span></span>
<span id="cb5-1865"><a href="#cb5-1865"></a> &amp; =S_{0}\exp\left(\sigma B_{s}-\frac{1}{2}\sigma^{2}t+\frac{1}{2}\sigma^{2}(t-s)\right)<span class="sc">\\</span></span>
<span id="cb5-1866"><a href="#cb5-1866"></a> &amp; =S_{0}\exp(\sigma B_{s}-\frac{1}{2}\sigma^{2}s)<span class="sc">\\</span></span>
<span id="cb5-1867"><a href="#cb5-1867"></a> &amp; =S_{s}</span>
<span id="cb5-1868"><a href="#cb5-1868"></a>\end{aligned}$$</span>
<span id="cb5-1869"><a href="#cb5-1869"></a></span>
<span id="cb5-1870"><a href="#cb5-1870"></a>We will sometimes abuse terminology and refer to the martingale case of</span>
<span id="cb5-1871"><a href="#cb5-1871"></a>geometric brownian motion simply as geometric Brownian Motion when the</span>
<span id="cb5-1872"><a href="#cb5-1872"></a>context is clear.</span>
<span id="cb5-1873"><a href="#cb5-1873"></a></span>
<span id="cb5-1874"><a href="#cb5-1874"></a><span class="sc">\(</span>iii<span class="sc">\)</span> *The square of the Brownian motion, compensated*. It is easy to</span>
<span id="cb5-1875"><a href="#cb5-1875"></a>check $(B_{t}^{2},t\geq0)$ is a submartingale by direct computation</span>
<span id="cb5-1876"><a href="#cb5-1876"></a>using increments or by Jensen's inequality:</span>
<span id="cb5-1877"><a href="#cb5-1877"></a>$\mathbf{E}<span class="co">[</span><span class="ot">B_{t}^{2}|\mathcal{F}_{s}</span><span class="co">]</span>&gt;(\mathbf{E}<span class="co">[</span><span class="ot">B_{t}|\mathcal{F}_{s}</span><span class="co">]</span>)^{2}=B_{s}^{2}$,</span>
<span id="cb5-1878"><a href="#cb5-1878"></a>$s&lt;t$. It is nevertheless possible to compensate to get a martingale:</span>
<span id="cb5-1879"><a href="#cb5-1879"></a></span>
<span id="cb5-1880"><a href="#cb5-1880"></a>$$\begin{aligned}</span>
<span id="cb5-1881"><a href="#cb5-1881"></a>M_{t} &amp; =B_{t}^{2}-t</span>
<span id="cb5-1882"><a href="#cb5-1882"></a>\end{aligned}$$</span>
<span id="cb5-1883"><a href="#cb5-1883"></a></span>
<span id="cb5-1884"><a href="#cb5-1884"></a>It is an easy exercise to verify that $(M_{t}:t\geq0)$ is a martingale</span>
<span id="cb5-1885"><a href="#cb5-1885"></a>for the Brownian filtration $(\mathcal{F}_{t}:t\geq0)$.</span>
<span id="cb5-1886"><a href="#cb5-1886"></a></span>
<span id="cb5-1887"><a href="#cb5-1887"></a>$$\begin{aligned}</span>
<span id="cb5-1888"><a href="#cb5-1888"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{t}|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{t}^{2}-t|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1889"><a href="#cb5-1889"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{t}^{2}|\mathcal{F}_{s}</span><span class="co">]</span>-t<span class="sc">\\</span></span>
<span id="cb5-1890"><a href="#cb5-1890"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(B_{t}-B_{s}+B_{s})^{2}|\mathcal{F}_{s}</span><span class="co">]</span>-t<span class="sc">\\</span></span>
<span id="cb5-1891"><a href="#cb5-1891"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(B_{t}-B_{s})^{2}|\mathcal{F}_{s}</span><span class="co">]</span>+2\mathbf{E}<span class="co">[</span><span class="ot">(B_{t}-B_{s})B_{s}|\mathcal{F}_{s}</span><span class="co">]</span>+\mathbf{E}<span class="co">[</span><span class="ot">B_{s}^{2}|\mathcal{F}_{s}</span><span class="co">]</span>-t<span class="sc">\\</span></span>
<span id="cb5-1892"><a href="#cb5-1892"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(B_{t}-B_{s})^{2}</span><span class="co">]</span>+2B_{s}\mathbf{E}<span class="co">[</span><span class="ot">(B_{t}-B_{s})|\mathcal{F}_{s}</span><span class="co">]</span>+B_{s}^{2}-t<span class="sc">\\</span></span>
<span id="cb5-1893"><a href="#cb5-1893"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(B_{t}-B_{s})^{2}</span><span class="co">]</span>+2B_{s}\mathbf{E}<span class="co">[</span><span class="ot">(B_{t}-B_{s})</span><span class="co">]</span>+B_{s}^{2}-t<span class="sc">\\</span></span>
<span id="cb5-1894"><a href="#cb5-1894"></a> &amp; \left<span class="sc">\{</span> \begin{array}{c}</span>
<span id="cb5-1895"><a href="#cb5-1895"></a>\text{\ensuremath{(B_{t}-B_{s})} is independent of \ensuremath{\mathcal{F}_{s}}}<span class="sc">\\</span></span>
<span id="cb5-1896"><a href="#cb5-1896"></a>\text{Also, \ensuremath{B_{s}} is known at time \ensuremath{s}}</span>
<span id="cb5-1897"><a href="#cb5-1897"></a>\end{array}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb5-1898"><a href="#cb5-1898"></a> &amp; =(t-s)+2B_{s}\cdot0+B_{s}^{2}-t<span class="sc">\\</span></span>
<span id="cb5-1899"><a href="#cb5-1899"></a> &amp; =B_{s}^{2}-s<span class="sc">\\</span></span>
<span id="cb5-1900"><a href="#cb5-1900"></a> &amp; =M_{s}</span>
<span id="cb5-1901"><a href="#cb5-1901"></a>\end{aligned}$$</span>
<span id="cb5-1902"><a href="#cb5-1902"></a>:::</span>
<span id="cb5-1903"><a href="#cb5-1903"></a></span>
<span id="cb5-1904"><a href="#cb5-1904"></a>::: example</span>
<span id="cb5-1905"><a href="#cb5-1905"></a>(Other important martingales).</span>
<span id="cb5-1906"><a href="#cb5-1906"></a></span>
<span id="cb5-1907"><a href="#cb5-1907"></a><span class="sc">\(</span>1<span class="sc">\)</span> *Symmetric random walks.* This is an example of a martingale in</span>
<span id="cb5-1908"><a href="#cb5-1908"></a>discrete time. Take $(X_{i}:i\in\mathbf{N})$ to be IID random variables</span>
<span id="cb5-1909"><a href="#cb5-1909"></a>with $\mathbf{E}<span class="co">[</span><span class="ot">X_{i}</span><span class="co">]</span>=0$ and $\mathbf{E}<span class="co">[</span><span class="ot">|X_{i}|</span><span class="co">]</span>&lt;\infty$. Take</span>
<span id="cb5-1910"><a href="#cb5-1910"></a>$\mathcal{F}_{n}=\sigma(X_{i},i\leq n)$ and</span>
<span id="cb5-1911"><a href="#cb5-1911"></a></span>
<span id="cb5-1912"><a href="#cb5-1912"></a>$$\begin{aligned}</span>
<span id="cb5-1913"><a href="#cb5-1913"></a>S_{n} &amp; =X_{1}+X_{2}+\ldots+X_{n},\quad S_{0}=0</span>
<span id="cb5-1914"><a href="#cb5-1914"></a>\end{aligned}$$</span>
<span id="cb5-1915"><a href="#cb5-1915"></a></span>
<span id="cb5-1916"><a href="#cb5-1916"></a>Firstly, the information learned by observing the outcomes of</span>
<span id="cb5-1917"><a href="#cb5-1917"></a>$X_{1}$,$\ldots$,$X_{n}$ is enough to completely determine $S_{n}$.</span>
<span id="cb5-1918"><a href="#cb5-1918"></a>Hence, $S_{n}$ is $\mathcal{F}_{n}-$measurable.</span>
<span id="cb5-1919"><a href="#cb5-1919"></a></span>
<span id="cb5-1920"><a href="#cb5-1920"></a>Next, $$\begin{aligned}</span>
<span id="cb5-1921"><a href="#cb5-1921"></a>|S_{n}| &amp; =\left|\sum_{i=1}^{n}X_{i}\right|<span class="sc">\\</span></span>
<span id="cb5-1922"><a href="#cb5-1922"></a> &amp; \leq\sum_{i=1}^{n}|X_{i}|</span>
<span id="cb5-1923"><a href="#cb5-1923"></a>\end{aligned}$$</span>
<span id="cb5-1924"><a href="#cb5-1924"></a></span>
<span id="cb5-1925"><a href="#cb5-1925"></a>Consequently, by the montonocity of expectations, we have:</span>
<span id="cb5-1926"><a href="#cb5-1926"></a></span>
<span id="cb5-1927"><a href="#cb5-1927"></a>$$\begin{aligned}</span>
<span id="cb5-1928"><a href="#cb5-1928"></a>\mathbf{E}<span class="co">[</span><span class="ot">|S_{n}|</span><span class="co">]</span> &amp; \leq\sum_{i=1}^{n}\mathbf{E}<span class="co">[</span><span class="ot">|X_{i}|</span><span class="co">]</span>&lt;\infty</span>
<span id="cb5-1929"><a href="#cb5-1929"></a>\end{aligned}$$</span>
<span id="cb5-1930"><a href="#cb5-1930"></a></span>
<span id="cb5-1931"><a href="#cb5-1931"></a>The martingale property is also satisfied. We have:</span>
<span id="cb5-1932"><a href="#cb5-1932"></a></span>
<span id="cb5-1933"><a href="#cb5-1933"></a>$$\begin{aligned}</span>
<span id="cb5-1934"><a href="#cb5-1934"></a>\mathbf{E}<span class="co">[</span><span class="ot">S_{n+1}|\mathcal{F}_{n}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">S_{n}+X_{n+1}|\mathcal{F}_{n}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1935"><a href="#cb5-1935"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">S_{n}|\mathcal{F}_{n}</span><span class="co">]</span>+\mathbf{E}<span class="co">[</span><span class="ot">X_{n+1}|\mathcal{F}_{n}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1936"><a href="#cb5-1936"></a> &amp; =S_{n}+\mathbf{E}<span class="co">[</span><span class="ot">X_{n+1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1937"><a href="#cb5-1937"></a> &amp; \left<span class="sc">\{</span> \begin{array}{c}</span>
<span id="cb5-1938"><a href="#cb5-1938"></a>\text{\ensuremath{S_{n}} is \ensuremath{\mathcal{F}_{n}}-measurable}<span class="sc">\\</span></span>
<span id="cb5-1939"><a href="#cb5-1939"></a>\text{\ensuremath{X_{n+1}} is independent of \ensuremath{\mathcal{F}_{n}}}</span>
<span id="cb5-1940"><a href="#cb5-1940"></a>\end{array}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb5-1941"><a href="#cb5-1941"></a> &amp; =S_{n}+0<span class="sc">\\</span></span>
<span id="cb5-1942"><a href="#cb5-1942"></a> &amp; =S_{n}</span>
<span id="cb5-1943"><a href="#cb5-1943"></a>\end{aligned}$$</span>
<span id="cb5-1944"><a href="#cb5-1944"></a></span>
<span id="cb5-1945"><a href="#cb5-1945"></a><span class="sc">\(</span>2<span class="sc">\)</span> *Compensated Poisson process*. Let $(N_{t}:t\geq0)$ be a Poisson</span>
<span id="cb5-1946"><a href="#cb5-1946"></a>process with rate $\lambda$ and $\mathcal{F}_{t}=\sigma(N_{s},s\leq t)$.</span>
<span id="cb5-1947"><a href="#cb5-1947"></a>Then, $N_{t}$ is a submartingale for its natural filtration. Again,</span>
<span id="cb5-1948"><a href="#cb5-1948"></a>properties (1) and (2) are easily checked. $N_{t}$ is $\mathcal{F}_{t}$</span>
<span id="cb5-1949"><a href="#cb5-1949"></a>measurable. Moreover,</span>
<span id="cb5-1950"><a href="#cb5-1950"></a>$\mathbf{E}<span class="co">[</span><span class="ot">|N_{t}|</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">N_{t}</span><span class="co">]</span>=\frac{1}{\lambda t}&lt;\infty$. The</span>
<span id="cb5-1951"><a href="#cb5-1951"></a>submartingale property follows by the independence of increments : for</span>
<span id="cb5-1952"><a href="#cb5-1952"></a>$s\leq t$,</span>
<span id="cb5-1953"><a href="#cb5-1953"></a></span>
<span id="cb5-1954"><a href="#cb5-1954"></a>$$\begin{aligned}</span>
<span id="cb5-1955"><a href="#cb5-1955"></a>\mathbf{E}<span class="co">[</span><span class="ot">N_{t}|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">N_{t}-N_{s}+N_{s}|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1956"><a href="#cb5-1956"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">N_{t}-N_{s}|\mathcal{F}_{s}</span><span class="co">]</span>+\mathbf{E}<span class="co">[</span><span class="ot">N_{s}|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1957"><a href="#cb5-1957"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">N_{t}-N_{s}</span><span class="co">]</span>+N_{s}<span class="sc">\\</span></span>
<span id="cb5-1958"><a href="#cb5-1958"></a> &amp; =\lambda(t-s)+N_{s}<span class="sc">\\</span></span>
<span id="cb5-1959"><a href="#cb5-1959"></a> &amp; \left<span class="sc">\{</span> \because\mathbf{E}<span class="co">[</span><span class="ot">N_{t}</span><span class="co">]</span>=\lambda t\right<span class="sc">\}</span> </span>
<span id="cb5-1960"><a href="#cb5-1960"></a>\end{aligned}$$</span>
<span id="cb5-1961"><a href="#cb5-1961"></a></span>
<span id="cb5-1962"><a href="#cb5-1962"></a>More importantly, we get a martingale by slightly modifying the process.</span>
<span id="cb5-1963"><a href="#cb5-1963"></a>Indeed, if we subtract $\lambda t$, we have that the process :</span>
<span id="cb5-1964"><a href="#cb5-1964"></a></span>
<span id="cb5-1965"><a href="#cb5-1965"></a>$$\begin{aligned}</span>
<span id="cb5-1966"><a href="#cb5-1966"></a>M_{t} &amp; =N_{t}-\lambda t</span>
<span id="cb5-1967"><a href="#cb5-1967"></a>\end{aligned}$$</span>
<span id="cb5-1968"><a href="#cb5-1968"></a></span>
<span id="cb5-1969"><a href="#cb5-1969"></a>is a martingale. We have:</span>
<span id="cb5-1970"><a href="#cb5-1970"></a></span>
<span id="cb5-1971"><a href="#cb5-1971"></a>$$\begin{aligned}</span>
<span id="cb5-1972"><a href="#cb5-1972"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{t}|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">N_{t}-\lambda t|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-1973"><a href="#cb5-1973"></a> &amp; =\lambda t-\lambda s+N_{s}-\lambda t<span class="sc">\\</span></span>
<span id="cb5-1974"><a href="#cb5-1974"></a> &amp; =N_{s}-\lambda s<span class="sc">\\</span></span>
<span id="cb5-1975"><a href="#cb5-1975"></a> &amp; =M_{s}</span>
<span id="cb5-1976"><a href="#cb5-1976"></a>\end{aligned}$$</span>
<span id="cb5-1977"><a href="#cb5-1977"></a></span>
<span id="cb5-1978"><a href="#cb5-1978"></a>This is called the *compensated Poisson process*. Let us simulate $10$</span>
<span id="cb5-1979"><a href="#cb5-1979"></a>paths of the compensated poisson process on $<span class="co">[</span><span class="ot">0,10</span><span class="co">]</span>$.</span>
<span id="cb5-1980"><a href="#cb5-1980"></a>:::</span>
<span id="cb5-1981"><a href="#cb5-1981"></a></span>
<span id="cb5-1982"><a href="#cb5-1982"></a><span class="in">``` {caption="Generating 10 paths of a compensated Poisson process"}</span></span>
<span id="cb5-1983"><a href="#cb5-1983"></a><span class="in">import numpy as np</span></span>
<span id="cb5-1984"><a href="#cb5-1984"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb5-1985"><a href="#cb5-1985"></a></span>
<span id="cb5-1986"><a href="#cb5-1986"></a><span class="in"># Generates a sample path of a compensated poisson process </span></span>
<span id="cb5-1987"><a href="#cb5-1987"></a><span class="in"># with rate : `lambda_` per unit time</span></span>
<span id="cb5-1988"><a href="#cb5-1988"></a><span class="in"># on the interval [0,T], and subintervals of size `stepSize`.</span></span>
<span id="cb5-1989"><a href="#cb5-1989"></a></span>
<span id="cb5-1990"><a href="#cb5-1990"></a><span class="in">def generateCompensatedPoissonPath(lambda_,T,stepSize):</span></span>
<span id="cb5-1991"><a href="#cb5-1991"></a><span class="in">    N = int(T/stepSize)   </span></span>
<span id="cb5-1992"><a href="#cb5-1992"></a></span>
<span id="cb5-1993"><a href="#cb5-1993"></a><span class="in">    poissonParam = lambda_ * stepSize        </span></span>
<span id="cb5-1994"><a href="#cb5-1994"></a></span>
<span id="cb5-1995"><a href="#cb5-1995"></a><span class="in">    x = np.random.poisson(lam=poissonParam,size=N)  </span></span>
<span id="cb5-1996"><a href="#cb5-1996"></a><span class="in">    x = np.concatenate([[0.0], x])</span></span>
<span id="cb5-1997"><a href="#cb5-1997"></a><span class="in">    N_t = np.cumsum(x)  </span></span>
<span id="cb5-1998"><a href="#cb5-1998"></a><span class="in">    t = np.linspace(start=0.0,stop=10.0,num=1001)</span></span>
<span id="cb5-1999"><a href="#cb5-1999"></a></span>
<span id="cb5-2000"><a href="#cb5-2000"></a><span class="in">    M_t = np.subtract(N_t,lambda_ * t)  </span></span>
<span id="cb5-2001"><a href="#cb5-2001"></a><span class="in">    return M_t</span></span>
<span id="cb5-2002"><a href="#cb5-2002"></a></span>
<span id="cb5-2003"><a href="#cb5-2003"></a></span>
<span id="cb5-2004"><a href="#cb5-2004"></a><span class="in">t = np.linspace(0,10,1001)</span></span>
<span id="cb5-2005"><a href="#cb5-2005"></a><span class="in">plt.grid(True)</span></span>
<span id="cb5-2006"><a href="#cb5-2006"></a></span>
<span id="cb5-2007"><a href="#cb5-2007"></a><span class="in">plt.xlabel(r'Time $t$')</span></span>
<span id="cb5-2008"><a href="#cb5-2008"></a><span class="in">plt.ylabel(r'Compensated poisson process $M(t)$')</span></span>
<span id="cb5-2009"><a href="#cb5-2009"></a><span class="in">plt.grid(True)</span></span>
<span id="cb5-2010"><a href="#cb5-2010"></a><span class="in">plt.title(r'$10$ paths of the compensated Poisson process on $[0,10]$')</span></span>
<span id="cb5-2011"><a href="#cb5-2011"></a></span>
<span id="cb5-2012"><a href="#cb5-2012"></a><span class="in">for i in range(10):</span></span>
<span id="cb5-2013"><a href="#cb5-2013"></a><span class="in">    # Generate a poisson path with rate 1 /sec = 0.01 /millisec</span></span>
<span id="cb5-2014"><a href="#cb5-2014"></a><span class="in">    n_t = generateCompensatedPoissonPath(lambda_=1.0, T=10, stepSize=0.01)</span></span>
<span id="cb5-2015"><a href="#cb5-2015"></a><span class="in">    plt.plot(t, n_t)</span></span>
<span id="cb5-2016"><a href="#cb5-2016"></a></span>
<span id="cb5-2017"><a href="#cb5-2017"></a></span>
<span id="cb5-2018"><a href="#cb5-2018"></a><span class="in">plt.show()</span></span>
<span id="cb5-2019"><a href="#cb5-2019"></a><span class="in">plt.close()</span></span>
<span id="cb5-2020"><a href="#cb5-2020"></a><span class="in">```</span></span>
<span id="cb5-2021"><a href="#cb5-2021"></a></span>
<span id="cb5-2022"><a href="#cb5-2022"></a>We saw in the two examples, that, even though a process is not itself a</span>
<span id="cb5-2023"><a href="#cb5-2023"></a>martingale, we can sometimes *compensate* to obtain a martingale! Ito</span>
<span id="cb5-2024"><a href="#cb5-2024"></a>Calculus will greatly extend this perspective. We will have systematic</span>
<span id="cb5-2025"><a href="#cb5-2025"></a>rules that show when a function of Brownian motion is a martingale and</span>
<span id="cb5-2026"><a href="#cb5-2026"></a>if not, how to modify it to get one.</span>
<span id="cb5-2027"><a href="#cb5-2027"></a></span>
<span id="cb5-2028"><a href="#cb5-2028"></a>For now, we observe that a convex function of a martingale is always a</span>
<span id="cb5-2029"><a href="#cb5-2029"></a>submartingale by Jensen's inequality.</span>
<span id="cb5-2030"><a href="#cb5-2030"></a></span>
<span id="cb5-2031"><a href="#cb5-2031"></a>::: cor</span>
<span id="cb5-2032"><a href="#cb5-2032"></a>[]{#corollary:the-convex-function-of-martingale-is-a-submartingale</span>
<span id="cb5-2033"><a href="#cb5-2033"></a>label="corollary:the-convex-function-of-martingale-is-a-submartingale"}If</span>
<span id="cb5-2034"><a href="#cb5-2034"></a>$c$ is a convex function on $\mathbf{R}$ and $(M_{t}:t\geq0)$ is a</span>
<span id="cb5-2035"><a href="#cb5-2035"></a>martingale for $(\mathcal{F}_{t}:t\geq0)$, then the process</span>
<span id="cb5-2036"><a href="#cb5-2036"></a>$(c(M_{t}):t\geq0)$ is a submartingale for the same filtration, granted</span>
<span id="cb5-2037"><a href="#cb5-2037"></a>that $\mathbf{E}<span class="co">[</span><span class="ot">|c(M_{t})|</span><span class="co">]</span>&lt;\infty$.</span>
<span id="cb5-2038"><a href="#cb5-2038"></a>:::</span>
<span id="cb5-2039"><a href="#cb5-2039"></a></span>
<span id="cb5-2040"><a href="#cb5-2040"></a>::: proof</span>
<span id="cb5-2041"><a href="#cb5-2041"></a>*Proof.* The fact that $c(M_{t})$ is adapted to the filtration is clear</span>
<span id="cb5-2042"><a href="#cb5-2042"></a>since it is an explicit function of $M_{t}$. The integrability is by</span>
<span id="cb5-2043"><a href="#cb5-2043"></a>assumption. The submartingale property is checked as follows:</span>
<span id="cb5-2044"><a href="#cb5-2044"></a></span>
<span id="cb5-2045"><a href="#cb5-2045"></a>$$\begin{aligned}</span>
<span id="cb5-2046"><a href="#cb5-2046"></a>\mathbf{E}<span class="co">[</span><span class="ot">c(M_{t})|\mathcal{F}_{s}</span><span class="co">]</span> &amp; \geq c(\mathbf{E}<span class="co">[</span><span class="ot">M_{t}|\mathcal{F}_{s}</span><span class="co">]</span>)=c(M_{s})</span>
<span id="cb5-2047"><a href="#cb5-2047"></a>\end{aligned}$$ ◻</span>
<span id="cb5-2048"><a href="#cb5-2048"></a>:::</span>
<span id="cb5-2049"><a href="#cb5-2049"></a></span>
<span id="cb5-2050"><a href="#cb5-2050"></a>::: rem*</span>
<span id="cb5-2051"><a href="#cb5-2051"></a>(The Doob-Meyer Decomposition Theorem). Let $(X_{n}:n\in\mathbf{N})$ be</span>
<span id="cb5-2052"><a href="#cb5-2052"></a>a submartingale with respect to a filtration</span>
<span id="cb5-2053"><a href="#cb5-2053"></a>$(\mathcal{F}_{n}:n\in\mathbf{N})$. Define a sequence of random</span>
<span id="cb5-2054"><a href="#cb5-2054"></a>variables $(A_{n}:n\in\mathbf{N})$ by $A_{0}=0$ and</span>
<span id="cb5-2055"><a href="#cb5-2055"></a></span>
<span id="cb5-2056"><a href="#cb5-2056"></a>$$\begin{aligned}</span>
<span id="cb5-2057"><a href="#cb5-2057"></a>A_{n} &amp; =\sum_{i=1}^{n}(\mathbf{E}<span class="co">[</span><span class="ot">X_{i}|\mathcal{F}_{i-1}</span><span class="co">]</span>-X_{i-1}),\quad n\geq1</span>
<span id="cb5-2058"><a href="#cb5-2058"></a>\end{aligned}$$</span>
<span id="cb5-2059"><a href="#cb5-2059"></a></span>
<span id="cb5-2060"><a href="#cb5-2060"></a>Note that $A_{n}$ is $\mathcal{F}_{n-1}$-measurable. Moreover, since</span>
<span id="cb5-2061"><a href="#cb5-2061"></a>$(X_{n}:n\in\mathbf{N})$ is a submartingale, we have</span>
<span id="cb5-2062"><a href="#cb5-2062"></a>$\mathbf{E}<span class="co">[</span><span class="ot">X_{i}|\mathcal{F}_{i-1}</span><span class="co">]</span>-X_{i-1}\geq0$ almost surely. Hence,</span>
<span id="cb5-2063"><a href="#cb5-2063"></a>$(A_{n}:n\in\mathbf{N})$ is an increasing sequence almost surely. Let</span>
<span id="cb5-2064"><a href="#cb5-2064"></a>$M_{n}=X_{n}-A_{n}$.</span>
<span id="cb5-2065"><a href="#cb5-2065"></a></span>
<span id="cb5-2066"><a href="#cb5-2066"></a>We have:</span>
<span id="cb5-2067"><a href="#cb5-2067"></a></span>
<span id="cb5-2068"><a href="#cb5-2068"></a>$$\begin{aligned}</span>
<span id="cb5-2069"><a href="#cb5-2069"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">X_{n}-A_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2070"><a href="#cb5-2070"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">A_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2071"><a href="#cb5-2071"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span>-\mathbf{E}\left<span class="co">[</span><span class="ot">\left.\mathbf{E}[X_{n}|\mathcal{F}_{n-1}]-X_{n-1}+A_{n-1}\right|\mathcal{F}_{n-1}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2072"><a href="#cb5-2072"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span>+\mathbf{E}<span class="co">[</span><span class="ot">X_{n-1}|\mathcal{F}_{n-1}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">A_{n-1}|\mathcal{F}_{n-1}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2073"><a href="#cb5-2073"></a> &amp; =\cancel{\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span>}-\cancel{\mathbf{E}<span class="co">[</span><span class="ot">X_{n}|\mathcal{F}_{n-1}</span><span class="co">]</span>}+X_{n-1}-A_{n-1}<span class="sc">\\</span></span>
<span id="cb5-2074"><a href="#cb5-2074"></a> &amp; =M_{n-1}</span>
<span id="cb5-2075"><a href="#cb5-2075"></a>\end{aligned}$$</span>
<span id="cb5-2076"><a href="#cb5-2076"></a></span>
<span id="cb5-2077"><a href="#cb5-2077"></a>Thus, $(M_{n}:n\in\mathbf{N})$ is a martingale. Thus, we have obtained</span>
<span id="cb5-2078"><a href="#cb5-2078"></a>the Doob decomposition:</span>
<span id="cb5-2079"><a href="#cb5-2079"></a></span>
<span id="cb5-2080"><a href="#cb5-2080"></a>$$\begin{aligned}</span>
<span id="cb5-2081"><a href="#cb5-2081"></a>X_{n} &amp; =M_{n}+A_{n}\label{eq:doob-decomposition}</span>
<span id="cb5-2082"><a href="#cb5-2082"></a>\end{aligned}$$</span>
<span id="cb5-2083"><a href="#cb5-2083"></a></span>
<span id="cb5-2084"><a href="#cb5-2084"></a>This decomposition of a submartingale as a sum of a martingale and an</span>
<span id="cb5-2085"><a href="#cb5-2085"></a>adapted increasing sequence is unique, if we require that $A_{0}=0$ and</span>
<span id="cb5-2086"><a href="#cb5-2086"></a>that $A_{n}$ is $\mathcal{F}_{n-1}$-measurable.</span>
<span id="cb5-2087"><a href="#cb5-2087"></a></span>
<span id="cb5-2088"><a href="#cb5-2088"></a>For the continuous-time case, the situation is much more complicated.</span>
<span id="cb5-2089"><a href="#cb5-2089"></a>The analogue of equation</span>
<span id="cb5-2090"><a href="#cb5-2090"></a>(<span class="co">[</span><span class="ot">\[eq:doob-decomposition\]</span><span class="co">](#eq:doob-decomposition)</span>{reference-type="ref"</span>
<span id="cb5-2091"><a href="#cb5-2091"></a>reference="eq:doob-decomposition"}) is called the *Doob-Meyer</span>
<span id="cb5-2092"><a href="#cb5-2092"></a>decomposition*. We briefly describe this decomposition and avoid the</span>
<span id="cb5-2093"><a href="#cb5-2093"></a>technical details. All stochastic processes $X(t)$ are assumed to be</span>
<span id="cb5-2094"><a href="#cb5-2094"></a>right-continuous with left-hand limits $X(t-)$.</span>
<span id="cb5-2095"><a href="#cb5-2095"></a></span>
<span id="cb5-2096"><a href="#cb5-2096"></a>Let $X(t)$, $a\leq t\leq b$ be a submartingale with respect to a</span>
<span id="cb5-2097"><a href="#cb5-2097"></a>right-continuous filtration $(\mathcal{F}_{t}:a\leq t\leq b)$. If $X(t)$</span>
<span id="cb5-2098"><a href="#cb5-2098"></a>satisfies certain conditions, then it can be uniquely decomposed as:</span>
<span id="cb5-2099"><a href="#cb5-2099"></a></span>
<span id="cb5-2100"><a href="#cb5-2100"></a>$$\begin{aligned}</span>
<span id="cb5-2101"><a href="#cb5-2101"></a>X(t) &amp; =M(t)+C(t),\quad a\leq t\leq b</span>
<span id="cb5-2102"><a href="#cb5-2102"></a>\end{aligned}$$</span>
<span id="cb5-2103"><a href="#cb5-2103"></a></span>
<span id="cb5-2104"><a href="#cb5-2104"></a>where $M(t)$, $a\leq t\leq b$ is a martingale with respect to</span>
<span id="cb5-2105"><a href="#cb5-2105"></a>$(\mathcal{F}_{t};a\leq t\leq b)$, $C(t)$ is right-continuous and</span>
<span id="cb5-2106"><a href="#cb5-2106"></a>increasing almost surely with $\mathbf{E}<span class="co">[</span><span class="ot">C(t)</span><span class="co">]</span>&lt;\infty$.</span>
<span id="cb5-2107"><a href="#cb5-2107"></a>:::</span>
<span id="cb5-2108"><a href="#cb5-2108"></a></span>
<span id="cb5-2109"><a href="#cb5-2109"></a>::: example</span>
<span id="cb5-2110"><a href="#cb5-2110"></a>(Square of a Poisson Process). Let $(N_{t}:t\geq0)$ be a Poisson process</span>
<span id="cb5-2111"><a href="#cb5-2111"></a>with rate $\lambda$. We consider the compensated process</span>
<span id="cb5-2112"><a href="#cb5-2112"></a>$M_{t}=N_{t}-\lambda t$. By</span>
<span id="cb5-2113"><a href="#cb5-2113"></a>(<span class="co">[</span><span class="ot">\[corollary:the-convex-function-of-martingale-is-a-submartingale\]</span><span class="co">](#corollary:the-convex-function-of-martingale-is-a-submartingale)</span>{reference-type="ref"</span>
<span id="cb5-2114"><a href="#cb5-2114"></a>reference="corollary:the-convex-function-of-martingale-is-a-submartingale"}),</span>
<span id="cb5-2115"><a href="#cb5-2115"></a>the process $(M_{t}^{2}:t\geq0)$ is a submartingale for the filtration</span>
<span id="cb5-2116"><a href="#cb5-2116"></a>$(\mathcal{F}_{t}:t\geq0)$ of the Poisson process. How should we</span>
<span id="cb5-2117"><a href="#cb5-2117"></a>compensated $M_{t}^{2}$ to get a martingale? A direct computation using</span>
<span id="cb5-2118"><a href="#cb5-2118"></a>the properties of conditional expectation yields:</span>
<span id="cb5-2119"><a href="#cb5-2119"></a></span>
<span id="cb5-2120"><a href="#cb5-2120"></a>$$\begin{aligned}</span>
<span id="cb5-2121"><a href="#cb5-2121"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{t}^{2}|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(M_{t}-M_{s}+M_{s})^{2}|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2122"><a href="#cb5-2122"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(M_{t}-M_{s})^{2}+2(M_{t}-M_{s})M_{s}+M_{s}^{2}|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2123"><a href="#cb5-2123"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(M_{t}-M_{s})^{2}|\mathcal{F}_{s}</span><span class="co">]</span>+2\mathbf{E}<span class="co">[</span><span class="ot">(M_{t}-M_{s})M_{s}|\mathcal{F}_{s}</span><span class="co">]</span>+\mathbf{E}<span class="co">[</span><span class="ot">M_{s}^{2}|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2124"><a href="#cb5-2124"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(M_{t}-M_{s})^{2}</span><span class="co">]</span>+2M_{s}\underbrace{\mathbf{E}<span class="co">[</span><span class="ot">M_{t}-M_{s}</span><span class="co">]</span>}_{\text{equals \ensuremath{0}}}+M_{s}^{2}<span class="sc">\\</span></span>
<span id="cb5-2125"><a href="#cb5-2125"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">(M_{t}-M_{s})^{2}</span><span class="co">]</span>+M_{s}^{2}</span>
<span id="cb5-2126"><a href="#cb5-2126"></a>\end{aligned}$$</span>
<span id="cb5-2127"><a href="#cb5-2127"></a></span>
<span id="cb5-2128"><a href="#cb5-2128"></a>Now, if $X\sim\text{Poisson\ensuremath{(\lambda t)}}$, then</span>
<span id="cb5-2129"><a href="#cb5-2129"></a>$\mathbf{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>=\lambda t$ and</span>
<span id="cb5-2130"><a href="#cb5-2130"></a>$\mathbf{E}\ensuremath{<span class="co">[</span><span class="ot">X^{2}</span><span class="co">]</span>}=\lambda t(\lambda t+1)$.</span>
<span id="cb5-2131"><a href="#cb5-2131"></a></span>
<span id="cb5-2132"><a href="#cb5-2132"></a>$$\begin{aligned}</span>
<span id="cb5-2133"><a href="#cb5-2133"></a>\mathbf{E}<span class="co">[</span><span class="ot">(M_{t}-M_{s})^{2}</span><span class="co">]</span> &amp; =\mathbf{E}\left<span class="co">[</span><span class="ot">\left\{ (N_{t}-N_{s})-\lambda(t-s)\right\} ^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2134"><a href="#cb5-2134"></a> &amp; =\mathbf{E}\left<span class="co">[</span><span class="ot">(N_{t}-N_{s})^{2}\right</span><span class="co">]</span>-2\lambda(t-s)\mathbf{E}\left<span class="co">[</span><span class="ot">(N_{t}-N_{s})\right</span><span class="co">]</span>+\lambda^{2}(t-s)^{2}<span class="sc">\\</span></span>
<span id="cb5-2135"><a href="#cb5-2135"></a> &amp; =\lambda^{2}(t-s)^{2}+\lambda(t-s)-2\lambda(t-s)\cdot\lambda(t-s)+\lambda^{2}(t-s)^{2}<span class="sc">\\</span></span>
<span id="cb5-2136"><a href="#cb5-2136"></a> &amp; =\lambda(t-s)</span>
<span id="cb5-2137"><a href="#cb5-2137"></a>\end{aligned}$$</span>
<span id="cb5-2138"><a href="#cb5-2138"></a></span>
<span id="cb5-2139"><a href="#cb5-2139"></a>Thus,</span>
<span id="cb5-2140"><a href="#cb5-2140"></a></span>
<span id="cb5-2141"><a href="#cb5-2141"></a>$$\begin{aligned}</span>
<span id="cb5-2142"><a href="#cb5-2142"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{t}^{2}-\lambda t|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =M_{s}^{2}-\lambda s</span>
<span id="cb5-2143"><a href="#cb5-2143"></a>\end{aligned}$$</span>
<span id="cb5-2144"><a href="#cb5-2144"></a></span>
<span id="cb5-2145"><a href="#cb5-2145"></a>We conclude that the process $(M_{t}^{2}-\lambda t:t\geq0)$ is a</span>
<span id="cb5-2146"><a href="#cb5-2146"></a>martingale. The Doob-Meyer decomposition of the submartingale</span>
<span id="cb5-2147"><a href="#cb5-2147"></a>$M_{t}^{2}$ is then:</span>
<span id="cb5-2148"><a href="#cb5-2148"></a></span>
<span id="cb5-2149"><a href="#cb5-2149"></a>$$\begin{aligned}</span>
<span id="cb5-2150"><a href="#cb5-2150"></a>M_{t}^{2} &amp; =(M_{t}^{2}-\lambda t)+\lambda t</span>
<span id="cb5-2151"><a href="#cb5-2151"></a>\end{aligned}$$</span>
<span id="cb5-2152"><a href="#cb5-2152"></a>:::</span>
<span id="cb5-2153"><a href="#cb5-2153"></a></span>
<span id="cb5-2154"><a href="#cb5-2154"></a>::: example</span>
<span id="cb5-2155"><a href="#cb5-2155"></a>Consider a Brownian motion $B(t)$. The quadratic variation of the</span>
<span id="cb5-2156"><a href="#cb5-2156"></a>process $(B(t):t\geq0)$ over the interval $<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>$ is given by</span>
<span id="cb5-2157"><a href="#cb5-2157"></a>$<span class="co">[</span><span class="ot">B</span><span class="co">]</span>_{t}=t$. On the other hand, we saw, that the square of Brownian</span>
<span id="cb5-2158"><a href="#cb5-2158"></a>motion compensated, $(B_{t}^{2}-t:t\geq0)$ is a martingale. Hence, the</span>
<span id="cb5-2159"><a href="#cb5-2159"></a>Doob-Meyer decomposition of $B(t)^{2}$ is given by:</span>
<span id="cb5-2160"><a href="#cb5-2160"></a></span>
<span id="cb5-2161"><a href="#cb5-2161"></a>$$\begin{aligned}</span>
<span id="cb5-2162"><a href="#cb5-2162"></a>B(t)^{2} &amp; =(B(t)^{2}-t)+t</span>
<span id="cb5-2163"><a href="#cb5-2163"></a>\end{aligned}$$</span>
<span id="cb5-2164"><a href="#cb5-2164"></a>:::</span>
<span id="cb5-2165"><a href="#cb5-2165"></a></span>
<span id="cb5-2166"><a href="#cb5-2166"></a><span class="fu">## Computations with Martingales.</span></span>
<span id="cb5-2167"><a href="#cb5-2167"></a></span>
<span id="cb5-2168"><a href="#cb5-2168"></a>Martingales are not only conceptually interesting, they are also</span>
<span id="cb5-2169"><a href="#cb5-2169"></a>formidable tools to compute probabilities and expectations of processes.</span>
<span id="cb5-2170"><a href="#cb5-2170"></a>For example, in this section, we will solve the *gambler's ruin* problem</span>
<span id="cb5-2171"><a href="#cb5-2171"></a>for Brownian motion. For convenience, we introduce the notion of</span>
<span id="cb5-2172"><a href="#cb5-2172"></a>*stopping time* before doing so.</span>
<span id="cb5-2173"><a href="#cb5-2173"></a></span>
<span id="cb5-2174"><a href="#cb5-2174"></a>::: defn</span>
<span id="cb5-2175"><a href="#cb5-2175"></a>A random variable $\tau:\Omega\to\mathbf{N}\cup<span class="sc">\{</span>+\infty<span class="sc">\}</span>$ is said to</span>
<span id="cb5-2176"><a href="#cb5-2176"></a>be a *stopping time* for the filtration $(\mathcal{F}_{t}:t\geq0)$ if</span>
<span id="cb5-2177"><a href="#cb5-2177"></a>and only if:</span>
<span id="cb5-2178"><a href="#cb5-2178"></a></span>
<span id="cb5-2179"><a href="#cb5-2179"></a>$$\begin{aligned}</span>
<span id="cb5-2180"><a href="#cb5-2180"></a><span class="sc">\{</span>\omega:\tau(\omega)\leq t<span class="sc">\}</span> &amp; \in\mathcal{F}_{t},\quad\forall t\geq0</span>
<span id="cb5-2181"><a href="#cb5-2181"></a>\end{aligned}$$ Note that since $\mathcal{F}_{t}$ is a sigma-field, if</span>
<span id="cb5-2182"><a href="#cb5-2182"></a>$\tau$ is a stopping time, then we must also have that</span>
<span id="cb5-2183"><a href="#cb5-2183"></a>$<span class="sc">\{</span>\omega:\tau(\omega)&gt;t<span class="sc">\}</span>\in\mathcal{F}_{t}$.</span>
<span id="cb5-2184"><a href="#cb5-2184"></a></span>
<span id="cb5-2185"><a href="#cb5-2185"></a>In other words, $\tau$ is a stopping time, if we can decide if the</span>
<span id="cb5-2186"><a href="#cb5-2186"></a>events $<span class="sc">\{</span>\tau\leq t<span class="sc">\}</span>$ occurred or not based on the information</span>
<span id="cb5-2187"><a href="#cb5-2187"></a>available at time $t$.</span>
<span id="cb5-2188"><a href="#cb5-2188"></a></span>
<span id="cb5-2189"><a href="#cb5-2189"></a>The term *stopping time* comes from gambling: a gambler can decide to</span>
<span id="cb5-2190"><a href="#cb5-2190"></a>stop playing at a random time (depending for example on previous gains</span>
<span id="cb5-2191"><a href="#cb5-2191"></a>or losses), but when he or she decides to stop, his/her decision is</span>
<span id="cb5-2192"><a href="#cb5-2192"></a>based solely upon the knowledge of what happened before, and does not</span>
<span id="cb5-2193"><a href="#cb5-2193"></a>depend on future outcomes. In other words, the stopping policy/strategy</span>
<span id="cb5-2194"><a href="#cb5-2194"></a>can only depend on past outcomes. Otherwise, it would mean that he/she</span>
<span id="cb5-2195"><a href="#cb5-2195"></a>has a crystall ball.</span>
<span id="cb5-2196"><a href="#cb5-2196"></a>:::</span>
<span id="cb5-2197"><a href="#cb5-2197"></a></span>
<span id="cb5-2198"><a href="#cb5-2198"></a>::: example</span>
<span id="cb5-2199"><a href="#cb5-2199"></a>(Examples of stopping times).</span>
<span id="cb5-2200"><a href="#cb5-2200"></a></span>
<span id="cb5-2201"><a href="#cb5-2201"></a><span class="sc">\(</span>i<span class="sc">\)</span> *First passage time*. This is the first time when a process</span>
<span id="cb5-2202"><a href="#cb5-2202"></a>reaches a certain value. To be precise, let $X=(X_{t}:t\geq0)$ be a</span>
<span id="cb5-2203"><a href="#cb5-2203"></a>process and $(\mathcal{F}_{t}:t\geq0)$ be its natural filtration. For</span>
<span id="cb5-2204"><a href="#cb5-2204"></a>$a&gt;0$, we define the first passage time at $a$ to be:</span>
<span id="cb5-2205"><a href="#cb5-2205"></a></span>
<span id="cb5-2206"><a href="#cb5-2206"></a>$$\begin{aligned}</span>
<span id="cb5-2207"><a href="#cb5-2207"></a>\tau(\omega) &amp; =\inf<span class="sc">\{</span>s\geq0:X_{s}(\omega)\geq a<span class="sc">\}</span></span>
<span id="cb5-2208"><a href="#cb5-2208"></a>\end{aligned}$$</span>
<span id="cb5-2209"><a href="#cb5-2209"></a></span>
<span id="cb5-2210"><a href="#cb5-2210"></a>If the path $\omega$ never reaches $a$, we set $\tau(\omega)=\infty$.</span>
<span id="cb5-2211"><a href="#cb5-2211"></a>Now, for $t$ fixed and for a given path $X(\omega)$, it is possible to</span>
<span id="cb5-2212"><a href="#cb5-2212"></a>know if $<span class="sc">\{</span>\tau(\omega)\leq t<span class="sc">\}</span>$ (the path has reached $a$ before time</span>
<span id="cb5-2213"><a href="#cb5-2213"></a>$t$) or $<span class="sc">\{</span>\tau(\omega)&gt;t<span class="sc">\}</span>$ (the path has not reached $a$ before time</span>
<span id="cb5-2214"><a href="#cb5-2214"></a>$t$) with the information available at time $t$, since we are looking at</span>
<span id="cb5-2215"><a href="#cb5-2215"></a>the first time the process reaches $a$. Hence, we conclude that $\tau$</span>
<span id="cb5-2216"><a href="#cb5-2216"></a>is a stopping time.</span>
<span id="cb5-2217"><a href="#cb5-2217"></a></span>
<span id="cb5-2218"><a href="#cb5-2218"></a><span class="sc">\(</span>ii<span class="sc">\)</span> *Hitting time*. More generally, we can consider the first time</span>
<span id="cb5-2219"><a href="#cb5-2219"></a>(if ever) that the path of a process $(X_{t}:t\geq0)$ enters or hits a</span>
<span id="cb5-2220"><a href="#cb5-2220"></a>subset $B$ of $\mathbf{R}$:</span>
<span id="cb5-2221"><a href="#cb5-2221"></a></span>
<span id="cb5-2222"><a href="#cb5-2222"></a>$$\begin{aligned}</span>
<span id="cb5-2223"><a href="#cb5-2223"></a>\tau(\omega) &amp; =\min<span class="sc">\{</span>s\geq0:X_{s}(\omega)\in B<span class="sc">\}</span></span>
<span id="cb5-2224"><a href="#cb5-2224"></a>\end{aligned}$$</span>
<span id="cb5-2225"><a href="#cb5-2225"></a></span>
<span id="cb5-2226"><a href="#cb5-2226"></a>The first passage time is the particular case in which $B=[a,\infty)$.</span>
<span id="cb5-2227"><a href="#cb5-2227"></a></span>
<span id="cb5-2228"><a href="#cb5-2228"></a><span class="sc">\(</span>iii<span class="sc">\)</span> *Minimum of two stopping times.* If $\tau$ and $\tau'$ are two</span>
<span id="cb5-2229"><a href="#cb5-2229"></a>stopping times for the same filtration $(\mathcal{F}_{t}:t\geq0)$, then</span>
<span id="cb5-2230"><a href="#cb5-2230"></a>so is the minimum $\tau\land\tau'$ between the two, where</span>
<span id="cb5-2231"><a href="#cb5-2231"></a></span>
<span id="cb5-2232"><a href="#cb5-2232"></a>$$\begin{aligned}</span>
<span id="cb5-2233"><a href="#cb5-2233"></a>(\tau\land\tau')(\omega) &amp; =\min<span class="sc">\{</span>\tau(\omega),\tau'(\omega)<span class="sc">\}</span></span>
<span id="cb5-2234"><a href="#cb5-2234"></a>\end{aligned}$$</span>
<span id="cb5-2235"><a href="#cb5-2235"></a></span>
<span id="cb5-2236"><a href="#cb5-2236"></a>This is because for any $t\geq0$:</span>
<span id="cb5-2237"><a href="#cb5-2237"></a></span>
<span id="cb5-2238"><a href="#cb5-2238"></a>$$\begin{aligned}</span>
<span id="cb5-2239"><a href="#cb5-2239"></a><span class="sc">\{</span>\omega: &amp; (\tau\land\tau')(\omega)\leq t<span class="sc">\}</span>=<span class="sc">\{</span>\omega:\tau(\omega)\leq t<span class="sc">\}</span>\cup<span class="sc">\{</span>\omega:\tau'(\omega)\leq t<span class="sc">\}</span></span>
<span id="cb5-2240"><a href="#cb5-2240"></a>\end{aligned}$$</span>
<span id="cb5-2241"><a href="#cb5-2241"></a></span>
<span id="cb5-2242"><a href="#cb5-2242"></a>Since the right hand side is the union of two events in</span>
<span id="cb5-2243"><a href="#cb5-2243"></a>$\mathcal{F}_{t}$, it must also be in $\mathcal{F}_{t}$ by the</span>
<span id="cb5-2244"><a href="#cb5-2244"></a>properties of a sigma-field. We conclude that $\tau\land\tau'$ is a</span>
<span id="cb5-2245"><a href="#cb5-2245"></a>stopping time. Is it also the case that the maximum $\tau\lor\tau'$ is a</span>
<span id="cb5-2246"><a href="#cb5-2246"></a>stopping time?</span>
<span id="cb5-2247"><a href="#cb5-2247"></a></span>
<span id="cb5-2248"><a href="#cb5-2248"></a>For any fixed $t\geq0$, we have:</span>
<span id="cb5-2249"><a href="#cb5-2249"></a></span>
<span id="cb5-2250"><a href="#cb5-2250"></a>$$\begin{aligned}</span>
<span id="cb5-2251"><a href="#cb5-2251"></a><span class="sc">\{</span>\omega:(\tau\lor\tau')(\omega)\leq t<span class="sc">\}</span> &amp; =<span class="sc">\{</span>\omega:\tau(\omega)\leq t<span class="sc">\}</span>\cap<span class="sc">\{</span>\omega:\tau'(\omega)\leq t<span class="sc">\}</span></span>
<span id="cb5-2252"><a href="#cb5-2252"></a>\end{aligned}$$</span>
<span id="cb5-2253"><a href="#cb5-2253"></a></span>
<span id="cb5-2254"><a href="#cb5-2254"></a>Since the right hand side is the intersection of two events in</span>
<span id="cb5-2255"><a href="#cb5-2255"></a>$\mathcal{F}_{t}$, it must also be in $\mathcal{F}_{t}$ by the</span>
<span id="cb5-2256"><a href="#cb5-2256"></a>properties of a sigma-field. We conclude that $\tau\lor\tau'$ is a</span>
<span id="cb5-2257"><a href="#cb5-2257"></a>stopping time.</span>
<span id="cb5-2258"><a href="#cb5-2258"></a>:::</span>
<span id="cb5-2259"><a href="#cb5-2259"></a></span>
<span id="cb5-2260"><a href="#cb5-2260"></a>::: example</span>
<span id="cb5-2261"><a href="#cb5-2261"></a>(Last passage time is not a stopping time). What if we look at the last</span>
<span id="cb5-2262"><a href="#cb5-2262"></a>time the process reaches $a$, that is:</span>
<span id="cb5-2263"><a href="#cb5-2263"></a></span>
<span id="cb5-2264"><a href="#cb5-2264"></a>$$\begin{aligned}</span>
<span id="cb5-2265"><a href="#cb5-2265"></a>\rho(\omega) &amp; =\sup<span class="sc">\{</span>t\geq0:X_{t}(\omega)\geq a<span class="sc">\}</span></span>
<span id="cb5-2266"><a href="#cb5-2266"></a>\end{aligned}$$</span>
<span id="cb5-2267"><a href="#cb5-2267"></a></span>
<span id="cb5-2268"><a href="#cb5-2268"></a>This is a well-defined random variable, but it is not a stopping time.</span>
<span id="cb5-2269"><a href="#cb5-2269"></a>Based on the information available at time $t$, we are not able to</span>
<span id="cb5-2270"><a href="#cb5-2270"></a>decide whether or not $<span class="sc">\{</span>\rho(\omega)\leq t<span class="sc">\}</span>$ occurred or not, as the</span>
<span id="cb5-2271"><a href="#cb5-2271"></a>path can always reach $a$ one more time after $t$.</span>
<span id="cb5-2272"><a href="#cb5-2272"></a>:::</span>
<span id="cb5-2273"><a href="#cb5-2273"></a></span>
<span id="cb5-2274"><a href="#cb5-2274"></a>It turns out that a martingale that is stopped when the stopping time is</span>
<span id="cb5-2275"><a href="#cb5-2275"></a>attained remains a martingale.</span>
<span id="cb5-2276"><a href="#cb5-2276"></a></span>
<span id="cb5-2277"><a href="#cb5-2277"></a>::: prop</span>
<span id="cb5-2278"><a href="#cb5-2278"></a>(Stopped Martingale). If $(M_{t}:t\geq0)$ is a continuous martingale for</span>
<span id="cb5-2279"><a href="#cb5-2279"></a>the filtration $(\mathcal{F}_{t}:t\geq0)$ and $\tau$ is a stopping time</span>
<span id="cb5-2280"><a href="#cb5-2280"></a>for the same filtration, then the stopped process defined by</span>
<span id="cb5-2281"><a href="#cb5-2281"></a>$$\begin{aligned}</span>
<span id="cb5-2282"><a href="#cb5-2282"></a>M_{t\land\tau} &amp; =\begin{cases}</span>
<span id="cb5-2283"><a href="#cb5-2283"></a>M_{t} &amp; t\leq\tau<span class="sc">\\</span></span>
<span id="cb5-2284"><a href="#cb5-2284"></a>M_{\tau} &amp; t&gt;\tau</span>
<span id="cb5-2285"><a href="#cb5-2285"></a>\end{cases}</span>
<span id="cb5-2286"><a href="#cb5-2286"></a>\end{aligned}$$</span>
<span id="cb5-2287"><a href="#cb5-2287"></a></span>
<span id="cb5-2288"><a href="#cb5-2288"></a>is also a continuous martingale for the same filtration.</span>
<span id="cb5-2289"><a href="#cb5-2289"></a>:::</span>
<span id="cb5-2290"><a href="#cb5-2290"></a></span>
<span id="cb5-2291"><a href="#cb5-2291"></a>::: thm</span>
<span id="cb5-2292"><a href="#cb5-2292"></a>[]{#th:doob's-optional-sampling-theorem</span>
<span id="cb5-2293"><a href="#cb5-2293"></a>label="th:doob's-optional-sampling-theorem"}(Doob's Optional sampling</span>
<span id="cb5-2294"><a href="#cb5-2294"></a>theorem). If $(M_{t}:t\geq0)$ is a continuous martingale for the</span>
<span id="cb5-2295"><a href="#cb5-2295"></a>filtration $(\mathcal{F}_{t}:t\geq0)$ and $\tau$ is a stopping time such</span>
<span id="cb5-2296"><a href="#cb5-2296"></a>that $\tau&lt;\infty$ and the stopped process $(M_{t\land\tau}:t\geq0)$ is</span>
<span id="cb5-2297"><a href="#cb5-2297"></a>bounded, then:</span>
<span id="cb5-2298"><a href="#cb5-2298"></a></span>
<span id="cb5-2299"><a href="#cb5-2299"></a>$$\begin{aligned}</span>
<span id="cb5-2300"><a href="#cb5-2300"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{\tau}</span><span class="co">]</span> &amp; =M_{0}</span>
<span id="cb5-2301"><a href="#cb5-2301"></a>\end{aligned}$$</span>
<span id="cb5-2302"><a href="#cb5-2302"></a>:::</span>
<span id="cb5-2303"><a href="#cb5-2303"></a></span>
<span id="cb5-2304"><a href="#cb5-2304"></a>::: proof</span>
<span id="cb5-2305"><a href="#cb5-2305"></a>*Proof.* Since $(M_{\tau\land t}:t\geq0)$ is a martingale, we always</span>
<span id="cb5-2306"><a href="#cb5-2306"></a>have:</span>
<span id="cb5-2307"><a href="#cb5-2307"></a></span>
<span id="cb5-2308"><a href="#cb5-2308"></a>$$\begin{aligned}</span>
<span id="cb5-2309"><a href="#cb5-2309"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{\tau\land t}</span><span class="co">]</span> &amp; =M_{0}</span>
<span id="cb5-2310"><a href="#cb5-2310"></a>\end{aligned}$$</span>
<span id="cb5-2311"><a href="#cb5-2311"></a></span>
<span id="cb5-2312"><a href="#cb5-2312"></a>Now, since $\tau(\omega)&lt;\infty$, we must</span>
<span id="cb5-2313"><a href="#cb5-2313"></a></span>
<span id="cb5-2314"><a href="#cb5-2314"></a>have that $\lim_{t\to\infty}M_{\tau\land t}=M_{\tau}$ almost surely. In</span>
<span id="cb5-2315"><a href="#cb5-2315"></a>particular, we have:</span>
<span id="cb5-2316"><a href="#cb5-2316"></a></span>
<span id="cb5-2317"><a href="#cb5-2317"></a>$$\begin{aligned}</span>
<span id="cb5-2318"><a href="#cb5-2318"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{\tau}</span><span class="co">]</span> &amp; =\mathbf{E}\left<span class="co">[</span><span class="ot">\lim_{t\to\infty}M_{\tau\land t}\right</span><span class="co">]</span>=\lim_{t\to\infty}\mathbf{E}<span class="co">[</span><span class="ot">M_{\tau\land t}</span><span class="co">]</span>=\lim_{t\to\infty}M_{0}</span>
<span id="cb5-2319"><a href="#cb5-2319"></a>\end{aligned}$$</span>
<span id="cb5-2320"><a href="#cb5-2320"></a></span>
<span id="cb5-2321"><a href="#cb5-2321"></a>where we passed to the limit, using the dominated convergence theorem</span>
<span id="cb5-2322"><a href="#cb5-2322"></a>(<span class="co">[</span><span class="ot">\[th:dominated-convergence-theorem\]</span><span class="co">](#th:dominated-convergence-theorem)</span>{reference-type="ref"</span>
<span id="cb5-2323"><a href="#cb5-2323"></a>reference="th:dominated-convergence-theorem"}). ◻</span>
<span id="cb5-2324"><a href="#cb5-2324"></a>:::</span>
<span id="cb5-2325"><a href="#cb5-2325"></a></span>
<span id="cb5-2326"><a href="#cb5-2326"></a>::: example</span>
<span id="cb5-2327"><a href="#cb5-2327"></a>[]{#example:probability-of-hitting-times</span>
<span id="cb5-2328"><a href="#cb5-2328"></a>label="example:probability-of-hitting-times"}(Gambler's ruin with</span>
<span id="cb5-2329"><a href="#cb5-2329"></a>Brownian motion). The *gambler's ruin problem* is known in different</span>
<span id="cb5-2330"><a href="#cb5-2330"></a>forms. Roughly speaking, it refers to the problem of computing the</span>
<span id="cb5-2331"><a href="#cb5-2331"></a>probability of a gambler making a series of bets reaching a certain</span>
<span id="cb5-2332"><a href="#cb5-2332"></a>amount before going broke. In terms of Brownian motion (and stochastic</span>
<span id="cb5-2333"><a href="#cb5-2333"></a>processes in general), it translates to the following questions: Let</span>
<span id="cb5-2334"><a href="#cb5-2334"></a>$(B_{t}:t\geq0)$ be a standard brownian motion starting at $B_{0}=0$ and</span>
<span id="cb5-2335"><a href="#cb5-2335"></a>$a,b&gt;0$.</span>
<span id="cb5-2336"><a href="#cb5-2336"></a></span>
<span id="cb5-2337"><a href="#cb5-2337"></a><span class="sc">\(</span>1<span class="sc">\)</span> What is the probability that a Brownian path reaches $a$ before</span>
<span id="cb5-2338"><a href="#cb5-2338"></a>$-b$?</span>
<span id="cb5-2339"><a href="#cb5-2339"></a></span>
<span id="cb5-2340"><a href="#cb5-2340"></a><span class="sc">\(</span>2<span class="sc">\)</span> What is the expected waiting time for the path to reach $a$ or</span>
<span id="cb5-2341"><a href="#cb5-2341"></a>$-b$?</span>
<span id="cb5-2342"><a href="#cb5-2342"></a></span>
<span id="cb5-2343"><a href="#cb5-2343"></a>For the first question, it is a simple computation using stopping time</span>
<span id="cb5-2344"><a href="#cb5-2344"></a>and martingale properties. Define the hitting time:</span>
<span id="cb5-2345"><a href="#cb5-2345"></a></span>
<span id="cb5-2346"><a href="#cb5-2346"></a>$$\begin{aligned}</span>
<span id="cb5-2347"><a href="#cb5-2347"></a>\tau(\omega) &amp; =\inf<span class="sc">\{</span>t\geq0:B_{t}(\omega)\geq a\text{ or }B_{t}(\omega)\leq-b<span class="sc">\}</span></span>
<span id="cb5-2348"><a href="#cb5-2348"></a>\end{aligned}$$</span>
<span id="cb5-2349"><a href="#cb5-2349"></a></span>
<span id="cb5-2350"><a href="#cb5-2350"></a>Note that $\tau$ is the minimum between the first passage time at $a$</span>
<span id="cb5-2351"><a href="#cb5-2351"></a>and the one at $-b$.</span>
<span id="cb5-2352"><a href="#cb5-2352"></a></span>
<span id="cb5-2353"><a href="#cb5-2353"></a>We first show that $\tau&lt;\infty$ almost surely. In other words, all</span>
<span id="cb5-2354"><a href="#cb5-2354"></a>Brownian paths reach $a$ or $-b$ eventually. To see this, consider the</span>
<span id="cb5-2355"><a href="#cb5-2355"></a>event $E_{n}$ that the $n$-th increment exceeds $a+b$</span>
<span id="cb5-2356"><a href="#cb5-2356"></a></span>
<span id="cb5-2357"><a href="#cb5-2357"></a>$$\begin{aligned}</span>
<span id="cb5-2358"><a href="#cb5-2358"></a>E_{n} &amp; :=\left<span class="sc">\{</span> |B_{n}-B_{n-1}|&gt;a+b\right<span class="sc">\}</span> </span>
<span id="cb5-2359"><a href="#cb5-2359"></a>\end{aligned}$$</span>
<span id="cb5-2360"><a href="#cb5-2360"></a></span>
<span id="cb5-2361"><a href="#cb5-2361"></a>Note that, if $E_{n}$ occurs, then we must have that the Brownian motion</span>
<span id="cb5-2362"><a href="#cb5-2362"></a>path exits the interval $<span class="co">[</span><span class="ot">-b,a</span><span class="co">]</span>.$ Moreover, we have</span>
<span id="cb5-2363"><a href="#cb5-2363"></a>$\mathbb{P}(E_{n})=\mathbb{P}(E_{1})$ for all $n$. Call this probability</span>
<span id="cb5-2364"><a href="#cb5-2364"></a>$p$.</span>
<span id="cb5-2365"><a href="#cb5-2365"></a></span>
<span id="cb5-2366"><a href="#cb5-2366"></a>Since the events $E_{n}$ are independent, we have:</span>
<span id="cb5-2367"><a href="#cb5-2367"></a></span>
<span id="cb5-2368"><a href="#cb5-2368"></a>$$\begin{aligned}</span>
<span id="cb5-2369"><a href="#cb5-2369"></a>\mathbb{P}(E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}) &amp; =(1-p)^{n}</span>
<span id="cb5-2370"><a href="#cb5-2370"></a>\end{aligned}$$</span>
<span id="cb5-2371"><a href="#cb5-2371"></a></span>
<span id="cb5-2372"><a href="#cb5-2372"></a>As $n\to\infty$ we have:</span>
<span id="cb5-2373"><a href="#cb5-2373"></a></span>
<span id="cb5-2374"><a href="#cb5-2374"></a>$$\begin{aligned}</span>
<span id="cb5-2375"><a href="#cb5-2375"></a>\lim_{n\to\infty}\mathbb{P}(E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}) &amp; =0</span>
<span id="cb5-2376"><a href="#cb5-2376"></a>\end{aligned}$$</span>
<span id="cb5-2377"><a href="#cb5-2377"></a></span>
<span id="cb5-2378"><a href="#cb5-2378"></a>The sequence of events $(F_{n})$ where</span>
<span id="cb5-2379"><a href="#cb5-2379"></a>$F_{n}=E_{1}^{C}\cap E_{2}^{C}\cap\ldots\cap E_{n}^{C}$ is a decreasing</span>
<span id="cb5-2380"><a href="#cb5-2380"></a>sequence of events. By the continuity of probability measure lemma</span>
<span id="cb5-2381"><a href="#cb5-2381"></a>(<span class="co">[</span><span class="ot">\[th:continuity-property-of-lebesgue-measure\]</span><span class="co">](#th:continuity-property-of-lebesgue-measure)</span>{reference-type="ref"</span>
<span id="cb5-2382"><a href="#cb5-2382"></a>reference="th:continuity-property-of-lebesgue-measure"}), we conclude</span>
<span id="cb5-2383"><a href="#cb5-2383"></a>that:</span>
<span id="cb5-2384"><a href="#cb5-2384"></a></span>
<span id="cb5-2385"><a href="#cb5-2385"></a>$$\begin{aligned}</span>
<span id="cb5-2386"><a href="#cb5-2386"></a>\lim_{n\to\infty}\mathbb{P}\left(F_{n}\right) &amp; =\mathbb{P}\left(\bigcap_{n=1}^{\infty}F_{n}\right)=0</span>
<span id="cb5-2387"><a href="#cb5-2387"></a>\end{aligned}$$</span>
<span id="cb5-2388"><a href="#cb5-2388"></a></span>
<span id="cb5-2389"><a href="#cb5-2389"></a>Therefore, it must be the case $\mathbb{P}(\cup_{n=1}^{\infty}E_{n})=1$.</span>
<span id="cb5-2390"><a href="#cb5-2390"></a>So, $E_{n}$ must occur for some $n$, so all brownian motion paths reach</span>
<span id="cb5-2391"><a href="#cb5-2391"></a>$a$ or $-b$ almost surely.</span>
<span id="cb5-2392"><a href="#cb5-2392"></a></span>
<span id="cb5-2393"><a href="#cb5-2393"></a>Since $\tau&lt;\infty$ with probability one, the random variable $B_{\tau}$</span>
<span id="cb5-2394"><a href="#cb5-2394"></a>is well-defined : $B_{\tau}(\omega)=B_{t}(\omega)$ if $\tau(\omega)=t$.</span>
<span id="cb5-2395"><a href="#cb5-2395"></a>It can only take two values: $a$ or $-b$. Question (1) above translates</span>
<span id="cb5-2396"><a href="#cb5-2396"></a>into computing $\mathbb{P}(B_{\tau}=a)$. On one hand, we have:</span>
<span id="cb5-2397"><a href="#cb5-2397"></a></span>
<span id="cb5-2398"><a href="#cb5-2398"></a>$$\begin{aligned}</span>
<span id="cb5-2399"><a href="#cb5-2399"></a>\mathbf{E}<span class="co">[</span><span class="ot">B_{\tau}</span><span class="co">]</span> &amp; =a\mathbb{P}(B_{\tau}=a)+(-b)(1-\mathbb{P}(B_{\tau}=a))</span>
<span id="cb5-2400"><a href="#cb5-2400"></a>\end{aligned}$$</span>
<span id="cb5-2401"><a href="#cb5-2401"></a></span>
<span id="cb5-2402"><a href="#cb5-2402"></a>On the other hand, by corollary</span>
<span id="cb5-2403"><a href="#cb5-2403"></a>(<span class="co">[</span><span class="ot">\[th:doob\'s-optional-sampling-theorem\]</span><span class="co">](#th:doob's-optional-sampling-theorem)</span>{reference-type="ref"</span>
<span id="cb5-2404"><a href="#cb5-2404"></a>reference="th:doob's-optional-sampling-theorem"}), we have</span>
<span id="cb5-2405"><a href="#cb5-2405"></a>$\mathbf{E}<span class="co">[</span><span class="ot">B_{\tau}</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">B_{0}</span><span class="co">]</span>=0$. (Note that the stopped</span>
<span id="cb5-2406"><a href="#cb5-2406"></a>process $(B_{t\land\tau}:t\geq0)$ is bounded above by $a$ and by $-b$</span>
<span id="cb5-2407"><a href="#cb5-2407"></a>below). Putting these two observations together, we get:</span>
<span id="cb5-2408"><a href="#cb5-2408"></a></span>
<span id="cb5-2409"><a href="#cb5-2409"></a>$$\begin{aligned}</span>
<span id="cb5-2410"><a href="#cb5-2410"></a>\mathbb{P}(B_{\tau}=a) &amp; =\frac{b}{a+b}</span>
<span id="cb5-2411"><a href="#cb5-2411"></a>\end{aligned}$$</span>
<span id="cb5-2412"><a href="#cb5-2412"></a></span>
<span id="cb5-2413"><a href="#cb5-2413"></a>A very simple and elegant answer!</span>
<span id="cb5-2414"><a href="#cb5-2414"></a></span>
<span id="cb5-2415"><a href="#cb5-2415"></a>We will revisit this problem again and again. In particular, we will</span>
<span id="cb5-2416"><a href="#cb5-2416"></a>answer the question above for Brownian motion with a drift at length</span>
<span id="cb5-2417"><a href="#cb5-2417"></a>further ahead.</span>
<span id="cb5-2418"><a href="#cb5-2418"></a>:::</span>
<span id="cb5-2419"><a href="#cb5-2419"></a></span>
<span id="cb5-2420"><a href="#cb5-2420"></a>::: example</span>
<span id="cb5-2421"><a href="#cb5-2421"></a>[]{#ex:expected-waiting-times</span>
<span id="cb5-2422"><a href="#cb5-2422"></a>label="ex:expected-waiting-times"}(Expected Waiting Time). Let $\tau$ be</span>
<span id="cb5-2423"><a href="#cb5-2423"></a>as in the last example. We now answer question (2) of the gambler's ruin</span>
<span id="cb5-2424"><a href="#cb5-2424"></a>problem:</span>
<span id="cb5-2425"><a href="#cb5-2425"></a></span>
<span id="cb5-2426"><a href="#cb5-2426"></a>$$\begin{aligned}</span>
<span id="cb5-2427"><a href="#cb5-2427"></a>\mathbf{E}<span class="co">[</span><span class="ot">\tau</span><span class="co">]</span> &amp; =ab</span>
<span id="cb5-2428"><a href="#cb5-2428"></a>\end{aligned}$$</span>
<span id="cb5-2429"><a href="#cb5-2429"></a></span>
<span id="cb5-2430"><a href="#cb5-2430"></a>Note that the expected waiting time is consistent with the rough</span>
<span id="cb5-2431"><a href="#cb5-2431"></a>heuristic that Brownian motion travels a distance $\sqrt{t}$ by time</span>
<span id="cb5-2432"><a href="#cb5-2432"></a>$t$. We now use the martingale $M_{t}=B_{t}^{2}-t$. On the one hand, if</span>
<span id="cb5-2433"><a href="#cb5-2433"></a>we apply optional stopping in corollary</span>
<span id="cb5-2434"><a href="#cb5-2434"></a>(<span class="co">[</span><span class="ot">\[th:doob\'s-optional-sampling-theorem\]</span><span class="co">](#th:doob's-optional-sampling-theorem)</span>{reference-type="ref"</span>
<span id="cb5-2435"><a href="#cb5-2435"></a>reference="th:doob's-optional-sampling-theorem"}), we get:</span>
<span id="cb5-2436"><a href="#cb5-2436"></a></span>
<span id="cb5-2437"><a href="#cb5-2437"></a>$$\begin{aligned}</span>
<span id="cb5-2438"><a href="#cb5-2438"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{\tau}</span><span class="co">]</span> &amp; =M_{0}=0</span>
<span id="cb5-2439"><a href="#cb5-2439"></a>\end{aligned}$$</span>
<span id="cb5-2440"><a href="#cb5-2440"></a></span>
<span id="cb5-2441"><a href="#cb5-2441"></a>Moreover, we know the distribution of $B_{\tau}$, thanks to the</span>
<span id="cb5-2442"><a href="#cb5-2442"></a>probability calculated in the last example. We can therefore compute</span>
<span id="cb5-2443"><a href="#cb5-2443"></a>$\mathbf{E}<span class="co">[</span><span class="ot">M_{\tau}</span><span class="co">]</span>$ directly:</span>
<span id="cb5-2444"><a href="#cb5-2444"></a></span>
<span id="cb5-2445"><a href="#cb5-2445"></a>$$\begin{aligned}</span>
<span id="cb5-2446"><a href="#cb5-2446"></a>0 &amp; =\mathbf{E}<span class="co">[</span><span class="ot">M_{\tau}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2447"><a href="#cb5-2447"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{\tau}^{2}-\tau</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2448"><a href="#cb5-2448"></a> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{\tau}^{2}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">\tau</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2449"><a href="#cb5-2449"></a> &amp; =a^{2}\cdot\frac{b}{a+b}+b^{2}\cdot\frac{a}{a+b}-\mathbf{E}<span class="co">[</span><span class="ot">\tau</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-2450"><a href="#cb5-2450"></a>\mathbf{E}<span class="co">[</span><span class="ot">\tau</span><span class="co">]</span> &amp; =\frac{a^{2}b+b^{2}a}{a+b}<span class="sc">\\</span></span>
<span id="cb5-2451"><a href="#cb5-2451"></a> &amp; =\frac{ab\cancel{(a+b)}}{\cancel{(a+b)}}=ab</span>
<span id="cb5-2452"><a href="#cb5-2452"></a>\end{aligned}$$</span>
<span id="cb5-2453"><a href="#cb5-2453"></a></span>
<span id="cb5-2454"><a href="#cb5-2454"></a>Why can we apply optional stopping here? The random variable $\tau$ is</span>
<span id="cb5-2455"><a href="#cb5-2455"></a>finite with probability $1$ as before. However, the stopped martingale</span>
<span id="cb5-2456"><a href="#cb5-2456"></a>is not necessarily bounded as before: $B_{\tau\land t}$ is bounded but</span>
<span id="cb5-2457"><a href="#cb5-2457"></a>$\tau$ is not. However, the conclusion of optional stopping still holds.</span>
<span id="cb5-2458"><a href="#cb5-2458"></a>Indeed, we have:</span>
<span id="cb5-2459"><a href="#cb5-2459"></a></span>
<span id="cb5-2460"><a href="#cb5-2460"></a>$$\begin{aligned}</span>
<span id="cb5-2461"><a href="#cb5-2461"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{t\land\tau}</span><span class="co">]</span> &amp; =\mathbf{E}<span class="co">[</span><span class="ot">B_{t\land\tau}^{2}</span><span class="co">]</span>-\mathbf{E}<span class="co">[</span><span class="ot">t\land\tau</span><span class="co">]</span></span>
<span id="cb5-2462"><a href="#cb5-2462"></a>\end{aligned}$$</span>
<span id="cb5-2463"><a href="#cb5-2463"></a></span>
<span id="cb5-2464"><a href="#cb5-2464"></a>By the bounded convergence theorem, we get</span>
<span id="cb5-2465"><a href="#cb5-2465"></a>$\lim_{t\to\infty}\mathbf{E}<span class="co">[</span><span class="ot">B_{t\land\tau}^{2}</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">\lim_{t\to\infty}B_{t\land\tau}^{2}</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">B_{\tau}^{2}</span><span class="co">]</span>$.</span>
<span id="cb5-2466"><a href="#cb5-2466"></a>Since $\tau\land t$ is a non-decreasing sequence and as $t\to\infty$,</span>
<span id="cb5-2467"><a href="#cb5-2467"></a>$t\land\tau\to\tau$ almost surely, as $\tau&lt;\infty$, by the monotone</span>
<span id="cb5-2468"><a href="#cb5-2468"></a>convergence theorem,</span>
<span id="cb5-2469"><a href="#cb5-2469"></a>$\lim_{t\to\infty}\mathbf{E}<span class="co">[</span><span class="ot">t\land\tau</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">\tau</span><span class="co">]</span>$.</span>
<span id="cb5-2470"><a href="#cb5-2470"></a>:::</span>
<span id="cb5-2471"><a href="#cb5-2471"></a></span>
<span id="cb5-2472"><a href="#cb5-2472"></a>::: example</span>
<span id="cb5-2473"><a href="#cb5-2473"></a>(First passage time of Brownian Motion.) We can use the previous two</span>
<span id="cb5-2474"><a href="#cb5-2474"></a>examples to get some very interesting information on the first passage</span>
<span id="cb5-2475"><a href="#cb5-2475"></a>time:</span>
<span id="cb5-2476"><a href="#cb5-2476"></a></span>
<span id="cb5-2477"><a href="#cb5-2477"></a>$$\begin{aligned}</span>
<span id="cb5-2478"><a href="#cb5-2478"></a>\tau_{a} &amp; =\inf<span class="sc">\{</span>t\geq0:B_{t}\geq a<span class="sc">\}</span></span>
<span id="cb5-2479"><a href="#cb5-2479"></a>\end{aligned}$$</span>
<span id="cb5-2480"><a href="#cb5-2480"></a></span>
<span id="cb5-2481"><a href="#cb5-2481"></a>Let $\tau=\tau_{a}\land\tau_{-b}$ be as in the previous examples with</span>
<span id="cb5-2482"><a href="#cb5-2482"></a>$\tau_{-b}=\inf<span class="sc">\{</span>t\geq0:B_{t}\leq-b<span class="sc">\}</span>$. Note that</span>
<span id="cb5-2483"><a href="#cb5-2483"></a>$(\tau_{-b},b\in\mathbf{R}_{+})$ is a sequence of random variables that</span>
<span id="cb5-2484"><a href="#cb5-2484"></a>is increasing in $b$. A brownian motion path must cross through $-1$</span>
<span id="cb5-2485"><a href="#cb5-2485"></a>before it hits $-2$ for the first time and in general</span>
<span id="cb5-2486"><a href="#cb5-2486"></a>$\tau_{-n}(\omega)\leq\tau_{-(n+1)}(\omega)$. Moreover, we have</span>
<span id="cb5-2487"><a href="#cb5-2487"></a>$\tau_{-b}\to\infty$ almost surely as $b\to\infty$. That's because,</span>
<span id="cb5-2488"><a href="#cb5-2488"></a>$\mathbb{P}<span class="sc">\{</span>\tau&lt;\infty<span class="sc">\}</span>=1$. Moreover, the event $<span class="sc">\{</span>B_{\tau}=a<span class="sc">\}</span>$ is</span>
<span id="cb5-2489"><a href="#cb5-2489"></a>the same as $<span class="sc">\{</span>\tau_{a}&lt;\tau_{-b}<span class="sc">\}</span>$. Now, the events</span>
<span id="cb5-2490"><a href="#cb5-2490"></a>$<span class="sc">\{</span>\tau_{a}&lt;\tau_{-b}<span class="sc">\}</span>$ are increasing in $b$, since if a path reaches</span>
<span id="cb5-2491"><a href="#cb5-2491"></a>$a$ before $-b$, it will do so as well for a more negative value of</span>
<span id="cb5-2492"><a href="#cb5-2492"></a>$-b$. On one hand, this means by the continuity of probability measure</span>
<span id="cb5-2493"><a href="#cb5-2493"></a>lemma</span>
<span id="cb5-2494"><a href="#cb5-2494"></a>(<span class="co">[</span><span class="ot">\[th:continuity-property-of-lebesgue-measure\]</span><span class="co">](#th:continuity-property-of-lebesgue-measure)</span>{reference-type="ref"</span>
<span id="cb5-2495"><a href="#cb5-2495"></a>reference="th:continuity-property-of-lebesgue-measure"}) that:</span>
<span id="cb5-2496"><a href="#cb5-2496"></a></span>
<span id="cb5-2497"><a href="#cb5-2497"></a>$$\begin{aligned}</span>
<span id="cb5-2498"><a href="#cb5-2498"></a>\lim_{b\to\infty}\mathbb{P}\left<span class="sc">\{</span> \tau_{a}&lt;\tau_{-b}\right<span class="sc">\}</span>  &amp; =\mathbb{P}<span class="sc">\{</span>\lim_{b\to\infty}\tau_{a}&lt;\tau_{-b}<span class="sc">\}\\</span></span>
<span id="cb5-2499"><a href="#cb5-2499"></a> &amp; =\mathbb{P}<span class="sc">\{</span>\tau_{a}&lt;\infty<span class="sc">\}</span></span>
<span id="cb5-2500"><a href="#cb5-2500"></a>\end{aligned}$$</span>
<span id="cb5-2501"><a href="#cb5-2501"></a></span>
<span id="cb5-2502"><a href="#cb5-2502"></a>On the other hand, we have by example</span>
<span id="cb5-2503"><a href="#cb5-2503"></a>(<span class="co">[</span><span class="ot">\[example:probability-of-hitting-times\]</span><span class="co">](#example:probability-of-hitting-times)</span>{reference-type="ref"</span>
<span id="cb5-2504"><a href="#cb5-2504"></a>reference="example:probability-of-hitting-times"})</span>
<span id="cb5-2505"><a href="#cb5-2505"></a></span>
<span id="cb5-2506"><a href="#cb5-2506"></a>$$\begin{aligned}</span>
<span id="cb5-2507"><a href="#cb5-2507"></a>\lim_{b\to\infty}\mathbb{P}\left<span class="sc">\{</span> \tau_{a}&lt;\tau_{-b}\right<span class="sc">\}</span>  &amp; =\lim_{b\to\infty}\mathbb{P}<span class="sc">\{</span>B_{\tau}=a<span class="sc">\}\\</span></span>
<span id="cb5-2508"><a href="#cb5-2508"></a> &amp; =\lim_{b\to\infty}\frac{b}{b+a}<span class="sc">\\</span></span>
<span id="cb5-2509"><a href="#cb5-2509"></a> &amp; =1</span>
<span id="cb5-2510"><a href="#cb5-2510"></a>\end{aligned}$$</span>
<span id="cb5-2511"><a href="#cb5-2511"></a></span>
<span id="cb5-2512"><a href="#cb5-2512"></a>We just showed that:</span>
<span id="cb5-2513"><a href="#cb5-2513"></a></span>
<span id="cb5-2514"><a href="#cb5-2514"></a>$$\begin{aligned}</span>
<span id="cb5-2515"><a href="#cb5-2515"></a>\mathbb{P}\left<span class="sc">\{</span> \tau_{a}&lt;\infty\right<span class="sc">\}</span>  &amp; =1\label{eq:first-passage-time-to-a-is-finite-almost-surely}</span>
<span id="cb5-2516"><a href="#cb5-2516"></a>\end{aligned}$$</span>
<span id="cb5-2517"><a href="#cb5-2517"></a></span>
<span id="cb5-2518"><a href="#cb5-2518"></a>In other words, every Brownian path will reach $a$, no matter how large</span>
<span id="cb5-2519"><a href="#cb5-2519"></a>$a$ is!</span>
<span id="cb5-2520"><a href="#cb5-2520"></a></span>
<span id="cb5-2521"><a href="#cb5-2521"></a>How long will it take to reach $a$ on average? Well, we know from</span>
<span id="cb5-2522"><a href="#cb5-2522"></a>example</span>
<span id="cb5-2523"><a href="#cb5-2523"></a>(<span class="co">[</span><span class="ot">\[ex:expected-waiting-times\]</span><span class="co">](#ex:expected-waiting-times)</span>{reference-type="ref"</span>
<span id="cb5-2524"><a href="#cb5-2524"></a>reference="ex:expected-waiting-times"}) that</span>
<span id="cb5-2525"><a href="#cb5-2525"></a>$\mathbf{E}<span class="co">[</span><span class="ot">\tau_{a}\land\tau_{-b}</span><span class="co">]</span>=ab$. On one hand this means,</span>
<span id="cb5-2526"><a href="#cb5-2526"></a></span>
<span id="cb5-2527"><a href="#cb5-2527"></a>$$\begin{aligned}</span>
<span id="cb5-2528"><a href="#cb5-2528"></a>\lim_{b\to\infty}\mathbf{E}<span class="co">[</span><span class="ot">\tau_{a}\land\tau_{-b}</span><span class="co">]</span> &amp; =\lim_{b\to\infty}ab=\infty</span>
<span id="cb5-2529"><a href="#cb5-2529"></a>\end{aligned}$$</span>
<span id="cb5-2530"><a href="#cb5-2530"></a></span>
<span id="cb5-2531"><a href="#cb5-2531"></a>On the other hand, since the random variables $\tau_{-b}$ are</span>
<span id="cb5-2532"><a href="#cb5-2532"></a>increasing,</span>
<span id="cb5-2533"><a href="#cb5-2533"></a></span>
<span id="cb5-2534"><a href="#cb5-2534"></a>$$\begin{aligned}</span>
<span id="cb5-2535"><a href="#cb5-2535"></a>\lim_{b\to\infty}\mathbf{E}<span class="co">[</span><span class="ot">\tau_{a}\land\tau_{-b}</span><span class="co">]</span> &amp; =\mathbf{E}\left<span class="co">[</span><span class="ot">\lim_{b\to\infty}\tau_{a}\land\tau_{-b}\right</span><span class="co">]</span>=\mathbf{E}<span class="co">[</span><span class="ot">\tau_{a}</span><span class="co">]</span></span>
<span id="cb5-2536"><a href="#cb5-2536"></a>\end{aligned}$$</span>
<span id="cb5-2537"><a href="#cb5-2537"></a></span>
<span id="cb5-2538"><a href="#cb5-2538"></a>by the monotone convergence theorem</span>
<span id="cb5-2539"><a href="#cb5-2539"></a>(<span class="co">[</span><span class="ot">\[th:monotone-convergence-theorem\]</span><span class="co">](#th:monotone-convergence-theorem)</span>{reference-type="ref"</span>
<span id="cb5-2540"><a href="#cb5-2540"></a>reference="th:monotone-convergence-theorem"}). We just proved that:</span>
<span id="cb5-2541"><a href="#cb5-2541"></a></span>
<span id="cb5-2542"><a href="#cb5-2542"></a>$$\begin{aligned}</span>
<span id="cb5-2543"><a href="#cb5-2543"></a>\mathbf{E}<span class="co">[</span><span class="ot">\tau_{a}</span><span class="co">]</span> &amp; =\infty</span>
<span id="cb5-2544"><a href="#cb5-2544"></a>\end{aligned}$$</span>
<span id="cb5-2545"><a href="#cb5-2545"></a></span>
<span id="cb5-2546"><a href="#cb5-2546"></a>In other words, any Brownian motion path will reach $a$, but the</span>
<span id="cb5-2547"><a href="#cb5-2547"></a>expected waiting time for this to occur is infinite, no matter, how</span>
<span id="cb5-2548"><a href="#cb5-2548"></a>small $a$ is! What is happening here? No matter, how small $a$ is, there</span>
<span id="cb5-2549"><a href="#cb5-2549"></a>is always paths that reach very large negative values before hitting</span>
<span id="cb5-2550"><a href="#cb5-2550"></a>$a$. These paths might be unlikely. However, the first passage time for</span>
<span id="cb5-2551"><a href="#cb5-2551"></a>these paths is so large that they affect the value of the expectation</span>
<span id="cb5-2552"><a href="#cb5-2552"></a>substantially. In other words, $\tau_{a}$ is a *heavy-tailed random</span>
<span id="cb5-2553"><a href="#cb5-2553"></a>variable*. We look at the distribution of $\tau_{a}$ in more detail in</span>
<span id="cb5-2554"><a href="#cb5-2554"></a>the next section.</span>
<span id="cb5-2555"><a href="#cb5-2555"></a>:::</span>
<span id="cb5-2556"><a href="#cb5-2556"></a></span>
<span id="cb5-2557"><a href="#cb5-2557"></a>::: example</span>
<span id="cb5-2558"><a href="#cb5-2558"></a>(When option stopping fails). Consider $\tau_{a}$, the first passage</span>
<span id="cb5-2559"><a href="#cb5-2559"></a>time at $a&gt;0$. The random variable $B_{\tau_{a}}$ is well-defined since</span>
<span id="cb5-2560"><a href="#cb5-2560"></a>$\tau_{a}&lt;\infty$. In fact, we have $B_{\tau_{a}}=a$ with probability</span>
<span id="cb5-2561"><a href="#cb5-2561"></a>one. Therefore, the following must hold:</span>
<span id="cb5-2562"><a href="#cb5-2562"></a></span>
<span id="cb5-2563"><a href="#cb5-2563"></a>$$\begin{aligned}</span>
<span id="cb5-2564"><a href="#cb5-2564"></a>\mathbf{E}<span class="co">[</span><span class="ot">B_{\tau_{a}}</span><span class="co">]</span> &amp; =a\neq B_{0}</span>
<span id="cb5-2565"><a href="#cb5-2565"></a>\end{aligned}$$</span>
<span id="cb5-2566"><a href="#cb5-2566"></a></span>
<span id="cb5-2567"><a href="#cb5-2567"></a>Optional stopping theorem corollary</span>
<span id="cb5-2568"><a href="#cb5-2568"></a>(<span class="co">[</span><span class="ot">\[th:doob\'s-optional-sampling-theorem\]</span><span class="co">](#th:doob's-optional-sampling-theorem)</span>{reference-type="ref"</span>
<span id="cb5-2569"><a href="#cb5-2569"></a>reference="th:doob's-optional-sampling-theorem"}) does not apply here,</span>
<span id="cb5-2570"><a href="#cb5-2570"></a>since the stopped process $(B_{t\land\tau_{a}}:t\geq0)$ is not bounded.</span>
<span id="cb5-2571"><a href="#cb5-2571"></a>$B_{t\land\tau_{a}}$ can become infinitely negative before hitting $a$.</span>
<span id="cb5-2572"><a href="#cb5-2572"></a>:::</span>
<span id="cb5-2573"><a href="#cb5-2573"></a></span>
<span id="cb5-2574"><a href="#cb5-2574"></a><span class="fu">## Reflection principle for Brownian motion.</span></span>
<span id="cb5-2575"><a href="#cb5-2575"></a></span>
<span id="cb5-2576"><a href="#cb5-2576"></a>::: prop</span>
<span id="cb5-2577"><a href="#cb5-2577"></a>[]{#prop:bacheliers-formula label="prop:bacheliers-formula"}(Bachelier's</span>
<span id="cb5-2578"><a href="#cb5-2578"></a>formula). Let $(B_{t}:t\leq T)$ be a standard brownian motion on</span>
<span id="cb5-2579"><a href="#cb5-2579"></a>$<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>.$ Then, the CDF of the random variable</span>
<span id="cb5-2580"><a href="#cb5-2580"></a>$\sup_{0\leq t\leq T}B_{t}$ is:</span>
<span id="cb5-2581"><a href="#cb5-2581"></a></span>
<span id="cb5-2582"><a href="#cb5-2582"></a>$$\begin{aligned}</span>
<span id="cb5-2583"><a href="#cb5-2583"></a>\mathbb{P}\left(\sup_{0\leq t\leq T}B_{t}\leq a\right) &amp; =\mathbb{P}\left(|B_{T}|\leq a\right)</span>
<span id="cb5-2584"><a href="#cb5-2584"></a>\end{aligned}$$</span>
<span id="cb5-2585"><a href="#cb5-2585"></a></span>
<span id="cb5-2586"><a href="#cb5-2586"></a>In particular, its PDF is:</span>
<span id="cb5-2587"><a href="#cb5-2587"></a></span>
<span id="cb5-2588"><a href="#cb5-2588"></a>$$\begin{aligned}</span>
<span id="cb5-2589"><a href="#cb5-2589"></a>f_{\max}(a) &amp; =\frac{2}{\sqrt{2\pi T}}e^{-\frac{a^{2}}{2T}}</span>
<span id="cb5-2590"><a href="#cb5-2590"></a>\end{aligned}$$</span>
<span id="cb5-2591"><a href="#cb5-2591"></a>:::</span>
<span id="cb5-2592"><a href="#cb5-2592"></a></span>
<span id="cb5-2593"><a href="#cb5-2593"></a>::: rem*</span>
<span id="cb5-2594"><a href="#cb5-2594"></a>We can verify these results empirically. Note that the paths of the</span>
<span id="cb5-2595"><a href="#cb5-2595"></a>random variables $\max_{0\leq s\leq t}B_{s}$ and $|B_{t}|$ are very</span>
<span id="cb5-2596"><a href="#cb5-2596"></a>different as $t$ varies for a given $\omega$. One is increasing and the</span>
<span id="cb5-2597"><a href="#cb5-2597"></a>other is not. The equality holds in distribution for a fixed $t$. As a</span>
<span id="cb5-2598"><a href="#cb5-2598"></a>bonus corollary, we get the distribution of the first passage time at</span>
<span id="cb5-2599"><a href="#cb5-2599"></a>$a$.</span>
<span id="cb5-2600"><a href="#cb5-2600"></a>:::</span>
<span id="cb5-2601"><a href="#cb5-2601"></a></span>
<span id="cb5-2602"><a href="#cb5-2602"></a>::: cor</span>
<span id="cb5-2603"><a href="#cb5-2603"></a>Let $a\geq0$ and $\tau_{a}=\inf<span class="sc">\{</span>t\geq0:B_{t}\geq a<span class="sc">\}</span>$. Then:</span>
<span id="cb5-2604"><a href="#cb5-2604"></a></span>
<span id="cb5-2605"><a href="#cb5-2605"></a>$$\begin{aligned}</span>
<span id="cb5-2606"><a href="#cb5-2606"></a>\mathbb{P}\left(\tau_{a}\leq T\right) &amp; =\mathbb{P}\left(\max_{0\leq t\leq T}B_{t}\geq a\right)=\int_{a}^{\infty}\frac{2}{\sqrt{2\pi T}}e^{-\frac{x^{2}}{2T}}dx</span>
<span id="cb5-2607"><a href="#cb5-2607"></a>\end{aligned}$$</span>
<span id="cb5-2608"><a href="#cb5-2608"></a></span>
<span id="cb5-2609"><a href="#cb5-2609"></a>In particular, the random variable $\tau_{a}$ has the PDF:</span>
<span id="cb5-2610"><a href="#cb5-2610"></a></span>
<span id="cb5-2611"><a href="#cb5-2611"></a>$$\begin{aligned}</span>
<span id="cb5-2612"><a href="#cb5-2612"></a>f_{\tau_{a}}(t) &amp; =\frac{a}{\sqrt{2\pi}}\frac{e^{-\frac{a^{2}}{2t}}}{t^{3/2}},\quad t&gt;0</span>
<span id="cb5-2613"><a href="#cb5-2613"></a>\end{aligned}$$</span>
<span id="cb5-2614"><a href="#cb5-2614"></a></span>
<span id="cb5-2615"><a href="#cb5-2615"></a>This implies that it is heavy-tailed with $\mathbf{E}<span class="co">[</span><span class="ot">\tau_{a}</span><span class="co">]</span>=\infty$.</span>
<span id="cb5-2616"><a href="#cb5-2616"></a>:::</span>
<span id="cb5-2617"><a href="#cb5-2617"></a></span>
<span id="cb5-2618"><a href="#cb5-2618"></a>::: proof</span>
<span id="cb5-2619"><a href="#cb5-2619"></a>*Proof.* The maximum on $<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>$ is larger than or equal to $a$ if and</span>
<span id="cb5-2620"><a href="#cb5-2620"></a>only if $\tau_{a}\leq T$. Therefore, the events</span>
<span id="cb5-2621"><a href="#cb5-2621"></a>$<span class="sc">\{</span>\max_{0\leq t\leq T}B_{t}\geq a<span class="sc">\}</span>$ and $<span class="sc">\{</span>\tau_{a}\leq T<span class="sc">\}</span>$ are the</span>
<span id="cb5-2622"><a href="#cb5-2622"></a>same. So, the CDF $\mathbb{P}(\tau_{a}\leq t)$ of $\tau_{a}$, by</span>
<span id="cb5-2623"><a href="#cb5-2623"></a>proposition</span>
<span id="cb5-2624"><a href="#cb5-2624"></a>(<span class="co">[</span><span class="ot">\[prop:bacheliers-formula\]</span><span class="co">](#prop:bacheliers-formula)</span>{reference-type="ref"</span>
<span id="cb5-2625"><a href="#cb5-2625"></a>reference="prop:bacheliers-formula"})</span>
<span id="cb5-2626"><a href="#cb5-2626"></a>$\int_{a}^{\infty}f_{\max}(x)dx=\int_{a}^{\infty}\frac{2}{\sqrt{2\pi T}}e^{-\frac{x^{2}}{2T}}dx$.</span>
<span id="cb5-2627"><a href="#cb5-2627"></a></span>
<span id="cb5-2628"><a href="#cb5-2628"></a>$$\begin{aligned}</span>
<span id="cb5-2629"><a href="#cb5-2629"></a>f_{\tau_{a}}(t) &amp; =-2\phi(a/\sqrt{t})\cdot a\cdot\left(-\frac{1}{2t^{3/2}}\right)<span class="sc">\\</span></span>
<span id="cb5-2630"><a href="#cb5-2630"></a> &amp; =\frac{a}{t^{3/2}}\phi\left(\frac{a}{\sqrt{t}}\right)<span class="sc">\\</span></span>
<span id="cb5-2631"><a href="#cb5-2631"></a> &amp; =\frac{a}{t^{3/2}}\cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{a^{2}}{2t}}</span>
<span id="cb5-2632"><a href="#cb5-2632"></a>\end{aligned}$$</span>
<span id="cb5-2633"><a href="#cb5-2633"></a></span>
<span id="cb5-2634"><a href="#cb5-2634"></a>To estimate the expectation, it suffices to realize that for $t\geq1$,</span>
<span id="cb5-2635"><a href="#cb5-2635"></a>$e^{-\frac{a^{2}}{2t}}$ is larger than $e^{-\frac{a^{2}}{2}}$.</span>
<span id="cb5-2636"><a href="#cb5-2636"></a>Therefore, we have:</span>
<span id="cb5-2637"><a href="#cb5-2637"></a></span>
<span id="cb5-2638"><a href="#cb5-2638"></a>$$\begin{aligned}</span>
<span id="cb5-2639"><a href="#cb5-2639"></a>\mathbf{E}<span class="co">[</span><span class="ot">\tau_{a}</span><span class="co">]</span> &amp; =\int_{0}^{\infty}t\frac{a}{\sqrt{2\pi}}\frac{e^{-a^{2}/2t}}{t^{3/2}}dt\geq\frac{ae^{-a^{2}/2}}{\sqrt{2\pi}}\int_{1}^{\infty}t^{-1/2}dt</span>
<span id="cb5-2640"><a href="#cb5-2640"></a>\end{aligned}$$</span>
<span id="cb5-2641"><a href="#cb5-2641"></a></span>
<span id="cb5-2642"><a href="#cb5-2642"></a>This is an improper integral and it diverges like $\sqrt{t}$ and is</span>
<span id="cb5-2643"><a href="#cb5-2643"></a>infinite as claimed. ◻</span>
<span id="cb5-2644"><a href="#cb5-2644"></a>:::</span>
<span id="cb5-2645"><a href="#cb5-2645"></a></span>
<span id="cb5-2646"><a href="#cb5-2646"></a>To prove proposition</span>
<span id="cb5-2647"><a href="#cb5-2647"></a>(<span class="co">[</span><span class="ot">\[prop:bacheliers-formula\]</span><span class="co">](#prop:bacheliers-formula)</span>{reference-type="ref"</span>
<span id="cb5-2648"><a href="#cb5-2648"></a>reference="prop:bacheliers-formula"}), we will need an important</span>
<span id="cb5-2649"><a href="#cb5-2649"></a>property of Brownian motion called the *reflection principle*. To</span>
<span id="cb5-2650"><a href="#cb5-2650"></a>motivate it, recall the reflection symmetry of Brownian motion at time</span>
<span id="cb5-2651"><a href="#cb5-2651"></a>$s$ in proposition</span>
<span id="cb5-2652"><a href="#cb5-2652"></a>(<span class="co">[</span><span class="ot">\[prop:brownian-motion-symmetry-of-reflection-at-time-s\]</span><span class="co">](#prop:brownian-motion-symmetry-of-reflection-at-time-s)</span>{reference-type="ref"</span>
<span id="cb5-2653"><a href="#cb5-2653"></a>reference="prop:brownian-motion-symmetry-of-reflection-at-time-s"}). It</span>
<span id="cb5-2654"><a href="#cb5-2654"></a>turns out that this reflection property also holds if $s$ is replaced by</span>
<span id="cb5-2655"><a href="#cb5-2655"></a>a stopping time.</span>
<span id="cb5-2656"><a href="#cb5-2656"></a></span>
<span id="cb5-2657"><a href="#cb5-2657"></a>::: lem</span>
<span id="cb5-2658"><a href="#cb5-2658"></a>[]{#lemma:BM-reflection-principle</span>
<span id="cb5-2659"><a href="#cb5-2659"></a>label="lemma:BM-reflection-principle"}(Reflection principle). Let</span>
<span id="cb5-2660"><a href="#cb5-2660"></a>$(B_{t}:t\geq0)$ be a standard Brownian motion and let $\tau$ be a</span>
<span id="cb5-2661"><a href="#cb5-2661"></a>stopping time for its filtration. Then, the process</span>
<span id="cb5-2662"><a href="#cb5-2662"></a>$(\tilde{B}_{t}:t\geq0)$ defined by the reflection at time $\tau$:</span>
<span id="cb5-2663"><a href="#cb5-2663"></a></span>
<span id="cb5-2664"><a href="#cb5-2664"></a>$$\begin{aligned}</span>
<span id="cb5-2665"><a href="#cb5-2665"></a>\tilde{B}_{t} &amp; =\begin{cases}</span>
<span id="cb5-2666"><a href="#cb5-2666"></a>B_{t} &amp; \text{if \ensuremath{t\leq\tau}}<span class="sc">\\</span></span>
<span id="cb5-2667"><a href="#cb5-2667"></a>B_{\tau}-(B_{t}-B_{\tau}) &amp; \text{if \ensuremath{t&gt;\tau}}</span>
<span id="cb5-2668"><a href="#cb5-2668"></a>\end{cases}</span>
<span id="cb5-2669"><a href="#cb5-2669"></a>\end{aligned}$$</span>
<span id="cb5-2670"><a href="#cb5-2670"></a></span>
<span id="cb5-2671"><a href="#cb5-2671"></a>is also a standard brownian motion.</span>
<span id="cb5-2672"><a href="#cb5-2672"></a>:::</span>
<span id="cb5-2673"><a href="#cb5-2673"></a></span>
<span id="cb5-2674"><a href="#cb5-2674"></a>::: rem*</span>
<span id="cb5-2675"><a href="#cb5-2675"></a>We defer the proof of the reflection property of Brownian motion to a</span>
<span id="cb5-2676"><a href="#cb5-2676"></a>further section. It is intuitive and instructive to quickly picture this</span>
<span id="cb5-2677"><a href="#cb5-2677"></a>in the discrete-time setting. I adopt the approach as in Shreve-I.</span>
<span id="cb5-2678"><a href="#cb5-2678"></a></span>
<span id="cb5-2679"><a href="#cb5-2679"></a>We repeatedly toss a fair coin ($p$, the probability of $H$ on each</span>
<span id="cb5-2680"><a href="#cb5-2680"></a>toss, and $q=1-p$, the probability of $T$ on each toss, are both equal</span>
<span id="cb5-2681"><a href="#cb5-2681"></a>to $\frac{1}{2}$). We denote the successive outcomes of the tosses by</span>
<span id="cb5-2682"><a href="#cb5-2682"></a>$\omega_{1}\omega_{2}\omega_{3}\ldots$. Let</span>
<span id="cb5-2683"><a href="#cb5-2683"></a></span>
<span id="cb5-2684"><a href="#cb5-2684"></a>$$\begin{aligned}</span>
<span id="cb5-2685"><a href="#cb5-2685"></a>X_{j} &amp; =\begin{cases}</span>
<span id="cb5-2686"><a href="#cb5-2686"></a>-1 &amp; \text{if \ensuremath{\omega_{j}=H}}<span class="sc">\\</span></span>
<span id="cb5-2687"><a href="#cb5-2687"></a>+1 &amp; \text{if \ensuremath{\omega_{j}=T}}</span>
<span id="cb5-2688"><a href="#cb5-2688"></a>\end{cases}</span>
<span id="cb5-2689"><a href="#cb5-2689"></a>\end{aligned}$$</span>
<span id="cb5-2690"><a href="#cb5-2690"></a></span>
<span id="cb5-2691"><a href="#cb5-2691"></a>and define $M_{0}=0$, $M_{n}=\sum_{j=1}^{n}X_{n}$. The process</span>
<span id="cb5-2692"><a href="#cb5-2692"></a>$(M_{n}:n\in\mathbf{N})$ is a symmetric random walk.</span>
<span id="cb5-2693"><a href="#cb5-2693"></a></span>
<span id="cb5-2694"><a href="#cb5-2694"></a>Suppose we toss a coin an odd number $(2j-1)$ of times. Some of the</span>
<span id="cb5-2695"><a href="#cb5-2695"></a>paths will reach level $1$ in the first $2j-1$ steps and other will not</span>
<span id="cb5-2696"><a href="#cb5-2696"></a>reach. In the case of $3$ tosses, there are $2^{3}=8$ possible paths and</span>
<span id="cb5-2697"><a href="#cb5-2697"></a>$5$ of these reach level $1$ at some time $\tau_{1}\leq2j-1$. From that</span>
<span id="cb5-2698"><a href="#cb5-2698"></a>moment on, we can create a reflected path, which steps up each time the</span>
<span id="cb5-2699"><a href="#cb5-2699"></a>original path steps down and steps down each time the original path</span>
<span id="cb5-2700"><a href="#cb5-2700"></a>steps up. If the original path ends above $1$ at the final time $2j-1$,</span>
<span id="cb5-2701"><a href="#cb5-2701"></a>the reflected path ends below $1$ and vice versa. If the original path</span>
<span id="cb5-2702"><a href="#cb5-2702"></a>ends at $1$, the reflected path does also. In fact, the reflection at</span>
<span id="cb5-2703"><a href="#cb5-2703"></a>the first hitting time has the same distribution as the original random</span>
<span id="cb5-2704"><a href="#cb5-2704"></a>walk.</span>
<span id="cb5-2705"><a href="#cb5-2705"></a></span>
<span id="cb5-2706"><a href="#cb5-2706"></a>The key here is, out of the $5$ paths that reach level $1$ at some time,</span>
<span id="cb5-2707"><a href="#cb5-2707"></a>there are as many reflected paths that exceed $1$ at time $(2j-1)$ as</span>
<span id="cb5-2708"><a href="#cb5-2708"></a>there are original paths that exceed $1$ at time $(2j-1)$. So, to count</span>
<span id="cb5-2709"><a href="#cb5-2709"></a>the total number of paths that reach level $1$ by time $(2j-1)$, we can</span>
<span id="cb5-2710"><a href="#cb5-2710"></a>count the paths that are at $1$ at time $(2j-1)$ and then add on *twice*</span>
<span id="cb5-2711"><a href="#cb5-2711"></a>the number of paths that exceed $1$ at time $(2j-1)$.</span>
<span id="cb5-2712"><a href="#cb5-2712"></a>:::</span>
<span id="cb5-2713"><a href="#cb5-2713"></a></span>
<span id="cb5-2714"><a href="#cb5-2714"></a>With this new tool, we can now prove proposition</span>
<span id="cb5-2715"><a href="#cb5-2715"></a>(<span class="co">[</span><span class="ot">\[prop:bacheliers-formula\]</span><span class="co">](#prop:bacheliers-formula)</span>{reference-type="ref"</span>
<span id="cb5-2716"><a href="#cb5-2716"></a>reference="prop:bacheliers-formula"}).</span>
<span id="cb5-2717"><a href="#cb5-2717"></a></span>
<span id="cb5-2718"><a href="#cb5-2718"></a>::: proof</span>
<span id="cb5-2719"><a href="#cb5-2719"></a>*Proof.* Consider $\mathbb{P}(\max_{t\leq T}B_{t}\geq a)$. By splitting</span>
<span id="cb5-2720"><a href="#cb5-2720"></a>this probability over the event of the endpoint, we have:</span>
<span id="cb5-2721"><a href="#cb5-2721"></a></span>
<span id="cb5-2722"><a href="#cb5-2722"></a>$$\begin{aligned}</span>
<span id="cb5-2723"><a href="#cb5-2723"></a>\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}&gt;a\right)+\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right)</span>
<span id="cb5-2724"><a href="#cb5-2724"></a>\end{aligned}$$</span>
<span id="cb5-2725"><a href="#cb5-2725"></a></span>
<span id="cb5-2726"><a href="#cb5-2726"></a>Note also, that $\mathbb{P}(B_{T}=a)=0$. Hence, the first probability</span>
<span id="cb5-2727"><a href="#cb5-2727"></a>equals $\mathbb{P}(B_{T}\geq a)$. As for the second, consider the time</span>
<span id="cb5-2728"><a href="#cb5-2728"></a>$\tau_{a}$. On the event considered, we have $\tau_{a}\leq T$ and using</span>
<span id="cb5-2729"><a href="#cb5-2729"></a>lemma</span>
<span id="cb5-2730"><a href="#cb5-2730"></a>(<span class="co">[</span><span class="ot">\[lemma:BM-reflection-principle\]</span><span class="co">](#lemma:BM-reflection-principle)</span>{reference-type="ref"</span>
<span id="cb5-2731"><a href="#cb5-2731"></a>reference="lemma:BM-reflection-principle"}) at that time, we get</span>
<span id="cb5-2732"><a href="#cb5-2732"></a></span>
<span id="cb5-2733"><a href="#cb5-2733"></a>$$\begin{aligned}</span>
<span id="cb5-2734"><a href="#cb5-2734"></a>\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,\tilde{B}_{T}\geq a\right)</span>
<span id="cb5-2735"><a href="#cb5-2735"></a>\end{aligned}$$</span>
<span id="cb5-2736"><a href="#cb5-2736"></a></span>
<span id="cb5-2737"><a href="#cb5-2737"></a>Observe that the event $<span class="sc">\{</span>\max_{t\leq T}B_{t}\geq a<span class="sc">\}</span>$ is the same as</span>
<span id="cb5-2738"><a href="#cb5-2738"></a>$<span class="sc">\{</span>\max_{t\leq T}\tilde{B}_{T}\geq a<span class="sc">\}</span>$. (A rough picture might help</span>
<span id="cb5-2739"><a href="#cb5-2739"></a>here.) Thereforem the above probability is</span>
<span id="cb5-2740"><a href="#cb5-2740"></a></span>
<span id="cb5-2741"><a href="#cb5-2741"></a>$$\begin{aligned}</span>
<span id="cb5-2742"><a href="#cb5-2742"></a>\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\leq a\right) &amp; =\mathbb{P}\left(\max_{t\leq T}\tilde{B}_{t}\geq a,\tilde{B}_{T}\geq a\right)=\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a,B_{T}\geq a\right)</span>
<span id="cb5-2743"><a href="#cb5-2743"></a>\end{aligned}$$</span>
<span id="cb5-2744"><a href="#cb5-2744"></a></span>
<span id="cb5-2745"><a href="#cb5-2745"></a>where the last equality follows from the reflection principle</span>
<span id="cb5-2746"><a href="#cb5-2746"></a>($\tilde{B}_{t}$ is also a standard brownian motion, and $B_{T}$ and</span>
<span id="cb5-2747"><a href="#cb5-2747"></a>$\tilde{B}_{T}$ have the same distribution.) But, as above, the last</span>
<span id="cb5-2748"><a href="#cb5-2748"></a>probability is equal to $\mathbb{P}(B_{T}\geq a)$. We conclude that:</span>
<span id="cb5-2749"><a href="#cb5-2749"></a></span>
<span id="cb5-2750"><a href="#cb5-2750"></a>$$\begin{aligned}</span>
<span id="cb5-2751"><a href="#cb5-2751"></a>\mathbb{P}\left(\max_{t\leq T}B_{t}\geq a\right) &amp; =2\mathbb{P}(B_{T}\geq a)=\frac{2}{\sqrt{2\pi T}}\int_{a}^{\infty}e^{-\frac{x^{2}}{2T}}dx=\mathbb{P}(|B_{T}|\geq a)</span>
<span id="cb5-2752"><a href="#cb5-2752"></a>\end{aligned}$$</span>
<span id="cb5-2753"><a href="#cb5-2753"></a></span>
<span id="cb5-2754"><a href="#cb5-2754"></a>This implies in particular that</span>
<span id="cb5-2755"><a href="#cb5-2755"></a>$\mathbb{P}\left(\max_{t\leq T}B_{t}=a\right)=0$. Thus, we also have</span>
<span id="cb5-2756"><a href="#cb5-2756"></a>$\mathbb{P}(\max_{t\leq T}B_{t}\leq a)=\mathbb{P}(|B_{T}|\leq a)$ as</span>
<span id="cb5-2757"><a href="#cb5-2757"></a>claimed. ◻</span>
<span id="cb5-2758"><a href="#cb5-2758"></a>:::</span>
<span id="cb5-2759"><a href="#cb5-2759"></a></span>
<span id="cb5-2760"><a href="#cb5-2760"></a>::: example</span>
<span id="cb5-2761"><a href="#cb5-2761"></a>(Simulating Martingales) Sample $10$ paths of the following process with</span>
<span id="cb5-2762"><a href="#cb5-2762"></a>a step-size of $0.01$:</span>
<span id="cb5-2763"><a href="#cb5-2763"></a></span>
<span id="cb5-2764"><a href="#cb5-2764"></a><span class="sc">\(</span>a<span class="sc">\)</span> $B_{t}^{2}-t$, $t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$</span>
<span id="cb5-2765"><a href="#cb5-2765"></a></span>
<span id="cb5-2766"><a href="#cb5-2766"></a><span class="sc">\(</span>b<span class="sc">\)</span> Geometric Brownian motion : $S_{t}=\exp(B_{t}-t/2)$, $t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$.</span>
<span id="cb5-2767"><a href="#cb5-2767"></a></span>
<span id="cb5-2768"><a href="#cb5-2768"></a>Let's write a simple $\texttt{BrownianMotion}$ class, that we shall use</span>
<span id="cb5-2769"><a href="#cb5-2769"></a>to generate sample paths.</span>
<span id="cb5-2770"><a href="#cb5-2770"></a>:::</span>
<span id="cb5-2771"><a href="#cb5-2771"></a></span>
<span id="cb5-2772"><a href="#cb5-2772"></a><span class="in">``` {caption="10 paths of $B_t^2 - t$"}</span></span>
<span id="cb5-2773"><a href="#cb5-2773"></a><span class="in">import numpy as np</span></span>
<span id="cb5-2774"><a href="#cb5-2774"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb5-2775"><a href="#cb5-2775"></a></span>
<span id="cb5-2776"><a href="#cb5-2776"></a><span class="in">import attrs</span></span>
<span id="cb5-2777"><a href="#cb5-2777"></a><span class="in">from attrs import define, field</span></span>
<span id="cb5-2778"><a href="#cb5-2778"></a></span>
<span id="cb5-2779"><a href="#cb5-2779"></a><span class="in">@define</span></span>
<span id="cb5-2780"><a href="#cb5-2780"></a><span class="in">class BrownianMotion:</span></span>
<span id="cb5-2781"><a href="#cb5-2781"></a><span class="in">    _step_size = field(validator=attrs.validators.and_(attrs.validators.instance_of(float),</span></span>
<span id="cb5-2782"><a href="#cb5-2782"></a><span class="in">                                                       attrs.validators.ge(0.0)))</span></span>
<span id="cb5-2783"><a href="#cb5-2783"></a><span class="in">    # Time T</span></span>
<span id="cb5-2784"><a href="#cb5-2784"></a><span class="in">    _T = field(validator=attrs.validators.and_(attrs.validators.instance_of(float),</span></span>
<span id="cb5-2785"><a href="#cb5-2785"></a><span class="in">                                               attrs.validators.ge(0.0)))</span></span>
<span id="cb5-2786"><a href="#cb5-2786"></a><span class="in">    # number of paths</span></span>
<span id="cb5-2787"><a href="#cb5-2787"></a><span class="in">    _N = field(validator=attrs.validators.and_(attrs.validators.instance_of(int),</span></span>
<span id="cb5-2788"><a href="#cb5-2788"></a><span class="in">                                               attrs.validators.gt(0)))</span></span>
<span id="cb5-2789"><a href="#cb5-2789"></a></span>
<span id="cb5-2790"><a href="#cb5-2790"></a><span class="in">    _num_steps = field(init=False)</span></span>
<span id="cb5-2791"><a href="#cb5-2791"></a></span>
<span id="cb5-2792"><a href="#cb5-2792"></a><span class="in">    def __attrs_post_init__(self):</span></span>
<span id="cb5-2793"><a href="#cb5-2793"></a><span class="in">        self._num_steps = int(self._T/self._step_size)</span></span>
<span id="cb5-2794"><a href="#cb5-2794"></a></span>
<span id="cb5-2795"><a href="#cb5-2795"></a><span class="in">    def covariance_matrix(self):</span></span>
<span id="cb5-2796"><a href="#cb5-2796"></a><span class="in">        C = np.zeros((self._num_steps,self._num_steps))</span></span>
<span id="cb5-2797"><a href="#cb5-2797"></a></span>
<span id="cb5-2798"><a href="#cb5-2798"></a><span class="in">        for i in range(self._num_steps):</span></span>
<span id="cb5-2799"><a href="#cb5-2799"></a><span class="in">            for j in range(self._num_steps):</span></span>
<span id="cb5-2800"><a href="#cb5-2800"></a><span class="in">                s = (i+1) * self._step_size</span></span>
<span id="cb5-2801"><a href="#cb5-2801"></a><span class="in">                t = (j+1) * self._step_size</span></span>
<span id="cb5-2802"><a href="#cb5-2802"></a><span class="in">                C[i,j] = min(s,t)</span></span>
<span id="cb5-2803"><a href="#cb5-2803"></a><span class="in">        return C</span></span>
<span id="cb5-2804"><a href="#cb5-2804"></a></span>
<span id="cb5-2805"><a href="#cb5-2805"></a><span class="in">    # Each column vector represents a sample path</span></span>
<span id="cb5-2806"><a href="#cb5-2806"></a><span class="in">    def generate_paths(self):</span></span>
<span id="cb5-2807"><a href="#cb5-2807"></a><span class="in">        C = self.covariance_matrix()</span></span>
<span id="cb5-2808"><a href="#cb5-2808"></a><span class="in">        A = np.linalg.cholesky(C)</span></span>
<span id="cb5-2809"><a href="#cb5-2809"></a><span class="in">        Z = np.random.standard_normal((self._num_steps, self._N))</span></span>
<span id="cb5-2810"><a href="#cb5-2810"></a><span class="in">        X = np.matmul(A,Z)</span></span>
<span id="cb5-2811"><a href="#cb5-2811"></a><span class="in">        X = np.concatenate((np.zeros((1,self._N)),X),axis=0)</span></span>
<span id="cb5-2812"><a href="#cb5-2812"></a><span class="in">        return X.transpose()</span></span>
<span id="cb5-2813"><a href="#cb5-2813"></a><span class="in">```</span></span>
<span id="cb5-2814"><a href="#cb5-2814"></a></span>
<span id="cb5-2815"><a href="#cb5-2815"></a>Now, the process $B_{t}^{2}-t$ can be sampled as follows:</span>
<span id="cb5-2816"><a href="#cb5-2816"></a></span>
<span id="cb5-2817"><a href="#cb5-2817"></a><span class="in">``` {caption="10 paths of $B_t^2 - t$"}</span></span>
<span id="cb5-2818"><a href="#cb5-2818"></a></span>
<span id="cb5-2819"><a href="#cb5-2819"></a><span class="in">def generateSquareOfBMCompensated(numOfPaths,stepSize,T):</span></span>
<span id="cb5-2820"><a href="#cb5-2820"></a><span class="in">    N = int(T/stepSize)</span></span>
<span id="cb5-2821"><a href="#cb5-2821"></a></span>
<span id="cb5-2822"><a href="#cb5-2822"></a><span class="in">    X = []</span></span>
<span id="cb5-2823"><a href="#cb5-2823"></a><span class="in">    brownianMotion = BrownianMotion(stepSize,T)</span></span>
<span id="cb5-2824"><a href="#cb5-2824"></a><span class="in">    for n in range(numOfPaths):</span></span>
<span id="cb5-2825"><a href="#cb5-2825"></a></span>
<span id="cb5-2826"><a href="#cb5-2826"></a><span class="in">        B_t = brownianMotion.samplePath()</span></span>
<span id="cb5-2827"><a href="#cb5-2827"></a></span>
<span id="cb5-2828"><a href="#cb5-2828"></a><span class="in">        B_t_sq = np.square(B_t)</span></span>
<span id="cb5-2829"><a href="#cb5-2829"></a></span>
<span id="cb5-2830"><a href="#cb5-2830"></a><span class="in">        t = np.linspace(start=0.0,stop=1.0,num=N+1)</span></span>
<span id="cb5-2831"><a href="#cb5-2831"></a><span class="in">        M_t = np.subtract(B_t_sq,t)</span></span>
<span id="cb5-2832"><a href="#cb5-2832"></a><span class="in">        X.append(M_t)</span></span>
<span id="cb5-2833"><a href="#cb5-2833"></a></span>
<span id="cb5-2834"><a href="#cb5-2834"></a><span class="in">    return X</span></span>
<span id="cb5-2835"><a href="#cb5-2835"></a><span class="in">```</span></span>
<span id="cb5-2836"><a href="#cb5-2836"></a></span>
<span id="cb5-2837"><a href="#cb5-2837"></a>The gBM process can be sampled similarly, with</span>
<span id="cb5-2838"><a href="#cb5-2838"></a>$\texttt{\ensuremath{M_{t}} = np.exp(np.subtract(\ensuremath{B_{t}},t/2))}$.</span>
<span id="cb5-2839"><a href="#cb5-2839"></a></span>
<span id="cb5-2840"><a href="#cb5-2840"></a>::: example</span>
<span id="cb5-2841"><a href="#cb5-2841"></a>**(Maximum of Brownian Motion.)** Consider the maximum of Brownian</span>
<span id="cb5-2842"><a href="#cb5-2842"></a>motion on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$: $\max_{s\leq1}B_{s}$.</span>
<span id="cb5-2843"><a href="#cb5-2843"></a></span>
<span id="cb5-2844"><a href="#cb5-2844"></a><span class="sc">\(</span>a<span class="sc">\)</span> Draw the histogram of the random variable</span>
<span id="cb5-2845"><a href="#cb5-2845"></a>$\max_{s\leq1}B_{s}$using $10,0000$ sampled Brownian paths with a step</span>
<span id="cb5-2846"><a href="#cb5-2846"></a>size of $0.01$.</span>
<span id="cb5-2847"><a href="#cb5-2847"></a></span>
<span id="cb5-2848"><a href="#cb5-2848"></a><span class="sc">\(</span>b<span class="sc">\)</span> Compare this to the PDF of the random variable $|B_{1}|$.</span>
<span id="cb5-2849"><a href="#cb5-2849"></a>:::</span>
<span id="cb5-2850"><a href="#cb5-2850"></a></span>
<span id="cb5-2851"><a href="#cb5-2851"></a>*Solution.*</span>
<span id="cb5-2852"><a href="#cb5-2852"></a></span>
<span id="cb5-2853"><a href="#cb5-2853"></a>I use the $\texttt{itertools}$ python library to compute the running</span>
<span id="cb5-2854"><a href="#cb5-2854"></a>maximum of a brownian motion path.</span>
<span id="cb5-2855"><a href="#cb5-2855"></a></span>
<span id="cb5-2856"><a href="#cb5-2856"></a><span class="in">``` {caption="The process $\\sup_{s\\leq 1}B_s$"}</span></span>
<span id="cb5-2857"><a href="#cb5-2857"></a></span>
<span id="cb5-2858"><a href="#cb5-2858"></a><span class="in">brownianMotion = BrownianMotion(stepSize=0.01,T=1)</span></span>
<span id="cb5-2859"><a href="#cb5-2859"></a><span class="in">data = []</span></span>
<span id="cb5-2860"><a href="#cb5-2860"></a></span>
<span id="cb5-2861"><a href="#cb5-2861"></a><span class="in">for i in range(10000):</span></span>
<span id="cb5-2862"><a href="#cb5-2862"></a><span class="in">    B_t = brownianMotion.samplePath()</span></span>
<span id="cb5-2863"><a href="#cb5-2863"></a><span class="in">    max_B_t = list(itertools.accumulate(B_t,max))</span></span>
<span id="cb5-2864"><a href="#cb5-2864"></a><span class="in">    data.append(max_B_t[100])</span></span>
<span id="cb5-2865"><a href="#cb5-2865"></a><span class="in">```</span></span>
<span id="cb5-2866"><a href="#cb5-2866"></a></span>
<span id="cb5-2867"><a href="#cb5-2867"></a>Analytically, we know that $B_{1}$ is a gaussian random variable with</span>
<span id="cb5-2868"><a href="#cb5-2868"></a>mean $0$ and variance $1$.</span>
<span id="cb5-2869"><a href="#cb5-2869"></a></span>
<span id="cb5-2870"><a href="#cb5-2870"></a>$$\begin{aligned}</span>
<span id="cb5-2871"><a href="#cb5-2871"></a>\mathbb{P}(|B_{1}|\leq z) &amp; =\mathbb{P}(|Z|\leq z)<span class="sc">\\</span></span>
<span id="cb5-2872"><a href="#cb5-2872"></a> &amp; =\mathbb{P}(-z\leq Z\leq z)<span class="sc">\\</span></span>
<span id="cb5-2873"><a href="#cb5-2873"></a> &amp; =\mathbb{P}(Z\leq z)-\mathbb{P}(Z\leq-z)<span class="sc">\\</span></span>
<span id="cb5-2874"><a href="#cb5-2874"></a> &amp; =\mathbb{P}(Z\leq z)-(1-\mathbb{P}(Z\leq z))<span class="sc">\\</span></span>
<span id="cb5-2875"><a href="#cb5-2875"></a>F_{|B_{1}|}(z) &amp; =2\Phi(z)-1</span>
<span id="cb5-2876"><a href="#cb5-2876"></a>\end{aligned}$$</span>
<span id="cb5-2877"><a href="#cb5-2877"></a></span>
<span id="cb5-2878"><a href="#cb5-2878"></a>Differentiating on both sides, we get:</span>
<span id="cb5-2879"><a href="#cb5-2879"></a></span>
<span id="cb5-2880"><a href="#cb5-2880"></a>$$\begin{aligned}</span>
<span id="cb5-2881"><a href="#cb5-2881"></a>f_{|B_{1}|}(z) &amp; =2\phi(z)=\frac{2}{\sqrt{2\pi}}e^{-\frac{z^{2}}{2}},\quad z\in[0,\infty)</span>
<span id="cb5-2882"><a href="#cb5-2882"></a>\end{aligned}$$</span>
<span id="cb5-2883"><a href="#cb5-2883"></a></span>
<span id="cb5-2884"><a href="#cb5-2884"></a>::: example</span>
<span id="cb5-2885"><a href="#cb5-2885"></a>(First passage time.) Let $(B_{t}:t\geq0)$ be a standard brownian</span>
<span id="cb5-2886"><a href="#cb5-2886"></a>motion. Consider the random variable:</span>
<span id="cb5-2887"><a href="#cb5-2887"></a></span>
<span id="cb5-2888"><a href="#cb5-2888"></a>$$\begin{aligned}</span>
<span id="cb5-2889"><a href="#cb5-2889"></a>\tau &amp; =\min<span class="sc">\{</span>t\geq0:B_{t}\geq1<span class="sc">\}</span></span>
<span id="cb5-2890"><a href="#cb5-2890"></a>\end{aligned}$$</span>
<span id="cb5-2891"><a href="#cb5-2891"></a></span>
<span id="cb5-2892"><a href="#cb5-2892"></a>This is the first time that $B_{t}$ reaches $1$.</span>
<span id="cb5-2893"><a href="#cb5-2893"></a></span>
<span id="cb5-2894"><a href="#cb5-2894"></a><span class="sc">\(</span>a<span class="sc">\)</span> Draw a histogram for the distribution of $\tau\land10$ on the</span>
<span id="cb5-2895"><a href="#cb5-2895"></a>time-interval $<span class="co">[</span><span class="ot">0,10</span><span class="co">]</span>$ using $10,000$ brownian motion paths on $<span class="co">[</span><span class="ot">0,10</span><span class="co">]</span>$</span>
<span id="cb5-2896"><a href="#cb5-2896"></a>with discretization $0.01$.</span>
<span id="cb5-2897"><a href="#cb5-2897"></a></span>
<span id="cb5-2898"><a href="#cb5-2898"></a>*The notation $\tau\land10$ means that if the path does not reach $1$ on</span>
<span id="cb5-2899"><a href="#cb5-2899"></a>$<span class="co">[</span><span class="ot">0,10</span><span class="co">]</span>$, then give the value $10$ to the stopping time.*</span>
<span id="cb5-2900"><a href="#cb5-2900"></a></span>
<span id="cb5-2901"><a href="#cb5-2901"></a><span class="sc">\(</span>b<span class="sc">\)</span> Estimate $\mathbf{E}<span class="co">[</span><span class="ot">\tau\land10</span><span class="co">]</span>$.</span>
<span id="cb5-2902"><a href="#cb5-2902"></a></span>
<span id="cb5-2903"><a href="#cb5-2903"></a><span class="sc">\(</span>c<span class="sc">\)</span> What proportion of paths never reach $1$ in the time interval</span>
<span id="cb5-2904"><a href="#cb5-2904"></a>$<span class="co">[</span><span class="ot">0,10</span><span class="co">]</span>$?</span>
<span id="cb5-2905"><a href="#cb5-2905"></a>:::</span>
<span id="cb5-2906"><a href="#cb5-2906"></a></span>
<span id="cb5-2907"><a href="#cb5-2907"></a>*Solution.*</span>
<span id="cb5-2908"><a href="#cb5-2908"></a></span>
<span id="cb5-2909"><a href="#cb5-2909"></a>To compute the expectation, we classify the hitting times of all paths</span>
<span id="cb5-2910"><a href="#cb5-2910"></a>into $50$ bins. I simply did</span>
<span id="cb5-2911"><a href="#cb5-2911"></a></span>
<span id="cb5-2912"><a href="#cb5-2912"></a>$\texttt{frequency, bins = np.histogram(firstPassageTimes,bins=50,range=(0,10))}$</span>
<span id="cb5-2913"><a href="#cb5-2913"></a></span>
<span id="cb5-2914"><a href="#cb5-2914"></a>and then computed</span>
<span id="cb5-2915"><a href="#cb5-2915"></a></span>
<span id="cb5-2916"><a href="#cb5-2916"></a>$\texttt{expectation=np.dot(frequency,bins<span class="co">[</span><span class="ot">1:</span><span class="co">]</span>)/10000}$.</span>
<span id="cb5-2917"><a href="#cb5-2917"></a></span>
<span id="cb5-2918"><a href="#cb5-2918"></a>This expectation estimate on my machine is</span>
<span id="cb5-2919"><a href="#cb5-2919"></a>$\mathbf{E}<span class="co">[</span><span class="ot">\tau\land10</span><span class="co">]</span>=4.34$ secs. There were approximately $2600$</span>
<span id="cb5-2920"><a href="#cb5-2920"></a>paths out of $10,000$ that did not reach $1$.</span>
<span id="cb5-2921"><a href="#cb5-2921"></a></span>
<span id="cb5-2922"><a href="#cb5-2922"></a>::: example</span>
<span id="cb5-2923"><a href="#cb5-2923"></a>**Gambler's ruin at the French Roulette.** Consider the scenario in</span>
<span id="cb5-2924"><a href="#cb5-2924"></a>which you are gambling $\$1$ at the French roulette on the reds: You</span>
<span id="cb5-2925"><a href="#cb5-2925"></a>gain $\$1$ with probability $18/38$ and you lose a dollar with</span>
<span id="cb5-2926"><a href="#cb5-2926"></a>probability $20/38$. We estimate the probability of your fortune</span>
<span id="cb5-2927"><a href="#cb5-2927"></a>reaching $\$200$ before it reaches $0$.</span>
<span id="cb5-2928"><a href="#cb5-2928"></a></span>
<span id="cb5-2929"><a href="#cb5-2929"></a><span class="sc">\(</span>a<span class="sc">\)</span> Write a function that samples the simple random walk path from</span>
<span id="cb5-2930"><a href="#cb5-2930"></a>time $0$ to time $5,000$ with a given starting point.</span>
<span id="cb5-2931"><a href="#cb5-2931"></a></span>
<span id="cb5-2932"><a href="#cb5-2932"></a><span class="sc">\(</span>b<span class="sc">\)</span> Use the above to estimate the probability of reaching $\$200$</span>
<span id="cb5-2933"><a href="#cb5-2933"></a>before $\$0$ on a sample of $100$ paths if you start with $\$100$.</span>
<span id="cb5-2934"><a href="#cb5-2934"></a>:::</span>
<span id="cb5-2935"><a href="#cb5-2935"></a></span>
<span id="cb5-2936"><a href="#cb5-2936"></a>::: example</span>
<span id="cb5-2937"><a href="#cb5-2937"></a>**[]{#ex:doob's-maximal-inequality</span>
<span id="cb5-2938"><a href="#cb5-2938"></a>label="ex:doob's-maximal-inequality"}Doob's maximal inequalities**. We</span>
<span id="cb5-2939"><a href="#cb5-2939"></a>prove the following: Let $(M_{k}:k\geq1)$ be positive submartingale for</span>
<span id="cb5-2940"><a href="#cb5-2940"></a>the filtration $(\mathcal{F}_{k}:k\in\mathbf{N})$. Then, for any</span>
<span id="cb5-2941"><a href="#cb5-2941"></a>$1\leq p&lt;\infty$ and $a&gt;0$</span>
<span id="cb5-2942"><a href="#cb5-2942"></a></span>
<span id="cb5-2943"><a href="#cb5-2943"></a>$$\begin{aligned}</span>
<span id="cb5-2944"><a href="#cb5-2944"></a>\mathbb{P}\left(\max_{k\leq n}M_{k}&gt;a\right) &amp; \leq\frac{1}{a^{p}}\mathbf{E}<span class="co">[</span><span class="ot">M_{n}^{p}</span><span class="co">]</span></span>
<span id="cb5-2945"><a href="#cb5-2945"></a>\end{aligned}$$</span>
<span id="cb5-2946"><a href="#cb5-2946"></a></span>
<span id="cb5-2947"><a href="#cb5-2947"></a><span class="sc">\(</span>a<span class="sc">\)</span> Use Jensen's inequality to show that if $(M_{k}:k\geq1)$ is a</span>
<span id="cb5-2948"><a href="#cb5-2948"></a>positive submartingale, then so is $(M_{k}^{p}:k\geq1)$ for</span>
<span id="cb5-2949"><a href="#cb5-2949"></a>$1\leq p&lt;\infty$. Conclude that it suffices to prove the statement for</span>
<span id="cb5-2950"><a href="#cb5-2950"></a>$p=1$.</span>
<span id="cb5-2951"><a href="#cb5-2951"></a>:::</span>
<span id="cb5-2952"><a href="#cb5-2952"></a></span>
<span id="cb5-2953"><a href="#cb5-2953"></a>*Solution.*</span>
<span id="cb5-2954"><a href="#cb5-2954"></a></span>
<span id="cb5-2955"><a href="#cb5-2955"></a>The function $f(x)=x^{p}$ is convex. By conditional Jensen's inequality,</span>
<span id="cb5-2956"><a href="#cb5-2956"></a></span>
<span id="cb5-2957"><a href="#cb5-2957"></a>$$\begin{aligned}</span>
<span id="cb5-2958"><a href="#cb5-2958"></a>\left(\mathbf{E}<span class="co">[</span><span class="ot">M_{k+1}|\mathcal{F}_{k}</span><span class="co">]</span>\right)^{p} &amp; \leq\mathbf{E}<span class="co">[</span><span class="ot">M_{k}^{p}|\mathcal{F}_{k}</span><span class="co">]</span></span>
<span id="cb5-2959"><a href="#cb5-2959"></a>\end{aligned}$$</span>
<span id="cb5-2960"><a href="#cb5-2960"></a></span>
<span id="cb5-2961"><a href="#cb5-2961"></a>Thus,</span>
<span id="cb5-2962"><a href="#cb5-2962"></a></span>
<span id="cb5-2963"><a href="#cb5-2963"></a>$$\begin{aligned}</span>
<span id="cb5-2964"><a href="#cb5-2964"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{k+1}^{p}|\mathcal{F}_{k}</span><span class="co">]</span> &amp; \geq\left(\mathbf{E}<span class="co">[</span><span class="ot">M_{k+1}|\mathcal{F}_{k}</span><span class="co">]</span>\right)^{p}\geq M_{k}^{p}</span>
<span id="cb5-2965"><a href="#cb5-2965"></a>\end{aligned}$$</span>
<span id="cb5-2966"><a href="#cb5-2966"></a></span>
<span id="cb5-2967"><a href="#cb5-2967"></a>where the last inequality follows from the fact that $(M_{k}:k\geq1)$ is</span>
<span id="cb5-2968"><a href="#cb5-2968"></a>a positive submartingale, so</span>
<span id="cb5-2969"><a href="#cb5-2969"></a>$\mathbf{E}<span class="co">[</span><span class="ot">M_{k+1}|\mathcal{F}_{k}</span><span class="co">]</span>\geq M_{k}$. Consequently,</span>
<span id="cb5-2970"><a href="#cb5-2970"></a>$(M_{k}^{p}:k\geq1)$ is also a positive submartingale.</span>
<span id="cb5-2971"><a href="#cb5-2971"></a></span>
<span id="cb5-2972"><a href="#cb5-2972"></a><span class="sc">\(</span>b<span class="sc">\)</span> Consider the events</span>
<span id="cb5-2973"><a href="#cb5-2973"></a></span>
<span id="cb5-2974"><a href="#cb5-2974"></a>$$\begin{aligned}</span>
<span id="cb5-2975"><a href="#cb5-2975"></a>B_{k} &amp; =\bigcap_{j&lt;k}<span class="sc">\{</span>\omega:M_{j}(\omega)\leq a<span class="sc">\}</span>\cap<span class="sc">\{</span>\omega:M_{k}(\omega)&gt;a<span class="sc">\}</span></span>
<span id="cb5-2976"><a href="#cb5-2976"></a>\end{aligned}$$</span>
<span id="cb5-2977"><a href="#cb5-2977"></a></span>
<span id="cb5-2978"><a href="#cb5-2978"></a>Argue that the $B_{k}$'s are disjoint and that</span>
<span id="cb5-2979"><a href="#cb5-2979"></a>$\bigcup_{k\leq n}B_{k}=<span class="sc">\{</span>\max_{k\leq n}M_{k}&gt;a<span class="sc">\}</span>=B$.</span>
<span id="cb5-2980"><a href="#cb5-2980"></a></span>
<span id="cb5-2981"><a href="#cb5-2981"></a>*Solution.*</span>
<span id="cb5-2982"><a href="#cb5-2982"></a></span>
<span id="cb5-2983"><a href="#cb5-2983"></a>Clearly, $B_{k}$ is the event that the first time to cross $a$ is $k$.</span>
<span id="cb5-2984"><a href="#cb5-2984"></a>If $B_{k}$ occurs, $B_{k+1},B_{k+2},\ldots$ fail to occur. Hence, all</span>
<span id="cb5-2985"><a href="#cb5-2985"></a>$B_{k}'s$ are pairwise disjoint. The event $\bigcup_{k\leq n}B_{k}$ is</span>
<span id="cb5-2986"><a href="#cb5-2986"></a>the event that the random walk crosses $a$ at any time $k\leq n$. Thus,</span>
<span id="cb5-2987"><a href="#cb5-2987"></a>the running maximum of the Brownian motion at time $n$ exceeds $a$.</span>
<span id="cb5-2988"><a href="#cb5-2988"></a></span>
<span id="cb5-2989"><a href="#cb5-2989"></a><span class="sc">\(</span>c<span class="sc">\)</span> Show that</span>
<span id="cb5-2990"><a href="#cb5-2990"></a></span>
<span id="cb5-2991"><a href="#cb5-2991"></a>$$\begin{aligned}</span>
<span id="cb5-2992"><a href="#cb5-2992"></a>\mathbf{E}<span class="co">[</span><span class="ot">M_{n}</span><span class="co">]</span>\geq\mathbf{E}<span class="co">[</span><span class="ot">M_{n}\mathbf{1}_{B}</span><span class="co">]</span> &amp; \geq a\sum_{k\leq n}\mathbb{P}(B_{k})=a\mathbb{P}(B)</span>
<span id="cb5-2993"><a href="#cb5-2993"></a>\end{aligned}$$</span>
<span id="cb5-2994"><a href="#cb5-2994"></a></span>
<span id="cb5-2995"><a href="#cb5-2995"></a>by decomposing $B$ in $B_{k}$'s and by using the properties of</span>
<span id="cb5-2996"><a href="#cb5-2996"></a>expectations, as well as the submartingale property.</span>
<span id="cb5-2997"><a href="#cb5-2997"></a></span>
<span id="cb5-2998"><a href="#cb5-2998"></a>*Solution.*</span>
<span id="cb5-2999"><a href="#cb5-2999"></a></span>
<span id="cb5-3000"><a href="#cb5-3000"></a>Clearly, $M_{n}\geq M_{n}\mathbf{1}_{B}\geq a\mathbf{1}_{B}$. And</span>
<span id="cb5-3001"><a href="#cb5-3001"></a>$M_{n}$ is a positive random variable. By monotonicity of expectations,</span>
<span id="cb5-3002"><a href="#cb5-3002"></a>$\mathbf{E}<span class="co">[</span><span class="ot">M_{n}</span><span class="co">]</span>\geq\mathbf{E}<span class="co">[</span><span class="ot">M_{n}\mathbf{1}_{B}</span><span class="co">]</span>\geq a\mathbf{E}<span class="co">[</span><span class="ot">\mathbf{1}_{B}</span><span class="co">]</span>=a\mathbb{P}(B)=a\sum_{k\leq n}\mathbb{P}(B_{k})$,</span>
<span id="cb5-3003"><a href="#cb5-3003"></a>where the last equality holds because the $B_{k}$'s are disjoint.</span>
<span id="cb5-3004"><a href="#cb5-3004"></a></span>
<span id="cb5-3005"><a href="#cb5-3005"></a><span class="sc">\(</span>d<span class="sc">\)</span> Argue that the inequality holds for continuous paths by</span>
<span id="cb5-3006"><a href="#cb5-3006"></a>discretizing time and using convergence theorems : If $(M_{t}:t\geq0)$</span>
<span id="cb5-3007"><a href="#cb5-3007"></a>is a positive submartingale with continuous paths for the filtration</span>
<span id="cb5-3008"><a href="#cb5-3008"></a>$(\mathcal{F}_{t}:t\geq0)$, then for any $1\leq p&lt;\infty$ and $a&gt;0$:</span>
<span id="cb5-3009"><a href="#cb5-3009"></a></span>
<span id="cb5-3010"><a href="#cb5-3010"></a>$$\begin{aligned}</span>
<span id="cb5-3011"><a href="#cb5-3011"></a>\mathbb{P}\left(\max_{s\leq t}M_{s}&gt;a\right) &amp; \leq\frac{1}{a^{p}}\mathbf{E}<span class="co">[</span><span class="ot">M_{t}^{p}</span><span class="co">]</span></span>
<span id="cb5-3012"><a href="#cb5-3012"></a>\end{aligned}$$</span>
<span id="cb5-3013"><a href="#cb5-3013"></a></span>
<span id="cb5-3014"><a href="#cb5-3014"></a>*Solution.*</span>
<span id="cb5-3015"><a href="#cb5-3015"></a></span>
<span id="cb5-3016"><a href="#cb5-3016"></a>Let $(M_{t}:t\geq0)$ be a positive submartingale with continuous paths</span>
<span id="cb5-3017"><a href="#cb5-3017"></a>for the filtration $(\mathcal{F}_{t}:t\geq0)$. Consider a sequence of</span>
<span id="cb5-3018"><a href="#cb5-3018"></a>partitions of the interval $<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>$ into $2^{r}$ subintervals :</span>
<span id="cb5-3019"><a href="#cb5-3019"></a></span>
<span id="cb5-3020"><a href="#cb5-3020"></a>$$\begin{aligned}</span>
<span id="cb5-3021"><a href="#cb5-3021"></a>D_{r} &amp; =\left<span class="sc">\{</span> \frac{kt}{2^{r}}:k=0,1,2,\ldots,2^{n}\right<span class="sc">\}</span> </span>
<span id="cb5-3022"><a href="#cb5-3022"></a>\end{aligned}$$</span>
<span id="cb5-3023"><a href="#cb5-3023"></a></span>
<span id="cb5-3024"><a href="#cb5-3024"></a>And consider a sequence of discrete positive sub-martingales:</span>
<span id="cb5-3025"><a href="#cb5-3025"></a></span>
<span id="cb5-3026"><a href="#cb5-3026"></a>$$\begin{aligned}</span>
<span id="cb5-3027"><a href="#cb5-3027"></a>M_{kt/2^{r}}^{(r)} &amp; =M_{kt/2^{r}},\quad k\in\mathbf{N},0\leq k\leq2^{r}</span>
<span id="cb5-3028"><a href="#cb5-3028"></a>\end{aligned}$$</span>
<span id="cb5-3029"><a href="#cb5-3029"></a></span>
<span id="cb5-3030"><a href="#cb5-3030"></a>Next, we define for $r=1,2,3,\ldots$</span>
<span id="cb5-3031"><a href="#cb5-3031"></a></span>
<span id="cb5-3032"><a href="#cb5-3032"></a>$$\begin{aligned}</span>
<span id="cb5-3033"><a href="#cb5-3033"></a>A_{r} &amp; =\left<span class="sc">\{</span> \sup_{s\in D_{r}}|M_{s}^{(r)}|&gt;a\right<span class="sc">\}</span> </span>
<span id="cb5-3034"><a href="#cb5-3034"></a>\end{aligned}$$</span>
<span id="cb5-3035"><a href="#cb5-3035"></a></span>
<span id="cb5-3036"><a href="#cb5-3036"></a>By using the maximal inequality in discrete time, gives us:</span>
<span id="cb5-3037"><a href="#cb5-3037"></a></span>
<span id="cb5-3038"><a href="#cb5-3038"></a>$$\begin{aligned}</span>
<span id="cb5-3039"><a href="#cb5-3039"></a>\mathbb{P}(A_{r})=\mathbb{P}\left<span class="sc">\{</span> \sup_{s\in D_{r}}|M_{s}^{(r)}|&gt;a\right<span class="sc">\}</span>  &amp; \leq\frac{1}{a^{p}}\mathbf{E}\left<span class="co">[</span><span class="ot">\left(M_{s}^{(r)}\right)^{p}\right</span><span class="co">]</span>=\frac{1}{a^{p}}\mathbf{E}\left<span class="co">[</span><span class="ot">M_{t}^{p}\right</span><span class="co">]</span></span>
<span id="cb5-3040"><a href="#cb5-3040"></a>\end{aligned}$$</span>
<span id="cb5-3041"><a href="#cb5-3041"></a></span>
<span id="cb5-3042"><a href="#cb5-3042"></a>$$\begin{aligned}</span>
<span id="cb5-3043"><a href="#cb5-3043"></a>\mathbb{P}\left(\max_{s\leq t}M_{s}&gt;a\right) &amp; =\mathbb{P}\left(\bigcup_{r=1}^{\infty}A_{r}\right)<span class="sc">\\</span></span>
<span id="cb5-3044"><a href="#cb5-3044"></a> &amp; =\lim_{r\to\infty}\mathbb{P}\left(A_{r}\right)<span class="sc">\\</span></span>
<span id="cb5-3045"><a href="#cb5-3045"></a> &amp; \left<span class="sc">\{</span> \text{Continuity of probability measure}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb5-3046"><a href="#cb5-3046"></a> &amp; \leq\lim_{r\to\infty}\frac{1}{a^{p}}\mathbf{E}\left<span class="co">[</span><span class="ot">M_{t}^{p}\right</span><span class="co">]</span></span>
<span id="cb5-3047"><a href="#cb5-3047"></a>\end{aligned}$$</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>
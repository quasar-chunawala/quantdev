<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-12-01">

<title>A gentle introduction to the Girsanov Theorem - Back to the basics â€“ quantdev.blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-f2a1071e85750ec973bbb8a8f120da0f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-e2cfcb35d2364ed86e6f554fe2526fa3.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">A gentle introduction to the Girsanov Theorem - Back to the basics</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Stochastic Calculus</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 1, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#change-of-probability." id="toc-change-of-probability." class="nav-link" data-scroll-target="#change-of-probability.">Change of Probability.</a>
  <ul class="collapse">
  <li><a href="#change-of-probability-for-a-random-variable." id="toc-change-of-probability-for-a-random-variable." class="nav-link" data-scroll-target="#change-of-probability-for-a-random-variable.">Change of Probability for a Random Variable.</a></li>
  <li><a href="#the-cameron-martin-theorem." id="toc-the-cameron-martin-theorem." class="nav-link" data-scroll-target="#the-cameron-martin-theorem.">The Cameron-Martin Theorem.</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>One of the most popular technical tools in financial engineering is the Girsanov theorem. In this blog-post, I intend to provide the dear reader a beginner-friendly introduction and an intuitive gut feel for these tools.</p>
<p>The change of measure technique was used by <a href="https://en.wikipedia.org/wiki/H%C3%A9lyette_Geman">Heylette Geman</a>, <a href="https://en.wikipedia.org/wiki/Nicole_El_Karoui">Nicole El Karoui</a> and <a href="">Jean-Charles Rochet</a> in their seminal note <a href="https://www.cambridge.org/core/journals/journal-of-applied-probability/article/abs/changes-of-numeraire-changes-of-probability-measure-and-option-pricing/EA730D6C18D56426D491B6A25563C0B3">Changes of Numeraire, Changes of Probability Measure and Option Pricing</a>.</p>
<div id="97850431" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext itikz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="change-of-probability." class="level1">
<h1>Change of Probability.</h1>
<section id="change-of-probability-for-a-random-variable." class="level2">
<h2 class="anchored" data-anchor-id="change-of-probability-for-a-random-variable.">Change of Probability for a Random Variable.</h2>
<p>Consider a random variable <span class="math inline">\(X\)</span> defined on a sample space <span class="math inline">\(\Omega\)</span> having zero mean. We want to change the mean of <span class="math inline">\(X\)</span> so that <span class="math inline">\(\mu\neq 0\)</span>. Of course, it is easy to change the mean of a random variable: If <span class="math inline">\(X\)</span> has mean <span class="math inline">\(0\)</span>, then the random variable <span class="math inline">\(X+\mu\)</span> has mean <span class="math inline">\(\mu\)</span>. However, it might be that the variable <span class="math inline">\(X+\mu\)</span> does not share the same possible values as <span class="math inline">\(X\)</span>. For example, take <span class="math inline">\(X\)</span> to be a uniform random variable on <span class="math inline">\([-1,1]\)</span>. While <span class="math inline">\(X+1\)</span> has mean <span class="math inline">\(1\)</span>, the density of <span class="math inline">\(X+1\)</span> would be non-zero on <span class="math inline">\([0,2]\)</span> instead of <span class="math inline">\([-1,1]\)</span>.</p>
<p>Our goal is to find a good way to change the underlying probability <span class="math inline">\(\mathbb{P}\)</span>, and thus the distribution of <span class="math inline">\(X\)</span>, so that the set of outcomes is unchanged. If <span class="math inline">\(X\)</span> is a discrete random variable, say with <span class="math inline">\(\mathbb{P}(X=-1)=\mathbb{P}(X=1)=1/2\)</span>, we can change the probability in order to change the mean easily. It suffices to take <span class="math inline">\(\tilde{\mathbb{P}}\)</span> so that <span class="math inline">\(\tilde{\mathbb{P}}(X=1)=p\)</span> and <span class="math inline">\(\mathbb{P}(X=-1)=1-p\)</span> for some appropriate <span class="math inline">\(0\leq p\leq1\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is a continuous random variable, with a PDF <span class="math inline">\(f_{X}\)</span>, the probabilities can be changed by modifying the PDF. Consider the a new PDF:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{f}_{X}(x) &amp; =f_{X}(x)g(x)
\end{aligned}\]</span></p>
<p>for some function <span class="math inline">\(g(x)&gt;0\)</span> such that <span class="math inline">\(\int f(x)g(x)dx=1\)</span>. Clearly, <span class="math inline">\(f_{X}(x)g(x)\)</span> is also a PDF and <span class="math inline">\(f_{X}(x)&gt;0\)</span> if and only if <span class="math inline">\(f_{X}(x)g(x)&gt;0\)</span>, so that the possible values of <span class="math inline">\(X\)</span> are unchanged. A convenient (and important!) choice of function <span class="math inline">\(g\)</span> is:</p>
<p><span id="eq-change-of-probability-of-an-rv"><span class="math display">\[\begin{aligned}
g(x) &amp; =\frac{e^{ax}}{\int_{\mathbf{R}}e^{ax}f_{X}(x)dx}=\frac{e^{ax}}{\mathbb{E}[e^{aX}]},\quad a\in\mathbf{R}
\end{aligned} \tag{1}\]</span></span></p>
<p>assuming <span class="math inline">\(X\)</span> has a well-defined MGF. Here <span class="math inline">\(a\)</span> is a parameter that can be tuned to fit to a specific mean. The normalization factor in the denominator is the MGF of <span class="math inline">\(X\)</span>. It ensures that <span class="math inline">\(f_{X}(x)g(x)\)</span> is a PDF. Note that if <span class="math inline">\(a&gt;0\)</span>, the function <span class="math inline">\(g\)</span> gives a bigger weight to large values of <span class="math inline">\(X\)</span>. We say that <span class="math inline">\(g\)</span> is biased towards the large values.</p>
<div id="exm-biasing-a-uniform-random-variable" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Biasing a uniform random variable)</strong></span> Let <span class="math inline">\(X\)</span> be a uniform random variable on <span class="math inline">\([0,1]\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Clearly, <span class="math inline">\(\mathbb{E}[X]=1/2\)</span>. How can we change the PDF of <span class="math inline">\(X\)</span> so that the possible values are still <span class="math inline">\([0,1]\)</span>, but the mean is <span class="math inline">\(1/4\)</span>. We have that the PDF is <span class="math inline">\(f_{X}(x)=1\)</span> if <span class="math inline">\(x\in[0,1]\)</span> and <span class="math inline">\(0\)</span> elsewhere. Therefore, the mean with the new PDF with parameter <span class="math inline">\(a\)</span> as in the <a href="#eq-change-of-probability-of-an-rv" class="quarto-xref">Equation&nbsp;1</a> is:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[X] &amp; =\int_{0}^{1}x\tilde{f}(x)dx\\
&amp; =\int_{0}^{1}\frac{xe^{ax}}{\mathbb{E}[e^{aX}]}dx\\
&amp; =\frac{a}{e^{a}-1}\int_{0}^{1}xe^{ax}dx\\
&amp; =\frac{a}{e^{a}-1}\left(\left[x\frac{e^{ax}}{a}\right]_{0}^{1}-\frac{1}{a}\int_{0}^{1}e^{ax}dx\right)\\
&amp; =\frac{a}{e^{a}-1}\left(\frac{e^{a}}{a}-\frac{1}{a}\frac{e^{a}-1}{a}\right)\\
&amp; =\frac{e^{a}}{e^{a}-1}-\frac{1}{a}
\end{aligned}\]</span></p>
<p>For <span class="math inline">\(\tilde{\mathbb{E}[X]}\)</span>to be equal to <span class="math inline">\(1/4\)</span>, we get numerically <span class="math inline">\(a\approx-3.6\)</span>. Note that the possible values of <span class="math inline">\(X\)</span> remain the same under the new probability. However, the new distribution is no longer uniform! It has bias towards values closer to zero, as it should.</p>
</div>
<div id="exm-biasing-a-gaussian-random-variable" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Biasing a Gaussian random variable.)</strong></span> Let <span class="math inline">\(X\)</span> be a Gaussian random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>. How can we change the PDF of <span class="math inline">\(X\)</span> to have mean <span class="math inline">\(0\)</span>? Going back to <a href="#eq-change-of-probability-of-an-rv" class="quarto-xref">Equation&nbsp;1</a>, the mean <span class="math inline">\(\mu\)</span> under the new PDF with parameter <span class="math inline">\(a\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[X] &amp; =\int_{-\infty}^{\infty}x\tilde{f}(x)dx\\
&amp; =\int_{-\infty}^{\infty}xg(x)f(x)dx\\
&amp; =\int_{-\infty}^{\infty}x\cdot\frac{e^{ax}}{\mathbb{E}[e^{aX}]}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}dx\\
&amp; =\frac{1}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\cdot\exp\left[-\frac{1}{2}\left(\frac{x^{2}-2\mu x+\mu^{2}-2a\sigma^{2}x}{\sigma^{2}}\right)\right]dx\\
&amp; =\frac{1}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\cdot\exp\left[-\frac{1}{2}\left(\frac{x^{2}-2(\mu+a\sigma^{2})x+(\mu+a\sigma^{2})^{2}-2\mu a\sigma^{2}-a^{2}\sigma^{4}}{\sigma^{2}}\right)\right]dx\\
&amp; =\frac{e^{\mu a+a^{2}\sigma^{2}/2}}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left[-\frac{1}{2}\left(\frac{x-(\mu+a\sigma^{2})}{\sigma}\right)^{2}\right]dx\\
&amp; =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left[-\frac{1}{2}\left(\frac{x-(\mu+a\sigma^{2})}{\sigma}\right)^{2}\right]dx
\end{aligned}\]</span></p>
<p>For the specific choice of the parameter <span class="math inline">\(a=\mu/\sigma^{2}\)</span>, we recover the PDF of a Gaussian random variable with mean <span class="math inline">\(0\)</span>. But, we can deduce more. The new PDF is also Gaussian. This was not the case for uniform random variables. In fact, the new PDF is exactly the same as the one of <span class="math inline">\(X-\mu\)</span>. For if, <span class="math inline">\(a=\mu/\sigma^{2}\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[X] &amp; =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left[-\frac{x^{2}}{2\sigma^{2}}\right]dx
\end{aligned}\]</span></p>
<p>and observe that if <span class="math inline">\(Y=X-\mu\)</span>, then:</p>
<p><span class="math display">\[\begin{aligned}
F_{Y}(x) &amp; =\mathbb{P}(X-\mu&lt;x)\\
&amp; =\mathbb{P}(X\leq x+\mu)\\
&amp; =F_{X}(x+\mu)\\
\frac{d}{dx}(F_{Y}(x)) &amp; =\frac{d}{dx}(F_{X}(x+\mu))\\
f_{Y}(x) &amp; =f_{X}(x+\mu)\cdot\frac{d}{dx}(x+\mu)\\
f_{Y}(x) &amp; =f_{X}(x+\mu)\\
&amp; =\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{1}{2}\left(\frac{x+\mu-\mu}{\sigma}\right)^{2}\right]\\
&amp; =\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{x^{2}}{2\sigma^{2}}\right]
\end{aligned}\]</span></p>
<p>In other words:</p>
<p><em>For Gaussians, changing the mean by recentering is equivalent to changing the probability as in</em> <a href="#eq-change-of-probability-of-an-rv" class="quarto-xref">Equation&nbsp;1</a>.</p>
<section id="visualization" class="level3">
<h3 class="anchored" data-anchor-id="visualization">Visualization</h3>
<p>Let <span class="math inline">\(X\)</span> be gaussian with mean <span class="math inline">\(\mu=1\)</span> and variance <span class="math inline">\(\sigma^2 = 1\)</span>. The PDF of <span class="math inline">\(X\)</span> is:</p>
<div id="b2300e17" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>\begin{axis}[xlabel <span class="op">=</span> $x$, ylabel<span class="op">=</span>{$f_X(x)$}, title<span class="op">=</span>{The PDF of $X \sim \mathcal{N}<span class="op">^</span>{P}(\mu<span class="op">=</span><span class="dv">1</span>,\sigma<span class="op">^</span><span class="dv">2</span><span class="op">=</span><span class="dv">1</span>)$},domain<span class="op">=-</span><span class="dv">3</span>:<span class="dv">3</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>\addplot[color<span class="op">=</span>black,samples<span class="op">=</span><span class="dv">100</span>]{<span class="dv">1</span><span class="op">/</span>(sqrt(<span class="dv">2</span><span class="op">*</span><span class="fl">3.14</span>))<span class="op">*</span>exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>((x<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(x<span class="op">-</span><span class="dv">1</span>))}<span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>\end{axis}</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-3-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I choose <span class="math inline">\(g(x) = \frac{e^{ax}}{\mathbb{E}[e^{aX}]} = \frac{e^{ax}}{e^{\mu a + \frac{1}{2}a^2 \sigma^2}}\)</span>, <span class="math inline">\(\mu=1\)</span>, <span class="math inline">\(\sigma^2 = 1\)</span> and set the value of the parameter <span class="math inline">\(a = \frac{\mu}{\sigma^2} = -1\)</span>.</p>
<div id="aea671b0" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>\begin{axis}[xlabel <span class="op">=</span> $x$, ylabel<span class="op">=</span>{$g(x)$}, title<span class="op">=</span>{The density scaling $g(x)<span class="op">=</span>\frac{e<span class="op">^</span>{ax}}{E[e<span class="op">^</span>{ax}]}$, <span class="cf">with</span> parameter value $a<span class="op">=-</span>\frac{\mu}{\sigma<span class="op">^</span><span class="dv">2</span>}$},domain<span class="op">=-</span><span class="dv">3</span>:<span class="dv">3</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>\addplot[color<span class="op">=</span>black,samples<span class="op">=</span><span class="dv">100</span>]{exp(<span class="op">-</span>x)<span class="op">/</span>exp(<span class="op">-</span><span class="fl">0.5</span>)}<span class="op">;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>\end{axis}</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The new density <span class="math inline">\(\tilde{f}_X(x)\)</span> obtained multiplying <span class="math inline">\(f_X(x)\)</span> by the weights <span class="math inline">\(g(x)\)</span> is the same as a gaussian centered at <span class="math inline">\(0\)</span> with variance <span class="math inline">\(1\)</span> :</p>
<div id="de09c454" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>\begin{axis}[xlabel <span class="op">=</span> $x$, ylabel<span class="op">=</span>{$\tilde{f}_X(x)$}, title<span class="op">=</span>{The new PDF of $X$, after multiplying the density ${f}_X(x)$ by weights $g(x)$.},domain<span class="op">=-</span><span class="dv">3</span>:<span class="dv">3</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>\addplot[color<span class="op">=</span>black,samples<span class="op">=</span><span class="dv">100</span>]{<span class="dv">1</span><span class="op">/</span>(sqrt(<span class="dv">2</span><span class="op">*</span><span class="fl">3.14</span>))<span class="op">*</span>exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>(x<span class="op">*</span>x))}<span class="op">;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>\end{axis}</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-5-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This is a very special property of the Gaussian distribution. The exponential and Poisson distributions have a similar property.</p>
<p>Intuitively, if we apply this idea to each time-slice <span class="math inline">\((B_t - B_s)\)</span> of the Brownian motion which has a drift, we can recenter the gaussians to have mean <span class="math inline">\(0\)</span> (driftless).</p>
<p><a href="#exm-biasing-a-gaussian-random-variable" class="quarto-xref">Example&nbsp;2</a> is very important and we will state it as a theorem. Before doing so, we notice that the change of PDF (<a href="#eq-change-of-probability-of-an-rv" class="quarto-xref">Equation&nbsp;1</a>) can be expressed more generally by changing the underlying probability measure(length, area, weights) <span class="math inline">\(\mathbb{P}\)</span> on the sample space <span class="math inline">\(\Omega\)</span> on which the random variables are defined. More precisely, let <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> be a probability space, and let <span class="math inline">\(X\)</span> be a random variable defined on <span class="math inline">\(\Omega\)</span>. We define a new probability <span class="math inline">\(\tilde{\mathbb{P}}\)</span> on <span class="math inline">\(\Omega\)</span> as follows:</p>
<p>If <span class="math inline">\(\mathcal{E}\)</span> is an event in <span class="math inline">\(\mathcal{F}\)</span>, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{\tilde{E}}[1_{\mathcal{E}}]=\int_{\mathbf{R}}1_{\mathcal{E}}\cdot\tilde{f}(x)dx\nonumber \\
&amp; =\int_{\mathbf{R}}1_{\mathcal{E}}\cdot g(x)f_{X}(x)dx\nonumber \\
&amp; =\int_{\mathbf{R}}1_{\mathcal{E}}\cdot\frac{e^{ax}}{\mathbb{E}[e^{aX}]}f_{X}(x)dx\nonumber \\
&amp; =\mathbb{E}\left[1_{\mathcal{E}}\frac{e^{aX}}{\mathbb{E}[e^{aX}]}\right]
\end{aligned}\]</span></p>
<p>Intuitively, we are changing the probability of each outcome <span class="math inline">\(\omega\in\mathcal{E}\)</span>, by the factor</p>
<p><span class="math display">\[\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\label{eq:girsanov-probability-scaling}\]</span></p>
<p>In other words, if <span class="math inline">\(a&gt;0\)</span>, the outcomes <span class="math inline">\(\omega\)</span> for which <span class="math inline">\(X\)</span> has large values are favored. Note that equation (<a href="#eq:change-of-probability-of-an-rv" data-reference-type="ref" data-reference="eq:change-of-probability-of-an-rv">[eq:change-of-probability-of-an-rv]</a>) for the PDF is recovered, since for any function <span class="math inline">\(h\)</span> of <span class="math inline">\(X\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[h(X)] &amp; =\mathbb{E}\left[\frac{e^{aX}}{\mathbb{E}[e^{aX}]}h(X)\right]\\
&amp; =\int_{\mathbf{R}}h(x)\frac{e^{ax}}{\mathbb{E}[e^{aX}]}f_{X}(x)dx
\end{aligned}\]</span></p>
<p>In this setting, the above example becomes the preliminary version of the Cameron-Martin-Girsanov theorem:</p>
</section>
</div>
<div class="thm">
<p><span id="th:change-of-probability-for-a-random-variable" data-label="th:change-of-probability-for-a-random-variable"></span>Let <span class="math inline">\(X\)</span> be a Gaussian random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Then, under the probability <span class="math inline">\(\mathbb{\tilde{P}}\)</span> given by:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{E}\left[1_{\mathcal{E}}e^{-\frac{\mu}{\sigma^{2}}X+\frac{1}{2}\frac{\mu^{2}}{\sigma^{2}}}\right],\quad\mathcal{E}\in\mathcal{F}\label{eq:preliminary-version-girsanov-theorem}
\end{aligned}\]</span></p>
<p>the random variable <span class="math inline">\(X\)</span> is Gaussian with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>.</p>
<p>Moreover, since <span class="math inline">\(X\)</span> can be written as <span class="math inline">\(X=Y+\mu\)</span> where <span class="math inline">\(Y\)</span> is Gaussian with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> under <span class="math inline">\(\mathbb{P}\)</span>, we have that <span class="math inline">\(\mathbb{\tilde{P}}\)</span> can be written as:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{E}\left[1_{\mathcal{E}}e^{-\frac{\mu}{\sigma^{2}}Y-\frac{1}{2}\frac{\mu^{2}}{\sigma^{2}}}\right],\quad\mathcal{E}\in\mathcal{F}\label{eq:preliminary-version-girsanov-theorem-II}
\end{aligned}\]</span></p>
</div>
<p>It is good to pause for a second and look at the signs in the exponential of equations (<a href="#eq:preliminary-version-girsanov-theorem" data-reference-type="ref" data-reference="eq:preliminary-version-girsanov-theorem">[eq:preliminary-version-girsanov-theorem]</a>) and (<a href="#eq:preliminary-version-girsanov-theorem-II" data-reference-type="ref" data-reference="eq:preliminary-version-girsanov-theorem-II">[eq:preliminary-version-girsanov-theorem-II]</a>). The signs in the exponential might be very confusing and is the source of many mistakes in the Cameron-Martin-Girsanov theorem. A good trick is to say that, if we want to remove <span class="math inline">\(\mu\)</span>, then the sign in front of <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> must be negative. Then, we add the exponential factor needed for <span class="math inline">\(\tilde{\mathbb{P}}\)</span> to be a probability. This is given by the MGF of <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> depending on how we want to express it.</p>
<p>The probabilities <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span>, as defined in the equation (<a href="#eq:preliminary-version-girsanov-theorem" data-reference-type="ref" data-reference="eq:preliminary-version-girsanov-theorem">[eq:preliminary-version-girsanov-theorem]</a>) are obviously not equal since they differ by a factor in (<a href="#eq:girsanov-probability-scaling" data-reference-type="ref" data-reference="eq:girsanov-probability-scaling">[eq:girsanov-probability-scaling]</a>). However, they share some similarities. Most notably, if <span class="math inline">\(\mathcal{E}\)</span> is an event of positive <span class="math inline">\(\mathbb{P}\)</span>-probability, <span class="math inline">\(\mathbb{P}(\mathcal{E})&gt;0\)</span>, then we must have <span class="math inline">\(\tilde{\mathbb{P}}(\mathcal{E})&gt;0\)</span>, since the factor in (<a href="#eq:girsanov-probability-scaling" data-reference-type="ref" data-reference="eq:girsanov-probability-scaling">[eq:girsanov-probability-scaling]</a>) is always strictly positive. The converse is also true: if <span class="math inline">\(\mathcal{E}\)</span> is an event of positive <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-probability, <span class="math inline">\(\tilde{\mathbb{P}}(\mathcal{E})&gt;0\)</span>, then we must have that <span class="math inline">\(\mathbb{P}(\mathcal{E})&gt;0\)</span>. This is because the factor in (<a href="#eq:girsanov-probability-scaling" data-reference-type="ref" data-reference="eq:girsanov-probability-scaling">[eq:girsanov-probability-scaling]</a>) can be inverted, being strictly positive. More precisely, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(\mathcal{E}) &amp; =\mathbb{E}[1_{\mathcal{E}}]\\
&amp; =\mathbb{E}\left[1_{\mathcal{E}}\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\left(\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\right)^{-1}\right]\\
&amp; =\tilde{\mathbb{E}}\left[1_{\mathcal{E}}\left(\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\right)^{-1}\right]
\end{aligned}\]</span></p>
<p>The factor <span class="math inline">\(\left(\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\right)^{-1}\)</span> is also strictly positive, proving the claim. To sum it all up, the probabilities <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span> essentially share the same possible outcomes. Such probability measures are said to be equivalent measures.</p>
<div class="defn">
<p>Consider the two probabilities <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span> on <span class="math inline">\((\Omega,\mathcal{F})\)</span>. They are said to be equivalent, if for any event <span class="math inline">\(\mathcal{E}\in\mathcal{F}\)</span>, we have <span class="math inline">\(\mathbb{P}(\mathcal{E})&gt;0\)</span> if and only if <span class="math inline">\(\mathbb{P}(\mathcal{E})&gt;0\)</span>. Thus, <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span> agree on the null sets. If <span class="math inline">\(A\in\mathcal{F}\)</span> is such that <span class="math inline">\(\mathbb{P}(A)=0\)</span>, then <span class="math inline">\(\mathbb{\tilde{P}}(A)=0\)</span> and vice-versa.</p>
</div>
<p>Keep in mind that two probabilities that are equivalent might still be very far from being equal!</p>
</section>
<section id="the-cameron-martin-theorem." class="level2">
<h2 class="anchored" data-anchor-id="the-cameron-martin-theorem.">The Cameron-Martin Theorem.</h2>
<div class="thm">
<p><span id="th:girsanov-theorem-for-constant-drift-case" data-label="th:girsanov-theorem-for-constant-drift-case"></span>(Cameron-Martin Theorem for constant drift). Let <span class="math inline">\((\tilde{B(t)},t\in[0,T])\)</span> be a <span class="math inline">\(\mathbb{P}-\)</span>Brownian motion with constant drift <span class="math inline">\(\theta\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Consider the probability <span class="math inline">\(\tilde{\mathbb{P}}\)</span>on <span class="math inline">\(\Omega\)</span> given by:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{P}}(\mathcal{E}) &amp; =\mathbb{E}\left[e^{-\theta\tilde{B}(T)+\frac{\theta^{2}}{2}T}1_{\mathcal{E}}\right],\quad\mathcal{E}\in\mathcal{F}\label{eq:girsanov-theorem-constant-drift-case}
\end{aligned}\]</span></p>
<p>Then, the process <span class="math inline">\((\tilde{B}(t),t\in[0,T])\)</span> under <span class="math inline">\(\mathbb{\tilde{P}}\)</span>is distributed like a standard brownian motion. Moreover, since we can write <span class="math inline">\(\tilde{B_{t}}=\theta t+B_{t}\)</span> for some standard brownian motion <span class="math inline">\((B_{t},t\in[0,T])\)</span> on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>, the probability <span class="math inline">\(\tilde{\mathbb{P}}\)</span> can also be written as:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{P}}(\mathcal{E}) &amp; =\mathbb{E}\left[e^{-\theta B(T)-\frac{\theta^{2}}{2}T}1_{\mathcal{E}}\right]\label{eq:girsanov-theorem-constant-drift-case-II}
\end{aligned}\]</span></p>
</div>
<p>It is a good idea to pause again and look at the signs in the exponential in equations (<a href="#eq:girsanov-theorem-constant-drift-case" data-reference-type="ref" data-reference="eq:girsanov-theorem-constant-drift-case">[eq:girsanov-theorem-constant-drift-case]</a>) and (<a href="#eq:girsanov-theorem-constant-drift-case-II" data-reference-type="ref" data-reference="eq:girsanov-theorem-constant-drift-case-II">[eq:girsanov-theorem-constant-drift-case-II]</a>). They behave the same way as in theorem (<a href="#th:change-of-probability-for-a-random-variable" data-reference-type="ref" data-reference="th:change-of-probability-for-a-random-variable">[th:change-of-probability-for-a-random-variable]</a>). There is a minus sign in front of <span class="math inline">\(B_{T}\)</span> to remove the drift. Before proving the theorem, we make some important remarks.</p>
<p>(1) <strong>The end-point</strong>. Note that only the endpoint <span class="math inline">\(\tilde{B}(T)\)</span> of the Brownian motion is involved in the change of probability. In particular, <span class="math inline">\(T\)</span> cannot be <span class="math inline">\(+\infty\)</span>. The Cameron-Martin theorem can only be applied on a finite interval.</p>
<p>(2) <strong>A martingale.</strong> The factor <span class="math inline">\(M_{T}=e^{-\theta B(T)-\frac{\theta^{2}}{2}T}=e^{-\theta\tilde{B}(T)+\frac{1}{2}\theta^{2}T}\)</span> involved in the change of probability is the end-point of a <span class="math inline">\(\mathbb{P}-\)</span>martingale, that is, it is a martingale under the original probability <span class="math inline">\(\mathbb{P}\)</span>. To see this:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[M_{T}|\mathcal{F}_{t}] &amp; =\mathbb{E}\left[e^{-\theta B(T)-\frac{1}{2}\theta^{2}T}|\mathcal{F}_{t}\right]\\
&amp; =e^{-\theta B(t)}\mathbb{E}\left[e^{-\theta(B(T)-B(t))}|\mathcal{F}_{t}\right]e^{-\frac{\theta^{2}}{2}T}\\
&amp; \{\text{Using }B(T)-B(t)\perp\mathcal{F}_{t}\}\\
&amp; =e^{-\theta B(t)}\mathbb{E}\left[e^{-\theta(B(T)-B(t))}\right]e^{-\frac{\theta^{2}}{2}T}\\
&amp; =e^{-\theta B(t)}e^{\frac{\theta^{2}}{2}(T-t)}e^{-\frac{\theta^{2}}{2}T}\\
&amp; =e^{-\theta B(t)-\frac{\theta^{2}}{2}t}
\end{aligned}\]</span></p>
<p>In fact, since <span class="math inline">\(B(t)\)</span> is a <span class="math inline">\(\mathbb{P}\)</span>-standard Brownian motion, <span class="math inline">\(M(t)=e^{-\theta B(t)-\frac{\theta^{2}}{2}t}\)</span> is a geometric brownian motion.</p>
<p>Interestingly, the drift of <span class="math inline">\(\tilde{B}(t)\)</span> becomes the volatility factor in <span class="math inline">\(M_{T}\)</span>! <span class="math inline">\(\mathbb{E}[M_{T}^{2}]=\mathbb{E}[e^{-2\theta B(T)-\theta^{2}T}]=e^{-\theta^{2}T}\cdot\mathbb{E}[e^{-2\theta B(T)}]=e^{-\theta^{2}T}\cdot e^{2\theta^{2}T}=e^{\theta^{2}T}\)</span>.</p>
<p>The fact that <span class="math inline">\(M(t)\)</span> is a martingale is very helpful in calculations. Indeed, suppose we want to compute the expectation of a function <span class="math inline">\(F(\tilde{B}(s))\)</span> of a Brownian motion with drift at time <span class="math inline">\(s&lt;T\)</span>. Then, we have by theorem (<a href="#th:girsanov-theorem-for-constant-drift-case" data-reference-type="ref" data-reference="th:girsanov-theorem-for-constant-drift-case">[th:girsanov-theorem-for-constant-drift-case]</a>):</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[F(\tilde{B}(s))] &amp; =\mathbb{E}[M_{T}M_{T}^{-1}F(\tilde{B}(s))]\\
&amp; =\tilde{\mathbb{E}}[M_{T}^{-1}F(\tilde{B}(s))]\\
&amp; =\tilde{\mathbb{E}}[e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))]
\end{aligned}\]</span></p>
<p>Now, we know that under <span class="math inline">\(\tilde{\mathbb{P}}\)</span>probability, <span class="math inline">\((\tilde{B}(t),t\in[0,T])\)</span> is a standard brownian motion, or <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-standard brownian motion for short. Therefore, the process <span class="math inline">\(e^{\theta\tilde{B}(t)-\frac{1}{2}\theta^{2}t}\)</span> is a martingale under the new probability measure <span class="math inline">\(\tilde{\mathbb{P}}\)</span>, or a <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-martingale for short. By conditioning over <span class="math inline">\(\mathcal{F}_{s}\)</span> and applying the martingale property, we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[F(\tilde{B}_{s})\right] &amp; =\tilde{\mathbb{E}}[e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))]\\
&amp; =\tilde{\mathbb{E}}[\tilde{\mathbb{E}}[e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))|\mathcal{F}_{s}]]\\
&amp; =\tilde{\mathbb{E}}[e^{\theta\tilde{B}(s)-\frac{1}{2}\theta^{2}s}F(\tilde{B}(s))]\\
&amp; =\mathbb{E}[e^{\theta B(s)-\frac{1}{2}\theta^{2}s}F(B(s))]
\end{aligned}\]</span></p>
<p>The last equality may seem wrong as removed all the tildes. It is not! It holds because <span class="math inline">\((\tilde{B}(t))\)</span> under <span class="math inline">\(\tilde{\mathbb{P}}\)</span> has the same distribution as <span class="math inline">\((B(t))\)</span> under <span class="math inline">\(\mathbb{P}\)</span>: a standard brownian motion. Of course, it would be possible to directly evaluate <span class="math inline">\(\mathbb{E}[F(\tilde{B}(s))]\)</span> here as we know the distribution of a Brownian motion with drift. However, when the function will involve more than one point (such as the maximum of the path), the Cameron-Martin theorem is a powerful tool to evaluate expectations.</p>
<p>(3) <strong>The paths with or without the drift are the same.</strong> Let <span class="math inline">\((B(t),t\leq T)\)</span> be a standard Brownian motion defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Heuristically, it is fruitful to think of the sample space of <span class="math inline">\(\Omega\)</span> as the different continuous paths of Brownian motion. Since, the change of probability from <span class="math inline">\(\mathbb{P}\)</span> to <span class="math inline">\(\tilde{\mathbb{P}}\)</span>simply changes the relative weights of the paths (and this change of weight is never zero, similarly to equation (<a href="#eq:girsanov-probability-scaling" data-reference-type="ref" data-reference="eq:girsanov-probability-scaling">[eq:girsanov-probability-scaling]</a>) for a single random variable), the theorem suggests that the paths of a standard Brownian motion and those of a Brownian motion with a constant drift <span class="math inline">\(\theta\)</span> (with volatility <span class="math inline">\(1\)</span>) are essentially the same.</p>
<p>The form of the factor <span class="math inline">\(M_{T}=e^{-\theta\tilde{B}_{T}+\theta^{2}T}\)</span> can be easily understood at the heuristic level. For each outcome <span class="math inline">\(\omega\)</span>, it is proportional to <span class="math inline">\(e^{-\theta\tilde{B}_{T}(\omega)}\)</span> (The term <span class="math inline">\(e^{(\theta^{2}/2)T}\)</span> is simply to ensure that <span class="math inline">\(\mathbb{P}(\Omega)=1\)</span>) Therefore, the factor <span class="math inline">\(M_{T}\)</span> penalizes the paths for which <span class="math inline">\(\tilde{B}_{T}(\omega)\)</span> is large and positive (if <span class="math inline">\(\theta&gt;0\)</span>). In particular, it is conceivable that the Brownian motion with positive drift is reduced to standard Brownian motion under the new probability.</p>
<p>(4) <strong>Changing the volatility.</strong> What about the volatility? Is it possible to change the probability <span class="math inline">\(\mathbb{P}\)</span> to <span class="math inline">\(\tilde{\mathbb{P}}\)</span> in such a way that the Brownian motion under <span class="math inline">\(\mathbb{P}\)</span> has volatility <span class="math inline">\(\sigma\neq1\)</span> under <span class="math inline">\(\tilde{\mathbb{P}}\)</span>? The answer is no! The paths of the Brownian motions with different volatilities are inherently different. Indeed, it suffices to compute the quadratic variation. If <span class="math inline">\((B_{t}:t\in[0,T])\)</span> has volatility <span class="math inline">\(1\)</span> and <span class="math inline">\((\tilde{B_{t}},t\in[0,T])\)</span> has volatility <span class="math inline">\(2\)</span>. then the following convergence holds for <span class="math inline">\(\omega\)</span> in a set of probability one (for a partition fine enough, say <span class="math inline">\(t_{j+1}-t_{j}=2^{-n}\)</span>. Then <span class="math inline">\(B_{t}=\int1\cdot dB_{t}\)</span> and <span class="math inline">\(\tilde{B_{t}}=\int2\cdot dB_{t}\)</span></p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\sum_{j=0}^{n-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))^{2} &amp; =\int_{0}^{T}1^{2}\cdot ds=T
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\sum_{j=0}^{n-1}(\tilde{B}_{t_{j+1}}(\omega)-\tilde{B}_{t_{j}}(\omega))^{2} &amp; =\int_{0}^{T}2^{2}\cdot ds=4T
\end{aligned}\]</span></p>
<p>In other words, the distribution of the standard brownian motion on <span class="math inline">\([0,T]\)</span> is supported on paths whose quadratic variation is <span class="math inline">\(T\)</span>, whereas the distribution of <span class="math inline">\((\tilde{B}_{t},t\geq0)\)</span> is supported on paths where the quadratic variation is <span class="math inline">\(4T\)</span>. These paths are very different. We conclude that the distributions of the two processes are not equivalent. Hence, a change of probability from <span class="math inline">\(\mathbb{P}\)</span> to <span class="math inline">\(\mathbb{\tilde{P}}\)</span> is not possible. In fact, we say that they are mutually singular, meaning the set of paths on which they are supported are disjoint.</p>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\((\tilde{B}_{t}:t\in[0,T])\)</span> be a Brownian motion with constant drift <span class="math inline">\(\theta\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Thus, <span class="math inline">\(\tilde{B}_{t}=\theta t+B_{t}\)</span>.</p>
<p><strong>Claim</strong>. <span class="math inline">\(\tilde{B}_{t}\)</span> is a <span class="math inline">\(\mathbb{\tilde{P}}\)</span>-martingale.</p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
M_{t} &amp; =f(t,B_{t})=\exp(-\theta B_{t}-(\theta^{2}/2)t)
\end{aligned}\]</span></p>
<p>So:</p>
<p><span class="math display">\[\begin{aligned}
dM_{t} &amp; =-\frac{\theta^{2}}{2}M_{t}dt-\theta M_{t}dB_{t}+\frac{1}{2}\theta^{2}M(t)dt\\
&amp; =-\theta M_{t}dB_{t}
\end{aligned}\]</span></p>
<p>Consider the product <span class="math inline">\((M_{t}\tilde{B}_{t})\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
d(M_{t}\tilde{B}_{t}) &amp; =\tilde{B}_{t}dM_{t}+M_{t}d\tilde{B}_{t}+dM_{t}\cdot d\tilde{B}_{t}\\
&amp; =-\tilde{B}_{t}\theta M_{t}dB_{t}+M_{t}(\theta dt+dB_{t})-\theta M_{t}dB_{t}(\theta dt+dB_{t})\\
&amp; =-\tilde{B}_{t}\theta M_{t}dB_{t}+\theta M_{t}dt+M_{t}dB_{t}-\theta M_{t}dt\\
&amp; =(-\tilde{B}_{t}\theta+1)M_{t}dB_{t}
\end{aligned}\]</span></p>
<p>Thus, by the properties of Ito integral,<span class="math inline">\(M_{t}\tilde{B}_{t}\)</span> is a martingale under <span class="math inline">\(\mathbb{P}\)</span>. By the abstract Bayes formula (<a href="#th:abstract-bayes-formula" data-reference-type="ref" data-reference="th:abstract-bayes-formula">[th:abstract-bayes-formula]</a>):</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[\tilde{B}_{t}|\mathcal{F}_{s}] &amp; =\frac{1}{M_{s}}\mathbb{E}[M_{t}\tilde{B}_{t}|\mathcal{F}_{s}]\\
&amp; =\frac{1}{M_{s}}\cdot M_{s}\tilde{B}_{s}\\
&amp; =\tilde{B}_{s}
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(\tilde{B}_{t}\)</span> is a <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-martingale.</p>
<p><strong>Claim</strong>. Our claim is that under the <span class="math inline">\(\tilde{\mathbb{P}}\)</span> measure, <span class="math inline">\(\tilde{B}_{t}\sim\mathcal{N}^{\mathbb{\tilde{P}}}(0,t)\)</span> and to do this we rely on the the moment-generating function.</p>
<p>By definition, for a constant <span class="math inline">\(\Psi\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
M_{\tilde{B}_{t}}(\Psi) &amp; =\tilde{\mathbb{E}}\left[\exp\left(\Psi\tilde{B}_{t}\right)\right]\\
&amp; =\mathbb{E}\left[M_{T}\exp\left(\Psi\tilde{B}_{t}\right)\right]\\
&amp; =\mathbb{E}\left[\exp\left(-\theta\tilde{B}_{T}+\frac{\theta^{2}}{2}T+\Psi\tilde{B}_{t}\right)\right]\\
&amp; =\mathbb{E}\left[\exp\left(-\theta(\theta T+B_{T})+\frac{\theta^{2}}{2}T+\Psi(\theta t+B_{t})\right)\right]\\
&amp; =\mathbb{E}\left[\exp\left(-\theta B_{T}-\frac{\theta^{2}}{2}T+\Psi\theta t+\Psi B_{t})\right)\right]\\
&amp; =\mathbb{E}\left[\exp\left(-\theta(B_{T}-B_{t})-\frac{\theta^{2}}{2}T+\Psi\theta t+(\Psi-\theta)B_{t})\right)\right]\\
&amp; =\exp\left(-\frac{\theta^{2}}{2}T+\Psi\theta t\right)\mathbb{E}\left(-\theta(B_{T}-B_{t})\right)\mathbb{E}\left((\Psi-\theta)B_{t}\right)\\
&amp; =\exp\left(-\frac{\theta^{2}}{2}T+\Psi\theta t\right)\exp\left[\frac{1}{2}\theta^{2}(T-t)\right]\exp\left[\frac{1}{2}(\Psi-\theta)^{2}t\right]\\
&amp; =\exp\left[-\frac{1}{2}\left(\theta^{2}-2\Psi\theta-(\Psi-\theta)^{2}\right)t\right]\\
&amp; =\exp\left[-\frac{1}{2}\left(\theta^{2}-2\Psi\theta-(\Psi^{2}-2\Psi\theta+\theta^{2}\right)t\right]\\
&amp; =\exp(-\Psi^{2}t)
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(\tilde{B}_{t}\sim\mathcal{N}^{\tilde{\mathbb{P}}}(0,t)\)</span>.</p>
<p><strong>Claim</strong>. Finally, to show that <span class="math inline">\(\tilde{B}_{t}\)</span> is indeed a <span class="math inline">\(\mathbb{\tilde{P}}-\)</span>standard brownian motion, we have the following:</p>
<p>(a) <span class="math inline">\(\tilde{B}_{0}=\theta(0)+B_{0}=0\)</span> and <span class="math inline">\(\tilde{B}_{t}\)</span> has almost surely continuous paths.</p>
<p>(b) We would like to prove that, for <span class="math inline">\(s&lt;t\)</span>, <span class="math inline">\(\tilde{B}_{t}-\tilde{B}_{s}\sim\mathcal{N}^{\tilde{\mathbb{P}}}(0,t-s)\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[\tilde{B}_{t}-\tilde{B}_{s}] &amp; =\tilde{\mathbb{E}}[\tilde{B}_{t}]-\tilde{\mathbb{E}}[B_{s}]\\
&amp; =0
\end{aligned}\]</span></p>
<p>And,</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[(\tilde{B}_{t}-\tilde{B}_{s})^{2}] &amp; =\tilde{\mathbb{E}}[\tilde{B}_{t}^{2}-2\tilde{B}_{t}\tilde{B}_{s}+\tilde{B}_{s}^{2}]\\
&amp; =\tilde{\mathbb{E}}[B_{t}^{2}]-2\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}]+\tilde{\mathbb{E}}[\tilde{B}_{s}^{2}]\\
&amp; =t+s-2\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}]
\end{aligned}\]</span></p>
<p>(c) The non-overlapping increments of a <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-martingale are independent. To see this, suppose <span class="math inline">\(t_{1}\leq t_{2}\leq t_{3}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[(B_{t_{3}}-B_{t_{2}})(B_{t_{2}}-B_{t_{1}})] &amp; =\tilde{\mathbb{E}}[\tilde{\mathbb{E}}[(B_{t_{3}}-B_{t_{2}})(B_{t_{2}}-B_{t_{1}})|\mathcal{F}_{t_{2}}]]\\
&amp; =\tilde{\mathbb{E}}[(B_{t_{2}}-B_{t_{1}})\tilde{\mathbb{E}}[(B_{t_{3}}-B_{t_{2}})|\mathcal{F}_{t_{2}}]]\\
&amp; =\tilde{\mathbb{E}}[(B_{t_{2}}-B_{t_{1}})(B_{t_{2}}-B_{t_{2}})]]=0
\end{aligned}\]</span></p>
<p>Also, the covariance</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}] &amp; =\tilde{\mathbb{E}}[(\tilde{B}_{t}-\tilde{B}_{s})\tilde{B}_{s}]+\tilde{\mathbb{E}}[\tilde{B}_{s}^{2}]\\
&amp; =0+s
\end{aligned}\]</span></p>
<p>So, <span class="math inline">\(\mathbb{E}[(\tilde{B}_{t}-\tilde{B}_{s})^{2}]=t+s-2s=t-s\)</span>.</p>
<p>Consequently, <span class="math inline">\(\tilde{B}_{t}\)</span> is a <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-standard brownian motion.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb5" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "A gentle introduction to the Girsanov Theorem - Back to the basics"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Quasar"</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-12-01"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Stochastic Calculus]      </span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "image.jpg"</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="an">comments:</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">  giscus: </span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">    repo: quasar-chunawala/quantdev</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>One of the most popular technical tools in financial engineering is the Girsanov theorem. In this blog-post, I intend to provide the dear reader a beginner-friendly introduction and an intuitive gut feel for these tools. </span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>The change of measure technique was used by <span class="co">[</span><span class="ot">Heylette Geman</span><span class="co">](https://en.wikipedia.org/wiki/H%C3%A9lyette_Geman)</span>, <span class="co">[</span><span class="ot">Nicole El Karoui</span><span class="co">](https://en.wikipedia.org/wiki/Nicole_El_Karoui)</span> and <span class="co">[</span><span class="ot">Jean-Charles Rochet</span><span class="co">]()</span> in their seminal note <span class="co">[</span><span class="ot">Changes of Numeraire, Changes of Probability Measure and Option Pricing</span><span class="co">](https://www.cambridge.org/core/journals/journal-of-applied-probability/article/abs/changes-of-numeraire-changes-of-probability-measure-and-option-pricing/EA730D6C18D56426D491B6A25563C0B3)</span>.</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext itikz</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="fu"># Change of Probability.</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## Change of Probability for a Random Variable.</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>Consider a random variable $X$ defined on a sample space $\Omega$ having zero mean. We want to change the mean of $X$ so that $\mu\neq 0$. Of course, it is easy to change the mean of a random variable: If $X$ has mean $0$, then the random variable $X+\mu$ has mean $\mu$. However, it might be that the variable $X+\mu$ does not share the same possible values as $X$. For example, take $X$ to be a uniform random variable on $<span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span>$. While $X+1$ has mean $1$, the density of $X+1$ would be non-zero on $<span class="co">[</span><span class="ot">0,2</span><span class="co">]</span>$ instead of $<span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span>$.</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>Our goal is to find a good way to change the underlying probability $\mathbb{P}$, and thus the distribution of $X$, so that the set of outcomes is unchanged. If $X$ is a discrete random variable, say with $\mathbb{P}(X=-1)=\mathbb{P}(X=1)=1/2$, we can change the probability in order to change the mean easily. It suffices to take $\tilde{\mathbb{P}}$ so that $\tilde{\mathbb{P}}(X=1)=p$ and $\mathbb{P}(X=-1)=1-p$ for some appropriate $0\leq p\leq1$.</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>If $X$ is a continuous random variable, with a PDF $f_{X}$, the probabilities can be changed by modifying the PDF. Consider the a new PDF:</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>\tilde{f}_{X}(x) &amp; =f_{X}(x)g(x)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>for some function $g(x)&gt;0$ such that $\int f(x)g(x)dx=1$. Clearly, $f_{X}(x)g(x)$ is also a PDF and $f_{X}(x)&gt;0$ if and only if $f_{X}(x)g(x)&gt;0$, so that the possible values of $X$ are unchanged. A convenient (and important!) choice of function $g$ is:</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>g(x) &amp; =\frac{e^{ax}}{\int_{\mathbf{R}}e^{ax}f_{X}(x)dx}=\frac{e^{ax}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>},\quad a\in\mathbf{R}</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>\end{aligned}$${#eq-change-of-probability-of-an-rv}</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>assuming $X$ has a well-defined MGF. Here $a$ is a parameter that can be tuned to fit to a specific mean. The normalization factor in the denominator is the MGF of $X$. It ensures that $f_{X}(x)g(x)$ is a PDF. Note that if $a&gt;0$, the function $g$ gives a bigger weight to large values of $X$. We say that $g$ is biased towards the large values.</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>:::{#exm-biasing-a-uniform-random-variable}</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="fu">### Biasing a uniform random variable </span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>Let $X$ be a uniform random variable on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. Clearly, $\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>=1/2$. How can we change the PDF of $X$ so that the possible values are still $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$, but the mean is $1/4$. We have that the PDF is $f_{X}(x)=1$ if $x\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ and $0$ elsewhere. Therefore, the mean with the new PDF with parameter $a$ as in the @eq-change-of-probability-of-an-rv is:</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp; =\int_{0}^{1}x\tilde{f}(x)dx<span class="sc">\\</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{0}^{1}\frac{xe^{ax}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>}dx<span class="sc">\\</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{a}{e^{a}-1}\int_{0}^{1}xe^{ax}dx<span class="sc">\\</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{a}{e^{a}-1}\left(\left<span class="co">[</span><span class="ot">x\frac{e^{ax}}{a}\right</span><span class="co">]</span>_{0}^{1}-\frac{1}{a}\int_{0}^{1}e^{ax}dx\right)<span class="sc">\\</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{a}{e^{a}-1}\left(\frac{e^{a}}{a}-\frac{1}{a}\frac{e^{a}-1}{a}\right)<span class="sc">\\</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{e^{a}}{e^{a}-1}-\frac{1}{a}</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>For $\tilde{\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>}$to be equal to $1/4$, we get numerically $a\approx-3.6$. Note that the possible values of $X$ remain the same under the new probability. However, the new distribution is no longer uniform! It has bias towards values closer to zero, as it should.</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>:::{#exm-biasing-a-gaussian-random-variable}</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a><span class="fu">### Biasing a Gaussian random variable. </span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>Let $X$ be a Gaussian random variable with mean $\mu$ and variance $\sigma^{2}$. How can we change the PDF of $X$ to have mean $0$? Going back to @eq-change-of-probability-of-an-rv, the mean $\mu$ under the new PDF with parameter $a$ is:</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp; =\int_{-\infty}^{\infty}x\tilde{f}(x)dx<span class="sc">\\</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{-\infty}^{\infty}xg(x)f(x)dx<span class="sc">\\</span></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{-\infty}^{\infty}x\cdot\frac{e^{ax}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}dx<span class="sc">\\</span></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\cdot\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x^{2}-2\mu x+\mu^{2}-2a\sigma^{2}x}{\sigma^{2}}\right)\right</span><span class="co">]</span>dx<span class="sc">\\</span></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\cdot\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x^{2}-2(\mu+a\sigma^{2})x+(\mu+a\sigma^{2})^{2}-2\mu a\sigma^{2}-a^{2}\sigma^{4}}{\sigma^{2}}\right)\right</span><span class="co">]</span>dx<span class="sc">\\</span></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{e^{\mu a+a^{2}\sigma^{2}/2}}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x-(\mu+a\sigma^{2})}{\sigma}\right)^{2}\right</span><span class="co">]</span>dx<span class="sc">\\</span></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x-(\mu+a\sigma^{2})}{\sigma}\right)^{2}\right</span><span class="co">]</span>dx</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>For the specific choice of the parameter $a=\mu/\sigma^{2}$, we recover the PDF of a Gaussian random variable with mean $0$. But, we can deduce more. The new PDF is also Gaussian. This was not the case for uniform random variables. In fact, the new PDF is exactly the same as the one of $X-\mu$. For if, $a=\mu/\sigma^{2}$, we have:</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp; =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left<span class="co">[</span><span class="ot">-\frac{x^{2}}{2\sigma^{2}}\right</span><span class="co">]</span>dx</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>and observe that if $Y=X-\mu$, then:</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>F_{Y}(x) &amp; =\mathbb{P}(X-\mu&lt;x)<span class="sc">\\</span></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{P}(X\leq x+\mu)<span class="sc">\\</span></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a> &amp; =F_{X}(x+\mu)<span class="sc">\\</span></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>\frac{d}{dx}(F_{Y}(x)) &amp; =\frac{d}{dx}(F_{X}(x+\mu))<span class="sc">\\</span></span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>f_{Y}(x) &amp; =f_{X}(x+\mu)\cdot\frac{d}{dx}(x+\mu)<span class="sc">\\</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>f_{Y}(x) &amp; =f_{X}(x+\mu)<span class="sc">\\</span></span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{\sqrt{2\pi}\sigma}\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x+\mu-\mu}{\sigma}\right)^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{\sqrt{2\pi}\sigma}\exp\left<span class="co">[</span><span class="ot">-\frac{x^{2}}{2\sigma^{2}}\right</span><span class="co">]</span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>In other words:</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>*For Gaussians, changing the mean by recentering is equivalent to changing the probability as in* @eq-change-of-probability-of-an-rv.</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualization</span></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>Let $X$ be gaussian with mean $\mu=1$ and variance $\sigma^2 = 1$. The PDF of $X$ is: </span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: true</span></span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>\begin{axis}[xlabel <span class="op">=</span> $x$, ylabel<span class="op">=</span>{$f_X(x)$}, title<span class="op">=</span>{The PDF of $X \sim \mathcal{N}<span class="op">^</span>{P}(\mu<span class="op">=</span><span class="dv">1</span>,\sigma<span class="op">^</span><span class="dv">2</span><span class="op">=</span><span class="dv">1</span>)$},domain<span class="op">=-</span><span class="dv">3</span>:<span class="dv">3</span>]</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>\addplot[color<span class="op">=</span>black,samples<span class="op">=</span><span class="dv">100</span>]{<span class="dv">1</span><span class="op">/</span>(sqrt(<span class="dv">2</span><span class="op">*</span><span class="fl">3.14</span>))<span class="op">*</span>exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>((x<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(x<span class="op">-</span><span class="dv">1</span>))}<span class="op">;</span></span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>\end{axis}</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a>\end{tikzpicture}</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>```</span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>I choose $g(x) <span class="op">=</span> \frac{e<span class="op">^</span>{ax}}{\mathbb{E}[e<span class="op">^</span>{aX}]} <span class="op">=</span> \frac{e<span class="op">^</span>{ax}}{e<span class="op">^</span>{\mu a <span class="op">+</span> \frac{<span class="dv">1</span>}{<span class="dv">2</span>}a<span class="op">^</span><span class="dv">2</span> \sigma<span class="op">^</span><span class="dv">2</span>}}$, $\mu<span class="op">=</span><span class="dv">1</span>$, $\sigma<span class="op">^</span><span class="dv">2</span> <span class="op">=</span> <span class="dv">1</span>$ <span class="kw">and</span> <span class="bu">set</span> the value of the parameter $a <span class="op">=</span> \frac{\mu}{\sigma<span class="op">^</span><span class="dv">2</span>} <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>$.</span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: true</span></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>\begin{axis}[xlabel <span class="op">=</span> $x$, ylabel<span class="op">=</span>{$g(x)$}, title<span class="op">=</span>{The density scaling $g(x)<span class="op">=</span>\frac{e<span class="op">^</span>{ax}}{E[e<span class="op">^</span>{ax}]}$, <span class="cf">with</span> parameter value $a<span class="op">=-</span>\frac{\mu}{\sigma<span class="op">^</span><span class="dv">2</span>}$},domain<span class="op">=-</span><span class="dv">3</span>:<span class="dv">3</span>]</span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>\addplot[color<span class="op">=</span>black,samples<span class="op">=</span><span class="dv">100</span>]{exp(<span class="op">-</span>x)<span class="op">/</span>exp(<span class="op">-</span><span class="fl">0.5</span>)}<span class="op">;</span></span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a>\end{axis}</span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a>\end{tikzpicture}</span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a>```</span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>The new density $\tilde{f}_X(x)$ obtained multiplying $f_X(x)$ by the weights $g(x)$ <span class="kw">is</span> the same <span class="im">as</span> a gaussian centered at $<span class="dv">0</span>$ <span class="cf">with</span> variance $<span class="dv">1</span>$ :</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: true</span></span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-summary: "Show the code"</span></span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a>\begin{axis}[xlabel <span class="op">=</span> $x$, ylabel<span class="op">=</span>{$\tilde{f}_X(x)$}, title<span class="op">=</span>{The new PDF of $X$, after multiplying the density ${f}_X(x)$ by weights $g(x)$.},domain<span class="op">=-</span><span class="dv">3</span>:<span class="dv">3</span>]</span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a>\addplot[color<span class="op">=</span>black,samples<span class="op">=</span><span class="dv">100</span>]{<span class="dv">1</span><span class="op">/</span>(sqrt(<span class="dv">2</span><span class="op">*</span><span class="fl">3.14</span>))<span class="op">*</span>exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>(x<span class="op">*</span>x))}<span class="op">;</span></span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a>\end{axis}</span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a>\end{tikzpicture}</span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a>```</span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a>This <span class="kw">is</span> a very special <span class="bu">property</span> of the Gaussian distribution. The exponential <span class="kw">and</span> Poisson distributions have a similar <span class="bu">property</span>.</span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a>Intuitively, <span class="cf">if</span> we <span class="bu">apply</span> this idea to each time<span class="op">-</span><span class="bu">slice</span> $(B_t <span class="op">-</span> B_s)$ of the Brownian motion which has a drift, we can recenter the gaussians to have mean $<span class="dv">0</span>$ (driftless).</span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a><span class="at">@exm</span><span class="op">-</span>biasing<span class="op">-</span>a<span class="op">-</span>gaussian<span class="op">-</span>random<span class="op">-</span>variable <span class="kw">is</span> very important <span class="kw">and</span> we will state it <span class="im">as</span> a theorem. Before doing so, we notice that the change of PDF (<span class="op">@</span>eq<span class="op">-</span>change<span class="op">-</span>of<span class="op">-</span>probability<span class="op">-</span>of<span class="op">-</span>an<span class="op">-</span>rv) can be expressed more generally by changing the underlying probability measure(length, area, weights) $\mathbb{P}$ on the sample space $\Omega$ on which the random variables are defined. More precisely, let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, <span class="kw">and</span> let $X$ be a random variable defined on $\Omega$. We define a new probability $\tilde{\mathbb{P}}$ on $\Omega$ <span class="im">as</span> follows:</span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a>If $\mathcal{E}$ <span class="kw">is</span> an event <span class="kw">in</span> $\mathcal{F}$, then:</span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a>\mathbb{\tilde{P}}(\mathcal{E}) <span class="op">&amp;</span> <span class="op">=</span>\mathbb{\tilde{E}}[<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}]<span class="op">=</span>\int_{\mathbf{R}}<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}\cdot\tilde{f}(x)dx\nonumber \<span class="op">\</span></span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\int_{\mathbf{R}}<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}\cdot g(x)f_{X}(x)dx\nonumber \<span class="op">\</span></span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\int_{\mathbf{R}}<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}\cdot\frac{e<span class="op">^</span>{ax}}{\mathbb{E}[e<span class="op">^</span>{aX}]}f_{X}(x)dx\nonumber \<span class="op">\</span></span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}\frac{e<span class="op">^</span>{aX}}{\mathbb{E}[e<span class="op">^</span>{aX}]}\right]</span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a>Intuitively, we are changing the probability of each outcome $\omega\<span class="kw">in</span>\mathcal{E}$, by the factor</span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a>$$\frac{e<span class="op">^</span>{aX(\omega)}}{\mathbb{E}[e<span class="op">^</span>{aX}]}\label{eq:girsanov<span class="op">-</span>probability<span class="op">-</span>scaling}$$</span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a>In other words, <span class="cf">if</span> $a<span class="op">&gt;</span><span class="dv">0</span>$, the outcomes $\omega$ <span class="cf">for</span> which $X$ has large values are favored. Note that equation ([\[eq:change<span class="op">-</span>of<span class="op">-</span>probability<span class="op">-</span>of<span class="op">-</span>an<span class="op">-</span>rv\]](<span class="co">#eq:change-of-probability-of-an-rv){reference-type="ref" reference="eq:change-of-probability-of-an-rv"}) for the PDF is recovered, since for any function $h$ of $X$, we have:</span></span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}[h(X)] <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[\frac{e<span class="op">^</span>{aX}}{\mathbb{E}[e<span class="op">^</span>{aX}]}h(X)\right]\<span class="op">\</span></span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\int_{\mathbf{R}}h(x)\frac{e<span class="op">^</span>{ax}}{\mathbb{E}[e<span class="op">^</span>{aX}]}f_{X}(x)dx</span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a>In this setting, the above example becomes the preliminary version of the Cameron<span class="op">-</span>Martin<span class="op">-</span>Girsanov theorem:</span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a>[]{<span class="co">#th:change-of-probability-for-a-random-variable label="th:change-of-probability-for-a-random-variable"}Let $X$ be a Gaussian random variable with mean $\mu$ and variance $\sigma^{2}$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. Then, under the probability $\mathbb{\tilde{P}}$ given by:</span></span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a>\mathbb{\tilde{P}}(\mathcal{E}) <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}e<span class="op">^</span>{<span class="op">-</span>\frac{\mu}{\sigma<span class="op">^</span>{<span class="dv">2</span>}}X<span class="op">+</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\frac{\mu<span class="op">^</span>{<span class="dv">2</span>}}{\sigma<span class="op">^</span>{<span class="dv">2</span>}}}\right],\quad\mathcal{E}\<span class="kw">in</span>\mathcal{F}\label{eq:preliminary<span class="op">-</span>version<span class="op">-</span>girsanov<span class="op">-</span>theorem}</span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a>the random variable $X$ <span class="kw">is</span> Gaussian <span class="cf">with</span> mean $<span class="dv">0</span>$ <span class="kw">and</span> variance $\sigma<span class="op">^</span>{<span class="dv">2</span>}$.</span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a>Moreover, since $X$ can be written <span class="im">as</span> $X<span class="op">=</span>Y<span class="op">+</span>\mu$ where $Y$ <span class="kw">is</span> Gaussian <span class="cf">with</span> mean $<span class="dv">0</span>$ <span class="kw">and</span> variance $\sigma<span class="op">^</span>{<span class="dv">2</span>}$ under $\mathbb{P}$, we have that $\mathbb{\tilde{P}}$ can be written <span class="im">as</span>:</span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a>\mathbb{\tilde{P}}(\mathcal{E}) <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}e<span class="op">^</span>{<span class="op">-</span>\frac{\mu}{\sigma<span class="op">^</span>{<span class="dv">2</span>}}Y<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\frac{\mu<span class="op">^</span>{<span class="dv">2</span>}}{\sigma<span class="op">^</span>{<span class="dv">2</span>}}}\right],\quad\mathcal{E}\<span class="kw">in</span>\mathcal{F}\label{eq:preliminary<span class="op">-</span>version<span class="op">-</span>girsanov<span class="op">-</span>theorem<span class="op">-</span>II}</span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a>It <span class="kw">is</span> good to pause <span class="cf">for</span> a second <span class="kw">and</span> look at the signs <span class="kw">in</span> the exponential of equations ([\[eq:preliminary<span class="op">-</span>version<span class="op">-</span>girsanov<span class="op">-</span>theorem\]](<span class="co">#eq:preliminary-version-girsanov-theorem){reference-type="ref" reference="eq:preliminary-version-girsanov-theorem"}) and ([\[eq:preliminary-version-girsanov-theorem-II\]](#eq:preliminary-version-girsanov-theorem-II){reference-type="ref" reference="eq:preliminary-version-girsanov-theorem-II"}). The signs in the exponential might be very confusing and is the source of many mistakes in the Cameron-Martin-Girsanov theorem. A good trick is to say that, if we want to remove $\mu$, then the sign in front of $X$ or $Y$ must be negative. Then, we add the exponential factor needed for $\tilde{\mathbb{P}}$ to be a probability. This is given by the MGF of $X$ or $Y$ depending on how we want to express it.</span></span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a>The probabilities $\mathbb{P}$ <span class="kw">and</span> $\tilde{\mathbb{P}}$, <span class="im">as</span> defined <span class="kw">in</span> the equation ([\[eq:preliminary<span class="op">-</span>version<span class="op">-</span>girsanov<span class="op">-</span>theorem\]](<span class="co">#eq:preliminary-version-girsanov-theorem){reference-type="ref" reference="eq:preliminary-version-girsanov-theorem"}) are obviously not equal since they differ by a factor in ([\[eq:girsanov-probability-scaling\]](#eq:girsanov-probability-scaling){reference-type="ref" reference="eq:girsanov-probability-scaling"}). However, they share some similarities. Most notably, if $\mathcal{E}$ is an event of positive $\mathbb{P}$-probability, $\mathbb{P}(\mathcal{E})&gt;0$, then we must have $\tilde{\mathbb{P}}(\mathcal{E})&gt;0$, since the factor in ([\[eq:girsanov-probability-scaling\]](#eq:girsanov-probability-scaling){reference-type="ref" reference="eq:girsanov-probability-scaling"}) is always strictly positive. The converse is also true: if $\mathcal{E}$ is an event of positive $\tilde{\mathbb{P}}$-probability, $\tilde{\mathbb{P}}(\mathcal{E})&gt;0$, then we must have that $\mathbb{P}(\mathcal{E})&gt;0$. This is because the factor in ([\[eq:girsanov-probability-scaling\]](#eq:girsanov-probability-scaling){reference-type="ref" reference="eq:girsanov-probability-scaling"}) can be inverted, being strictly positive. More precisely, we have:</span></span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(\mathcal{E}) <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}[<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}]\<span class="op">\</span></span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}\frac{e<span class="op">^</span>{aX(\omega)}}{\mathbb{E}[e<span class="op">^</span>{aX}]}\left(\frac{e<span class="op">^</span>{aX(\omega)}}{\mathbb{E}[e<span class="op">^</span>{aX}]}\right)<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>}\right]\<span class="op">\</span></span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}\left[<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}\left(\frac{e<span class="op">^</span>{aX(\omega)}}{\mathbb{E}[e<span class="op">^</span>{aX}]}\right)<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>}\right]</span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a>The factor $\left(\frac{e<span class="op">^</span>{aX(\omega)}}{\mathbb{E}[e<span class="op">^</span>{aX}]}\right)<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>}$ <span class="kw">is</span> also strictly positive, proving the claim. To <span class="bu">sum</span> it <span class="bu">all</span> up, the probabilities $\mathbb{P}$ <span class="kw">and</span> $\tilde{\mathbb{P}}$ essentially share the same possible outcomes. Such probability measures are said to be equivalent measures.</span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a>Consider the two probabilities $\mathbb{P}$ <span class="kw">and</span> $\tilde{\mathbb{P}}$ on $(\Omega,\mathcal{F})$. They are said to be equivalent, <span class="cf">if</span> <span class="cf">for</span> <span class="bu">any</span> event $\mathcal{E}\<span class="kw">in</span>\mathcal{F}$, we have $\mathbb{P}(\mathcal{E})<span class="op">&gt;</span><span class="dv">0</span>$ <span class="cf">if</span> <span class="kw">and</span> only <span class="cf">if</span> $\mathbb{P}(\mathcal{E})<span class="op">&gt;</span><span class="dv">0</span>$. Thus, $\mathbb{P}$ <span class="kw">and</span> $\tilde{\mathbb{P}}$ agree on the null sets. If $A\<span class="kw">in</span>\mathcal{F}$ <span class="kw">is</span> such that $\mathbb{P}(A)<span class="op">=</span><span class="dv">0</span>$, then $\mathbb{\tilde{P}}(A)<span class="op">=</span><span class="dv">0</span>$ <span class="kw">and</span> vice<span class="op">-</span>versa.</span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a>Keep <span class="kw">in</span> mind that two probabilities that are equivalent might still be very far <span class="im">from</span> being equal<span class="op">!</span></span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a><span class="co">## The Cameron-Martin Theorem.</span></span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a>[]{<span class="co">#th:girsanov-theorem-for-constant-drift-case label="th:girsanov-theorem-for-constant-drift-case"}(Cameron-Martin Theorem for constant drift). Let $(\tilde{B(t)},t\in[0,T])$ be a $\mathbb{P}-$Brownian motion with constant drift $\theta$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. Consider the probability $\tilde{\mathbb{P}}$on $\Omega$ given by:</span></span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{P}}(\mathcal{E}) <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[e<span class="op">^</span>{<span class="op">-</span>\theta\tilde{B}(T)<span class="op">+</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T}<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}\right],\quad\mathcal{E}\<span class="kw">in</span>\mathcal{F}\label{eq:girsanov<span class="op">-</span>theorem<span class="op">-</span>constant<span class="op">-</span>drift<span class="op">-</span>case}</span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a>Then, the process $(\tilde{B}(t),t\<span class="kw">in</span>[<span class="dv">0</span>,T])$ under $\mathbb{\tilde{P}}$is distributed like a standard brownian motion. Moreover, since we can write $\tilde{B_{t}}<span class="op">=</span>\theta t<span class="op">+</span>B_{t}$ <span class="cf">for</span> some standard brownian motion $(B_{t},t\<span class="kw">in</span>[<span class="dv">0</span>,T])$ on $(\Omega,\mathcal{F},\mathbb{P})$, the probability $\tilde{\mathbb{P}}$ can also be written <span class="im">as</span>:</span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{P}}(\mathcal{E}) <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[e<span class="op">^</span>{<span class="op">-</span>\theta B(T)<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T}<span class="dv">1</span><span class="er">_</span>{\mathcal{E}}\right]\label{eq:girsanov<span class="op">-</span>theorem<span class="op">-</span>constant<span class="op">-</span>drift<span class="op">-</span>case<span class="op">-</span>II}</span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a>It <span class="kw">is</span> a good idea to pause again <span class="kw">and</span> look at the signs <span class="kw">in</span> the exponential <span class="kw">in</span> equations ([\[eq:girsanov<span class="op">-</span>theorem<span class="op">-</span>constant<span class="op">-</span>drift<span class="op">-</span>case\]](<span class="co">#eq:girsanov-theorem-constant-drift-case){reference-type="ref" reference="eq:girsanov-theorem-constant-drift-case"}) and ([\[eq:girsanov-theorem-constant-drift-case-II\]](#eq:girsanov-theorem-constant-drift-case-II){reference-type="ref" reference="eq:girsanov-theorem-constant-drift-case-II"}). They behave the same way as in theorem ([\[th:change-of-probability-for-a-random-variable\]](#th:change-of-probability-for-a-random-variable){reference-type="ref" reference="th:change-of-probability-for-a-random-variable"}). There is a minus sign in front of $B_{T}$ to remove the drift. Before proving the theorem, we make some important remarks.</span></span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a>\(<span class="dv">1</span>\) <span class="op">**</span>The end<span class="op">-</span>point<span class="op">**</span>. Note that only the endpoint $\tilde{B}(T)$ of the Brownian motion <span class="kw">is</span> involved <span class="kw">in</span> the change of probability. In particular, $T$ cannot be $<span class="op">+</span>\infty$. The Cameron<span class="op">-</span>Martin theorem can only be applied on a finite interval.</span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a>\(<span class="dv">2</span>\) <span class="op">**</span>A martingale.<span class="op">**</span> The factor $M_{T}<span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta B(T)<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T}<span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta\tilde{B}(T)<span class="op">+</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}T}$ involved <span class="kw">in</span> the change of probability <span class="kw">is</span> the end<span class="op">-</span>point of a $\mathbb{P}<span class="op">-</span>$martingale, that <span class="kw">is</span>, it <span class="kw">is</span> a martingale under the original probability $\mathbb{P}$. To see this:</span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a>\mathbb{E}[M_{T}<span class="op">|</span>\mathcal{F}_{t}] <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[e<span class="op">^</span>{<span class="op">-</span>\theta B(T)<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}T}<span class="op">|</span>\mathcal{F}_{t}\right]\<span class="op">\</span></span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta B(t)}\mathbb{E}\left[e<span class="op">^</span>{<span class="op">-</span>\theta(B(T)<span class="op">-</span>B(t))}<span class="op">|</span>\mathcal{F}_{t}\right]e<span class="op">^</span>{<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T}\<span class="op">\</span></span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> \{\text{Using }B(T)<span class="op">-</span>B(t)\perp\mathcal{F}_{t}\}\<span class="op">\</span></span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta B(t)}\mathbb{E}\left[e<span class="op">^</span>{<span class="op">-</span>\theta(B(T)<span class="op">-</span>B(t))}\right]e<span class="op">^</span>{<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T}\<span class="op">\</span></span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta B(t)}e<span class="op">^</span>{\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}(T<span class="op">-</span>t)}e<span class="op">^</span>{<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T}\<span class="op">\</span></span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta B(t)<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}t}</span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a>In fact, since $B(t)$ <span class="kw">is</span> a $\mathbb{P}$<span class="op">-</span>standard Brownian motion, $M(t)<span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta B(t)<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}t}$ <span class="kw">is</span> a geometric brownian motion.</span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a>Interestingly, the drift of $\tilde{B}(t)$ becomes the volatility factor <span class="kw">in</span> $M_{T}$<span class="op">!</span> $\mathbb{E}[M_{T}<span class="op">^</span>{<span class="dv">2</span>}]<span class="op">=</span>\mathbb{E}[e<span class="op">^</span>{<span class="op">-</span><span class="dv">2</span>\theta B(T)<span class="op">-</span>\theta<span class="op">^</span>{<span class="dv">2</span>}T}]<span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta<span class="op">^</span>{<span class="dv">2</span>}T}\cdot\mathbb{E}[e<span class="op">^</span>{<span class="op">-</span><span class="dv">2</span>\theta B(T)}]<span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta<span class="op">^</span>{<span class="dv">2</span>}T}\cdot e<span class="op">^</span>{<span class="dv">2</span>\theta<span class="op">^</span>{<span class="dv">2</span>}T}<span class="op">=</span>e<span class="op">^</span>{\theta<span class="op">^</span>{<span class="dv">2</span>}T}$.</span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a>The fact that $M(t)$ <span class="kw">is</span> a martingale <span class="kw">is</span> very helpful <span class="kw">in</span> calculations. Indeed, suppose we want to compute the expectation of a function $F(\tilde{B}(s))$ of a Brownian motion <span class="cf">with</span> drift at time $s<span class="op">&lt;</span>T$. Then, we have by theorem ([\[th:girsanov<span class="op">-</span>theorem<span class="op">-</span><span class="cf">for</span><span class="op">-</span>constant<span class="op">-</span>drift<span class="op">-</span>case\]](<span class="co">#th:girsanov-theorem-for-constant-drift-case){reference-type="ref" reference="th:girsanov-theorem-for-constant-drift-case"}):</span></span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a>\mathbb{E}[F(\tilde{B}(s))] <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}[M_{T}M_{T}<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>}F(\tilde{B}(s))]\<span class="op">\</span></span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[M_{T}<span class="op">^</span>{<span class="op">-</span><span class="dv">1</span>}F(\tilde{B}(s))]\<span class="op">\</span></span>
<span id="cb5-256"><a href="#cb5-256" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[e<span class="op">^</span>{\theta\tilde{B}(T)<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}T}F(\tilde{B}(s))]</span>
<span id="cb5-257"><a href="#cb5-257" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-258"><a href="#cb5-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-259"><a href="#cb5-259" aria-hidden="true" tabindex="-1"></a>Now, we know that under $\tilde{\mathbb{P}}$probability, $(\tilde{B}(t),t\<span class="kw">in</span>[<span class="dv">0</span>,T])$ <span class="kw">is</span> a standard brownian motion, <span class="kw">or</span> $\tilde{\mathbb{P}}$<span class="op">-</span>standard brownian motion <span class="cf">for</span> short. Therefore, the process $e<span class="op">^</span>{\theta\tilde{B}(t)<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}t}$ <span class="kw">is</span> a martingale under the new probability measure $\tilde{\mathbb{P}}$, <span class="kw">or</span> a $\tilde{\mathbb{P}}$<span class="op">-</span>martingale <span class="cf">for</span> short. By conditioning over $\mathcal{F}_{s}$ <span class="kw">and</span> applying the martingale <span class="bu">property</span>, we get:</span>
<span id="cb5-260"><a href="#cb5-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-261"><a href="#cb5-261" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-262"><a href="#cb5-262" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left[F(\tilde{B}_{s})\right] <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[e<span class="op">^</span>{\theta\tilde{B}(T)<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}T}F(\tilde{B}(s))]\<span class="op">\</span></span>
<span id="cb5-263"><a href="#cb5-263" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[\tilde{\mathbb{E}}[e<span class="op">^</span>{\theta\tilde{B}(T)<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}T}F(\tilde{B}(s))<span class="op">|</span>\mathcal{F}_{s}]]\<span class="op">\</span></span>
<span id="cb5-264"><a href="#cb5-264" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[e<span class="op">^</span>{\theta\tilde{B}(s)<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}s}F(\tilde{B}(s))]\<span class="op">\</span></span>
<span id="cb5-265"><a href="#cb5-265" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}[e<span class="op">^</span>{\theta B(s)<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}s}F(B(s))]</span>
<span id="cb5-266"><a href="#cb5-266" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-267"><a href="#cb5-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-268"><a href="#cb5-268" aria-hidden="true" tabindex="-1"></a>The last equality may seem wrong <span class="im">as</span> removed <span class="bu">all</span> the tildes. It <span class="kw">is</span> <span class="kw">not</span><span class="op">!</span> It holds because $(\tilde{B}(t))$ under $\tilde{\mathbb{P}}$ has the same distribution <span class="im">as</span> $(B(t))$ under $\mathbb{P}$: a standard brownian motion. Of course, it would be possible to directly evaluate $\mathbb{E}[F(\tilde{B}(s))]$ here <span class="im">as</span> we know the distribution of a Brownian motion <span class="cf">with</span> drift. However, when the function will involve more than one point (such <span class="im">as</span> the maximum of the path), the Cameron<span class="op">-</span>Martin theorem <span class="kw">is</span> a powerful tool to evaluate expectations.</span>
<span id="cb5-269"><a href="#cb5-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-270"><a href="#cb5-270" aria-hidden="true" tabindex="-1"></a>\(<span class="dv">3</span>\) <span class="op">**</span>The paths <span class="cf">with</span> <span class="kw">or</span> without the drift are the same.<span class="op">**</span> Let $(B(t),t\leq T)$ be a standard Brownian motion defined on $(\Omega,\mathcal{F},\mathbb{P})$. Heuristically, it <span class="kw">is</span> fruitful to think of the sample space of $\Omega$ <span class="im">as</span> the different continuous paths of Brownian motion. Since, the change of probability <span class="im">from</span> $\mathbb{P}$ to $\tilde{\mathbb{P}}$simply changes the relative weights of the paths (<span class="kw">and</span> this change of weight <span class="kw">is</span> never zero, similarly to equation ([\[eq:girsanov<span class="op">-</span>probability<span class="op">-</span>scaling\]](<span class="co">#eq:girsanov-probability-scaling){reference-type="ref" reference="eq:girsanov-probability-scaling"}) for a single random variable), the theorem suggests that the paths of a standard Brownian motion and those of a Brownian motion with a constant drift $\theta$ (with volatility $1$) are essentially the same.</span></span>
<span id="cb5-271"><a href="#cb5-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-272"><a href="#cb5-272" aria-hidden="true" tabindex="-1"></a>The form of the factor $M_{T}<span class="op">=</span>e<span class="op">^</span>{<span class="op">-</span>\theta\tilde{B}_{T}<span class="op">+</span>\theta<span class="op">^</span>{<span class="dv">2</span>}T}$ can be easily understood at the heuristic level. For each outcome $\omega$, it <span class="kw">is</span> proportional to $e<span class="op">^</span>{<span class="op">-</span>\theta\tilde{B}_{T}(\omega)}$ (The term $e<span class="op">^</span>{(\theta<span class="op">^</span>{<span class="dv">2</span>}<span class="op">/</span><span class="dv">2</span>)T}$ <span class="kw">is</span> simply to ensure that $\mathbb{P}(\Omega)<span class="op">=</span><span class="dv">1</span>$) Therefore, the factor $M_{T}$ penalizes the paths <span class="cf">for</span> which $\tilde{B}_{T}(\omega)$ <span class="kw">is</span> large <span class="kw">and</span> positive (<span class="cf">if</span> $\theta<span class="op">&gt;</span><span class="dv">0</span>$). In particular, it <span class="kw">is</span> conceivable that the Brownian motion <span class="cf">with</span> positive drift <span class="kw">is</span> reduced to standard Brownian motion under the new probability.</span>
<span id="cb5-273"><a href="#cb5-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-274"><a href="#cb5-274" aria-hidden="true" tabindex="-1"></a>\(<span class="dv">4</span>\) <span class="op">**</span>Changing the volatility.<span class="op">**</span> What about the volatility? Is it possible to change the probability $\mathbb{P}$ to $\tilde{\mathbb{P}}$ <span class="kw">in</span> such a way that the Brownian motion under $\mathbb{P}$ has volatility $\sigma\neq1$ under $\tilde{\mathbb{P}}$? The answer <span class="kw">is</span> no<span class="op">!</span> The paths of the Brownian motions <span class="cf">with</span> different volatilities are inherently different. Indeed, it suffices to compute the quadratic variation. If $(B_{t}:t\<span class="kw">in</span>[<span class="dv">0</span>,T])$ has volatility $<span class="dv">1</span>$ <span class="kw">and</span> $(\tilde{B_{t}},t\<span class="kw">in</span>[<span class="dv">0</span>,T])$ has volatility $<span class="dv">2</span>$. then the following convergence holds <span class="cf">for</span> $\omega$ <span class="kw">in</span> a <span class="bu">set</span> of probability one (<span class="cf">for</span> a partition fine enough, say $t_{j<span class="op">+</span><span class="dv">1</span>}<span class="op">-</span>t_{j}<span class="op">=</span><span class="dv">2</span><span class="op">^</span>{<span class="op">-</span>n}$. Then $B_{t}<span class="op">=</span>\int1\cdot dB_{t}$ <span class="kw">and</span> $\tilde{B_{t}}<span class="op">=</span>\int2\cdot dB_{t}$</span>
<span id="cb5-275"><a href="#cb5-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-276"><a href="#cb5-276" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-277"><a href="#cb5-277" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\sum_{j<span class="op">=</span><span class="dv">0</span>}<span class="op">^</span>{n<span class="op">-</span><span class="dv">1</span>}(B_{t_{j<span class="op">+</span><span class="dv">1</span>}}(\omega)<span class="op">-</span>B_{t_{j}}(\omega))<span class="op">^</span>{<span class="dv">2</span>} <span class="op">&amp;</span> <span class="op">=</span>\int_{<span class="dv">0</span>}<span class="op">^</span>{T}<span class="dv">1</span><span class="op">^</span>{<span class="dv">2</span>}\cdot ds<span class="op">=</span>T</span>
<span id="cb5-278"><a href="#cb5-278" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-279"><a href="#cb5-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-280"><a href="#cb5-280" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-281"><a href="#cb5-281" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\sum_{j<span class="op">=</span><span class="dv">0</span>}<span class="op">^</span>{n<span class="op">-</span><span class="dv">1</span>}(\tilde{B}_{t_{j<span class="op">+</span><span class="dv">1</span>}}(\omega)<span class="op">-</span>\tilde{B}_{t_{j}}(\omega))<span class="op">^</span>{<span class="dv">2</span>} <span class="op">&amp;</span> <span class="op">=</span>\int_{<span class="dv">0</span>}<span class="op">^</span>{T}<span class="dv">2</span><span class="op">^</span>{<span class="dv">2</span>}\cdot ds<span class="op">=</span><span class="dv">4</span><span class="er">T</span></span>
<span id="cb5-282"><a href="#cb5-282" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-283"><a href="#cb5-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-284"><a href="#cb5-284" aria-hidden="true" tabindex="-1"></a>In other words, the distribution of the standard brownian motion on $[<span class="dv">0</span>,T]$ <span class="kw">is</span> supported on paths whose quadratic variation <span class="kw">is</span> $T$, whereas the distribution of $(\tilde{B}_{t},t\geq0)$ <span class="kw">is</span> supported on paths where the quadratic variation <span class="kw">is</span> $<span class="dv">4</span><span class="er">T</span>$. These paths are very different. We conclude that the distributions of the two processes are <span class="kw">not</span> equivalent. Hence, a change of probability <span class="im">from</span> $\mathbb{P}$ to $\mathbb{\tilde{P}}$ <span class="kw">is</span> <span class="kw">not</span> possible. In fact, we say that they are mutually singular, meaning the <span class="bu">set</span> of paths on which they are supported are disjoint.</span>
<span id="cb5-285"><a href="#cb5-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-286"><a href="#cb5-286" aria-hidden="true" tabindex="-1"></a><span class="op">*</span>Proof.<span class="op">*</span></span>
<span id="cb5-287"><a href="#cb5-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-288"><a href="#cb5-288" aria-hidden="true" tabindex="-1"></a>Let $(\tilde{B}_{t}:t\<span class="kw">in</span>[<span class="dv">0</span>,T])$ be a Brownian motion <span class="cf">with</span> constant drift $\theta$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. Thus, $\tilde{B}_{t}<span class="op">=</span>\theta t<span class="op">+</span>B_{t}$.</span>
<span id="cb5-289"><a href="#cb5-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-290"><a href="#cb5-290" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Claim<span class="op">**</span>. $\tilde{B}_{t}$ <span class="kw">is</span> a $\mathbb{\tilde{P}}$<span class="op">-</span>martingale.</span>
<span id="cb5-291"><a href="#cb5-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-292"><a href="#cb5-292" aria-hidden="true" tabindex="-1"></a>Let</span>
<span id="cb5-293"><a href="#cb5-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-294"><a href="#cb5-294" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-295"><a href="#cb5-295" aria-hidden="true" tabindex="-1"></a>M_{t} <span class="op">&amp;</span> <span class="op">=</span>f(t,B_{t})<span class="op">=</span>\exp(<span class="op">-</span>\theta B_{t}<span class="op">-</span>(\theta<span class="op">^</span>{<span class="dv">2</span>}<span class="op">/</span><span class="dv">2</span>)t)</span>
<span id="cb5-296"><a href="#cb5-296" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-297"><a href="#cb5-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-298"><a href="#cb5-298" aria-hidden="true" tabindex="-1"></a>So:</span>
<span id="cb5-299"><a href="#cb5-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-300"><a href="#cb5-300" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-301"><a href="#cb5-301" aria-hidden="true" tabindex="-1"></a>dM_{t} <span class="op">&amp;</span> <span class="op">=-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}M_{t}dt<span class="op">-</span>\theta M_{t}dB_{t}<span class="op">+</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}M(t)dt\<span class="op">\</span></span>
<span id="cb5-302"><a href="#cb5-302" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=-</span>\theta M_{t}dB_{t}</span>
<span id="cb5-303"><a href="#cb5-303" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-304"><a href="#cb5-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-305"><a href="#cb5-305" aria-hidden="true" tabindex="-1"></a>Consider the product $(M_{t}\tilde{B}_{t})$. We have:</span>
<span id="cb5-306"><a href="#cb5-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-307"><a href="#cb5-307" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-308"><a href="#cb5-308" aria-hidden="true" tabindex="-1"></a>d(M_{t}\tilde{B}_{t}) <span class="op">&amp;</span> <span class="op">=</span>\tilde{B}_{t}dM_{t}<span class="op">+</span>M_{t}d\tilde{B}_{t}<span class="op">+</span>dM_{t}\cdot d\tilde{B}_{t}\<span class="op">\</span></span>
<span id="cb5-309"><a href="#cb5-309" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=-</span>\tilde{B}_{t}\theta M_{t}dB_{t}<span class="op">+</span>M_{t}(\theta dt<span class="op">+</span>dB_{t})<span class="op">-</span>\theta M_{t}dB_{t}(\theta dt<span class="op">+</span>dB_{t})\<span class="op">\</span></span>
<span id="cb5-310"><a href="#cb5-310" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=-</span>\tilde{B}_{t}\theta M_{t}dB_{t}<span class="op">+</span>\theta M_{t}dt<span class="op">+</span>M_{t}dB_{t}<span class="op">-</span>\theta M_{t}dt\<span class="op">\</span></span>
<span id="cb5-311"><a href="#cb5-311" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>(<span class="op">-</span>\tilde{B}_{t}\theta<span class="op">+</span><span class="dv">1</span>)M_{t}dB_{t}</span>
<span id="cb5-312"><a href="#cb5-312" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-313"><a href="#cb5-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-314"><a href="#cb5-314" aria-hidden="true" tabindex="-1"></a>Thus, by the properties of Ito integral,$M_{t}\tilde{B}_{t}$ <span class="kw">is</span> a martingale under $\mathbb{P}$. By the abstract Bayes formula ([\[th:abstract<span class="op">-</span>bayes<span class="op">-</span>formula\]](<span class="co">#th:abstract-bayes-formula){reference-type="ref" reference="th:abstract-bayes-formula"}):</span></span>
<span id="cb5-315"><a href="#cb5-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-316"><a href="#cb5-316" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-317"><a href="#cb5-317" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}[\tilde{B}_{t}<span class="op">|</span>\mathcal{F}_{s}] <span class="op">&amp;</span> <span class="op">=</span>\frac{<span class="dv">1</span>}{M_{s}}\mathbb{E}[M_{t}\tilde{B}_{t}<span class="op">|</span>\mathcal{F}_{s}]\<span class="op">\</span></span>
<span id="cb5-318"><a href="#cb5-318" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\frac{<span class="dv">1</span>}{M_{s}}\cdot M_{s}\tilde{B}_{s}\<span class="op">\</span></span>
<span id="cb5-319"><a href="#cb5-319" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{B}_{s}</span>
<span id="cb5-320"><a href="#cb5-320" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-321"><a href="#cb5-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-322"><a href="#cb5-322" aria-hidden="true" tabindex="-1"></a>Thus, $\tilde{B}_{t}$ <span class="kw">is</span> a $\tilde{\mathbb{P}}$<span class="op">-</span>martingale.</span>
<span id="cb5-323"><a href="#cb5-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-324"><a href="#cb5-324" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Claim<span class="op">**</span>. Our claim <span class="kw">is</span> that under the $\tilde{\mathbb{P}}$ measure, $\tilde{B}_{t}\sim\mathcal{N}<span class="op">^</span>{\mathbb{\tilde{P}}}(<span class="dv">0</span>,t)$ <span class="kw">and</span> to do this we rely on the the moment<span class="op">-</span>generating function.</span>
<span id="cb5-325"><a href="#cb5-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-326"><a href="#cb5-326" aria-hidden="true" tabindex="-1"></a>By definition, <span class="cf">for</span> a constant $\Psi$:</span>
<span id="cb5-327"><a href="#cb5-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-328"><a href="#cb5-328" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-329"><a href="#cb5-329" aria-hidden="true" tabindex="-1"></a>M_{\tilde{B}_{t}}(\Psi) <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}\left[\exp\left(\Psi\tilde{B}_{t}\right)\right]\<span class="op">\</span></span>
<span id="cb5-330"><a href="#cb5-330" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[M_{T}\exp\left(\Psi\tilde{B}_{t}\right)\right]\<span class="op">\</span></span>
<span id="cb5-331"><a href="#cb5-331" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[\exp\left(<span class="op">-</span>\theta\tilde{B}_{T}<span class="op">+</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T<span class="op">+</span>\Psi\tilde{B}_{t}\right)\right]\<span class="op">\</span></span>
<span id="cb5-332"><a href="#cb5-332" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[\exp\left(<span class="op">-</span>\theta(\theta T<span class="op">+</span>B_{T})<span class="op">+</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T<span class="op">+</span>\Psi(\theta t<span class="op">+</span>B_{t})\right)\right]\<span class="op">\</span></span>
<span id="cb5-333"><a href="#cb5-333" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[\exp\left(<span class="op">-</span>\theta B_{T}<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T<span class="op">+</span>\Psi\theta t<span class="op">+</span>\Psi B_{t})\right)\right]\<span class="op">\</span></span>
<span id="cb5-334"><a href="#cb5-334" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\mathbb{E}\left[\exp\left(<span class="op">-</span>\theta(B_{T}<span class="op">-</span>B_{t})<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T<span class="op">+</span>\Psi\theta t<span class="op">+</span>(\Psi<span class="op">-</span>\theta)B_{t})\right)\right]\<span class="op">\</span></span>
<span id="cb5-335"><a href="#cb5-335" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\exp\left(<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T<span class="op">+</span>\Psi\theta t\right)\mathbb{E}\left(<span class="op">-</span>\theta(B_{T}<span class="op">-</span>B_{t})\right)\mathbb{E}\left((\Psi<span class="op">-</span>\theta)B_{t}\right)\<span class="op">\</span></span>
<span id="cb5-336"><a href="#cb5-336" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\exp\left(<span class="op">-</span>\frac{\theta<span class="op">^</span>{<span class="dv">2</span>}}{<span class="dv">2</span>}T<span class="op">+</span>\Psi\theta t\right)\exp\left[\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\theta<span class="op">^</span>{<span class="dv">2</span>}(T<span class="op">-</span>t)\right]\exp\left[\frac{<span class="dv">1</span>}{<span class="dv">2</span>}(\Psi<span class="op">-</span>\theta)<span class="op">^</span>{<span class="dv">2</span>}t\right]\<span class="op">\</span></span>
<span id="cb5-337"><a href="#cb5-337" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\exp\left[<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\left(\theta<span class="op">^</span>{<span class="dv">2</span>}<span class="op">-</span><span class="dv">2</span>\Psi\theta<span class="op">-</span>(\Psi<span class="op">-</span>\theta)<span class="op">^</span>{<span class="dv">2</span>}\right)t\right]\<span class="op">\</span></span>
<span id="cb5-338"><a href="#cb5-338" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\exp\left[<span class="op">-</span>\frac{<span class="dv">1</span>}{<span class="dv">2</span>}\left(\theta<span class="op">^</span>{<span class="dv">2</span>}<span class="op">-</span><span class="dv">2</span>\Psi\theta<span class="op">-</span>(\Psi<span class="op">^</span>{<span class="dv">2</span>}<span class="op">-</span><span class="dv">2</span>\Psi\theta<span class="op">+</span>\theta<span class="op">^</span>{<span class="dv">2</span>}\right)t\right]\<span class="op">\</span></span>
<span id="cb5-339"><a href="#cb5-339" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\exp(<span class="op">-</span>\Psi<span class="op">^</span>{<span class="dv">2</span>}t)</span>
<span id="cb5-340"><a href="#cb5-340" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-341"><a href="#cb5-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-342"><a href="#cb5-342" aria-hidden="true" tabindex="-1"></a>Thus, $\tilde{B}_{t}\sim\mathcal{N}<span class="op">^</span>{\tilde{\mathbb{P}}}(<span class="dv">0</span>,t)$.</span>
<span id="cb5-343"><a href="#cb5-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-344"><a href="#cb5-344" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Claim<span class="op">**</span>. Finally, to show that $\tilde{B}_{t}$ <span class="kw">is</span> indeed a $\mathbb{\tilde{P}}<span class="op">-</span>$standard brownian motion, we have the following:</span>
<span id="cb5-345"><a href="#cb5-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-346"><a href="#cb5-346" aria-hidden="true" tabindex="-1"></a>\(a\) $\tilde{B}_{<span class="dv">0</span>}<span class="op">=</span>\theta(<span class="dv">0</span>)<span class="op">+</span>B_{<span class="dv">0</span>}<span class="op">=</span><span class="dv">0</span>$ <span class="kw">and</span> $\tilde{B}_{t}$ has almost surely continuous paths.</span>
<span id="cb5-347"><a href="#cb5-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-348"><a href="#cb5-348" aria-hidden="true" tabindex="-1"></a>\(b\) We would like to prove that, <span class="cf">for</span> $s<span class="op">&lt;</span>t$, $\tilde{B}_{t}<span class="op">-</span>\tilde{B}_{s}\sim\mathcal{N}<span class="op">^</span>{\tilde{\mathbb{P}}}(<span class="dv">0</span>,t<span class="op">-</span>s)$. We have:</span>
<span id="cb5-349"><a href="#cb5-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-350"><a href="#cb5-350" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-351"><a href="#cb5-351" aria-hidden="true" tabindex="-1"></a>\mathbb{E}[\tilde{B}_{t}<span class="op">-</span>\tilde{B}_{s}] <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[\tilde{B}_{t}]<span class="op">-</span>\tilde{\mathbb{E}}[B_{s}]\<span class="op">\</span></span>
<span id="cb5-352"><a href="#cb5-352" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span><span class="dv">0</span></span>
<span id="cb5-353"><a href="#cb5-353" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-354"><a href="#cb5-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-355"><a href="#cb5-355" aria-hidden="true" tabindex="-1"></a>And,</span>
<span id="cb5-356"><a href="#cb5-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-357"><a href="#cb5-357" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-358"><a href="#cb5-358" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}[(\tilde{B}_{t}<span class="op">-</span>\tilde{B}_{s})<span class="op">^</span>{<span class="dv">2</span>}] <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[\tilde{B}_{t}<span class="op">^</span>{<span class="dv">2</span>}<span class="op">-</span><span class="dv">2</span>\tilde{B}_{t}\tilde{B}_{s}<span class="op">+</span>\tilde{B}_{s}<span class="op">^</span>{<span class="dv">2</span>}]\<span class="op">\</span></span>
<span id="cb5-359"><a href="#cb5-359" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[B_{t}<span class="op">^</span>{<span class="dv">2</span>}]<span class="op">-</span><span class="dv">2</span>\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}]<span class="op">+</span>\tilde{\mathbb{E}}[\tilde{B}_{s}<span class="op">^</span>{<span class="dv">2</span>}]\<span class="op">\</span></span>
<span id="cb5-360"><a href="#cb5-360" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>t<span class="op">+</span>s<span class="op">-</span><span class="dv">2</span>\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}]</span>
<span id="cb5-361"><a href="#cb5-361" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-362"><a href="#cb5-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-363"><a href="#cb5-363" aria-hidden="true" tabindex="-1"></a>\(c\) The non<span class="op">-</span>overlapping increments of a $\tilde{\mathbb{P}}$<span class="op">-</span>martingale are independent. To see this, suppose $t_{<span class="dv">1</span>}\leq t_{<span class="dv">2</span>}\leq t_{<span class="dv">3</span>}$:</span>
<span id="cb5-364"><a href="#cb5-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-365"><a href="#cb5-365" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-366"><a href="#cb5-366" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}[(B_{t_{<span class="dv">3</span>}}<span class="op">-</span>B_{t_{<span class="dv">2</span>}})(B_{t_{<span class="dv">2</span>}}<span class="op">-</span>B_{t_{<span class="dv">1</span>}})] <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[\tilde{\mathbb{E}}[(B_{t_{<span class="dv">3</span>}}<span class="op">-</span>B_{t_{<span class="dv">2</span>}})(B_{t_{<span class="dv">2</span>}}<span class="op">-</span>B_{t_{<span class="dv">1</span>}})<span class="op">|</span>\mathcal{F}_{t_{<span class="dv">2</span>}}]]\<span class="op">\</span></span>
<span id="cb5-367"><a href="#cb5-367" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[(B_{t_{<span class="dv">2</span>}}<span class="op">-</span>B_{t_{<span class="dv">1</span>}})\tilde{\mathbb{E}}[(B_{t_{<span class="dv">3</span>}}<span class="op">-</span>B_{t_{<span class="dv">2</span>}})<span class="op">|</span>\mathcal{F}_{t_{<span class="dv">2</span>}}]]\<span class="op">\</span></span>
<span id="cb5-368"><a href="#cb5-368" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[(B_{t_{<span class="dv">2</span>}}<span class="op">-</span>B_{t_{<span class="dv">1</span>}})(B_{t_{<span class="dv">2</span>}}<span class="op">-</span>B_{t_{<span class="dv">2</span>}})]]<span class="op">=</span><span class="dv">0</span></span>
<span id="cb5-369"><a href="#cb5-369" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-370"><a href="#cb5-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-371"><a href="#cb5-371" aria-hidden="true" tabindex="-1"></a>Also, the covariance</span>
<span id="cb5-372"><a href="#cb5-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-373"><a href="#cb5-373" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb5-374"><a href="#cb5-374" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}] <span class="op">&amp;</span> <span class="op">=</span>\tilde{\mathbb{E}}[(\tilde{B}_{t}<span class="op">-</span>\tilde{B}_{s})\tilde{B}_{s}]<span class="op">+</span>\tilde{\mathbb{E}}[\tilde{B}_{s}<span class="op">^</span>{<span class="dv">2</span>}]\<span class="op">\</span></span>
<span id="cb5-375"><a href="#cb5-375" aria-hidden="true" tabindex="-1"></a> <span class="op">&amp;</span> <span class="op">=</span><span class="dv">0</span><span class="op">+</span>s</span>
<span id="cb5-376"><a href="#cb5-376" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb5-377"><a href="#cb5-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-378"><a href="#cb5-378" aria-hidden="true" tabindex="-1"></a>So, $\mathbb{E}[(\tilde{B}_{t}<span class="op">-</span>\tilde{B}_{s})<span class="op">^</span>{<span class="dv">2</span>}]<span class="op">=</span>t<span class="op">+</span>s<span class="op">-</span><span class="dv">2</span><span class="er">s</span><span class="op">=</span>t<span class="op">-</span>s$.</span>
<span id="cb5-379"><a href="#cb5-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-380"><a href="#cb5-380" aria-hidden="true" tabindex="-1"></a>Consequently, $\tilde{B}_{t}$ <span class="kw">is</span> a $\tilde{\mathbb{P}}$<span class="op">-</span>standard brownian motion.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-12-01">

<title>A gentle introduction to the Girsanov Theorem - Back to the basics – quantdev.blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-f2a1071e85750ec973bbb8a8f120da0f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-e2cfcb35d2364ed86e6f554fe2526fa3.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">A gentle introduction to the Girsanov Theorem - Back to the basics</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Stochastic Calculus</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 1, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#change-of-probability." id="toc-change-of-probability." class="nav-link" data-scroll-target="#change-of-probability.">Change of Probability.</a>
  <ul class="collapse">
  <li><a href="#change-of-probability-for-a-random-variable." id="toc-change-of-probability-for-a-random-variable." class="nav-link" data-scroll-target="#change-of-probability-for-a-random-variable.">Change of Probability for a Random Variable.</a></li>
  <li><a href="#the-cameron-martin-theorem." id="toc-the-cameron-martin-theorem." class="nav-link" data-scroll-target="#the-cameron-martin-theorem.">The Cameron-Martin Theorem.</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>One of the most popular technical tools in financial engineering is the change of numeraire. In this blog-post, I intend to provide the dear reader a beginner-friendly introduction and an intuitive gut feel for these tools.</p>
<p>You take a convenient asset e.g.&nbsp;a share of stock or a bond as the <em>numeraire</em>, as if it were a medium of exchange, and express the prices of other assets and option prices in units of this numeraire. When pricing derivatives analytically, switching numeraires is done for computational ease.</p>
<p>The technique was used by <a href="https://en.wikipedia.org/wiki/H%C3%A9lyette_Geman">Heylette Geman</a>, <a href="https://en.wikipedia.org/wiki/Nicole_El_Karoui">Nicole El Karoui</a> and <a href="">Jean-Charles Rochet</a> in their seminal note <a href="https://www.cambridge.org/core/journals/journal-of-applied-probability/article/abs/changes-of-numeraire-changes-of-probability-measure-and-option-pricing/EA730D6C18D56426D491B6A25563C0B3">Changes of Numeraire, Changes of Probability Measure and Option Pricing</a>.</p>
</section>
<section id="change-of-probability." class="level1">
<h1>Change of Probability.</h1>
<section id="change-of-probability-for-a-random-variable." class="level2">
<h2 class="anchored" data-anchor-id="change-of-probability-for-a-random-variable.">Change of Probability for a Random Variable.</h2>
<p>Consider a random variable <span class="math inline">\(X\)</span> defined on a sample space <span class="math inline">\(\Omega\)</span> having zero mean. We want to change the mean of <span class="math inline">\(X\)</span> so that <span class="math inline">\(\mu\neq 0\)</span>. Of course, it is easy to change the mean of a random variable: If <span class="math inline">\(X\)</span> has mean <span class="math inline">\(0\)</span>, then the random variable <span class="math inline">\(X+\mu\)</span> has mean <span class="math inline">\(\mu\)</span>. However, it might be that the variable <span class="math inline">\(X+\mu\)</span> does not share the same possible values as <span class="math inline">\(X\)</span>. For example, take <span class="math inline">\(X\)</span> to be a uniform random variable on <span class="math inline">\([-1,1]\)</span>. While <span class="math inline">\(X+1\)</span> has mean <span class="math inline">\(1\)</span>, the density of <span class="math inline">\(X+1\)</span> would be non-zero on <span class="math inline">\([0,2]\)</span> instead of <span class="math inline">\([-1,1]\)</span>.</p>
<p>Our goal is to find a good way to change the underlying probability <span class="math inline">\(\mathbb{P}\)</span>, and thus the distribution of <span class="math inline">\(X\)</span>, so that the set of outcomes is unchanged. If <span class="math inline">\(X\)</span> is a discrete random variable, say with <span class="math inline">\(\mathbb{P}(X=-1)=\mathbb{P}(X=1)=1/2\)</span>, we can change the probability in order to change the mean easily. It suffices to take <span class="math inline">\(\tilde{\mathbb{P}}\)</span> so that <span class="math inline">\(\tilde{\mathbb{P}}(X=1)=p\)</span> and <span class="math inline">\(\mathbb{P}(X=-1)=1-p\)</span> for some appropriate <span class="math inline">\(0\leq p\leq1\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is a continuous random variable, with a PDF <span class="math inline">\(f_{X}\)</span>, the probabilities can be changed by modifying the PDF. Consider the a new PDF:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{f}_{X}(x) &amp; =f_{X}(x)g(x)
\end{aligned}\]</span></p>
<p>for some function <span class="math inline">\(g(x)&gt;0\)</span> such that <span class="math inline">\(\int f(x)g(x)dx=1\)</span>. Clearly, <span class="math inline">\(f_{X}(x)g(x)\)</span> is also a PDF and <span class="math inline">\(f_{X}(x)&gt;0\)</span> if and only if <span class="math inline">\(f_{X}(x)g(x)&gt;0\)</span>, so that the possible values of <span class="math inline">\(X\)</span> are unchanged. A convenient (and important!) choice of function <span class="math inline">\(g\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
g(x) &amp; =\frac{e^{ax}}{\int_{\mathbf{R}}e^{ax}f_{X}(x)dx}=\frac{e^{ax}}{\mathbb{E}[e^{aX}]},\quad a\in\mathbf{R}\label{eq:change-of-probability-of-an-rv}
\end{aligned}\]</span></p>
<p>assuming <span class="math inline">\(X\)</span> has a well-defined MGF. Here <span class="math inline">\(a\)</span> is a parameter that can be tuned to fit to a specific mean. The normalization factor in the denominator is the MGF of <span class="math inline">\(X\)</span>. It ensures that <span class="math inline">\(f_{X}(x)g(x)\)</span> is a PDF. Note that if <span class="math inline">\(a&gt;0\)</span>, the function <span class="math inline">\(g\)</span> gives a bigger weight to large values of <span class="math inline">\(X\)</span>. We say that <span class="math inline">\(g\)</span> is biased towards the large values.</p>
<div id="exm-biasing-a-uniform-random-variable" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Biasing a uniform random variable)</strong></span> Let <span class="math inline">\(X\)</span> be a uniform random variable on <span class="math inline">\([0,1]\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Clearly, <span class="math inline">\(\mathbb{E}[X]=1/2\)</span>. How can we change the PDF of <span class="math inline">\(X\)</span> so that the possible values are still <span class="math inline">\([0,1]\)</span>, but the mean is <span class="math inline">\(1/4\)</span>. We have that the PDF is <span class="math inline">\(f_{X}(x)=1\)</span> if <span class="math inline">\(x\in[0,1]\)</span> and <span class="math inline">\(0\)</span> elsewhere. Therefore, the mean with the new PDF with parameter <span class="math inline">\(a\)</span> as in the equation (<a href="#eq:change-of-probability-of-an-rv" data-reference-type="ref" data-reference="eq:change-of-probability-of-an-rv">[eq:change-of-probability-of-an-rv]</a>) is:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[X] &amp; =\int_{0}^{1}x\tilde{f}(x)dx\\
&amp; =\int_{0}^{1}\frac{xe^{ax}}{\mathbb{E}[e^{aX}]}dx\\
&amp; =\frac{a}{e^{a}-1}\int_{0}^{1}xe^{ax}dx\\
&amp; =\frac{a}{e^{a}-1}\left(\left[x\frac{e^{ax}}{a}\right]_{0}^{1}-\frac{1}{a}\int_{0}^{1}e^{ax}dx\right)\\
&amp; =\frac{a}{e^{a}-1}\left(\frac{e^{a}}{a}-\frac{1}{a}\frac{e^{a}-1}{a}\right)\\
&amp; =\frac{e^{a}}{e^{a}-1}-\frac{1}{a}
\end{aligned}\]</span></p>
<p>For <span class="math inline">\(\tilde{\mathbb{E}[X]}\)</span>to be equal to <span class="math inline">\(1/4\)</span>, we get numerically <span class="math inline">\(a\approx-3.6\)</span>. Note that the possible values of <span class="math inline">\(X\)</span> remain the same under the new probability. However, the new distribution is no longer uniform! It has bias towards values closer to zero, as it should.</p>
</div>
<div id="exm-biasing-a-gaussian-random-variable" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Biasing a Gaussian random variable.)</strong></span> Let <span class="math inline">\(X\)</span> be a Gaussian random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>. How can we change the PDF of <span class="math inline">\(X\)</span> to have mean <span class="math inline">\(0\)</span>? Going back to (<a href="#eq:change-of-probability-of-an-rv" data-reference-type="ref" data-reference="eq:change-of-probability-of-an-rv">[eq:change-of-probability-of-an-rv]</a>), the mean <span class="math inline">\(\mu\)</span> under the new PDF with parameter <span class="math inline">\(a\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[X] &amp; =\int_{-\infty}^{\infty}x\tilde{f}(x)dx\\
&amp; =\int_{-\infty}^{\infty}xg(x)f(x)dx\\
&amp; =\int_{-\infty}^{\infty}x\cdot\frac{e^{ax}}{\mathbb{E}[e^{aX}]}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}dx\\
&amp; =\frac{1}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\cdot\exp\left[-\frac{1}{2}\left(\frac{x^{2}-2\mu x+\mu^{2}-2a\sigma^{2}x}{\sigma^{2}}\right)\right]dx\\
&amp; =\frac{1}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\cdot\exp\left[-\frac{1}{2}\left(\frac{x^{2}-2(\mu+a\sigma^{2})x+(\mu+a\sigma^{2})^{2}-2\mu a\sigma^{2}-a^{2}\sigma^{4}}{\sigma^{2}}\right)\right]dx\\
&amp; =\frac{e^{\mu a+a^{2}\sigma^{2}/2}}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left[-\frac{1}{2}\left(\frac{x-(\mu+a\sigma^{2})}{\sigma}\right)^{2}\right]dx\\
&amp; =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left[-\frac{1}{2}\left(\frac{x-(\mu+a\sigma^{2})}{\sigma}\right)^{2}\right]dx
\end{aligned}\]</span></p>
<p>For the specific choice of the parameter <span class="math inline">\(a=\mu/\sigma^{2}\)</span>, we recover the PDF of a Gaussian random variable with mean <span class="math inline">\(0\)</span>. But, we can deduce more. The new PDF is also Gaussian. This was not the case for uniform random variables. In fact, the new PDF is exactly the same as the one of <span class="math inline">\(X-\mu\)</span>. For if, <span class="math inline">\(a=\mu/\sigma^{2}\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[X] &amp; =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left[-\frac{x^{2}}{2\sigma^{2}}\right]dx
\end{aligned}\]</span></p>
<p>and observe that if <span class="math inline">\(Y=X-\mu\)</span>, then:</p>
<p><span class="math display">\[\begin{aligned}
F_{Y}(x) &amp; =\mathbb{P}(X-\mu&lt;x)\\
&amp; =\mathbb{P}(X\leq x+\mu)\\
&amp; =F_{X}(x+\mu)\\
\frac{d}{dx}(F_{Y}(x)) &amp; =\frac{d}{dx}(F_{X}(x+\mu))\\
f_{Y}(x) &amp; =f_{X}(x+\mu)\cdot\frac{d}{dx}(x+\mu)\\
f_{Y}(x) &amp; =f_{X}(x+\mu)\\
&amp; =\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{1}{2}\left(\frac{x+\mu-\mu}{\sigma}\right)^{2}\right]\\
&amp; =\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{x^{2}}{2\sigma^{2}}\right]
\end{aligned}\]</span></p>
<p>In other words:</p>
<p><em>For Gaussians, changing the mean by recentering is equivalent to changing the probability as in</em> (<a href="#eq:change-of-probability-of-an-rv" data-reference-type="ref" data-reference="eq:change-of-probability-of-an-rv">[eq:change-of-probability-of-an-rv]</a>).</p>
<p>This is a very special property of the Gaussian distribution. The exponential and Poisson distributions have a similar property.</p>
<p>Example (<a href="#ex:biasing-a-gaussian-random-variable" data-reference-type="ref" data-reference="ex:biasing-a-gaussian-random-variable">[ex:biasing-a-gaussian-random-variable]</a>) is very important and we will state it as a theorem. Before doing so, we notice that the change of PDF (<a href="#eq:change-of-probability-of-an-rv" data-reference-type="ref" data-reference="eq:change-of-probability-of-an-rv">[eq:change-of-probability-of-an-rv]</a>) can be expressed more generally by changing the underlying probability measure(length, area, weights) <span class="math inline">\(\mathbb{P}\)</span> on the sample space <span class="math inline">\(\Omega\)</span> on which the random variables are defined. More precisely, let <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> be a probability space, and let <span class="math inline">\(X\)</span> be a random variable defined on <span class="math inline">\(\Omega\)</span>. We define a new probability <span class="math inline">\(\tilde{\mathbb{P}}\)</span> on <span class="math inline">\(\Omega\)</span> as follows:</p>
<p>If <span class="math inline">\(\mathcal{E}\)</span> is an event in <span class="math inline">\(\mathcal{F}\)</span>, then:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{\tilde{E}}[1_{\mathcal{E}}]=\int_{\mathbf{R}}1_{\mathcal{E}}\cdot\tilde{f}(x)dx\nonumber \\
&amp; =\int_{\mathbf{R}}1_{\mathcal{E}}\cdot g(x)f_{X}(x)dx\nonumber \\
&amp; =\int_{\mathbf{R}}1_{\mathcal{E}}\cdot\frac{e^{ax}}{\mathbb{E}[e^{aX}]}f_{X}(x)dx\nonumber \\
&amp; =\mathbb{E}\left[1_{\mathcal{E}}\frac{e^{aX}}{\mathbb{E}[e^{aX}]}\right]
\end{aligned}\]</span></p>
<p>Intuitively, we are changing the probability of each outcome <span class="math inline">\(\omega\in\mathcal{E}\)</span>, by the factor</p>
<p><span class="math display">\[\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\label{eq:girsanov-probability-scaling}\]</span></p>
<p>In other words, if <span class="math inline">\(a&gt;0\)</span>, the outcomes <span class="math inline">\(\omega\)</span> for which <span class="math inline">\(X\)</span> has large values are favored. Note that equation (<a href="#eq:change-of-probability-of-an-rv" data-reference-type="ref" data-reference="eq:change-of-probability-of-an-rv">[eq:change-of-probability-of-an-rv]</a>) for the PDF is recovered, since for any function <span class="math inline">\(h\)</span> of <span class="math inline">\(X\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[h(X)] &amp; =\mathbb{E}\left[\frac{e^{aX}}{\mathbb{E}[e^{aX}]}h(X)\right]\\
&amp; =\int_{\mathbf{R}}h(x)\frac{e^{ax}}{\mathbb{E}[e^{aX}]}f_{X}(x)dx
\end{aligned}\]</span></p>
<p>In this setting, the above example becomes the preliminary version of the Cameron-Martin-Girsanov theorem:</p>
</div>
<div class="thm">
<p><span id="th:change-of-probability-for-a-random-variable" data-label="th:change-of-probability-for-a-random-variable"></span>Let <span class="math inline">\(X\)</span> be a Gaussian random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Then, under the probability <span class="math inline">\(\mathbb{\tilde{P}}\)</span> given by:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{E}\left[1_{\mathcal{E}}e^{-\frac{\mu}{\sigma^{2}}X+\frac{1}{2}\frac{\mu^{2}}{\sigma^{2}}}\right],\quad\mathcal{E}\in\mathcal{F}\label{eq:preliminary-version-girsanov-theorem}
\end{aligned}\]</span></p>
<p>the random variable <span class="math inline">\(X\)</span> is Gaussian with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>.</p>
<p>Moreover, since <span class="math inline">\(X\)</span> can be written as <span class="math inline">\(X=Y+\mu\)</span> where <span class="math inline">\(Y\)</span> is Gaussian with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> under <span class="math inline">\(\mathbb{P}\)</span>, we have that <span class="math inline">\(\mathbb{\tilde{P}}\)</span> can be written as:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{E}\left[1_{\mathcal{E}}e^{-\frac{\mu}{\sigma^{2}}Y-\frac{1}{2}\frac{\mu^{2}}{\sigma^{2}}}\right],\quad\mathcal{E}\in\mathcal{F}\label{eq:preliminary-version-girsanov-theorem-II}
\end{aligned}\]</span></p>
</div>
<p>It is good to pause for a second and look at the signs in the exponential of equations (<a href="#eq:preliminary-version-girsanov-theorem" data-reference-type="ref" data-reference="eq:preliminary-version-girsanov-theorem">[eq:preliminary-version-girsanov-theorem]</a>) and (<a href="#eq:preliminary-version-girsanov-theorem-II" data-reference-type="ref" data-reference="eq:preliminary-version-girsanov-theorem-II">[eq:preliminary-version-girsanov-theorem-II]</a>). The signs in the exponential might be very confusing and is the source of many mistakes in the Cameron-Martin-Girsanov theorem. A good trick is to say that, if we want to remove <span class="math inline">\(\mu\)</span>, then the sign in front of <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> must be negative. Then, we add the exponential factor needed for <span class="math inline">\(\tilde{\mathbb{P}}\)</span> to be a probability. This is given by the MGF of <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> depending on how we want to express it.</p>
<p>The probabilities <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span>, as defined in the equation (<a href="#eq:preliminary-version-girsanov-theorem" data-reference-type="ref" data-reference="eq:preliminary-version-girsanov-theorem">[eq:preliminary-version-girsanov-theorem]</a>) are obviously not equal since they differ by a factor in (<a href="#eq:girsanov-probability-scaling" data-reference-type="ref" data-reference="eq:girsanov-probability-scaling">[eq:girsanov-probability-scaling]</a>). However, they share some similarities. Most notably, if <span class="math inline">\(\mathcal{E}\)</span> is an event of positive <span class="math inline">\(\mathbb{P}\)</span>-probability, <span class="math inline">\(\mathbb{P}(\mathcal{E})&gt;0\)</span>, then we must have <span class="math inline">\(\tilde{\mathbb{P}}(\mathcal{E})&gt;0\)</span>, since the factor in (<a href="#eq:girsanov-probability-scaling" data-reference-type="ref" data-reference="eq:girsanov-probability-scaling">[eq:girsanov-probability-scaling]</a>) is always strictly positive. The converse is also true: if <span class="math inline">\(\mathcal{E}\)</span> is an event of positive <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-probability, <span class="math inline">\(\tilde{\mathbb{P}}(\mathcal{E})&gt;0\)</span>, then we must have that <span class="math inline">\(\mathbb{P}(\mathcal{E})&gt;0\)</span>. This is because the factor in (<a href="#eq:girsanov-probability-scaling" data-reference-type="ref" data-reference="eq:girsanov-probability-scaling">[eq:girsanov-probability-scaling]</a>) can be inverted, being strictly positive. More precisely, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(\mathcal{E}) &amp; =\mathbb{E}[1_{\mathcal{E}}]\\
&amp; =\mathbb{E}\left[1_{\mathcal{E}}\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\left(\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\right)^{-1}\right]\\
&amp; =\tilde{\mathbb{E}}\left[1_{\mathcal{E}}\left(\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\right)^{-1}\right]
\end{aligned}\]</span></p>
<p>The factor <span class="math inline">\(\left(\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\right)^{-1}\)</span> is also strictly positive, proving the claim. To sum it all up, the probabilities <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span> essentially share the same possible outcomes. Such probability measures are said to be equivalent measures.</p>
<div class="defn">
<p>Consider the two probabilities <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span> on <span class="math inline">\((\Omega,\mathcal{F})\)</span>. They are said to be equivalent, if for any event <span class="math inline">\(\mathcal{E}\in\mathcal{F}\)</span>, we have <span class="math inline">\(\mathbb{P}(\mathcal{E})&gt;0\)</span> if and only if <span class="math inline">\(\mathbb{P}(\mathcal{E})&gt;0\)</span>. Thus, <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span> agree on the null sets. If <span class="math inline">\(A\in\mathcal{F}\)</span> is such that <span class="math inline">\(\mathbb{P}(A)=0\)</span>, then <span class="math inline">\(\mathbb{\tilde{P}}(A)=0\)</span> and vice-versa.</p>
</div>
<p>Keep in mind that two probabilities that are equivalent might still be very far from being equal!</p>
</section>
<section id="the-cameron-martin-theorem." class="level2">
<h2 class="anchored" data-anchor-id="the-cameron-martin-theorem.">The Cameron-Martin Theorem.</h2>
<div class="thm">
<p><span id="th:girsanov-theorem-for-constant-drift-case" data-label="th:girsanov-theorem-for-constant-drift-case"></span>(Cameron-Martin Theorem for constant drift). Let <span class="math inline">\((\tilde{B(t)},t\in[0,T])\)</span> be a <span class="math inline">\(\mathbb{P}-\)</span>Brownian motion with constant drift <span class="math inline">\(\theta\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Consider the probability <span class="math inline">\(\tilde{\mathbb{P}}\)</span>on <span class="math inline">\(\Omega\)</span> given by:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{P}}(\mathcal{E}) &amp; =\mathbb{E}\left[e^{-\theta\tilde{B}(T)+\frac{\theta^{2}}{2}T}1_{\mathcal{E}}\right],\quad\mathcal{E}\in\mathcal{F}\label{eq:girsanov-theorem-constant-drift-case}
\end{aligned}\]</span></p>
<p>Then, the process <span class="math inline">\((\tilde{B}(t),t\in[0,T])\)</span> under <span class="math inline">\(\mathbb{\tilde{P}}\)</span>is distributed like a standard brownian motion. Moreover, since we can write <span class="math inline">\(\tilde{B_{t}}=\theta t+B_{t}\)</span> for some standard brownian motion <span class="math inline">\((B_{t},t\in[0,T])\)</span> on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>, the probability <span class="math inline">\(\tilde{\mathbb{P}}\)</span> can also be written as:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{P}}(\mathcal{E}) &amp; =\mathbb{E}\left[e^{-\theta B(T)-\frac{\theta^{2}}{2}T}1_{\mathcal{E}}\right]\label{eq:girsanov-theorem-constant-drift-case-II}
\end{aligned}\]</span></p>
</div>
<p>It is a good idea to pause again and look at the signs in the exponential in equations (<a href="#eq:girsanov-theorem-constant-drift-case" data-reference-type="ref" data-reference="eq:girsanov-theorem-constant-drift-case">[eq:girsanov-theorem-constant-drift-case]</a>) and (<a href="#eq:girsanov-theorem-constant-drift-case-II" data-reference-type="ref" data-reference="eq:girsanov-theorem-constant-drift-case-II">[eq:girsanov-theorem-constant-drift-case-II]</a>). They behave the same way as in theorem (<a href="#th:change-of-probability-for-a-random-variable" data-reference-type="ref" data-reference="th:change-of-probability-for-a-random-variable">[th:change-of-probability-for-a-random-variable]</a>). There is a minus sign in front of <span class="math inline">\(B_{T}\)</span> to remove the drift. Before proving the theorem, we make some important remarks.</p>
<p>(1) <strong>The end-point</strong>. Note that only the endpoint <span class="math inline">\(\tilde{B}(T)\)</span> of the Brownian motion is involved in the change of probability. In particular, <span class="math inline">\(T\)</span> cannot be <span class="math inline">\(+\infty\)</span>. The Cameron-Martin theorem can only be applied on a finite interval.</p>
<p>(2) <strong>A martingale.</strong> The factor <span class="math inline">\(M_{T}=e^{-\theta B(T)-\frac{\theta^{2}}{2}T}=e^{-\theta\tilde{B}(T)+\frac{1}{2}\theta^{2}T}\)</span> involved in the change of probability is the end-point of a <span class="math inline">\(\mathbb{P}-\)</span>martingale, that is, it is a martingale under the original probability <span class="math inline">\(\mathbb{P}\)</span>. To see this:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[M_{T}|\mathcal{F}_{t}] &amp; =\mathbb{E}\left[e^{-\theta B(T)-\frac{1}{2}\theta^{2}T}|\mathcal{F}_{t}\right]\\
&amp; =e^{-\theta B(t)}\mathbb{E}\left[e^{-\theta(B(T)-B(t))}|\mathcal{F}_{t}\right]e^{-\frac{\theta^{2}}{2}T}\\
&amp; \{\text{Using }B(T)-B(t)\perp\mathcal{F}_{t}\}\\
&amp; =e^{-\theta B(t)}\mathbb{E}\left[e^{-\theta(B(T)-B(t))}\right]e^{-\frac{\theta^{2}}{2}T}\\
&amp; =e^{-\theta B(t)}e^{\frac{\theta^{2}}{2}(T-t)}e^{-\frac{\theta^{2}}{2}T}\\
&amp; =e^{-\theta B(t)-\frac{\theta^{2}}{2}t}
\end{aligned}\]</span></p>
<p>In fact, since <span class="math inline">\(B(t)\)</span> is a <span class="math inline">\(\mathbb{P}\)</span>-standard Brownian motion, <span class="math inline">\(M(t)=e^{-\theta B(t)-\frac{\theta^{2}}{2}t}\)</span> is a geometric brownian motion.</p>
<p>Interestingly, the drift of <span class="math inline">\(\tilde{B}(t)\)</span> becomes the volatility factor in <span class="math inline">\(M_{T}\)</span>! <span class="math inline">\(\mathbb{E}[M_{T}^{2}]=\mathbb{E}[e^{-2\theta B(T)-\theta^{2}T}]=e^{-\theta^{2}T}\cdot\mathbb{E}[e^{-2\theta B(T)}]=e^{-\theta^{2}T}\cdot e^{2\theta^{2}T}=e^{\theta^{2}T}\)</span>.</p>
<p>The fact that <span class="math inline">\(M(t)\)</span> is a martingale is very helpful in calculations. Indeed, suppose we want to compute the expectation of a function <span class="math inline">\(F(\tilde{B}(s))\)</span> of a Brownian motion with drift at time <span class="math inline">\(s&lt;T\)</span>. Then, we have by theorem (<a href="#th:girsanov-theorem-for-constant-drift-case" data-reference-type="ref" data-reference="th:girsanov-theorem-for-constant-drift-case">[th:girsanov-theorem-for-constant-drift-case]</a>):</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[F(\tilde{B}(s))] &amp; =\mathbb{E}[M_{T}M_{T}^{-1}F(\tilde{B}(s))]\\
&amp; =\tilde{\mathbb{E}}[M_{T}^{-1}F(\tilde{B}(s))]\\
&amp; =\tilde{\mathbb{E}}[e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))]
\end{aligned}\]</span></p>
<p>Now, we know that under <span class="math inline">\(\tilde{\mathbb{P}}\)</span>probability, <span class="math inline">\((\tilde{B}(t),t\in[0,T])\)</span> is a standard brownian motion, or <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-standard brownian motion for short. Therefore, the process <span class="math inline">\(e^{\theta\tilde{B}(t)-\frac{1}{2}\theta^{2}t}\)</span> is a martingale under the new probability measure <span class="math inline">\(\tilde{\mathbb{P}}\)</span>, or a <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-martingale for short. By conditioning over <span class="math inline">\(\mathcal{F}_{s}\)</span> and applying the martingale property, we get:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[F(\tilde{B}_{s})\right] &amp; =\tilde{\mathbb{E}}[e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))]\\
&amp; =\tilde{\mathbb{E}}[\tilde{\mathbb{E}}[e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))|\mathcal{F}_{s}]]\\
&amp; =\tilde{\mathbb{E}}[e^{\theta\tilde{B}(s)-\frac{1}{2}\theta^{2}s}F(\tilde{B}(s))]\\
&amp; =\mathbb{E}[e^{\theta B(s)-\frac{1}{2}\theta^{2}s}F(B(s))]
\end{aligned}\]</span></p>
<p>The last equality may seem wrong as removed all the tildes. It is not! It holds because <span class="math inline">\((\tilde{B}(t))\)</span> under <span class="math inline">\(\tilde{\mathbb{P}}\)</span> has the same distribution as <span class="math inline">\((B(t))\)</span> under <span class="math inline">\(\mathbb{P}\)</span>: a standard brownian motion. Of course, it would be possible to directly evaluate <span class="math inline">\(\mathbb{E}[F(\tilde{B}(s))]\)</span> here as we know the distribution of a Brownian motion with drift. However, when the function will involve more than one point (such as the maximum of the path), the Cameron-Martin theorem is a powerful tool to evaluate expectations.</p>
<p>(3) <strong>The paths with or without the drift are the same.</strong> Let <span class="math inline">\((B(t),t\leq T)\)</span> be a standard Brownian motion defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Heuristically, it is fruitful to think of the sample space of <span class="math inline">\(\Omega\)</span> as the different continuous paths of Brownian motion. Since, the change of probability from <span class="math inline">\(\mathbb{P}\)</span> to <span class="math inline">\(\tilde{\mathbb{P}}\)</span>simply changes the relative weights of the paths (and this change of weight is never zero, similarly to equation (<a href="#eq:girsanov-probability-scaling" data-reference-type="ref" data-reference="eq:girsanov-probability-scaling">[eq:girsanov-probability-scaling]</a>) for a single random variable), the theorem suggests that the paths of a standard Brownian motion and those of a Brownian motion with a constant drift <span class="math inline">\(\theta\)</span> (with volatility <span class="math inline">\(1\)</span>) are essentially the same.</p>
<p>The form of the factor <span class="math inline">\(M_{T}=e^{-\theta\tilde{B}_{T}+\theta^{2}T}\)</span> can be easily understood at the heuristic level. For each outcome <span class="math inline">\(\omega\)</span>, it is proportional to <span class="math inline">\(e^{-\theta\tilde{B}_{T}(\omega)}\)</span> (The term <span class="math inline">\(e^{(\theta^{2}/2)T}\)</span> is simply to ensure that <span class="math inline">\(\mathbb{P}(\Omega)=1\)</span>) Therefore, the factor <span class="math inline">\(M_{T}\)</span> penalizes the paths for which <span class="math inline">\(\tilde{B}_{T}(\omega)\)</span> is large and positive (if <span class="math inline">\(\theta&gt;0\)</span>). In particular, it is conceivable that the Brownian motion with positive drift is reduced to standard Brownian motion under the new probability.</p>
<p>(4) <strong>Changing the volatility.</strong> What about the volatility? Is it possible to change the probability <span class="math inline">\(\mathbb{P}\)</span> to <span class="math inline">\(\tilde{\mathbb{P}}\)</span> in such a way that the Brownian motion under <span class="math inline">\(\mathbb{P}\)</span> has volatility <span class="math inline">\(\sigma\neq1\)</span> under <span class="math inline">\(\tilde{\mathbb{P}}\)</span>? The answer is no! The paths of the Brownian motions with different volatilities are inherently different. Indeed, it suffices to compute the quadratic variation. If <span class="math inline">\((B_{t}:t\in[0,T])\)</span> has volatility <span class="math inline">\(1\)</span> and <span class="math inline">\((\tilde{B_{t}},t\in[0,T])\)</span> has volatility <span class="math inline">\(2\)</span>. then the following convergence holds for <span class="math inline">\(\omega\)</span> in a set of probability one (for a partition fine enough, say <span class="math inline">\(t_{j+1}-t_{j}=2^{-n}\)</span>. Then <span class="math inline">\(B_{t}=\int1\cdot dB_{t}\)</span> and <span class="math inline">\(\tilde{B_{t}}=\int2\cdot dB_{t}\)</span></p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\sum_{j=0}^{n-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))^{2} &amp; =\int_{0}^{T}1^{2}\cdot ds=T
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\sum_{j=0}^{n-1}(\tilde{B}_{t_{j+1}}(\omega)-\tilde{B}_{t_{j}}(\omega))^{2} &amp; =\int_{0}^{T}2^{2}\cdot ds=4T
\end{aligned}\]</span></p>
<p>In other words, the distribution of the standard brownian motion on <span class="math inline">\([0,T]\)</span> is supported on paths whose quadratic variation is <span class="math inline">\(T\)</span>, whereas the distribution of <span class="math inline">\((\tilde{B}_{t},t\geq0)\)</span> is supported on paths where the quadratic variation is <span class="math inline">\(4T\)</span>. These paths are very different. We conclude that the distributions of the two processes are not equivalent. Hence, a change of probability from <span class="math inline">\(\mathbb{P}\)</span> to <span class="math inline">\(\mathbb{\tilde{P}}\)</span> is not possible. In fact, we say that they are mutually singular, meaning the set of paths on which they are supported are disjoint.</p>
<p><em>Proof.</em></p>
<p>Let <span class="math inline">\((\tilde{B}_{t}:t\in[0,T])\)</span> be a Brownian motion with constant drift <span class="math inline">\(\theta\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span>. Thus, <span class="math inline">\(\tilde{B}_{t}=\theta t+B_{t}\)</span>.</p>
<p><strong>Claim</strong>. <span class="math inline">\(\tilde{B}_{t}\)</span> is a <span class="math inline">\(\mathbb{\tilde{P}}\)</span>-martingale.</p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
M_{t} &amp; =f(t,B_{t})=\exp(-\theta B_{t}-(\theta^{2}/2)t)
\end{aligned}\]</span></p>
<p>So:</p>
<p><span class="math display">\[\begin{aligned}
dM_{t} &amp; =-\frac{\theta^{2}}{2}M_{t}dt-\theta M_{t}dB_{t}+\frac{1}{2}\theta^{2}M(t)dt\\
&amp; =-\theta M_{t}dB_{t}
\end{aligned}\]</span></p>
<p>Consider the product <span class="math inline">\((M_{t}\tilde{B}_{t})\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
d(M_{t}\tilde{B}_{t}) &amp; =\tilde{B}_{t}dM_{t}+M_{t}d\tilde{B}_{t}+dM_{t}\cdot d\tilde{B}_{t}\\
&amp; =-\tilde{B}_{t}\theta M_{t}dB_{t}+M_{t}(\theta dt+dB_{t})-\theta M_{t}dB_{t}(\theta dt+dB_{t})\\
&amp; =-\tilde{B}_{t}\theta M_{t}dB_{t}+\theta M_{t}dt+M_{t}dB_{t}-\theta M_{t}dt\\
&amp; =(-\tilde{B}_{t}\theta+1)M_{t}dB_{t}
\end{aligned}\]</span></p>
<p>Thus, by the properties of Ito integral,<span class="math inline">\(M_{t}\tilde{B}_{t}\)</span> is a martingale under <span class="math inline">\(\mathbb{P}\)</span>. By the abstract Bayes formula (<a href="#th:abstract-bayes-formula" data-reference-type="ref" data-reference="th:abstract-bayes-formula">[th:abstract-bayes-formula]</a>):</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[\tilde{B}_{t}|\mathcal{F}_{s}] &amp; =\frac{1}{M_{s}}\mathbb{E}[M_{t}\tilde{B}_{t}|\mathcal{F}_{s}]\\
&amp; =\frac{1}{M_{s}}\cdot M_{s}\tilde{B}_{s}\\
&amp; =\tilde{B}_{s}
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(\tilde{B}_{t}\)</span> is a <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-martingale.</p>
<p><strong>Claim</strong>. Our claim is that under the <span class="math inline">\(\tilde{\mathbb{P}}\)</span> measure, <span class="math inline">\(\tilde{B}_{t}\sim\mathcal{N}^{\mathbb{\tilde{P}}}(0,t)\)</span> and to do this we rely on the the moment-generating function.</p>
<p>By definition, for a constant <span class="math inline">\(\Psi\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
M_{\tilde{B}_{t}}(\Psi) &amp; =\tilde{\mathbb{E}}\left[\exp\left(\Psi\tilde{B}_{t}\right)\right]\\
&amp; =\mathbb{E}\left[M_{T}\exp\left(\Psi\tilde{B}_{t}\right)\right]\\
&amp; =\mathbb{E}\left[\exp\left(-\theta\tilde{B}_{T}+\frac{\theta^{2}}{2}T+\Psi\tilde{B}_{t}\right)\right]\\
&amp; =\mathbb{E}\left[\exp\left(-\theta(\theta T+B_{T})+\frac{\theta^{2}}{2}T+\Psi(\theta t+B_{t})\right)\right]\\
&amp; =\mathbb{E}\left[\exp\left(-\theta B_{T}-\frac{\theta^{2}}{2}T+\Psi\theta t+\Psi B_{t})\right)\right]\\
&amp; =\mathbb{E}\left[\exp\left(-\theta(B_{T}-B_{t})-\frac{\theta^{2}}{2}T+\Psi\theta t+(\Psi-\theta)B_{t})\right)\right]\\
&amp; =\exp\left(-\frac{\theta^{2}}{2}T+\Psi\theta t\right)\mathbb{E}\left(-\theta(B_{T}-B_{t})\right)\mathbb{E}\left((\Psi-\theta)B_{t}\right)\\
&amp; =\exp\left(-\frac{\theta^{2}}{2}T+\Psi\theta t\right)\exp\left[\frac{1}{2}\theta^{2}(T-t)\right]\exp\left[\frac{1}{2}(\Psi-\theta)^{2}t\right]\\
&amp; =\exp\left[-\frac{1}{2}\left(\theta^{2}-2\Psi\theta-(\Psi-\theta)^{2}\right)t\right]\\
&amp; =\exp\left[-\frac{1}{2}\left(\theta^{2}-2\Psi\theta-(\Psi^{2}-2\Psi\theta+\theta^{2}\right)t\right]\\
&amp; =\exp(-\Psi^{2}t)
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(\tilde{B}_{t}\sim\mathcal{N}^{\tilde{\mathbb{P}}}(0,t)\)</span>.</p>
<p><strong>Claim</strong>. Finally, to show that <span class="math inline">\(\tilde{B}_{t}\)</span> is indeed a <span class="math inline">\(\mathbb{\tilde{P}}-\)</span>standard brownian motion, we have the following:</p>
<p>(a) <span class="math inline">\(\tilde{B}_{0}=\theta(0)+B_{0}=0\)</span> and <span class="math inline">\(\tilde{B}_{t}\)</span> has almost surely continuous paths.</p>
<p>(b) We would like to prove that, for <span class="math inline">\(s&lt;t\)</span>, <span class="math inline">\(\tilde{B}_{t}-\tilde{B}_{s}\sim\mathcal{N}^{\tilde{\mathbb{P}}}(0,t-s)\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[\tilde{B}_{t}-\tilde{B}_{s}] &amp; =\tilde{\mathbb{E}}[\tilde{B}_{t}]-\tilde{\mathbb{E}}[B_{s}]\\
&amp; =0
\end{aligned}\]</span></p>
<p>And,</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[(\tilde{B}_{t}-\tilde{B}_{s})^{2}] &amp; =\tilde{\mathbb{E}}[\tilde{B}_{t}^{2}-2\tilde{B}_{t}\tilde{B}_{s}+\tilde{B}_{s}^{2}]\\
&amp; =\tilde{\mathbb{E}}[B_{t}^{2}]-2\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}]+\tilde{\mathbb{E}}[\tilde{B}_{s}^{2}]\\
&amp; =t+s-2\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}]
\end{aligned}\]</span></p>
<p>(c) The non-overlapping increments of a <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-martingale are independent. To see this, suppose <span class="math inline">\(t_{1}\leq t_{2}\leq t_{3}\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[(B_{t_{3}}-B_{t_{2}})(B_{t_{2}}-B_{t_{1}})] &amp; =\tilde{\mathbb{E}}[\tilde{\mathbb{E}}[(B_{t_{3}}-B_{t_{2}})(B_{t_{2}}-B_{t_{1}})|\mathcal{F}_{t_{2}}]]\\
&amp; =\tilde{\mathbb{E}}[(B_{t_{2}}-B_{t_{1}})\tilde{\mathbb{E}}[(B_{t_{3}}-B_{t_{2}})|\mathcal{F}_{t_{2}}]]\\
&amp; =\tilde{\mathbb{E}}[(B_{t_{2}}-B_{t_{1}})(B_{t_{2}}-B_{t_{2}})]]=0
\end{aligned}\]</span></p>
<p>Also, the covariance</p>
<p><span class="math display">\[\begin{aligned}
\tilde{\mathbb{E}}[\tilde{B}_{t}\tilde{B}_{s}] &amp; =\tilde{\mathbb{E}}[(\tilde{B}_{t}-\tilde{B}_{s})\tilde{B}_{s}]+\tilde{\mathbb{E}}[\tilde{B}_{s}^{2}]\\
&amp; =0+s
\end{aligned}\]</span></p>
<p>So, <span class="math inline">\(\mathbb{E}[(\tilde{B}_{t}-\tilde{B}_{s})^{2}]=t+s-2s=t-s\)</span>.</p>
<p>Consequently, <span class="math inline">\(\tilde{B}_{t}\)</span> is a <span class="math inline">\(\tilde{\mathbb{P}}\)</span>-standard brownian motion.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "A gentle introduction to the Girsanov Theorem - Back to the basics"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Quasar"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-12-01"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Stochastic Calculus]      </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "image.jpg"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="an">comments:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">  giscus: </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    repo: quasar-chunawala/quantdev</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>One of the most popular technical tools in financial engineering is the change of numeraire. In this blog-post, I intend to provide the dear reader a beginner-friendly introduction and an intuitive gut feel for these tools. </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>You take a convenient asset e.g. a share of stock or a bond as the *numeraire*, as if it were a medium of exchange, and express the prices of other assets and option prices in units of this numeraire. When pricing derivatives analytically, switching numeraires is done for computational ease. </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>The technique was used by <span class="co">[</span><span class="ot">Heylette Geman</span><span class="co">](https://en.wikipedia.org/wiki/H%C3%A9lyette_Geman)</span>, <span class="co">[</span><span class="ot">Nicole El Karoui</span><span class="co">](https://en.wikipedia.org/wiki/Nicole_El_Karoui)</span> and <span class="co">[</span><span class="ot">Jean-Charles Rochet</span><span class="co">]()</span> in their seminal note <span class="co">[</span><span class="ot">Changes of Numeraire, Changes of Probability Measure and Option Pricing</span><span class="co">](https://www.cambridge.org/core/journals/journal-of-applied-probability/article/abs/changes-of-numeraire-changes-of-probability-measure-and-option-pricing/EA730D6C18D56426D491B6A25563C0B3)</span>.</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu"># Change of Probability.</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## Change of Probability for a Random Variable.</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>Consider a random variable $X$ defined on a sample space $\Omega$ having zero mean. We want to change the mean of $X$ so that $\mu\neq 0$. Of course, it is easy to change the mean of a random variable: If $X$ has mean $0$, then the random variable $X+\mu$ has mean $\mu$. However, it might be that the variable $X+\mu$ does not share the same possible values as $X$. For example, take $X$ to be a uniform random variable on $<span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span>$. While $X+1$ has mean $1$, the density of $X+1$ would be non-zero on $<span class="co">[</span><span class="ot">0,2</span><span class="co">]</span>$ instead of $<span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span>$.</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>Our goal is to find a good way to change the underlying probability $\mathbb{P}$, and thus the distribution of $X$, so that the set of outcomes is unchanged. If $X$ is a discrete random variable, say with $\mathbb{P}(X=-1)=\mathbb{P}(X=1)=1/2$, we can change the probability in order to change the mean easily. It suffices to take $\tilde{\mathbb{P}}$ so that $\tilde{\mathbb{P}}(X=1)=p$ and $\mathbb{P}(X=-1)=1-p$ for some appropriate $0\leq p\leq1$.</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>If $X$ is a continuous random variable, with a PDF $f_{X}$, the probabilities can be changed by modifying the PDF. Consider the a new PDF:</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>\tilde{f}_{X}(x) &amp; =f_{X}(x)g(x)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>for some function $g(x)&gt;0$ such that $\int f(x)g(x)dx=1$. Clearly, $f_{X}(x)g(x)$ is also a PDF and $f_{X}(x)&gt;0$ if and only if $f_{X}(x)g(x)&gt;0$, so that the possible values of $X$ are unchanged. A convenient (and important!) choice of function $g$ is:</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>g(x) &amp; =\frac{e^{ax}}{\int_{\mathbf{R}}e^{ax}f_{X}(x)dx}=\frac{e^{ax}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>},\quad a\in\mathbf{R}\label{eq:change-of-probability-of-an-rv}</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>assuming $X$ has a well-defined MGF. Here $a$ is a parameter that can be tuned to fit to a specific mean. The normalization factor in the denominator is the MGF of $X$. It ensures that $f_{X}(x)g(x)$ is a PDF. Note that if $a&gt;0$, the function $g$ gives a bigger weight to large values of $X$. We say that $g$ is biased towards the large values.</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>:::{#exm-biasing-a-uniform-random-variable}</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="fu">### Biasing a uniform random variable </span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>Let $X$ be a uniform random variable on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. Clearly, $\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>=1/2$. How can we change the PDF of $X$ so that the possible values are still $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$, but the mean is $1/4$. We have that the PDF is $f_{X}(x)=1$ if $x\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ and $0$ elsewhere. Therefore, the mean with the new PDF with parameter $a$ as in the equation (<span class="co">[</span><span class="ot">\[eq:change-of-probability-of-an-rv\]</span><span class="co">](#eq:change-of-probability-of-an-rv)</span>{reference-type="ref" reference="eq:change-of-probability-of-an-rv"}) is:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp; =\int_{0}^{1}x\tilde{f}(x)dx<span class="sc">\\</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{0}^{1}\frac{xe^{ax}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>}dx<span class="sc">\\</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{a}{e^{a}-1}\int_{0}^{1}xe^{ax}dx<span class="sc">\\</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{a}{e^{a}-1}\left(\left<span class="co">[</span><span class="ot">x\frac{e^{ax}}{a}\right</span><span class="co">]</span>_{0}^{1}-\frac{1}{a}\int_{0}^{1}e^{ax}dx\right)<span class="sc">\\</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{a}{e^{a}-1}\left(\frac{e^{a}}{a}-\frac{1}{a}\frac{e^{a}-1}{a}\right)<span class="sc">\\</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{e^{a}}{e^{a}-1}-\frac{1}{a}</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>For $\tilde{\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>}$to be equal to $1/4$, we get numerically $a\approx-3.6$. Note that the possible values of $X$ remain the same under the new probability. However, the new distribution is no longer uniform! It has bias towards values closer to zero, as it should.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>:::{#exm-biasing-a-gaussian-random-variable}</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="fu">### Biasing a Gaussian random variable. </span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>Let $X$ be a Gaussian random variable with mean $\mu$ and variance $\sigma^{2}$. How can we change the PDF of $X$ to have mean $0$? Going back to (<span class="co">[</span><span class="ot">\[eq:change-of-probability-of-an-rv\]</span><span class="co">](#eq:change-of-probability-of-an-rv)</span>{reference-type="ref" reference="eq:change-of-probability-of-an-rv"}), the mean $\mu$ under the new PDF with parameter $a$ is:</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp; =\int_{-\infty}^{\infty}x\tilde{f}(x)dx<span class="sc">\\</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{-\infty}^{\infty}xg(x)f(x)dx<span class="sc">\\</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{-\infty}^{\infty}x\cdot\frac{e^{ax}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}}dx<span class="sc">\\</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\cdot\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x^{2}-2\mu x+\mu^{2}-2a\sigma^{2}x}{\sigma^{2}}\right)\right</span><span class="co">]</span>dx<span class="sc">\\</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\cdot\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x^{2}-2(\mu+a\sigma^{2})x+(\mu+a\sigma^{2})^{2}-2\mu a\sigma^{2}-a^{2}\sigma^{4}}{\sigma^{2}}\right)\right</span><span class="co">]</span>dx<span class="sc">\\</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{e^{\mu a+a^{2}\sigma^{2}/2}}{e^{\mu a+\frac{1}{2}a^{2}\sigma^{2}}}\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x-(\mu+a\sigma^{2})}{\sigma}\right)^{2}\right</span><span class="co">]</span>dx<span class="sc">\\</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x-(\mu+a\sigma^{2})}{\sigma}\right)^{2}\right</span><span class="co">]</span>dx</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>For the specific choice of the parameter $a=\mu/\sigma^{2}$, we recover the PDF of a Gaussian random variable with mean $0$. But, we can deduce more. The new PDF is also Gaussian. This was not the case for uniform random variables. In fact, the new PDF is exactly the same as the one of $X-\mu$. For if, $a=\mu/\sigma^{2}$, we have:</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp; =\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}x\exp\left<span class="co">[</span><span class="ot">-\frac{x^{2}}{2\sigma^{2}}\right</span><span class="co">]</span>dx</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>and observe that if $Y=X-\mu$, then:</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>F_{Y}(x) &amp; =\mathbb{P}(X-\mu&lt;x)<span class="sc">\\</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{P}(X\leq x+\mu)<span class="sc">\\</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a> &amp; =F_{X}(x+\mu)<span class="sc">\\</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>\frac{d}{dx}(F_{Y}(x)) &amp; =\frac{d}{dx}(F_{X}(x+\mu))<span class="sc">\\</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>f_{Y}(x) &amp; =f_{X}(x+\mu)\cdot\frac{d}{dx}(x+\mu)<span class="sc">\\</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>f_{Y}(x) &amp; =f_{X}(x+\mu)<span class="sc">\\</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{\sqrt{2\pi}\sigma}\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x+\mu-\mu}{\sigma}\right)^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{\sqrt{2\pi}\sigma}\exp\left<span class="co">[</span><span class="ot">-\frac{x^{2}}{2\sigma^{2}}\right</span><span class="co">]</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>In other words:</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>*For Gaussians, changing the mean by recentering is equivalent to changing the probability as in* (<span class="co">[</span><span class="ot">\[eq:change-of-probability-of-an-rv\]</span><span class="co">](#eq:change-of-probability-of-an-rv)</span>{reference-type="ref" reference="eq:change-of-probability-of-an-rv"}).</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>This is a very special property of the Gaussian distribution. The exponential and Poisson distributions have a similar property.</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>Example (<span class="co">[</span><span class="ot">\[ex:biasing-a-gaussian-random-variable\]</span><span class="co">](#ex:biasing-a-gaussian-random-variable)</span>{reference-type="ref" reference="ex:biasing-a-gaussian-random-variable"}) is very important and we will state it as a theorem. Before doing so, we notice that the change of PDF (<span class="co">[</span><span class="ot">\[eq:change-of-probability-of-an-rv\]</span><span class="co">](#eq:change-of-probability-of-an-rv)</span>{reference-type="ref" reference="eq:change-of-probability-of-an-rv"}) can be expressed more generally by changing the underlying probability measure(length, area, weights) $\mathbb{P}$ on the sample space $\Omega$ on which the random variables are defined. More precisely, let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, and let $X$ be a random variable defined on $\Omega$. We define a new probability $\tilde{\mathbb{P}}$ on $\Omega$ as follows:</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>If $\mathcal{E}$ is an event in $\mathcal{F}$, then:</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{\tilde{E}}<span class="co">[</span><span class="ot">1_{\mathcal{E}}</span><span class="co">]</span>=\int_{\mathbf{R}}1_{\mathcal{E}}\cdot\tilde{f}(x)dx\nonumber <span class="sc">\\</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{\mathbf{R}}1_{\mathcal{E}}\cdot g(x)f_{X}(x)dx\nonumber <span class="sc">\\</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{\mathbf{R}}1_{\mathcal{E}}\cdot\frac{e^{ax}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>}f_{X}(x)dx\nonumber <span class="sc">\\</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">1_{\mathcal{E}}\frac{e^{aX}}{\mathbb{E}[e^{aX}]}\right</span><span class="co">]</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>Intuitively, we are changing the probability of each outcome $\omega\in\mathcal{E}$, by the factor</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>$$\frac{e^{aX(\omega)}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>}\label{eq:girsanov-probability-scaling}$$</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>In other words, if $a&gt;0$, the outcomes $\omega$ for which $X$ has large values are favored. Note that equation (<span class="co">[</span><span class="ot">\[eq:change-of-probability-of-an-rv\]</span><span class="co">](#eq:change-of-probability-of-an-rv)</span>{reference-type="ref" reference="eq:change-of-probability-of-an-rv"}) for the PDF is recovered, since for any function $h$ of $X$, we have:</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">h(X)</span><span class="co">]</span> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\frac{e^{aX}}{\mathbb{E}[e^{aX}]}h(X)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{\mathbf{R}}h(x)\frac{e^{ax}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>}f_{X}(x)dx</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>In this setting, the above example becomes the preliminary version of the Cameron-Martin-Girsanov theorem:</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>[]{#th:change-of-probability-for-a-random-variable label="th:change-of-probability-for-a-random-variable"}Let $X$ be a Gaussian random variable with mean $\mu$ and variance $\sigma^{2}$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. Then, under the probability $\mathbb{\tilde{P}}$ given by:</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">1_{\mathcal{E}}e^{-\frac{\mu}{\sigma^{2}}X+\frac{1}{2}\frac{\mu^{2}}{\sigma^{2}}}\right</span><span class="co">]</span>,\quad\mathcal{E}\in\mathcal{F}\label{eq:preliminary-version-girsanov-theorem}</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>the random variable $X$ is Gaussian with mean $0$ and variance $\sigma^{2}$.</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>Moreover, since $X$ can be written as $X=Y+\mu$ where $Y$ is Gaussian with mean $0$ and variance $\sigma^{2}$ under $\mathbb{P}$, we have that $\mathbb{\tilde{P}}$ can be written as:</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>\mathbb{\tilde{P}}(\mathcal{E}) &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">1_{\mathcal{E}}e^{-\frac{\mu}{\sigma^{2}}Y-\frac{1}{2}\frac{\mu^{2}}{\sigma^{2}}}\right</span><span class="co">]</span>,\quad\mathcal{E}\in\mathcal{F}\label{eq:preliminary-version-girsanov-theorem-II}</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>It is good to pause for a second and look at the signs in the exponential of equations (<span class="co">[</span><span class="ot">\[eq:preliminary-version-girsanov-theorem\]</span><span class="co">](#eq:preliminary-version-girsanov-theorem)</span>{reference-type="ref" reference="eq:preliminary-version-girsanov-theorem"}) and (<span class="co">[</span><span class="ot">\[eq:preliminary-version-girsanov-theorem-II\]</span><span class="co">](#eq:preliminary-version-girsanov-theorem-II)</span>{reference-type="ref" reference="eq:preliminary-version-girsanov-theorem-II"}). The signs in the exponential might be very confusing and is the source of many mistakes in the Cameron-Martin-Girsanov theorem. A good trick is to say that, if we want to remove $\mu$, then the sign in front of $X$ or $Y$ must be negative. Then, we add the exponential factor needed for $\tilde{\mathbb{P}}$ to be a probability. This is given by the MGF of $X$ or $Y$ depending on how we want to express it.</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>The probabilities $\mathbb{P}$ and $\tilde{\mathbb{P}}$, as defined in the equation (<span class="co">[</span><span class="ot">\[eq:preliminary-version-girsanov-theorem\]</span><span class="co">](#eq:preliminary-version-girsanov-theorem)</span>{reference-type="ref" reference="eq:preliminary-version-girsanov-theorem"}) are obviously not equal since they differ by a factor in (<span class="co">[</span><span class="ot">\[eq:girsanov-probability-scaling\]</span><span class="co">](#eq:girsanov-probability-scaling)</span>{reference-type="ref" reference="eq:girsanov-probability-scaling"}). However, they share some similarities. Most notably, if $\mathcal{E}$ is an event of positive $\mathbb{P}$-probability, $\mathbb{P}(\mathcal{E})&gt;0$, then we must have $\tilde{\mathbb{P}}(\mathcal{E})&gt;0$, since the factor in (<span class="co">[</span><span class="ot">\[eq:girsanov-probability-scaling\]</span><span class="co">](#eq:girsanov-probability-scaling)</span>{reference-type="ref" reference="eq:girsanov-probability-scaling"}) is always strictly positive. The converse is also true: if $\mathcal{E}$ is an event of positive $\tilde{\mathbb{P}}$-probability, $\tilde{\mathbb{P}}(\mathcal{E})&gt;0$, then we must have that $\mathbb{P}(\mathcal{E})&gt;0$. This is because the factor in (<span class="co">[</span><span class="ot">\[eq:girsanov-probability-scaling\]</span><span class="co">](#eq:girsanov-probability-scaling)</span>{reference-type="ref" reference="eq:girsanov-probability-scaling"}) can be inverted, being strictly positive. More precisely, we have:</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(\mathcal{E}) &amp; =\mathbb{E}<span class="co">[</span><span class="ot">1_{\mathcal{E}}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">1_{\mathcal{E}}\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\left(\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\right)^{-1}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{\mathbb{E}}\left<span class="co">[</span><span class="ot">1_{\mathcal{E}}\left(\frac{e^{aX(\omega)}}{\mathbb{E}[e^{aX}]}\right)^{-1}\right</span><span class="co">]</span></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>The factor $\left(\frac{e^{aX(\omega)}}{\mathbb{E}<span class="co">[</span><span class="ot">e^{aX}</span><span class="co">]</span>}\right)^{-1}$ is also strictly positive, proving the claim. To sum it all up, the probabilities $\mathbb{P}$ and $\tilde{\mathbb{P}}$ essentially share the same possible outcomes. Such probability measures are said to be equivalent measures.</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>Consider the two probabilities $\mathbb{P}$ and $\tilde{\mathbb{P}}$ on $(\Omega,\mathcal{F})$. They are said to be equivalent, if for any event $\mathcal{E}\in\mathcal{F}$, we have $\mathbb{P}(\mathcal{E})&gt;0$ if and only if $\mathbb{P}(\mathcal{E})&gt;0$. Thus, $\mathbb{P}$ and $\tilde{\mathbb{P}}$ agree on the null sets. If $A\in\mathcal{F}$ is such that $\mathbb{P}(A)=0$, then $\mathbb{\tilde{P}}(A)=0$ and vice-versa.</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>Keep in mind that two probabilities that are equivalent might still be very far from being equal!</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Cameron-Martin Theorem.</span></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>[]{#th:girsanov-theorem-for-constant-drift-case label="th:girsanov-theorem-for-constant-drift-case"}(Cameron-Martin Theorem for constant drift). Let $(\tilde{B(t)},t\in<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>)$ be a $\mathbb{P}-$Brownian motion with constant drift $\theta$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. Consider the probability $\tilde{\mathbb{P}}$on $\Omega$ given by:</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{P}}(\mathcal{E}) &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">e^{-\theta\tilde{B}(T)+\frac{\theta^{2}}{2}T}1_{\mathcal{E}}\right</span><span class="co">]</span>,\quad\mathcal{E}\in\mathcal{F}\label{eq:girsanov-theorem-constant-drift-case}</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>Then, the process $(\tilde{B}(t),t\in<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>)$ under $\mathbb{\tilde{P}}$is distributed like a standard brownian motion. Moreover, since we can write $\tilde{B_{t}}=\theta t+B_{t}$ for some standard brownian motion $(B_{t},t\in<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>)$ on $(\Omega,\mathcal{F},\mathbb{P})$, the probability $\tilde{\mathbb{P}}$ can also be written as:</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{P}}(\mathcal{E}) &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">e^{-\theta B(T)-\frac{\theta^{2}}{2}T}1_{\mathcal{E}}\right</span><span class="co">]</span>\label{eq:girsanov-theorem-constant-drift-case-II}</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>It is a good idea to pause again and look at the signs in the exponential in equations (<span class="co">[</span><span class="ot">\[eq:girsanov-theorem-constant-drift-case\]</span><span class="co">](#eq:girsanov-theorem-constant-drift-case)</span>{reference-type="ref" reference="eq:girsanov-theorem-constant-drift-case"}) and (<span class="co">[</span><span class="ot">\[eq:girsanov-theorem-constant-drift-case-II\]</span><span class="co">](#eq:girsanov-theorem-constant-drift-case-II)</span>{reference-type="ref" reference="eq:girsanov-theorem-constant-drift-case-II"}). They behave the same way as in theorem (<span class="co">[</span><span class="ot">\[th:change-of-probability-for-a-random-variable\]</span><span class="co">](#th:change-of-probability-for-a-random-variable)</span>{reference-type="ref" reference="th:change-of-probability-for-a-random-variable"}). There is a minus sign in front of $B_{T}$ to remove the drift. Before proving the theorem, we make some important remarks.</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>1<span class="sc">\)</span> **The end-point**. Note that only the endpoint $\tilde{B}(T)$ of the Brownian motion is involved in the change of probability. In particular, $T$ cannot be $+\infty$. The Cameron-Martin theorem can only be applied on a finite interval.</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>2<span class="sc">\)</span> **A martingale.** The factor $M_{T}=e^{-\theta B(T)-\frac{\theta^{2}}{2}T}=e^{-\theta\tilde{B}(T)+\frac{1}{2}\theta^{2}T}$ involved in the change of probability is the end-point of a $\mathbb{P}-$martingale, that is, it is a martingale under the original probability $\mathbb{P}$. To see this:</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">M_{T}|\mathcal{F}_{t}</span><span class="co">]</span> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">e^{-\theta B(T)-\frac{1}{2}\theta^{2}T}|\mathcal{F}_{t}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a> &amp; =e^{-\theta B(t)}\mathbb{E}\left<span class="co">[</span><span class="ot">e^{-\theta(B(T)-B(t))}|\mathcal{F}_{t}\right</span><span class="co">]</span>e^{-\frac{\theta^{2}}{2}T}<span class="sc">\\</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a> &amp; <span class="sc">\{</span>\text{Using }B(T)-B(t)\perp\mathcal{F}_{t}<span class="sc">\}\\</span></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a> &amp; =e^{-\theta B(t)}\mathbb{E}\left<span class="co">[</span><span class="ot">e^{-\theta(B(T)-B(t))}\right</span><span class="co">]</span>e^{-\frac{\theta^{2}}{2}T}<span class="sc">\\</span></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a> &amp; =e^{-\theta B(t)}e^{\frac{\theta^{2}}{2}(T-t)}e^{-\frac{\theta^{2}}{2}T}<span class="sc">\\</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a> &amp; =e^{-\theta B(t)-\frac{\theta^{2}}{2}t}</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>In fact, since $B(t)$ is a $\mathbb{P}$-standard Brownian motion, $M(t)=e^{-\theta B(t)-\frac{\theta^{2}}{2}t}$ is a geometric brownian motion.</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>Interestingly, the drift of $\tilde{B}(t)$ becomes the volatility factor in $M_{T}$! $\mathbb{E}<span class="co">[</span><span class="ot">M_{T}^{2}</span><span class="co">]</span>=\mathbb{E}<span class="co">[</span><span class="ot">e^{-2\theta B(T)-\theta^{2}T}</span><span class="co">]</span>=e^{-\theta^{2}T}\cdot\mathbb{E}<span class="co">[</span><span class="ot">e^{-2\theta B(T)}</span><span class="co">]</span>=e^{-\theta^{2}T}\cdot e^{2\theta^{2}T}=e^{\theta^{2}T}$.</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>The fact that $M(t)$ is a martingale is very helpful in calculations. Indeed, suppose we want to compute the expectation of a function $F(\tilde{B}(s))$ of a Brownian motion with drift at time $s&lt;T$. Then, we have by theorem (<span class="co">[</span><span class="ot">\[th:girsanov-theorem-for-constant-drift-case\]</span><span class="co">](#th:girsanov-theorem-for-constant-drift-case)</span>{reference-type="ref" reference="th:girsanov-theorem-for-constant-drift-case"}):</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">F(\tilde{B}(s))</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">M_{T}M_{T}^{-1}F(\tilde{B}(s))</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">M_{T}^{-1}F(\tilde{B}(s))</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))</span><span class="co">]</span></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>Now, we know that under $\tilde{\mathbb{P}}$probability, $(\tilde{B}(t),t\in<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>)$ is a standard brownian motion, or $\tilde{\mathbb{P}}$-standard brownian motion for short. Therefore, the process $e^{\theta\tilde{B}(t)-\frac{1}{2}\theta^{2}t}$ is a martingale under the new probability measure $\tilde{\mathbb{P}}$, or a $\tilde{\mathbb{P}}$-martingale for short. By conditioning over $\mathcal{F}_{s}$ and applying the martingale property, we get:</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">F(\tilde{B}_{s})\right</span><span class="co">]</span> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{\mathbb{E}}[e^{\theta\tilde{B}(T)-\frac{1}{2}\theta^{2}T}F(\tilde{B}(s))|\mathcal{F}_{s}]</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">e^{\theta\tilde{B}(s)-\frac{1}{2}\theta^{2}s}F(\tilde{B}(s))</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">e^{\theta B(s)-\frac{1}{2}\theta^{2}s}F(B(s))</span><span class="co">]</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>The last equality may seem wrong as removed all the tildes. It is not! It holds because $(\tilde{B}(t))$ under $\tilde{\mathbb{P}}$ has the same distribution as $(B(t))$ under $\mathbb{P}$: a standard brownian motion. Of course, it would be possible to directly evaluate $\mathbb{E}<span class="co">[</span><span class="ot">F(\tilde{B}(s))</span><span class="co">]</span>$ here as we know the distribution of a Brownian motion with drift. However, when the function will involve more than one point (such as the maximum of the path), the Cameron-Martin theorem is a powerful tool to evaluate expectations.</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>3<span class="sc">\)</span> **The paths with or without the drift are the same.** Let $(B(t),t\leq T)$ be a standard Brownian motion defined on $(\Omega,\mathcal{F},\mathbb{P})$. Heuristically, it is fruitful to think of the sample space of $\Omega$ as the different continuous paths of Brownian motion. Since, the change of probability from $\mathbb{P}$ to $\tilde{\mathbb{P}}$simply changes the relative weights of the paths (and this change of weight is never zero, similarly to equation (<span class="co">[</span><span class="ot">\[eq:girsanov-probability-scaling\]</span><span class="co">](#eq:girsanov-probability-scaling)</span>{reference-type="ref" reference="eq:girsanov-probability-scaling"}) for a single random variable), the theorem suggests that the paths of a standard Brownian motion and those of a Brownian motion with a constant drift $\theta$ (with volatility $1$) are essentially the same.</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>The form of the factor $M_{T}=e^{-\theta\tilde{B}_{T}+\theta^{2}T}$ can be easily understood at the heuristic level. For each outcome $\omega$, it is proportional to $e^{-\theta\tilde{B}_{T}(\omega)}$ (The term $e^{(\theta^{2}/2)T}$ is simply to ensure that $\mathbb{P}(\Omega)=1$) Therefore, the factor $M_{T}$ penalizes the paths for which $\tilde{B}_{T}(\omega)$ is large and positive (if $\theta&gt;0$). In particular, it is conceivable that the Brownian motion with positive drift is reduced to standard Brownian motion under the new probability.</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>4<span class="sc">\)</span> **Changing the volatility.** What about the volatility? Is it possible to change the probability $\mathbb{P}$ to $\tilde{\mathbb{P}}$ in such a way that the Brownian motion under $\mathbb{P}$ has volatility $\sigma\neq1$ under $\tilde{\mathbb{P}}$? The answer is no! The paths of the Brownian motions with different volatilities are inherently different. Indeed, it suffices to compute the quadratic variation. If $(B_{t}:t\in<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>)$ has volatility $1$ and $(\tilde{B_{t}},t\in<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>)$ has volatility $2$. then the following convergence holds for $\omega$ in a set of probability one (for a partition fine enough, say $t_{j+1}-t_{j}=2^{-n}$. Then $B_{t}=\int1\cdot dB_{t}$ and $\tilde{B_{t}}=\int2\cdot dB_{t}$</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\sum_{j=0}^{n-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))^{2} &amp; =\int_{0}^{T}1^{2}\cdot ds=T</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\sum_{j=0}^{n-1}(\tilde{B}_{t_{j+1}}(\omega)-\tilde{B}_{t_{j}}(\omega))^{2} &amp; =\int_{0}^{T}2^{2}\cdot ds=4T</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>In other words, the distribution of the standard brownian motion on $<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>$ is supported on paths whose quadratic variation is $T$, whereas the distribution of $(\tilde{B}_{t},t\geq0)$ is supported on paths where the quadratic variation is $4T$. These paths are very different. We conclude that the distributions of the two processes are not equivalent. Hence, a change of probability from $\mathbb{P}$ to $\mathbb{\tilde{P}}$ is not possible. In fact, we say that they are mutually singular, meaning the set of paths on which they are supported are disjoint.</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>Let $(\tilde{B}_{t}:t\in[0,T])$ be a Brownian motion with constant drift $\theta$ defined on $(\Omega,\mathcal{F},\mathbb{P})$. Thus, $\tilde{B}_{t}=\theta t+B_{t}$.</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>**Claim**. $\tilde{B}_{t}$ is a $\mathbb{\tilde{P}}$-martingale.</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>Let</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>M_{t} &amp; =f(t,B_{t})=\exp(-\theta B_{t}-(\theta^{2}/2)t)</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>So:</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>dM_{t} &amp; =-\frac{\theta^{2}}{2}M_{t}dt-\theta M_{t}dB_{t}+\frac{1}{2}\theta^{2}M(t)dt<span class="sc">\\</span></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a> &amp; =-\theta M_{t}dB_{t}</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>Consider the product $(M_{t}\tilde{B}_{t})$. We have:</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>d(M_{t}\tilde{B}_{t}) &amp; =\tilde{B}_{t}dM_{t}+M_{t}d\tilde{B}_{t}+dM_{t}\cdot d\tilde{B}_{t}<span class="sc">\\</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a> &amp; =-\tilde{B}_{t}\theta M_{t}dB_{t}+M_{t}(\theta dt+dB_{t})-\theta M_{t}dB_{t}(\theta dt+dB_{t})<span class="sc">\\</span></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a> &amp; =-\tilde{B}_{t}\theta M_{t}dB_{t}+\theta M_{t}dt+M_{t}dB_{t}-\theta M_{t}dt<span class="sc">\\</span></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a> &amp; =(-\tilde{B}_{t}\theta+1)M_{t}dB_{t}</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>Thus, by the properties of Ito integral,$M_{t}\tilde{B}_{t}$ is a martingale under $\mathbb{P}$. By the abstract Bayes formula (<span class="co">[</span><span class="ot">\[th:abstract-bayes-formula\]</span><span class="co">](#th:abstract-bayes-formula)</span>{reference-type="ref" reference="th:abstract-bayes-formula"}):</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{B}_{t}|\mathcal{F}_{s}</span><span class="co">]</span> &amp; =\frac{1}{M_{s}}\mathbb{E}<span class="co">[</span><span class="ot">M_{t}\tilde{B}_{t}|\mathcal{F}_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{M_{s}}\cdot M_{s}\tilde{B}_{s}<span class="sc">\\</span></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{B}_{s}</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>Thus, $\tilde{B}_{t}$ is a $\tilde{\mathbb{P}}$-martingale.</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>**Claim**. Our claim is that under the $\tilde{\mathbb{P}}$ measure, $\tilde{B}_{t}\sim\mathcal{N}^{\mathbb{\tilde{P}}}(0,t)$ and to do this we rely on the the moment-generating function.</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>By definition, for a constant $\Psi$:</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>M_{\tilde{B}_{t}}(\Psi) &amp; =\tilde{\mathbb{E}}\left[\exp\left(\Psi\tilde{B}_{t}\right)\right]<span class="sc">\\</span></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">M_{T}\exp\left(\Psi\tilde{B}_{t}\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\exp\left(-\theta\tilde{B}_{T}+\frac{\theta^{2}}{2}T+\Psi\tilde{B}_{t}\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\exp\left(-\theta(\theta T+B_{T})+\frac{\theta^{2}}{2}T+\Psi(\theta t+B_{t})\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\exp\left(-\theta B_{T}-\frac{\theta^{2}}{2}T+\Psi\theta t+\Psi B_{t})\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\exp\left(-\theta(B_{T}-B_{t})-\frac{\theta^{2}}{2}T+\Psi\theta t+(\Psi-\theta)B_{t})\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a> &amp; =\exp\left(-\frac{\theta^{2}}{2}T+\Psi\theta t\right)\mathbb{E}\left(-\theta(B_{T}-B_{t})\right)\mathbb{E}\left((\Psi-\theta)B_{t}\right)<span class="sc">\\</span></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a> &amp; =\exp\left(-\frac{\theta^{2}}{2}T+\Psi\theta t\right)\exp\left<span class="co">[</span><span class="ot">\frac{1}{2}\theta^{2}(T-t)\right</span><span class="co">]</span>\exp\left<span class="co">[</span><span class="ot">\frac{1}{2}(\Psi-\theta)^{2}t\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a> &amp; =\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\theta^{2}-2\Psi\theta-(\Psi-\theta)^{2}\right)t\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a> &amp; =\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\theta^{2}-2\Psi\theta-(\Psi^{2}-2\Psi\theta+\theta^{2}\right)t\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a> &amp; =\exp(-\Psi^{2}t)</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>Thus, $\tilde{B}_{t}\sim\mathcal{N}^{\tilde{\mathbb{P}}}(0,t)$.</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>**Claim**. Finally, to show that $\tilde{B}_{t}$ is indeed a $\mathbb{\tilde{P}}-$standard brownian motion, we have the following:</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>a<span class="sc">\)</span> $\tilde{B}_{0}=\theta(0)+B_{0}=0$ and $\tilde{B}_{t}$ has almost surely continuous paths.</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>b<span class="sc">\)</span> We would like to prove that, for $s&lt;t$, $\tilde{B}_{t}-\tilde{B}_{s}\sim\mathcal{N}^{\tilde{\mathbb{P}}}(0,t-s)$. We have:</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">\tilde{B}_{t}-\tilde{B}_{s}</span><span class="co">]</span> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{B}_{t}</span><span class="co">]</span>-\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">B_{s}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a> &amp; =0</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>And,</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">(\tilde{B}_{t}-\tilde{B}_{s})^{2}</span><span class="co">]</span> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{B}_{t}^{2}-2\tilde{B}_{t}\tilde{B}_{s}+\tilde{B}_{s}^{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">B_{t}^{2}</span><span class="co">]</span>-2\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{B}_{t}\tilde{B}_{s}</span><span class="co">]</span>+\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{B}_{s}^{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a> &amp; =t+s-2\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{B}_{t}\tilde{B}_{s}</span><span class="co">]</span></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>c<span class="sc">\)</span> The non-overlapping increments of a $\tilde{\mathbb{P}}$-martingale are independent. To see this, suppose $t_{1}\leq t_{2}\leq t_{3}$:</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">(B_{t_{3}}-B_{t_{2}})(B_{t_{2}}-B_{t_{1}})</span><span class="co">]</span> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{\mathbb{E}}[(B_{t_{3}}-B_{t_{2}})(B_{t_{2}}-B_{t_{1}})|\mathcal{F}_{t_{2}}]</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">(B_{t_{2}}-B_{t_{1}})\tilde{\mathbb{E}}[(B_{t_{3}}-B_{t_{2}})|\mathcal{F}_{t_{2}}]</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">(B_{t_{2}}-B_{t_{1}})(B_{t_{2}}-B_{t_{2}})</span><span class="co">]</span>]=0</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>Also, the covariance</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{B}_{t}\tilde{B}_{s}</span><span class="co">]</span> &amp; =\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">(\tilde{B}_{t}-\tilde{B}_{s})\tilde{B}_{s}</span><span class="co">]</span>+\tilde{\mathbb{E}}<span class="co">[</span><span class="ot">\tilde{B}_{s}^{2}</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a> &amp; =0+s</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>So, $\mathbb{E}<span class="co">[</span><span class="ot">(\tilde{B}_{t}-\tilde{B}_{s})^{2}</span><span class="co">]</span>=t+s-2s=t-s$.</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>Consequently, $\tilde{B}_{t}$ is a $\tilde{\mathbb{P}}$-standard brownian motion.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>
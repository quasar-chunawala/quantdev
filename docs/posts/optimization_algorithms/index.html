<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-06-10">

<title>quantdev.blog - Optimization Algorithms</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../.././symbol.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sell_side_quant_critical_path.html" rel="" target="">
 <span class="menu-text">Sell-side Quant</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../roadmap.html" rel="" target="">
 <span class="menu-text">C++ Roadmap</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://patreon.com/u59411143?utm_medium=unknown&amp;utm_source=join_link&amp;utm_campaign=creatorshare_creator&amp;utm_content=copyLink" rel="" target=""><i class="bi bi-patreon" role="img">
</i> 
 <span class="menu-text">Become a patreon</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/quasar-chunawala" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://linkedin.com/in/quasar-chunawala" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Optimization Algorithms</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 10, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gradient-vector" id="toc-gradient-vector" class="nav-link active" data-scroll-target="#gradient-vector">Gradient vector</a>
  <ul class="collapse">
  <li><a href="#the-directional-derivative" id="toc-the-directional-derivative" class="nav-link" data-scroll-target="#the-directional-derivative">The directional derivative</a></li>
  <li><a href="#gradients-and-steepest-ascent" id="toc-gradients-and-steepest-ascent" class="nav-link" data-scroll-target="#gradients-and-steepest-ascent">Gradients and steepest ascent</a></li>
  </ul></li>
  <li><a href="#gradient-descent---naive-implementation" id="toc-gradient-descent---naive-implementation" class="nav-link" data-scroll-target="#gradient-descent---naive-implementation">Gradient Descent - Naive Implementation</a></li>
  <li><a href="#convergence." id="toc-convergence." class="nav-link" data-scroll-target="#convergence.">Convergence.</a>
  <ul class="collapse">
  <li><a href="#decomposing-the-error" id="toc-decomposing-the-error" class="nav-link" data-scroll-target="#decomposing-the-error">Decomposing the error</a></li>
  <li><a href="#choosing-a-step-size" id="toc-choosing-a-step-size" class="nav-link" data-scroll-target="#choosing-a-step-size">Choosing a step size</a></li>
  </ul></li>
  <li><a href="#stochastic-gradient-descentsgd" id="toc-stochastic-gradient-descentsgd" class="nav-link" data-scroll-target="#stochastic-gradient-descentsgd">Stochastic Gradient Descent(SGD)</a></li>
  <li><a href="#sgdoptimizer-class" id="toc-sgdoptimizer-class" class="nav-link" data-scroll-target="#sgdoptimizer-class"><code>SGDOptimizer</code> class</a></li>
  <li><a href="#learning-rate-decay" id="toc-learning-rate-decay" class="nav-link" data-scroll-target="#learning-rate-decay">Learning Rate Decay</a></li>
  <li><a href="#stochastic-gradient-descent-with-momentum" id="toc-stochastic-gradient-descent-with-momentum" class="nav-link" data-scroll-target="#stochastic-gradient-descent-with-momentum">Stochastic Gradient Descent with Momentum</a>
  <ul class="collapse">
  <li><a href="#the-dynamics-of-momentum" id="toc-the-dynamics-of-momentum" class="nav-link" data-scroll-target="#the-dynamics-of-momentum">The dynamics of Momentum</a></li>
  </ul></li>
  <li><a href="#adding-momentum-to-the-sgdoptimizer-class" id="toc-adding-momentum-to-the-sgdoptimizer-class" class="nav-link" data-scroll-target="#adding-momentum-to-the-sgdoptimizer-class">Adding momentum to the <code>SGDOptimizer</code> class</a></li>
  <li><a href="#adagrad" id="toc-adagrad" class="nav-link" data-scroll-target="#adagrad">AdaGrad</a></li>
  <li><a href="#rmsprop" id="toc-rmsprop" class="nav-link" data-scroll-target="#rmsprop">RMSProp</a>
  <ul class="collapse">
  <li><a href="#the-algorithm" id="toc-the-algorithm" class="nav-link" data-scroll-target="#the-algorithm">The Algorithm</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="gradient-vector" class="level2">
<h2 class="anchored" data-anchor-id="gradient-vector">Gradient vector</h2>
<p><em>Definition</em>. Let <span class="math inline">\(f:\mathbf{R}^n \to \mathbf{R}\)</span> be a scalar-valued function. The gradient vector of <span class="math inline">\(f\)</span> is defined as:</p>
<p><span class="math display">\[\begin{align*}
\nabla f(\mathbf{x}) = \left[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\ldots,\frac{\partial f}{\partial x_n}\right]
\end{align*}\]</span></p>
<p>The graph of the function <span class="math inline">\(f:\mathbf{R}^n \to \mathbf{R}\)</span> is the <em>hypersurface</em> in <span class="math inline">\(\mathbf{R}^{n+1}\)</span> given by the equation <span class="math inline">\(x_{n+1}=f(x_1,\ldots,x_n)\)</span>.</p>
<p><em>Definition</em>. <span class="math inline">\(f\)</span> is said to be <em>differentiable</em> at <span class="math inline">\(\mathbf{a}\)</span> if all the partial derivatives <span class="math inline">\(f_{x_i}(\mathbf{a})\)</span> exist and if the function <span class="math inline">\(h(\mathbf{x})\)</span> defined by:</p>
<p><span class="math display">\[\begin{align*}
h(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a})
\end{align*}\]</span></p>
<p>is a good linear approximation to <span class="math inline">\(f\)</span> near <span class="math inline">\(a\)</span>, meaning that:</p>
<p><span class="math display">\[\begin{align*}
L = \lim_{\mathbf{x} \to \mathbf{a}} \frac{f(\mathbf{x}) - h(\mathbf{x})}{||\mathbf{x} - \mathbf{a}||} = 0
\end{align*}\]</span></p>
<p>If <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(\mathbf{a},f(\mathbf{a})\)</span>, then the hypersurface determined by the graph has a <em>tangent hyperplane</em> at <span class="math inline">\((\mathbf{a},f(\mathbf{a}))\)</span> given by the equation:</p>
<p><span class="math display">\[\begin{align*}
h(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a})
\end{align*}\]</span></p>
<section id="the-directional-derivative" class="level3">
<h3 class="anchored" data-anchor-id="the-directional-derivative">The directional derivative</h3>
<p>Let <span class="math inline">\(f(x,y)\)</span> be a scalar-valued function of two variables. We understand the partial derivative <span class="math inline">\(\frac{\partial f}{\partial x}(a,b)\)</span> as the slope at the point <span class="math inline">\((a,b,f(a,b))\)</span> of the curve obtained as the intersection of the surface <span class="math inline">\(z=f(x,y)\)</span> and the plane <span class="math inline">\(y=b\)</span>. The other partial derivative has a geometric interpretation. However, the surface <span class="math inline">\(z=f(x,y)\)</span> contains infinitely many curves passing through <span class="math inline">\((a,b,f(a,b))\)</span> whose slope we might choose to measure. The directional derivative enables us to do this.</p>
<p>Intuitively, <span class="math inline">\(\frac{\partial f}{\partial x}(a,b)\)</span> is as the rate of change of <span class="math inline">\(f\)</span> as we move <em>infinitesimally</em> from <span class="math inline">\(\mathbf{a}=(a,b)\)</span> in the <span class="math inline">\(\mathbf{i}\)</span> direction.</p>
<p>Mathematically, by the definition of the derivative of <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial x}(a,b) &amp;= \lim_{h \to 0} \frac{f(a+h,b) - f(a,b)}{h}\\
&amp;=\lim_{h \to 0} \frac{f((a,b) + (h,0)) - f(a,b)}{h}\\
&amp;=\lim_{h \to 0} \frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\
&amp;=\lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{i}) - f(\mathbf{a})}{h}
\end{align*}\]</span></p>
<p>Similarly, we have:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial y}(a,b) = \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{j})-f(\mathbf{a})}{h}
\end{align*}\]</span></p>
<p>Writing partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose <span class="math inline">\(\mathbf{v}\)</span> is a unit vector in <span class="math inline">\(\mathbf{R}^2\)</span>. The quantity:</p>
<p><span class="math display">\[\begin{align*}
\lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{v}) - f(\mathbf{a})}{h}
\end{align*}\]</span></p>
<p>is nothing more than the rate of change of <span class="math inline">\(f\)</span> as we move infinitesimally from <span class="math inline">\(\mathbf{a} = (a,b)\)</span> in the direction specified by <span class="math inline">\(\mathbf{v}=(A,B) = A\mathbf{i} + B\mathbf{j}\)</span>.</p>
<p><em>Definition</em>. Let <span class="math inline">\(\mathbf{v}\in \mathbf{R}^n\)</span> be any unit vector, then the <em>directional derivative</em> of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{a}\)</span> in the direction of <span class="math inline">\(\mathbf{v}\)</span>, denoted <span class="math inline">\(D_{\mathbf{v}}f(\mathbf{a})\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \lim_{h \to 0} \frac{f(\mathbf{a} + h\mathbf{v}) - f(\mathbf{a})}{h}
\end{align*}\]</span></p>
<p>Let’s define a new function <span class="math inline">\(F\)</span> of a single variable <span class="math inline">\(t\)</span>, by holding everything else constant:</p>
<p><span class="math display">\[\begin{align*}
F(t) = f(\mathbf{a} + t\mathbf{v})
\end{align*}\]</span></p>
<p>Then, by the definition of directional derivatives, we have:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) &amp;= \lim_{t\to 0} \frac{f(\mathbf{a} + t\mathbf{v}) - f(\mathbf{a})}{t}\\
&amp;= \lim_{t\to 0} \frac{F(t) - F(0)}{t - 0} \\
&amp;= F'(0)
\end{align*}\]</span></p>
<p>That is:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \frac{d}{dt} f(\mathbf{a} + t\mathbf{v})\vert_{t=0}
\end{align*}\]</span></p>
<p>Let <span class="math inline">\(\mathbf{x}(t) = \mathbf{a}+t\mathbf{v}\)</span>. Then, by the chain rule:</p>
<p><span class="math display">\[\begin{align*}
\frac{d}{dt} f(\mathbf{a} + t\mathbf{v}) &amp;= Df(\mathbf{x}) D\mathbf{x}(t) \\
&amp;= \nabla f(\mathbf{x}) \cdot \mathbf{v}
\end{align*}\]</span></p>
<p>This equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p><em>Theorem.</em> Let <span class="math inline">\(f:X\to\mathbf{R}\)</span> be differentiable at <span class="math inline">\(\mathbf{a}\in X\)</span>. Then, the directional derivative <span class="math inline">\(D_{\mathbf{v}}f(\mathbf{a})\)</span> exists for all directions <span class="math inline">\(\mathbf{v}\in\mathbf{R}^n\)</span> and moreover we have:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{v}}f(\mathbf{a}) = \nabla f(\mathbf{x})\cdot \mathbf{v}
\end{align*}\]</span></p>
</section>
<section id="gradients-and-steepest-ascent" class="level3">
<h3 class="anchored" data-anchor-id="gradients-and-steepest-ascent">Gradients and steepest ascent</h3>
<p>Suppose you are traveling in space near the planet Nilrebo and that one of your spaceship’s instruments measures the external atmospheric pressure on your ship as a function <span class="math inline">\(f(x,y,z)\)</span> of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point <span class="math inline">\(\mathbf{a}=(a,b,c)\)</span> in the direction of the unit vector <span class="math inline">\(\mathbf{u}=u\mathbf{i}+v\mathbf{j}+w\mathbf{k}\)</span>, the rate of change of pressure is given by:</p>
<p><span class="math display">\[\begin{align*}
D_{\mathbf{u}}f(\mathbf{a}) = \nabla f(\mathbf{a}) \cdot \mathbf{u} = ||\nabla f(\mathbf{a})|| \cdot ||\mathbf{u}|| \cos \theta
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(\mathbf{u}\)</span> and the gradient vector <span class="math inline">\(\nabla f(\mathbf{a})\)</span>. Because, <span class="math inline">\(-1 \leq \cos \theta \leq 1\)</span>, and <span class="math inline">\(||\mathbf{u}||=1\)</span>, we have:</p>
<p><span class="math display">\[\begin{align*}
- ||\nabla f(\mathbf{a})|| \leq D_{\mathbf{u}}f(\mathbf{a}) \leq ||\nabla f(\mathbf{a})||
\end{align*}\]</span></p>
<p>Moreover, <span class="math inline">\(\cos \theta = 1\)</span> when <span class="math inline">\(\theta = 0\)</span> and <span class="math inline">\(\cos \theta = -1\)</span> when <span class="math inline">\(\theta = \pi\)</span>.</p>
<p><em>Theorem</em>. The directional derivative <span class="math inline">\(D_{\mathbf{u}}f(\mathbf{a})\)</span> is maximized, with respect to the direction, when <span class="math inline">\(\mathbf{u}\)</span> points in the direction of the gradient vector <span class="math inline">\(f(\mathbf{a})\)</span> and is minimized when <span class="math inline">\(\mathbf{u}\)</span> points in the opposite direction. Furthermore, the maximum and minimum values of <span class="math inline">\(D_{\mathbf{u}}f(\mathbf{a})\)</span> are <span class="math inline">\(||\nabla f(\mathbf{a})||\)</span> and <span class="math inline">\(-||\nabla f(\mathbf{a})||\)</span>.</p>
<p><em>Theorem</em> Let <span class="math inline">\(f:X \subseteq \mathbf{R}^n \to \mathbf{R}\)</span> be a function of class <span class="math inline">\(C^1\)</span>. If <span class="math inline">\(\mathbf{x}_0\)</span> is a point on the level set <span class="math inline">\(S=\{\mathbf{x} \in X | f(\mathbf{x}) = c\}\)</span>, then the gradient vector <span class="math inline">\(\nabla f(\mathbf{x}_0) \in \mathbf{R}^n\)</span> is perpendicular to <span class="math inline">\(S\)</span>.</p>
<p><em>Proof.</em> We need to establish the following: if <span class="math inline">\(\mathbf{v}\)</span> is any vector tangent to <span class="math inline">\(S\)</span> at <span class="math inline">\(\mathbf{x}_0\)</span>, then <span class="math inline">\(\nabla f(\mathbf{x}_0)\)</span> is perpendicular to <span class="math inline">\(\mathbf{v}\)</span> (i.e.&nbsp;<span class="math inline">\(\nabla f(\mathbf{x}_0) \cdot \mathbf{v} = 0\)</span>). By a tangent vector to <span class="math inline">\(S\)</span> at <span class="math inline">\(\mathbf{x}_0\)</span>, we mean that <span class="math inline">\(\mathbf{v}\)</span> is the velocity vector of a curve <span class="math inline">\(C\)</span> that lies in <span class="math inline">\(S\)</span> and passes through <span class="math inline">\(\mathbf{x}_0\)</span>.</p>
<p>Let <span class="math inline">\(C\)</span> be given parametrically by <span class="math inline">\(\mathbf{x}(t)=(x_1(t),\ldots,x_n(t))\)</span> where <span class="math inline">\(a &lt; t &lt; b\)</span> and <span class="math inline">\(\mathbf{x}(t_0) = \mathbf{x}_0\)</span> for some number <span class="math inline">\(t_0\)</span> in <span class="math inline">\((a,b)\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\frac{d}{dt}[f(\mathbf{x}(t))] &amp;= Df(\mathbf{x}) \cdot \mathbf{x}'(t)\\
&amp;= \nabla f(\mathbf{x}) \cdot \mathbf{v}
\end{align*}\]</span></p>
<p>Evaluation at <span class="math inline">\(t = t_0\)</span>, yields:</p>
<p><span class="math display">\[\begin{align*}
\nabla f (\mathbf{x}(t_0)) \cdot \mathbf{x}'(t_0) = \nabla f(\mathbf{x}_0) \cdot \mathbf{v}
\end{align*}\]</span></p>
<p>On the other hand, since <span class="math inline">\(C\)</span> is contained in <span class="math inline">\(S\)</span>, <span class="math inline">\(f(\mathbf{x})=c\)</span>. So,</p>
<p><span class="math display">\[\begin{align*}
\frac{d}{dt}[f(\mathbf{x}(t))] &amp;= \frac{d}{dt}[c] = 0
\end{align*}\]</span></p>
<p>Putting the above two facts together, we have the desired result.</p>
</section>
</section>
<section id="gradient-descent---naive-implementation" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent---naive-implementation">Gradient Descent - Naive Implementation</h2>
<p>Beginning at <span class="math inline">\(\mathbf{x}_0\)</span>, optimization algorithms generate a sequence of iterates <span class="math inline">\(\{\mathbf{x}_k\}_{k=0}^{\infty}\)</span> that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. The <em>gradient descent method</em> is an optimization algorithm that moves along <span class="math inline">\(\mathbf{d}_k = -\nabla f(\mathbf{x}_k)\)</span> at every step. Thus,</p>
<p><span class="math display">\[\begin{align*}
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \mathbf{d}_k
\end{align*}\]</span></p>
<p>It can choose the step length <span class="math inline">\(\alpha_k\)</span> in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient <span class="math inline">\(\nabla f(\mathbf{x}_k)\)</span>, but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>load_ext itikz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="kw">def</span> gradient_descent(</span>
<span id="cb2-6"><a href="#cb2-6"></a>    func: Callable[[<span class="bu">float</span>], <span class="bu">float</span>],</span>
<span id="cb2-7"><a href="#cb2-7"></a>    alpha: <span class="bu">float</span>,</span>
<span id="cb2-8"><a href="#cb2-8"></a>    xval_0: np.array,</span>
<span id="cb2-9"><a href="#cb2-9"></a>    epsilon: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-5</span>,</span>
<span id="cb2-10"><a href="#cb2-10"></a>    n_iter: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10000</span>,</span>
<span id="cb2-11"><a href="#cb2-11"></a>    debug_step: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span>,</span>
<span id="cb2-12"><a href="#cb2-12"></a>):</span>
<span id="cb2-13"><a href="#cb2-13"></a>    <span class="co">"""</span></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="co">    The gradient descent algorithm.</span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="co">    """</span></span>
<span id="cb2-16"><a href="#cb2-16"></a></span>
<span id="cb2-17"><a href="#cb2-17"></a>    xval_hist <span class="op">=</span> []</span>
<span id="cb2-18"><a href="#cb2-18"></a>    funcval_hist <span class="op">=</span> []</span>
<span id="cb2-19"><a href="#cb2-19"></a></span>
<span id="cb2-20"><a href="#cb2-20"></a>    xval_curr <span class="op">=</span> xval_0</span>
<span id="cb2-21"><a href="#cb2-21"></a>    error <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb2-22"><a href="#cb2-22"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-23"><a href="#cb2-23"></a></span>
<span id="cb2-24"><a href="#cb2-24"></a>    <span class="cf">while</span> np.linalg.norm(error) <span class="op">&gt;</span> epsilon <span class="kw">and</span> i <span class="op">&lt;</span> n_iter:</span>
<span id="cb2-25"><a href="#cb2-25"></a>        <span class="co"># Save down x_curr and func(x_curr)</span></span>
<span id="cb2-26"><a href="#cb2-26"></a>        xval_hist.append(xval_curr)</span>
<span id="cb2-27"><a href="#cb2-27"></a>        funcval_hist.append(func(xval_curr))</span>
<span id="cb2-28"><a href="#cb2-28"></a></span>
<span id="cb2-29"><a href="#cb2-29"></a>        <span class="co"># Calculate the forward difference</span></span>
<span id="cb2-30"><a href="#cb2-30"></a>        bump <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb2-31"><a href="#cb2-31"></a>        num_dims <span class="op">=</span> <span class="bu">len</span>(xval_curr)</span>
<span id="cb2-32"><a href="#cb2-32"></a>        xval_bump <span class="op">=</span> xval_curr <span class="op">+</span> np.eye(num_dims) <span class="op">*</span> bump</span>
<span id="cb2-33"><a href="#cb2-33"></a>        xval_nobump <span class="op">=</span> np.full((num_dims, num_dims), xval_curr)</span>
<span id="cb2-34"><a href="#cb2-34"></a></span>
<span id="cb2-35"><a href="#cb2-35"></a>        grad <span class="op">=</span> np.array(</span>
<span id="cb2-36"><a href="#cb2-36"></a>            [</span>
<span id="cb2-37"><a href="#cb2-37"></a>                (func(xval_h) <span class="op">-</span> func(xval)) <span class="op">/</span> bump</span>
<span id="cb2-38"><a href="#cb2-38"></a>                <span class="cf">for</span> xval_h, xval <span class="kw">in</span> <span class="bu">zip</span>(xval_bump, xval_nobump)</span>
<span id="cb2-39"><a href="#cb2-39"></a>            ]</span>
<span id="cb2-40"><a href="#cb2-40"></a>        )</span>
<span id="cb2-41"><a href="#cb2-41"></a></span>
<span id="cb2-42"><a href="#cb2-42"></a>        <span class="co"># Compute the next iterate</span></span>
<span id="cb2-43"><a href="#cb2-43"></a>        xval_next <span class="op">=</span> xval_curr <span class="op">-</span> alpha <span class="op">*</span> grad</span>
<span id="cb2-44"><a href="#cb2-44"></a></span>
<span id="cb2-45"><a href="#cb2-45"></a>        <span class="co"># Compute the error vector</span></span>
<span id="cb2-46"><a href="#cb2-46"></a>        error <span class="op">=</span> xval_next <span class="op">-</span> xval_curr</span>
<span id="cb2-47"><a href="#cb2-47"></a></span>
<span id="cb2-48"><a href="#cb2-48"></a>        <span class="cf">if</span> i <span class="op">%</span> debug_step <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-49"><a href="#cb2-49"></a>            <span class="bu">print</span>(</span>
<span id="cb2-50"><a href="#cb2-50"></a>                <span class="ss">f"x[</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">] = </span><span class="sc">{</span>xval_curr<span class="sc">}</span><span class="ss">, f(</span><span class="sc">{</span>xval_curr<span class="sc">}</span><span class="ss">) = </span><span class="sc">{</span>func(xval_curr)<span class="sc">}</span><span class="ss">, f'(</span><span class="sc">{</span>xval_curr<span class="sc">}</span><span class="ss">) = </span><span class="sc">{</span>grad<span class="sc">}</span><span class="ss">, error=</span><span class="sc">{</span>error<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb2-51"><a href="#cb2-51"></a>            )</span>
<span id="cb2-52"><a href="#cb2-52"></a></span>
<span id="cb2-53"><a href="#cb2-53"></a>        xval_curr <span class="op">=</span> xval_next</span>
<span id="cb2-54"><a href="#cb2-54"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-55"><a href="#cb2-55"></a></span>
<span id="cb2-56"><a href="#cb2-56"></a>    <span class="cf">return</span> xval_hist, funcval_hist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>One infamous test function is the <em>Rosenbrock function</em> defined as:</p>
<p><span class="math display">\[\begin{align*}
f(x,y) = (a-x)^2 + b(y-x^2)^2
\end{align*}\]</span></p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> rosenbrock(x):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>x[<span class="dv">0</span>])<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">100</span><span class="op">*</span>(x[<span class="dv">1</span>]<span class="op">-</span>x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3"></a></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="kw">def</span> f(x):</span>
<span id="cb3-5"><a href="#cb3-5"></a>    <span class="cf">return</span> x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is the plot of the Rosenbrock function with parameters <span class="math inline">\(a=1,b=100\)</span>.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb4-2"><a href="#cb4-2"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb4-3"><a href="#cb4-3"></a>\begin{axis}[</span>
<span id="cb4-4"><a href="#cb4-4"></a>     title<span class="op">=</span>{Plot of $f(x,y)<span class="op">=</span>(<span class="dv">1</span><span class="op">-</span>x)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">100</span>(y<span class="op">-</span>x<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span>$},</span>
<span id="cb4-5"><a href="#cb4-5"></a>]</span>
<span id="cb4-6"><a href="#cb4-6"></a>    \addplot3 [surf] {(<span class="dv">1</span><span class="op">-</span>x)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">100</span><span class="op">*</span>(y<span class="op">-</span>x<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span>}<span class="op">;</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>\end{axis}</span>
<span id="cb4-8"><a href="#cb4-8"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<p><img src="index_files/figure-html/cell-5-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>x_history, f_x_history <span class="op">=</span> gradient_descent(</span>
<span id="cb5-2"><a href="#cb5-2"></a>    func<span class="op">=</span>rosenbrock,</span>
<span id="cb5-3"><a href="#cb5-3"></a>    alpha<span class="op">=</span><span class="fl">0.001</span>,</span>
<span id="cb5-4"><a href="#cb5-4"></a>    xval_0<span class="op">=</span>np.array([<span class="op">-</span><span class="fl">2.0</span>, <span class="fl">2.0</span>]),</span>
<span id="cb5-5"><a href="#cb5-5"></a>    epsilon<span class="op">=</span><span class="fl">1e-7</span>,</span>
<span id="cb5-6"><a href="#cb5-6"></a>    debug_step<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb5-7"><a href="#cb5-7"></a>)</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="bu">print</span>(<span class="ss">f"x* = </span><span class="sc">{</span>x_history[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">, f(x*)=</span><span class="sc">{</span>f_x_history[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x[0] = [-2.  2.], f([-2.  2.]) = 409.0, f'([-2.  2.]) = [-1603.9997999  -399.9      ], error=[1.6039998 0.3999   ]
x[1000] = [-0.34194164  0.12278388], f([-0.34194164  0.12278388]) = 1.804241076974863, f'([-0.34194164  0.12278388]) = [-1.8359394   1.27195859], error=[ 0.00183594 -0.00127196]
x[2000] = [0.59082668 0.34719456], f([0.59082668 0.34719456]) = 0.16777685109400048, f'([0.59082668 0.34719456]) = [-0.23242066 -0.27632251], error=[0.00023242 0.00027632]
x[3000] = [0.71914598 0.51617916], f([0.71914598 0.51617916]) = 0.0789773438798074, f'([0.71914598 0.51617916]) = [-0.06806067 -0.09835534], error=[6.80606659e-05 9.83553399e-05]
x[4000] = [0.7626568  0.58094326], f([0.7626568  0.58094326]) = 0.05638109494458334, f'([0.7626568  0.58094326]) = [-0.02638936 -0.04042575], error=[2.63893643e-05 4.04257465e-05]
x[5000] = [0.78028032 0.60825002], f([0.78028032 0.60825002]) = 0.04831123625687607, f'([0.78028032 0.60825002]) = [-0.01115051 -0.01747329], error=[1.11505139e-05 1.74732947e-05]
x[6000] = [0.78785296 0.62017375], f([0.78785296 0.62017375]) = 0.045035368749296534, f'([0.78785296 0.62017375]) = [-0.00487137 -0.00770719], error=[4.87136843e-06 7.70718502e-06]
x[7000] = [0.79118466 0.62545602], f([0.79118466 0.62545602]) = 0.04363059164103049, f'([0.79118466 0.62545602]) = [-0.00215834 -0.00342913], error=[2.1583377e-06 3.4291304e-06]
x[8000] = [0.79266536 0.62781071], f([0.79266536 0.62781071]) = 0.04301342477692797, f'([0.79266536 0.62781071]) = [-0.00096218 -0.00153153], error=[9.62177510e-07 1.53153219e-06]
x[9000] = [0.79332635 0.62886327], f([0.79332635 0.62886327]) = 0.042739342077472306, f'([0.79332635 0.62886327]) = [-0.0004301  -0.00068518], error=[4.30102710e-07 6.85176669e-07]
x* = [0.7936218  0.62933403], f(x*)=0.04261711392593988</code></pre>
</div>
</div>
</section>
<section id="convergence." class="level2">
<h2 class="anchored" data-anchor-id="convergence.">Convergence.</h2>
<p>When applying gradient descent in practice, we need to choose a value for the learning rate parameter <span class="math inline">\(\alpha\)</span>. An error surface <span class="math inline">\(E\)</span> is usually a convex function on the weight space <span class="math inline">\(\mathbf{w}\)</span>. Intuitively, we might expect that increasing the value of <span class="math inline">\(\alpha\)</span> should lead to bigger steps through the weight space and hence faster convergence. However, the successive steps oscillate back and forth across the valley, and if we increase <span class="math inline">\(\alpha\)</span> too much, these oscillations will become divergent. Because <span class="math inline">\(\alpha\)</span> must be kept sufficiently small to avoid divergent oscillations across the valley, progress along the valley is very slow. Gradient descent then takes many small steps to reach the minimum and is a very inefficient procedure.</p>
<p>We can gain deeper insight into this problem, by considering a quadratic approximation to the error function in the neighbourhood of the minimum. Let the error function be given by:</p>
<p><span class="math display">\[\begin{align*}
f(w) = \frac{1}{2}w^T A w - b^T w, \quad w\in\mathbf{R}^n
\end{align*}\]</span></p>
<p>where <span class="math inline">\(A\)</span> is symmetric and <span class="math inline">\(A \succ 0\)</span>.</p>
<p>Differentiating on both sides, the gradient of the error function is:</p>
<p><span class="math display">\[\begin{align*}
\nabla f(w) = Aw - b
\end{align*}\]</span></p>
<p>and the hessian is:</p>
<p><span class="math display">\[\begin{align*}
\nabla^2 f(w) = A
\end{align*}\]</span></p>
<p>The critical points of <span class="math inline">\(f\)</span> are given by:</p>
<p><span class="math display">\[\begin{align*}
\nabla f(w^*) &amp;= 0\\
Aw^{*} - b &amp;= 0\\
w^{*} &amp;= A^{-1}b
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
f(w^{*}) &amp;= \frac{1}{2}(A^{-1}b)^T A (A^{-1}b) - b^T (A^{-1} b)\\
&amp;= \frac{1}{2}b^T A^{-1} A A^{-1} b -b^T A^{-1} b \\
&amp;= \frac{1}{2}b^T A^{-1} b - b^T A^{-1} b \\
&amp;= -\frac{1}{2}b^T A^{-1} b
\end{align*}\]</span></p>
<p>Therefore, the iterates of <span class="math inline">\(w\)</span> are:</p>
<p><span class="math display">\[\begin{align*}
w^{(k+1)} = w^{(k)} - \alpha(Aw^{(k)} - b)
\end{align*}\]</span></p>
<p>By the <em>spectral theorem</em>, every symmetric matrix <span class="math inline">\(A\)</span> is orthogonally diagonalizable. So, <span class="math inline">\(A\)</span> admits a factorization:</p>
<p><span class="math display">\[\begin{align*}
A = Q \Lambda Q^T
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\Lambda = diag(\lambda_1,\ldots,\lambda_n)\)</span> and as per convention, we will assume that <span class="math inline">\(\lambda_i\)</span>are sorted from smallest <span class="math inline">\(\lambda_1\)</span> to biggest <span class="math inline">\(\lambda_n\)</span>.</p>
<p>Recall that <span class="math inline">\(Q=[q_1,\ldots,q_n]\)</span>, where <span class="math inline">\(q_i\)</span> are the eigenvectors of <span class="math inline">\(A\)</span> and <span class="math inline">\(Q\)</span> is the change of basis matrix from the standard basis to the eigenvector basis. So, if <span class="math inline">\(a \in \mathbf{R}^n\)</span> are the coordinates of a vector in the standard basis and <span class="math inline">\(b \in \mathbf{R}^n\)</span> are its coordinates in the eigenvector basis, then <span class="math inline">\(a = Qb\)</span> or <span class="math inline">\(b=Q^T a\)</span>.</p>
<p>Let <span class="math inline">\(x^{(k)}=Q^T(w^{(k)}-w^{*})\)</span>. Equivalently, <span class="math inline">\(w^{(k)} = Qx^{(k)} + w^{*}\)</span>. Thus, we are shifting the origin to <span class="math inline">\(w^{*}\)</span> and changing the axes to be aligned with the eigenvectors. In this new coordinate system,</p>
<p><span class="math display">\[\begin{align*}
Qx^{(k+1)} + w^{*} &amp;= Qx^{(k)} + w^{*} - \alpha(AQx^{(k)} + Aw^{*} - b)\\
Qx^{(k+1)} &amp;= Qx^{(k)} - \alpha(AQx^{(k)} + Aw^{*} - b)\\
Qx^{(k+1)} &amp;= Qx^{(k)} - \alpha(AQx^{(k)} + A(A^{-1}b) - b)\\
&amp; \quad \{\text{Substituting } w^{*}=A^{-1}b \}\\
Qx^{(k+1)} &amp;= Qx^{(k)} - \alpha(AQx^{(k)})\\
Qx^{(k+1)} &amp;= Qx^{(k)} - \alpha(Q\Lambda Q^T Qx^{(k)})\\
&amp; \quad \{\text{Substituting } A = Q\Lambda Q^T \}\\
Qx^{(k+1)} &amp;= Qx^{(k)} - \alpha(Q\Lambda x^{(k)})\\
&amp; \quad \{\text{Using } Q^T Q = I \}\\
x^{(k+1)} &amp;= x^{(k)} - \alpha\Lambda x^{(k)}
\end{align*}\]</span></p>
<p>The <span class="math inline">\(i\)</span>-th coordinate of this recursive system is given by:</p>
<p><span class="math display">\[\begin{align*}
x_i^{(k+1)} &amp;= x_i^{(k)} - \alpha\lambda_i x_i^{(k)}\\
&amp;= (1-\alpha \lambda_i)x_i^{(k)}\\
&amp;= (1-\alpha \lambda_i)^{k+1}x_i^{(0)}
\end{align*}\]</span></p>
<p>Moving back to our original space <span class="math inline">\(w\)</span>, we can see that:</p>
<p><span class="math display">\[\begin{align*}
w^{(k)} - w^{*} = Qx^{(k)} &amp;= \sum_i q_i x_i^{(k)}\\
&amp;= \sum_i q_i (1-\alpha \lambda_i)^{k+1} x_i^{(0)}
\end{align*}\]</span></p>
<p>and there we have it - gradient descent in the closed form.</p>
<section id="decomposing-the-error" class="level3">
<h3 class="anchored" data-anchor-id="decomposing-the-error">Decomposing the error</h3>
<p>The above equation admits a simple interpretation. Each element of <span class="math inline">\(x^{(0)}\)</span> is the component of the error in the initial guess in <span class="math inline">\(Q\)</span>-basis. There are <span class="math inline">\(n\)</span> such errors and each of these errors follow their own, solitary path to the minimum, decreasing exponentially with a compounding rate of <span class="math inline">\(1-\alpha \lambda_i\)</span>. The closer that number is to <span class="math inline">\(1\)</span>, the slower it converges.</p>
<p>For most step-sizes, the eigenvectors with the largest eigenvalues converge the fastest. This triggers an explosion of progress in the first few iterations, before things slow down, as the eigenvectors with smaller eigenvalues’ struggles are revealed. It’s easy to visualize this - look at the sequences of <span class="math inline">\(\frac{1}{2^k}\)</span> and <span class="math inline">\(\frac{1}{3^k}\)</span>.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb7-2"><a href="#cb7-2"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb7-3"><a href="#cb7-3"></a>\begin{axis}[</span>
<span id="cb7-4"><a href="#cb7-4"></a>     title<span class="op">=</span>{Comparison of the rates of convergence},</span>
<span id="cb7-5"><a href="#cb7-5"></a>     xlabel<span class="op">=</span>{$n$},</span>
<span id="cb7-6"><a href="#cb7-6"></a>     ylabel<span class="op">=</span>{$f(n)$}</span>
<span id="cb7-7"><a href="#cb7-7"></a>]</span>
<span id="cb7-8"><a href="#cb7-8"></a>    \addplot [domain<span class="op">=</span><span class="dv">0</span>:<span class="dv">5</span>,samples<span class="op">=</span><span class="dv">400</span>,blue] {<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">^</span>x)} node [midway,above] {$<span class="dv">2</span><span class="op">^</span>{<span class="op">-</span>n}$}<span class="op">;</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>    \addplot [domain<span class="op">=</span><span class="dv">0</span>:<span class="dv">5</span>,samples<span class="op">=</span><span class="dv">400</span>,red] {<span class="dv">1</span><span class="op">/</span>(<span class="dv">3</span><span class="op">^</span>x)} node [midway,below] {$<span class="dv">3</span><span class="op">^</span>{<span class="op">-</span>n}$}<span class="op">;</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>\end{axis}</span>
<span id="cb7-11"><a href="#cb7-11"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<p><img src="index_files/figure-html/cell-7-output-1.svg" class="img-fluid"></p>
</div>
</div>
</section>
<section id="choosing-a-step-size" class="level3">
<h3 class="anchored" data-anchor-id="choosing-a-step-size">Choosing a step size</h3>
<p>The above analysis gives us immediate guidance as to how to set a step-size <span class="math inline">\(\alpha\)</span>. In order to converge, each <span class="math inline">\(|1-\alpha \lambda_i| &lt; 1\)</span>. All workable step-sizes, therefore, fall in the interval:</p>
<p><span class="math display">\[\begin{align*}
-1 &amp;\leq 1 - \alpha \lambda_i &amp;\leq 1 \\
-2 &amp;\leq - \alpha \lambda_i &amp;\leq 0 \\
0 &amp;\leq \alpha \lambda_i &amp;\leq 2
\end{align*}\]</span></p>
<p>Because <span class="math inline">\((1-\alpha \lambda_i)\)</span> could be either positive or negative, the overall convergence rate is determined by the slowest error component, which must be either <span class="math inline">\(\lambda_1\)</span> or <span class="math inline">\(\lambda_n\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\text{rate}(\alpha) = \max \{|1-\alpha \lambda_1|,|1-\alpha \lambda_n|\}
\end{align*}\]</span></p>
<p>The optimal learning rate is that which balances the convergence rate. Setting the convergence rate to be equal for the smallest and largest eigenvalues, we can solve for the optimal step size.</p>
<p><span class="math display">\[\begin{align*}
|1- \alpha \lambda_1| = |1- \alpha \lambda_n|
\end{align*}\]</span></p>
<p>Assuming <span class="math inline">\(\lambda_1 \neq \lambda_n\)</span>:</p>
<p><span class="math display">\[\begin{align*}
1 - \alpha \lambda_1 &amp;= -1 + \alpha \lambda_n\\
\alpha (\lambda_1 + \lambda_n) &amp;= 2\\
\alpha^* &amp;= \frac{2}{\lambda_1 + \lambda_n}
\end{align*}\]</span></p>
<p>So, the optimal convergence rate equals:</p>
<p><span class="math display">\[\begin{align*}
\max \{|1-\alpha \lambda_1|,|1-\alpha \lambda_n|\} &amp;= 1 - \frac{2\lambda_1}{\lambda_1 + \lambda_n} \\
&amp;= \frac{\lambda_n - \lambda_1}{\lambda_n + \lambda_1}\\
&amp;= \frac{\kappa - 1}{\kappa + 1}
\end{align*}\]</span></p>
<p>The ratio <span class="math inline">\(\kappa = \lambda_n / \lambda_1\)</span> determines the convergence rate of the problem. Recall that the level curves of the error surface are ellipsoids. Hence, a poorly conditioned Hessian results in stretching one of the axes of the ellipses, and taken to its extreme, the contours are almost parallel. Since gradient vectors are orthogonal to the level curves, the optimizer keeps pin-balling between parallel lines and takes forever to reach the center.</p>
</section>
</section>
<section id="stochastic-gradient-descentsgd" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descentsgd">Stochastic Gradient Descent(SGD)</h2>
<p>In machine learning applications, we typically want to minimize the loss function <span class="math inline">\(\mathcal{L}(w)\)</span> that has the form of a sum:</p>
<p><span class="math display">\[\begin{align*}
\mathcal{L}(w) = \frac{1}{n}\sum_i L_i(w)
\end{align*}\]</span></p>
<p>where the weights <span class="math inline">\(w\)</span> (and the biases) are to be estimated. Each summand function <span class="math inline">\(L_i\)</span> is typically associated with the <span class="math inline">\(i\)</span>-th sample in the data-set used for training.</p>
<p>When we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:</p>
<p><span class="math display">\[\begin{align*}
w_{k+1} := w_k - \alpha_k \nabla \mathcal{L}(w_{k}) = w_k - \frac{\alpha_k}{n}\sum_{i} \nabla L_i(w_{k})
\end{align*}\]</span></p>
<p>In the stochastic (or online) gradient descent algorithm, the true gradient of <span class="math inline">\(\mathcal{L}(w)\)</span> is approximated by the gradient at a single sample:</p>
<p><span class="math display">\[\begin{align*}
w_{k+1} := w_k - \alpha_k \nabla \mathcal{L}(w_{k}) = w_k - \alpha_k \nabla L_i(w_{k})
\end{align*}\]</span></p>
</section>
<section id="sgdoptimizer-class" class="level2">
<h2 class="anchored" data-anchor-id="sgdoptimizer-class"><code>SGDOptimizer</code> class</h2>
<p>We are now in a position to code the <code>SGDOptimizer</code> class.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Global imports</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="im">import</span> nnfs</span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="im">from</span> nnfs.datasets <span class="im">import</span> spiral_data</span>
<span id="cb8-6"><a href="#cb8-6"></a></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="im">from</span> dense_layer <span class="im">import</span> DenseLayer</span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="im">from</span> relu_activation <span class="im">import</span> ReLUActivation</span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="im">from</span> softmax_activation <span class="im">import</span> SoftmaxActivation</span>
<span id="cb8-10"><a href="#cb8-10"></a></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="im">from</span> loss <span class="im">import</span> Loss</span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="im">from</span> categorical_cross_entropy_loss <span class="im">import</span> CategoricalCrossEntropyLoss</span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="im">from</span> categorical_cross_entropy_softmax <span class="im">import</span> CategoricalCrossEntropySoftmax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">class</span> SGDOptimizer:</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a>    <span class="co"># Initialize the optimizer</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb9-5"><a href="#cb9-5"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a>    <span class="co"># Update the parameters</span></span>
<span id="cb9-8"><a href="#cb9-8"></a>    <span class="kw">def</span> update_params(<span class="va">self</span>, layer):</span>
<span id="cb9-9"><a href="#cb9-9"></a>        layer.weights <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> layer.dloss_dweights</span>
<span id="cb9-10"><a href="#cb9-10"></a>        layer.biases <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> layer.dloss_dbiases</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s play around with our optimizer.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Create dataset</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>X, y <span class="op">=</span> spiral_data(samples<span class="op">=</span><span class="dv">100</span>, classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="co"># Create a DenseLayer with 2 input features and 64 neurons</span></span>
<span id="cb10-5"><a href="#cb10-5"></a>dense1 <span class="op">=</span> DenseLayer(<span class="dv">2</span>, <span class="dv">64</span>)</span>
<span id="cb10-6"><a href="#cb10-6"></a></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co"># Create ReLU Activation (to be used with DenseLayer 1)</span></span>
<span id="cb10-8"><a href="#cb10-8"></a>activation1 <span class="op">=</span> ReLUActivation()</span>
<span id="cb10-9"><a href="#cb10-9"></a></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="co"># Create the second DenseLayer with 64 inputs and 3 output values</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>dense2 <span class="op">=</span> DenseLayer(<span class="dv">64</span>,<span class="dv">3</span>)</span>
<span id="cb10-12"><a href="#cb10-12"></a></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="co"># Create SoftmaxClassifer's combined loss and activation</span></span>
<span id="cb10-14"><a href="#cb10-14"></a>loss_activation <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb10-15"><a href="#cb10-15"></a></span>
<span id="cb10-16"><a href="#cb10-16"></a><span class="co"># The next step is to create the optimizer object</span></span>
<span id="cb10-17"><a href="#cb10-17"></a>optimizer <span class="op">=</span> SGDOptimizer()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we perform a <em>forward pass</em> of our sample data.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Perform a forward pass for our sample data</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>dense1.forward(X)</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># Performs a forward pass through the activation function</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="co"># takes the output of the first dense layer here</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>activation1.forward(dense1.output)</span>
<span id="cb11-7"><a href="#cb11-7"></a></span>
<span id="cb11-8"><a href="#cb11-8"></a><span class="co"># Performs a forward pass through the second DenseLayer</span></span>
<span id="cb11-9"><a href="#cb11-9"></a>dense2.forward(activation1.output)</span>
<span id="cb11-10"><a href="#cb11-10"></a></span>
<span id="cb11-11"><a href="#cb11-11"></a><span class="co"># Performs a forward pass through the activation/loss function</span></span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="co"># takes the output of the second DenseLayer and returns the loss</span></span>
<span id="cb11-13"><a href="#cb11-13"></a>loss <span class="op">=</span> loss_activation.forward(dense2.output, y)</span>
<span id="cb11-14"><a href="#cb11-14"></a></span>
<span id="cb11-15"><a href="#cb11-15"></a><span class="co"># Let's print the loss value</span></span>
<span id="cb11-16"><a href="#cb11-16"></a><span class="bu">print</span>(<span class="ss">f"Loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-17"><a href="#cb11-17"></a></span>
<span id="cb11-18"><a href="#cb11-18"></a><span class="co"># Now we do our backward pass </span></span>
<span id="cb11-19"><a href="#cb11-19"></a>loss_activation.backward(loss_activation.output, y)</span>
<span id="cb11-20"><a href="#cb11-20"></a>dense2.backward(loss_activation.dloss_dz)</span>
<span id="cb11-21"><a href="#cb11-21"></a>activation1.backward(dense2.dloss_dinputs)</span>
<span id="cb11-22"><a href="#cb11-22"></a>dense1.backward(activation1.dloss_dz)</span>
<span id="cb11-23"><a href="#cb11-23"></a></span>
<span id="cb11-24"><a href="#cb11-24"></a><span class="co"># Then finally we use our optimizer to update the weights and biases</span></span>
<span id="cb11-25"><a href="#cb11-25"></a>optimizer.update_params(dense1)</span>
<span id="cb11-26"><a href="#cb11-26"></a>optimizer.update_params(dense2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss = 1.0986526582562541</code></pre>
</div>
</div>
<p>This is everything we need to train our model!</p>
<p>But why would we only perform this optimization only once, when we can perform it many times by leveraging Python’s looping capabilities? We will repeatedly perform a forward pass, backward pass and optimization until we reach some stopping point. Each full pass through all of the training data is called an <em>epoch</em>.</p>
<p>In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of our training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass and optimization calculations.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Create dataset</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>X, y <span class="op">=</span> spiral_data(samples<span class="op">=</span><span class="dv">100</span>, classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># Create a dense layer with 2 input features and 64 output values</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>dense1 <span class="op">=</span> DenseLayer(<span class="dv">2</span>, <span class="dv">64</span>)</span>
<span id="cb13-6"><a href="#cb13-6"></a></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co"># Create ReLU Activation (to be used with the DenseLayer)</span></span>
<span id="cb13-8"><a href="#cb13-8"></a>activation1 <span class="op">=</span> ReLUActivation()</span>
<span id="cb13-9"><a href="#cb13-9"></a></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co"># Create a second DenseLayer with 64 input features (as we take</span></span>
<span id="cb13-11"><a href="#cb13-11"></a><span class="co"># output of the previous layer here) and 3 output values (output values)</span></span>
<span id="cb13-12"><a href="#cb13-12"></a>dense2 <span class="op">=</span> DenseLayer(<span class="dv">64</span>, <span class="dv">3</span>)</span>
<span id="cb13-13"><a href="#cb13-13"></a></span>
<span id="cb13-14"><a href="#cb13-14"></a><span class="co"># Create Softmax classifier's combined loss and activation</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>loss_activation <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb13-16"><a href="#cb13-16"></a></span>
<span id="cb13-17"><a href="#cb13-17"></a><span class="co"># Create optimizer</span></span>
<span id="cb13-18"><a href="#cb13-18"></a>optimizer <span class="op">=</span> SGDOptimizer()</span>
<span id="cb13-19"><a href="#cb13-19"></a></span>
<span id="cb13-20"><a href="#cb13-20"></a><span class="co"># Train in loop</span></span>
<span id="cb13-21"><a href="#cb13-21"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10001</span>):</span>
<span id="cb13-22"><a href="#cb13-22"></a></span>
<span id="cb13-23"><a href="#cb13-23"></a>    <span class="co"># Perform a forward pass of our training data through this layer</span></span>
<span id="cb13-24"><a href="#cb13-24"></a>    dense1.forward(X)</span>
<span id="cb13-25"><a href="#cb13-25"></a></span>
<span id="cb13-26"><a href="#cb13-26"></a>    <span class="co"># Perform a forward pass through the activation function</span></span>
<span id="cb13-27"><a href="#cb13-27"></a>    <span class="co"># takes the output of the first dense layer here</span></span>
<span id="cb13-28"><a href="#cb13-28"></a>    activation1.forward(dense1.output)</span>
<span id="cb13-29"><a href="#cb13-29"></a></span>
<span id="cb13-30"><a href="#cb13-30"></a>    <span class="co"># Perform a forward pass through second DenseLayer</span></span>
<span id="cb13-31"><a href="#cb13-31"></a>    <span class="co"># takes the outputs of the activation function of first layer as inputs</span></span>
<span id="cb13-32"><a href="#cb13-32"></a>    dense2.forward(activation1.output)</span>
<span id="cb13-33"><a href="#cb13-33"></a></span>
<span id="cb13-34"><a href="#cb13-34"></a>    <span class="co"># Perform a forward pass through the activation/loss function</span></span>
<span id="cb13-35"><a href="#cb13-35"></a>    <span class="co"># takes the output of the second DenseLayer here and returns the loss</span></span>
<span id="cb13-36"><a href="#cb13-36"></a>    loss <span class="op">=</span> loss_activation.forward(dense2.output, y)</span>
<span id="cb13-37"><a href="#cb13-37"></a></span>
<span id="cb13-38"><a href="#cb13-38"></a>    <span class="cf">if</span> <span class="kw">not</span> epoch <span class="op">%</span> <span class="dv">1000</span>:</span>
<span id="cb13-39"><a href="#cb13-39"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">: .3f}</span><span class="ss">"</span>)</span>
<span id="cb13-40"><a href="#cb13-40"></a></span>
<span id="cb13-41"><a href="#cb13-41"></a>    <span class="co"># Backward pass</span></span>
<span id="cb13-42"><a href="#cb13-42"></a>    loss_activation.backward(loss_activation.output, y)</span>
<span id="cb13-43"><a href="#cb13-43"></a>    dense2.backward(loss_activation.dloss_dz)</span>
<span id="cb13-44"><a href="#cb13-44"></a>    activation1.backward(dense2.dloss_dinputs)</span>
<span id="cb13-45"><a href="#cb13-45"></a>    dense1.backward(activation1.dloss_dz)</span>
<span id="cb13-46"><a href="#cb13-46"></a></span>
<span id="cb13-47"><a href="#cb13-47"></a>    <span class="co"># Update the weights and the biases</span></span>
<span id="cb13-48"><a href="#cb13-48"></a>    optimizer.update_params(dense1)</span>
<span id="cb13-49"><a href="#cb13-49"></a>    optimizer.update_params(dense2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0, Loss:  1.099
Epoch: 1000, Loss:  1.029
Epoch: 2000, Loss:  0.962
Epoch: 3000, Loss:  0.848
Epoch: 4000, Loss:  0.699
Epoch: 5000, Loss:  0.544
Epoch: 6000, Loss:  0.508
Epoch: 7000, Loss:  0.478
Epoch: 8000, Loss:  0.460
Epoch: 9000, Loss:  0.443
Epoch: 10000, Loss:  0.419</code></pre>
</div>
</div>
<p>Our neural network mostly stays stuck at around a loss of <span class="math inline">\(1.0\)</span> and later around <span class="math inline">\(0.85\)</span>-<span class="math inline">\(0.90\)</span> Given that this loss didn’t decrease much, we can assume that this learning rate being too high, also caused the model to get stuck in a <strong>local minimum</strong>, which we’ll learn more about soon. Iterating over more epochs, doesn’t seem helpful at this point, which tells us that we’re likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?</p>
<p>Recall that we’re adjusting our weights and biases by applying some fraction, in this case <span class="math inline">\(1.0\)</span> to the gradient and subtracting this from the weights and biases. This fraction is called the <strong>learning rate</strong> (LR) and is the primary adjustable parameter for the optimizer as it decreases loss.</p>
</section>
<section id="learning-rate-decay" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate-decay">Learning Rate Decay</h2>
<p>The idea of a <em>learning rate decay</em> is to start with a large learning rate, say <span class="math inline">\(1.0\)</span> in our case and then decrease it during training. There are a few methods for doing this. One option is program a <strong>decay rate</strong>, which steadily decays the learning rate per batch or per epoch.</p>
<p>Let’s plan to decay per step. This can also be referred to as <span class="math inline">\(1/t\)</span> <strong>decaying</strong> or <strong>exponential decaying</strong>. Basically, we’re going to update the learning rate each step by the reciprocal of the step count fraction. This fraction is a new hyper parameter that we’ll add to the optimizer, called the <strong>learning rate decay</strong>.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>initial_learning_rate <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>learning_rate_decay <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb15-5"><a href="#cb15-5"></a>    learning_rate <span class="op">=</span> initial_learning_rate <span class="op">*</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> learning_rate_decay <span class="op">*</span> step)</span>
<span id="cb15-6"><a href="#cb15-6"></a>    <span class="bu">print</span>(learning_rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.0
0.9090909090909091
0.8333333333333334
0.7692307692307692
0.7142857142857143
0.6666666666666666
0.625
0.588235294117647
0.5555555555555556
0.5263157894736842</code></pre>
</div>
</div>
<p>The derivative of the function <span class="math inline">\(\frac{1}{1+x}\)</span> is <span class="math inline">\(-\frac{1}{(1+x)^2}\)</span>.</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="op">%%</span>itikz <span class="op">--</span>temp<span class="op">-</span><span class="bu">dir</span> <span class="op">--</span>tex<span class="op">-</span>packages<span class="op">=</span>tikz,pgfplots <span class="op">--</span>tikz<span class="op">-</span>libraries<span class="op">=</span>arrows <span class="op">--</span>implicit<span class="op">-</span>standalone</span>
<span id="cb17-2"><a href="#cb17-2"></a>\begin{tikzpicture}[scale<span class="op">=</span><span class="fl">1.5</span>]</span>
<span id="cb17-3"><a href="#cb17-3"></a>\begin{axis}[</span>
<span id="cb17-4"><a href="#cb17-4"></a>     title<span class="op">=</span>{Plot of $f(x)<span class="op">=-</span>\frac{<span class="dv">1</span>}{(<span class="dv">1</span><span class="op">+</span>x)<span class="op">^</span><span class="dv">2</span>}$},</span>
<span id="cb17-5"><a href="#cb17-5"></a>     xlabel<span class="op">=</span>{$x$},</span>
<span id="cb17-6"><a href="#cb17-6"></a>     ylabel<span class="op">=</span>{$f(x)$}</span>
<span id="cb17-7"><a href="#cb17-7"></a>]</span>
<span id="cb17-8"><a href="#cb17-8"></a>    \addplot [domain<span class="op">=</span><span class="dv">0</span>:<span class="dv">1</span>,samples<span class="op">=</span><span class="dv">400</span>] {<span class="op">-</span><span class="dv">1</span><span class="op">/</span>(( <span class="dv">1</span> <span class="op">+</span> x)<span class="op">^</span><span class="dv">2</span>)}<span class="op">;</span></span>
<span id="cb17-9"><a href="#cb17-9"></a>\end{axis}</span>
<span id="cb17-10"><a href="#cb17-10"></a>\end{tikzpicture}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<p><img src="index_files/figure-html/cell-14-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>The learning rate drops fast initially, but the change in the learning rate lowers in each step. We can update our <code>SGDOptimizer</code> class to allow for the learning rate decay.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">class</span> SGDOptimizer:</span>
<span id="cb18-2"><a href="#cb18-2"></a></span>
<span id="cb18-3"><a href="#cb18-3"></a>    <span class="co"># Initial optimizer - set settings</span></span>
<span id="cb18-4"><a href="#cb18-4"></a>    <span class="co"># learning rate of 1. is default for this optimizer</span></span>
<span id="cb18-5"><a href="#cb18-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">1.0</span>, decay<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb18-6"><a href="#cb18-6"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb18-7"><a href="#cb18-7"></a>        <span class="va">self</span>.current_learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb18-8"><a href="#cb18-8"></a>        <span class="va">self</span>.decay <span class="op">=</span> decay</span>
<span id="cb18-9"><a href="#cb18-9"></a>        <span class="va">self</span>.iterations <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-10"><a href="#cb18-10"></a></span>
<span id="cb18-11"><a href="#cb18-11"></a>    <span class="co"># Call once before any parameter updates</span></span>
<span id="cb18-12"><a href="#cb18-12"></a>    <span class="kw">def</span> pre_update_params(<span class="va">self</span>):</span>
<span id="cb18-13"><a href="#cb18-13"></a>        <span class="cf">if</span> <span class="va">self</span>.decay:</span>
<span id="cb18-14"><a href="#cb18-14"></a>            <span class="va">self</span>.current_learning_rate <span class="op">=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> (</span>
<span id="cb18-15"><a href="#cb18-15"></a>                <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> <span class="va">self</span>.decay <span class="op">*</span> <span class="va">self</span>.iterations)</span>
<span id="cb18-16"><a href="#cb18-16"></a>            )</span>
<span id="cb18-17"><a href="#cb18-17"></a></span>
<span id="cb18-18"><a href="#cb18-18"></a>    <span class="co"># Update parameters</span></span>
<span id="cb18-19"><a href="#cb18-19"></a>    <span class="kw">def</span> update_params(<span class="va">self</span>, layer):</span>
<span id="cb18-20"><a href="#cb18-20"></a>        layer.weights <span class="op">+=</span> <span class="op">-</span><span class="va">self</span>.current_learning_rate <span class="op">*</span> layer.dloss_dweights</span>
<span id="cb18-21"><a href="#cb18-21"></a>        layer.biases <span class="op">+=</span> <span class="op">-</span><span class="va">self</span>.current_learning_rate <span class="op">*</span> layer.dloss_dbiases</span>
<span id="cb18-22"><a href="#cb18-22"></a></span>
<span id="cb18-23"><a href="#cb18-23"></a>    <span class="kw">def</span> post_update_params(<span class="va">self</span>):</span>
<span id="cb18-24"><a href="#cb18-24"></a>        <span class="va">self</span>.iterations <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s use a decay rate of <span class="math inline">\(0.01\)</span> and train our neural network again.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="kw">def</span> train(decay):</span>
<span id="cb19-2"><a href="#cb19-2"></a>    <span class="co"># Create a dataset</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>    X, y <span class="op">=</span> spiral_data(samples<span class="op">=</span><span class="dv">100</span>, classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a>    <span class="co"># Create a dense layer with 2 input features and 64 output values</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>    dense1 <span class="op">=</span> DenseLayer(<span class="dv">2</span>, <span class="dv">64</span>)</span>
<span id="cb19-7"><a href="#cb19-7"></a></span>
<span id="cb19-8"><a href="#cb19-8"></a>    <span class="co"># Create ReLU activation (to be used with the dense layer)</span></span>
<span id="cb19-9"><a href="#cb19-9"></a>    activation1 <span class="op">=</span> ReLUActivation()</span>
<span id="cb19-10"><a href="#cb19-10"></a></span>
<span id="cb19-11"><a href="#cb19-11"></a>    <span class="co"># Create second DenseLayer with 64 input features (as we take output of the</span></span>
<span id="cb19-12"><a href="#cb19-12"></a>    <span class="co"># previous layer here) and 3 output values</span></span>
<span id="cb19-13"><a href="#cb19-13"></a>    dense2 <span class="op">=</span> DenseLayer(<span class="dv">64</span>, <span class="dv">3</span>)</span>
<span id="cb19-14"><a href="#cb19-14"></a></span>
<span id="cb19-15"><a href="#cb19-15"></a>    <span class="co"># Create Softmax classifier's combined loss and activation</span></span>
<span id="cb19-16"><a href="#cb19-16"></a>    loss_activation <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb19-17"><a href="#cb19-17"></a></span>
<span id="cb19-18"><a href="#cb19-18"></a>    <span class="co"># Create optimizer</span></span>
<span id="cb19-19"><a href="#cb19-19"></a>    optimizer <span class="op">=</span> SGDOptimizer(learning_rate<span class="op">=</span><span class="fl">1.0</span>,decay<span class="op">=</span>decay)</span>
<span id="cb19-20"><a href="#cb19-20"></a></span>
<span id="cb19-21"><a href="#cb19-21"></a>    acc_vals <span class="op">=</span> []</span>
<span id="cb19-22"><a href="#cb19-22"></a>    loss_vals <span class="op">=</span> []</span>
<span id="cb19-23"><a href="#cb19-23"></a>    lr_vals <span class="op">=</span> []</span>
<span id="cb19-24"><a href="#cb19-24"></a></span>
<span id="cb19-25"><a href="#cb19-25"></a>    <span class="co"># Train in a loop</span></span>
<span id="cb19-26"><a href="#cb19-26"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10001</span>):</span>
<span id="cb19-27"><a href="#cb19-27"></a>        <span class="co"># Perform a forward pass of our training data through this layer</span></span>
<span id="cb19-28"><a href="#cb19-28"></a>        dense1.forward(X)</span>
<span id="cb19-29"><a href="#cb19-29"></a></span>
<span id="cb19-30"><a href="#cb19-30"></a>        <span class="co"># Perform a forward pass through the activation function</span></span>
<span id="cb19-31"><a href="#cb19-31"></a>        <span class="co"># takes the output of the first dense layer here</span></span>
<span id="cb19-32"><a href="#cb19-32"></a>        activation1.forward(dense1.output)</span>
<span id="cb19-33"><a href="#cb19-33"></a></span>
<span id="cb19-34"><a href="#cb19-34"></a>        <span class="co"># Perform a forward pass through second DenseLayer</span></span>
<span id="cb19-35"><a href="#cb19-35"></a>        <span class="co"># takes the outputs of the activation function of first layer as inputs</span></span>
<span id="cb19-36"><a href="#cb19-36"></a>        dense2.forward(activation1.output)</span>
<span id="cb19-37"><a href="#cb19-37"></a></span>
<span id="cb19-38"><a href="#cb19-38"></a>        <span class="co"># Perform a forward pass through the activation/loss function</span></span>
<span id="cb19-39"><a href="#cb19-39"></a>        <span class="co"># takes the output of the second DenseLayer here and returns the loss</span></span>
<span id="cb19-40"><a href="#cb19-40"></a>        loss <span class="op">=</span> loss_activation.forward(dense2.output, y)</span>
<span id="cb19-41"><a href="#cb19-41"></a></span>
<span id="cb19-42"><a href="#cb19-42"></a>        <span class="co"># Calculate accuracy from output of activation2 and targets</span></span>
<span id="cb19-43"><a href="#cb19-43"></a>        <span class="co"># Calculate values along the first axis</span></span>
<span id="cb19-44"><a href="#cb19-44"></a>        predictions <span class="op">=</span> np.argmax(loss_activation.output, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-45"><a href="#cb19-45"></a>        <span class="cf">if</span> <span class="bu">len</span>(y.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb19-46"><a href="#cb19-46"></a>            y <span class="op">=</span> np.argmax(y, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-47"><a href="#cb19-47"></a></span>
<span id="cb19-48"><a href="#cb19-48"></a>        accuracy <span class="op">=</span> np.mean(predictions <span class="op">==</span> y)</span>
<span id="cb19-49"><a href="#cb19-49"></a></span>
<span id="cb19-50"><a href="#cb19-50"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb19-51"><a href="#cb19-51"></a>            <span class="bu">print</span>(</span>
<span id="cb19-52"><a href="#cb19-52"></a>                <span class="ss">f"epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, </span><span class="ch">\</span></span>
<span id="cb19-53"><a href="#cb19-53"></a><span class="ss">                acc : </span><span class="sc">{</span>accuracy<span class="sc">:.3f}</span><span class="ss">, </span><span class="ch">\</span></span>
<span id="cb19-54"><a href="#cb19-54"></a><span class="ss">                loss: </span><span class="sc">{</span>loss<span class="sc">: .3f}</span><span class="ss">, </span><span class="ch">\</span></span>
<span id="cb19-55"><a href="#cb19-55"></a><span class="ss">                lr : </span><span class="sc">{</span>optimizer<span class="sc">.</span>current_learning_rate<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb19-56"><a href="#cb19-56"></a>            )</span>
<span id="cb19-57"><a href="#cb19-57"></a></span>
<span id="cb19-58"><a href="#cb19-58"></a>        acc_vals.append(accuracy)</span>
<span id="cb19-59"><a href="#cb19-59"></a>        loss_vals.append(loss)</span>
<span id="cb19-60"><a href="#cb19-60"></a>        lr_vals.append(optimizer.current_learning_rate)</span>
<span id="cb19-61"><a href="#cb19-61"></a></span>
<span id="cb19-62"><a href="#cb19-62"></a>        <span class="co"># Backward pass</span></span>
<span id="cb19-63"><a href="#cb19-63"></a>        loss_activation.backward(loss_activation.output, y)</span>
<span id="cb19-64"><a href="#cb19-64"></a>        dense2.backward(loss_activation.dloss_dz)</span>
<span id="cb19-65"><a href="#cb19-65"></a>        activation1.backward(dense2.dloss_dinputs)</span>
<span id="cb19-66"><a href="#cb19-66"></a>        dense1.backward(activation1.dloss_dz)</span>
<span id="cb19-67"><a href="#cb19-67"></a></span>
<span id="cb19-68"><a href="#cb19-68"></a>        <span class="co"># Update the weights and the biases</span></span>
<span id="cb19-69"><a href="#cb19-69"></a>        optimizer.pre_update_params()</span>
<span id="cb19-70"><a href="#cb19-70"></a>        optimizer.update_params(dense1)</span>
<span id="cb19-71"><a href="#cb19-71"></a>        optimizer.update_params(dense2)</span>
<span id="cb19-72"><a href="#cb19-72"></a>        optimizer.post_update_params()</span>
<span id="cb19-73"><a href="#cb19-73"></a></span>
<span id="cb19-74"><a href="#cb19-74"></a>    <span class="cf">return</span> acc_vals, loss_vals, lr_vals</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>acc_vals, loss_vals, lr_vals <span class="op">=</span> train(decay<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch: 0,                 acc : 0.333,                 loss:  1.099,                 lr : 1.0
epoch: 1000,                 acc : 0.477,                 loss:  1.066,                 lr : 0.09099181073703366
epoch: 2000,                 acc : 0.457,                 loss:  1.065,                 lr : 0.047641734159123386
epoch: 3000,                 acc : 0.453,                 loss:  1.065,                 lr : 0.03226847370119393
epoch: 4000,                 acc : 0.450,                 loss:  1.064,                 lr : 0.02439619419370578
epoch: 5000,                 acc : 0.440,                 loss:  1.064,                 lr : 0.019611688566385566
epoch: 6000,                 acc : 0.443,                 loss:  1.063,                 lr : 0.016396130513198885
epoch: 7000,                 acc : 0.447,                 loss:  1.063,                 lr : 0.014086491055078181
epoch: 8000,                 acc : 0.447,                 loss:  1.063,                 lr : 0.012347203358439314
epoch: 9000,                 acc : 0.447,                 loss:  1.062,                 lr : 0.010990218705352238
epoch: 10000,                 acc : 0.447,                 loss:  1.062,                 lr : 0.009901970492127933</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb22-2"><a href="#cb22-2"></a>epochs <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">10000</span>,<span class="dv">10001</span>)</span>
<span id="cb22-3"><a href="#cb22-3"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb22-4"><a href="#cb22-4"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb22-5"><a href="#cb22-5"></a>plt.plot(epochs,acc_vals)</span>
<span id="cb22-6"><a href="#cb22-6"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-18-output-1.png" width="597" height="429"></p>
</div>
</div>
<div class="cell" data-execution_count="18">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb23-2"><a href="#cb23-2"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb23-3"><a href="#cb23-3"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb23-4"><a href="#cb23-4"></a>plt.plot(epochs,loss_vals)</span>
<span id="cb23-5"><a href="#cb23-5"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-1.png" width="606" height="430"></p>
</div>
</div>
<div class="cell" data-execution_count="19">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb24-2"><a href="#cb24-2"></a>plt.ylabel(<span class="st">"Learning rate"</span>)</span>
<span id="cb24-3"><a href="#cb24-3"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb24-4"><a href="#cb24-4"></a>plt.plot(epochs, lr_vals)</span>
<span id="cb24-5"><a href="#cb24-5"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-20-output-1.png" width="589" height="429"></p>
</div>
</div>
<p>The optimization algorithm appears to be stuck and the reason is because the learning rate decayed far too quickly and became too small, trapping the optimizer in some local minimum. We can, instead, try to decay a bit slower by making our decay a smaller number. For example, let’s go with <span class="math inline">\(10^{-3}\)</span>.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>acc_vals, loss_vals, lr_vals <span class="op">=</span> train(decay<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch: 0,                 acc : 0.327,                 loss:  1.099,                 lr : 1.0
epoch: 1000,                 acc : 0.410,                 loss:  1.066,                 lr : 0.5002501250625312
epoch: 2000,                 acc : 0.413,                 loss:  1.055,                 lr : 0.33344448149383127
epoch: 3000,                 acc : 0.457,                 loss:  1.014,                 lr : 0.25006251562890724
epoch: 4000,                 acc : 0.527,                 loss:  0.968,                 lr : 0.2000400080016003
epoch: 5000,                 acc : 0.547,                 loss:  0.935,                 lr : 0.16669444907484582
epoch: 6000,                 acc : 0.563,                 loss:  0.918,                 lr : 0.1428775539362766
epoch: 7000,                 acc : 0.573,                 loss:  0.900,                 lr : 0.12501562695336915
epoch: 8000,                 acc : 0.577,                 loss:  0.882,                 lr : 0.11112345816201799
epoch: 9000,                 acc : 0.590,                 loss:  0.860,                 lr : 0.1000100010001
epoch: 10000,                 acc : 0.603,                 loss:  0.845,                 lr : 0.09091735612328393</code></pre>
</div>
</div>
<div class="cell" data-execution_count="21">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb27-2"><a href="#cb27-2"></a>epochs <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">10000</span>,<span class="dv">10001</span>)</span>
<span id="cb27-3"><a href="#cb27-3"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb27-4"><a href="#cb27-4"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb27-5"><a href="#cb27-5"></a>plt.plot(epochs,acc_vals)</span>
<span id="cb27-6"><a href="#cb27-6"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-22-output-1.png" width="597" height="429"></p>
</div>
</div>
<div class="cell" data-execution_count="22">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb28-2"><a href="#cb28-2"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb28-3"><a href="#cb28-3"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb28-4"><a href="#cb28-4"></a>plt.plot(epochs,loss_vals)</span>
<span id="cb28-5"><a href="#cb28-5"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-23-output-1.png" width="597" height="429"></p>
</div>
</div>
<div class="cell" data-execution_count="23">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb29-2"><a href="#cb29-2"></a>plt.ylabel(<span class="st">"Learning rate"</span>)</span>
<span id="cb29-3"><a href="#cb29-3"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb29-4"><a href="#cb29-4"></a>plt.plot(epochs, lr_vals)</span>
<span id="cb29-5"><a href="#cb29-5"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-24-output-1.png" width="589" height="429"></p>
</div>
</div>
</section>
<section id="stochastic-gradient-descent-with-momentum" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descent-with-momentum">Stochastic Gradient Descent with Momentum</h2>
<p>Momentum proposes a small tweak to gradient descent. We give gradient descent a short-term memory. Let’s define the updated velocity <span class="math inline">\(z^{k+1}\)</span> to be weighted and controlled by the mass <span class="math inline">\(\beta\)</span>. When <span class="math inline">\(\beta\)</span> is high, we simply use the velocity from the last time, that is, we are entirely driven by momentum. When <span class="math inline">\(\beta=0\)</span>, the momentum is zero.</p>
<p><span class="math display">\[\begin{align*}
z^{(k+1)} &amp;= \beta z^{(k)} + \nabla f(w^{(k)})\\
w^{k+1} &amp;= w^k - \alpha z^{k+1}
\end{align*}\]</span></p>
<p><span class="math inline">\(z^{(k+1)}\)</span> is called the <em>velocity</em>. It accumulates the past gradients similar to how a heavy ball rolling down the error function landscape integrates over past forces. To see what’s happening in more detail, we can recursively write out:</p>
<p><span class="math display">\[\begin{align*}
z^{(k)} &amp;= \beta z^{k-1} + \nabla f(w^{(k-1)}) \\
&amp;= \beta(\beta z^{k-2} + \nabla f(w^{(k-2)})) + \nabla f(w^{(k-1)})\\
&amp;= \beta^2 z^{k-2} + \beta \nabla f(w^{(k-2)}) + \nabla f(w^{(k-1)})\\
&amp;= \beta^2 (\beta z^{k-3} + \nabla f(w^{(k-3)}) ) + \beta \nabla f(w^{(k-2)}) + \nabla f(w^{(k-1)})\\
&amp;= \sum_{t=0}^{k} \beta^t \nabla f(w^{(k-1-t)})
\end{align*}\]</span></p>
<p>The new gradient replacement no longer points into the direction of steepest descent on a particular instance any longer but rather in the direction of an exponentially weighted average of past gradients.</p>
<section id="the-dynamics-of-momentum" class="level3">
<h3 class="anchored" data-anchor-id="the-dynamics-of-momentum">The dynamics of Momentum</h3>
<p>Since <span class="math inline">\(\nabla f(w^k) = Aw^k - b\)</span>, the update on the quadratic is:</p>
<p><span class="math display">\[\begin{align*}
z^{k+1} &amp;= \beta z^k + (Aw^k - b)\\
w^{k+1} &amp;= w^k - \alpha z^{k+1}
\end{align*}\]</span></p>
<p>We go through the same motions as before with the change of basis <span class="math inline">\((w^k - w^{*})=Qx^k\)</span> and <span class="math inline">\(z^k = Q y^k\)</span> to yield the update rule:</p>
<p><span class="math display">\[\begin{align*}
Q y^{k+1} &amp;= \beta Q y^k + (AQx^k + Aw^* - b)\\
Q y^{k+1} &amp;= \beta Q y^k + (AQx^k + AA^{-1}b - b)\\
Q y^{k+1} &amp;= \beta Q y^k + Q\Lambda Q^T Q x^k\\
Q y^{k+1} &amp;= \beta Q y^k + Q\Lambda x^k\\
y^{k+1} &amp;= \beta y^k + \Lambda x^k
\end{align*}\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[\begin{align*}
y_i^{k+1} &amp;= \beta y_i^k + \lambda_i x_i^k
\end{align*}\]</span></p>
<p>Moreover,</p>
<p><span class="math display">\[\begin{align*}
Qx^{k+1} + w^* &amp;= Qx^k + w^* - \alpha Qy^{k+1}\\
x^{k+1} &amp;= x^k - \alpha y^{k+1}
\end{align*}\]</span></p>
<p>or equivalently:</p>
<p><span class="math display">\[\begin{align*}
x_i^{k+1} &amp;= x_i^k - \alpha y_i^{k+1}
\end{align*}\]</span></p>
<p>This lets us rewrite our iterates as:</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
y_i^{k+1}\\
x_i^{k+1}
\end{bmatrix} &amp;=
\begin{bmatrix}
\beta y_i^k + \lambda_i x_i^k\\
(1-\alpha\lambda_i)x_i^k - \alpha \beta y_i^k
\end{bmatrix}\\
&amp;=\begin{bmatrix}
\beta &amp; \lambda_i\\
- \alpha \beta &amp; (1-\alpha\lambda_i)
\end{bmatrix}
\begin{bmatrix}
y_i^k\\
x_i^k
\end{bmatrix}
\end{align*}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
y_i^k\\
x_i^k
\end{bmatrix} = R^k \begin{bmatrix}
y_i^0\\
x_i^0
\end{bmatrix},\quad
R = \begin{bmatrix}
\beta &amp; \lambda_i\\
- \alpha \beta &amp; (1-\alpha\lambda_i)
\end{bmatrix}
\end{align*}\]</span></p>
<p>In the case of <span class="math inline">\(2 \times 2\)</span> matrix, there is an elegant little known formula in terms of the eigenvalues of the matrix <span class="math inline">\(R\)</span>, <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>:</p>
<p><span class="math display">\[\begin{align*}
R^k = \begin{cases}
\sigma_1^k R_1 - \sigma_2^k R_2 &amp; \sigma_1 \neq \sigma_2,\\
\sigma_1^k(kR\sigma_1-(k-1)I) &amp; \sigma_1 = \sigma_2
\end{cases}
\quad
R_j = \frac{R-\sigma_j I}{\sigma_1 - \sigma_2}
\end{align*}\]</span></p>
<p>The formula is rather complicated, but the takeway here is that it plays the exact same role the individual convergence rates <span class="math inline">\((1-\alpha \lambda_i)\)</span> do in gradient descent. The convergence rate is therefore the slowest of the two rates, <span class="math inline">\(\max \{|\sigma_1|,|\sigma_2|\}\)</span>.</p>
<p>For what values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> does momentum converge? Since we need both <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span> to converge, our convergence criterion is now <span class="math inline">\(\max \{|\sigma_1|,|\sigma_2|\} &lt; 1\)</span>.</p>
<p>It can be shown that when we choose an optimal value of the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, the convergence rate is proportional to:</p>
<p><span class="math display">\[\begin{align*}
\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}
\end{align*}\]</span></p>
<p>With barely a modicum of extra effort, we have square-rooted the condition number.</p>
</section>
</section>
<section id="adding-momentum-to-the-sgdoptimizer-class" class="level2">
<h2 class="anchored" data-anchor-id="adding-momentum-to-the-sgdoptimizer-class">Adding momentum to the <code>SGDOptimizer</code> class</h2>
<p>We are now in a position to add momentum to the <code>SGDOptimizer</code> class.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="kw">class</span> SGDOptimizer:</span>
<span id="cb30-2"><a href="#cb30-2"></a></span>
<span id="cb30-3"><a href="#cb30-3"></a>    <span class="co"># Initial optimizer - set settings</span></span>
<span id="cb30-4"><a href="#cb30-4"></a>    <span class="co"># learning rate of 1. is default for this optimizer</span></span>
<span id="cb30-5"><a href="#cb30-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">1.0</span>, decay<span class="op">=</span><span class="fl">0.0</span>, momentum<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb30-6"><a href="#cb30-6"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb30-7"><a href="#cb30-7"></a>        <span class="va">self</span>.current_learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb30-8"><a href="#cb30-8"></a>        <span class="va">self</span>.decay <span class="op">=</span> decay</span>
<span id="cb30-9"><a href="#cb30-9"></a>        <span class="va">self</span>.iterations <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-10"><a href="#cb30-10"></a>        <span class="va">self</span>.beta <span class="op">=</span> momentum</span>
<span id="cb30-11"><a href="#cb30-11"></a></span>
<span id="cb30-12"><a href="#cb30-12"></a>    <span class="co"># Call once before any parameter updates</span></span>
<span id="cb30-13"><a href="#cb30-13"></a>    <span class="kw">def</span> pre_update_params(<span class="va">self</span>):</span>
<span id="cb30-14"><a href="#cb30-14"></a>        <span class="cf">if</span> <span class="va">self</span>.decay:</span>
<span id="cb30-15"><a href="#cb30-15"></a>            <span class="va">self</span>.current_learning_rate <span class="op">=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> (</span>
<span id="cb30-16"><a href="#cb30-16"></a>                <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> <span class="va">self</span>.decay <span class="op">*</span> <span class="va">self</span>.iterations)</span>
<span id="cb30-17"><a href="#cb30-17"></a>            )</span>
<span id="cb30-18"><a href="#cb30-18"></a></span>
<span id="cb30-19"><a href="#cb30-19"></a>    <span class="co"># Update parameters</span></span>
<span id="cb30-20"><a href="#cb30-20"></a>    <span class="kw">def</span> update_params(<span class="va">self</span>, layer):</span>
<span id="cb30-21"><a href="#cb30-21"></a></span>
<span id="cb30-22"><a href="#cb30-22"></a>        <span class="co"># If we use momentum</span></span>
<span id="cb30-23"><a href="#cb30-23"></a>        <span class="cf">if</span> <span class="va">self</span>.beta:</span>
<span id="cb30-24"><a href="#cb30-24"></a></span>
<span id="cb30-25"><a href="#cb30-25"></a>            <span class="co"># If the layer does not contain momentum arrays, create them</span></span>
<span id="cb30-26"><a href="#cb30-26"></a>            <span class="co"># filled with zeros</span></span>
<span id="cb30-27"><a href="#cb30-27"></a>            <span class="cf">if</span> <span class="kw">not</span> <span class="bu">hasattr</span>(layer, <span class="st">"weight_momentums"</span>):</span>
<span id="cb30-28"><a href="#cb30-28"></a>                layer.weight_momentums <span class="op">=</span> np.zeros_like(layer.dloss_dweights)</span>
<span id="cb30-29"><a href="#cb30-29"></a>                <span class="co"># If there is no momentumm array for weights</span></span>
<span id="cb30-30"><a href="#cb30-30"></a>                <span class="co"># the array doesnt exist for biases yet either</span></span>
<span id="cb30-31"><a href="#cb30-31"></a>                layer.bias_momentums <span class="op">=</span> np.zeros_like(layer.dloss_dbiases)</span>
<span id="cb30-32"><a href="#cb30-32"></a></span>
<span id="cb30-33"><a href="#cb30-33"></a>            <span class="co"># Build weight updates with momentum - take previous</span></span>
<span id="cb30-34"><a href="#cb30-34"></a>            <span class="co"># updates multiplied by retain factor and update with</span></span>
<span id="cb30-35"><a href="#cb30-35"></a>            <span class="co"># with current gradients</span></span>
<span id="cb30-36"><a href="#cb30-36"></a>            <span class="co"># v[t+1] = \beta * v[t] + \alpha * dL/dw</span></span>
<span id="cb30-37"><a href="#cb30-37"></a>            weight_updates <span class="op">=</span> (</span>
<span id="cb30-38"><a href="#cb30-38"></a>                <span class="va">self</span>.beta <span class="op">*</span> layer.weight_momentums</span>
<span id="cb30-39"><a href="#cb30-39"></a>                <span class="op">+</span> <span class="va">self</span>.current_learning_rate <span class="op">*</span> layer.dloss_dweights</span>
<span id="cb30-40"><a href="#cb30-40"></a>            )</span>
<span id="cb30-41"><a href="#cb30-41"></a>            layer.weight_momentums <span class="op">=</span> weight_updates</span>
<span id="cb30-42"><a href="#cb30-42"></a></span>
<span id="cb30-43"><a href="#cb30-43"></a>            <span class="co"># Build bias updates</span></span>
<span id="cb30-44"><a href="#cb30-44"></a>            bias_updates <span class="op">=</span> (</span>
<span id="cb30-45"><a href="#cb30-45"></a>                <span class="va">self</span>.beta <span class="op">*</span> layer.bias_momentums</span>
<span id="cb30-46"><a href="#cb30-46"></a>                <span class="op">+</span> <span class="va">self</span>.current_learning_rate <span class="op">*</span> layer.dloss_dbiases</span>
<span id="cb30-47"><a href="#cb30-47"></a>            )</span>
<span id="cb30-48"><a href="#cb30-48"></a>            layer.bias_momentums <span class="op">=</span> bias_updates</span>
<span id="cb30-49"><a href="#cb30-49"></a>        <span class="cf">else</span>:</span>
<span id="cb30-50"><a href="#cb30-50"></a>            <span class="co"># Vanilla SGD updates (as before momentum update)</span></span>
<span id="cb30-51"><a href="#cb30-51"></a>            weight_updates <span class="op">=</span> <span class="va">self</span>.current_learning_rate <span class="op">*</span> layer.dloss_dweights</span>
<span id="cb30-52"><a href="#cb30-52"></a>            bias_updates <span class="op">=</span> <span class="va">self</span>.current_learning_rate <span class="op">*</span> layer.dloss_dbiases</span>
<span id="cb30-53"><a href="#cb30-53"></a></span>
<span id="cb30-54"><a href="#cb30-54"></a>        layer.weights <span class="op">-=</span> weight_updates</span>
<span id="cb30-55"><a href="#cb30-55"></a>        layer.biases <span class="op">-=</span> bias_updates</span>
<span id="cb30-56"><a href="#cb30-56"></a></span>
<span id="cb30-57"><a href="#cb30-57"></a>    <span class="kw">def</span> post_update_params(<span class="va">self</span>):</span>
<span id="cb30-58"><a href="#cb30-58"></a>        <span class="va">self</span>.iterations <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see an example illustrating how adding momentum changes the learning process. Keeping the same <code>learning_rate=1.0</code> and <code>decay=1e-3</code> from the previous training attempt and using a momentum of <code>0.50</code>:</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="kw">def</span> train(decay, momentum):</span>
<span id="cb31-2"><a href="#cb31-2"></a>    <span class="co"># Create a dataset</span></span>
<span id="cb31-3"><a href="#cb31-3"></a>    X, y <span class="op">=</span> spiral_data(samples<span class="op">=</span><span class="dv">100</span>, classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb31-4"><a href="#cb31-4"></a></span>
<span id="cb31-5"><a href="#cb31-5"></a>    <span class="co"># Create a dense layer with 2 input features and 64 output values</span></span>
<span id="cb31-6"><a href="#cb31-6"></a>    dense1 <span class="op">=</span> DenseLayer(<span class="dv">2</span>, <span class="dv">64</span>)</span>
<span id="cb31-7"><a href="#cb31-7"></a></span>
<span id="cb31-8"><a href="#cb31-8"></a>    <span class="co"># Create ReLU activation (to be used with the dense layer)</span></span>
<span id="cb31-9"><a href="#cb31-9"></a>    activation1 <span class="op">=</span> ReLUActivation()</span>
<span id="cb31-10"><a href="#cb31-10"></a></span>
<span id="cb31-11"><a href="#cb31-11"></a>    <span class="co"># Create second DenseLayer with 64 input features (as we take output of the</span></span>
<span id="cb31-12"><a href="#cb31-12"></a>    <span class="co"># previous layer here) and 3 output values</span></span>
<span id="cb31-13"><a href="#cb31-13"></a>    dense2 <span class="op">=</span> DenseLayer(<span class="dv">64</span>, <span class="dv">3</span>)</span>
<span id="cb31-14"><a href="#cb31-14"></a></span>
<span id="cb31-15"><a href="#cb31-15"></a>    <span class="co"># Create Softmax classifier's combined loss and activation</span></span>
<span id="cb31-16"><a href="#cb31-16"></a>    loss_activation <span class="op">=</span> CategoricalCrossEntropySoftmax()</span>
<span id="cb31-17"><a href="#cb31-17"></a></span>
<span id="cb31-18"><a href="#cb31-18"></a>    <span class="co"># Create optimizer</span></span>
<span id="cb31-19"><a href="#cb31-19"></a>    optimizer <span class="op">=</span> SGDOptimizer(learning_rate<span class="op">=</span><span class="fl">1.0</span>,decay<span class="op">=</span>decay,momentum<span class="op">=</span>momentum)</span>
<span id="cb31-20"><a href="#cb31-20"></a></span>
<span id="cb31-21"><a href="#cb31-21"></a>    acc_vals <span class="op">=</span> []</span>
<span id="cb31-22"><a href="#cb31-22"></a>    loss_vals <span class="op">=</span> []</span>
<span id="cb31-23"><a href="#cb31-23"></a>    lr_vals <span class="op">=</span> []</span>
<span id="cb31-24"><a href="#cb31-24"></a></span>
<span id="cb31-25"><a href="#cb31-25"></a>    <span class="co"># Train in a loop</span></span>
<span id="cb31-26"><a href="#cb31-26"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10001</span>):</span>
<span id="cb31-27"><a href="#cb31-27"></a>        <span class="co"># Perform a forward pass of our training data through this layer</span></span>
<span id="cb31-28"><a href="#cb31-28"></a>        dense1.forward(X)</span>
<span id="cb31-29"><a href="#cb31-29"></a></span>
<span id="cb31-30"><a href="#cb31-30"></a>        <span class="co"># Perform a forward pass through the activation function</span></span>
<span id="cb31-31"><a href="#cb31-31"></a>        <span class="co"># takes the output of the first dense layer here</span></span>
<span id="cb31-32"><a href="#cb31-32"></a>        activation1.forward(dense1.output)</span>
<span id="cb31-33"><a href="#cb31-33"></a></span>
<span id="cb31-34"><a href="#cb31-34"></a>        <span class="co"># Perform a forward pass through second DenseLayer</span></span>
<span id="cb31-35"><a href="#cb31-35"></a>        <span class="co"># takes the outputs of the activation function of first layer as inputs</span></span>
<span id="cb31-36"><a href="#cb31-36"></a>        dense2.forward(activation1.output)</span>
<span id="cb31-37"><a href="#cb31-37"></a></span>
<span id="cb31-38"><a href="#cb31-38"></a>        <span class="co"># Perform a forward pass through the activation/loss function</span></span>
<span id="cb31-39"><a href="#cb31-39"></a>        <span class="co"># takes the output of the second DenseLayer here and returns the loss</span></span>
<span id="cb31-40"><a href="#cb31-40"></a>        loss <span class="op">=</span> loss_activation.forward(dense2.output, y)</span>
<span id="cb31-41"><a href="#cb31-41"></a></span>
<span id="cb31-42"><a href="#cb31-42"></a>        <span class="co"># Calculate accuracy from output of activation2 and targets</span></span>
<span id="cb31-43"><a href="#cb31-43"></a>        <span class="co"># Calculate values along the first axis</span></span>
<span id="cb31-44"><a href="#cb31-44"></a>        predictions <span class="op">=</span> np.argmax(loss_activation.output, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-45"><a href="#cb31-45"></a>        <span class="cf">if</span> <span class="bu">len</span>(y.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb31-46"><a href="#cb31-46"></a>            y <span class="op">=</span> np.argmax(y, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-47"><a href="#cb31-47"></a></span>
<span id="cb31-48"><a href="#cb31-48"></a>        accuracy <span class="op">=</span> np.mean(predictions <span class="op">==</span> y)</span>
<span id="cb31-49"><a href="#cb31-49"></a></span>
<span id="cb31-50"><a href="#cb31-50"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb31-51"><a href="#cb31-51"></a>            <span class="bu">print</span>(</span>
<span id="cb31-52"><a href="#cb31-52"></a>                <span class="ss">f"epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, </span><span class="ch">\</span></span>
<span id="cb31-53"><a href="#cb31-53"></a><span class="ss">                acc : </span><span class="sc">{</span>accuracy<span class="sc">:.3f}</span><span class="ss">, </span><span class="ch">\</span></span>
<span id="cb31-54"><a href="#cb31-54"></a><span class="ss">                loss: </span><span class="sc">{</span>loss<span class="sc">: .3f}</span><span class="ss">, </span><span class="ch">\</span></span>
<span id="cb31-55"><a href="#cb31-55"></a><span class="ss">                lr : </span><span class="sc">{</span>optimizer<span class="sc">.</span>current_learning_rate<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb31-56"><a href="#cb31-56"></a>            )</span>
<span id="cb31-57"><a href="#cb31-57"></a></span>
<span id="cb31-58"><a href="#cb31-58"></a>        acc_vals.append(accuracy)</span>
<span id="cb31-59"><a href="#cb31-59"></a>        loss_vals.append(loss)</span>
<span id="cb31-60"><a href="#cb31-60"></a>        lr_vals.append(optimizer.current_learning_rate)</span>
<span id="cb31-61"><a href="#cb31-61"></a></span>
<span id="cb31-62"><a href="#cb31-62"></a>        <span class="co"># Backward pass</span></span>
<span id="cb31-63"><a href="#cb31-63"></a>        loss_activation.backward(loss_activation.output, y)</span>
<span id="cb31-64"><a href="#cb31-64"></a>        dense2.backward(loss_activation.dloss_dz)</span>
<span id="cb31-65"><a href="#cb31-65"></a>        activation1.backward(dense2.dloss_dinputs)</span>
<span id="cb31-66"><a href="#cb31-66"></a>        dense1.backward(activation1.dloss_dz)</span>
<span id="cb31-67"><a href="#cb31-67"></a></span>
<span id="cb31-68"><a href="#cb31-68"></a>        <span class="co"># Update the weights and the biases</span></span>
<span id="cb31-69"><a href="#cb31-69"></a>        optimizer.pre_update_params()</span>
<span id="cb31-70"><a href="#cb31-70"></a>        optimizer.update_params(dense1)</span>
<span id="cb31-71"><a href="#cb31-71"></a>        optimizer.update_params(dense2)</span>
<span id="cb31-72"><a href="#cb31-72"></a>        optimizer.post_update_params()</span>
<span id="cb31-73"><a href="#cb31-73"></a></span>
<span id="cb31-74"><a href="#cb31-74"></a>    <span class="cf">return</span> acc_vals, loss_vals, lr_vals</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>acc_vals, loss_vals, lr_vals <span class="op">=</span> train(decay<span class="op">=</span><span class="fl">1e-3</span>, momentum<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch: 0,                 acc : 0.337,                 loss:  1.099,                 lr : 1.0
epoch: 1000,                 acc : 0.510,                 loss:  0.978,                 lr : 0.5002501250625312
epoch: 2000,                 acc : 0.557,                 loss:  0.879,                 lr : 0.33344448149383127
epoch: 3000,                 acc : 0.580,                 loss:  0.771,                 lr : 0.25006251562890724
epoch: 4000,                 acc : 0.630,                 loss:  0.735,                 lr : 0.2000400080016003
epoch: 5000,                 acc : 0.657,                 loss:  0.670,                 lr : 0.16669444907484582
epoch: 6000,                 acc : 0.753,                 loss:  0.573,                 lr : 0.1428775539362766
epoch: 7000,                 acc : 0.783,                 loss:  0.522,                 lr : 0.12501562695336915
epoch: 8000,                 acc : 0.790,                 loss:  0.481,                 lr : 0.11112345816201799
epoch: 9000,                 acc : 0.807,                 loss:  0.441,                 lr : 0.1000100010001
epoch: 10000,                 acc : 0.843,                 loss:  0.401,                 lr : 0.09091735612328393</code></pre>
</div>
</div>
<p>The model achieved the lowest loss and the highest accuracy that we’ve seen so far. Can we do better? Sure, we can! Let’s try to set the momentum to <span class="math inline">\(0.9\)</span>:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>acc_vals, loss_vals, lr_vals <span class="op">=</span> train(decay<span class="op">=</span><span class="fl">1e-3</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch: 0,                 acc : 0.340,                 loss:  1.099,                 lr : 1.0
epoch: 1000,                 acc : 0.763,                 loss:  0.463,                 lr : 0.5002501250625312
epoch: 2000,                 acc : 0.790,                 loss:  0.407,                 lr : 0.33344448149383127
epoch: 3000,                 acc : 0.803,                 loss:  0.396,                 lr : 0.25006251562890724
epoch: 4000,                 acc : 0.813,                 loss:  0.391,                 lr : 0.2000400080016003
epoch: 5000,                 acc : 0.813,                 loss:  0.386,                 lr : 0.16669444907484582
epoch: 6000,                 acc : 0.813,                 loss:  0.384,                 lr : 0.1428775539362766
epoch: 7000,                 acc : 0.813,                 loss:  0.375,                 lr : 0.12501562695336915
epoch: 8000,                 acc : 0.833,                 loss:  0.332,                 lr : 0.11112345816201799
epoch: 9000,                 acc : 0.880,                 loss:  0.285,                 lr : 0.1000100010001
epoch: 10000,                 acc : 0.880,                 loss:  0.277,                 lr : 0.09091735612328393</code></pre>
</div>
</div>
</section>
<section id="adagrad" class="level2">
<h2 class="anchored" data-anchor-id="adagrad">AdaGrad</h2>
<p>In real-world datasets, some input features are sparse and some features are dense. If we use the same learning rate <span class="math inline">\(\alpha\)</span> for all the weights, parameters associated with sparse features receive meaningful updates only when these features occur. Given a decreasing learning rate, we might end up with a situation where parameters for dense features converge rather quickly to their optimal values, whereas for sparse features, we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate decreases too slowly for dense features and too quickly for sparse features.</p>
<p>The update rule for adaptive step-size gradient descent is:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{g}_t &amp;= \frac{\partial \mathcal L}{\partial \mathbf{w}}\\
\mathbf{s}_t &amp;= \mathbf{s}_{t-1} + \mathbf{g}_{t}^2 \\
\mathbf{w}_t &amp;= \mathbf{w}_{t-1} + \frac{\alpha}{\sqrt{\mathbf{s}_t+\epsilon}}\cdot \mathbf{g}_t
\end{align*}\]</span></p>
<p>Here the operations are applied coordinate-wise. So, the jacobian <span class="math inline">\(\mathbf{g}_t^2\)</span> has entries <span class="math inline">\(g_t^2\)</span>. As before, <span class="math inline">\(\alpha\)</span> is the learning rate and <span class="math inline">\(\epsilon\)</span> is an additive constant that ensures that we do not divide by <span class="math inline">\(0\)</span>. Thus, the learning rate for features whose weights receive frequent updates is decreased faster, whilst for those features, whose weights receive infrequent updates, it is decreased slower.</p>
<p>Thus, Adagrad decreases the learning-rate dynamically on a per-coordinate basis.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a><span class="kw">class</span> AdagradOptimizer:</span>
<span id="cb36-2"><a href="#cb36-2"></a></span>
<span id="cb36-3"><a href="#cb36-3"></a>    <span class="co"># Initial optimizer - set settings</span></span>
<span id="cb36-4"><a href="#cb36-4"></a>    <span class="co"># learning rate of 1. is default for this optimizer</span></span>
<span id="cb36-5"><a href="#cb36-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">1.0</span>, decay<span class="op">=</span><span class="fl">0.0</span>, epsilon<span class="op">=</span><span class="fl">1e-7</span>):</span>
<span id="cb36-6"><a href="#cb36-6"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb36-7"><a href="#cb36-7"></a>        <span class="va">self</span>.current_learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb36-8"><a href="#cb36-8"></a>        <span class="va">self</span>.decay <span class="op">=</span> decay</span>
<span id="cb36-9"><a href="#cb36-9"></a>        <span class="va">self</span>.iterations <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-10"><a href="#cb36-10"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> epsilon</span>
<span id="cb36-11"><a href="#cb36-11"></a></span>
<span id="cb36-12"><a href="#cb36-12"></a>    <span class="co"># Call once before any parameter updates</span></span>
<span id="cb36-13"><a href="#cb36-13"></a>    <span class="kw">def</span> pre_update_params(<span class="va">self</span>):</span>
<span id="cb36-14"><a href="#cb36-14"></a>        <span class="cf">if</span> <span class="va">self</span>.decay:</span>
<span id="cb36-15"><a href="#cb36-15"></a>            <span class="va">self</span>.current_learning_rate <span class="op">=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> (</span>
<span id="cb36-16"><a href="#cb36-16"></a>                <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> <span class="va">self</span>.decay <span class="op">*</span> <span class="va">self</span>.iterations)</span>
<span id="cb36-17"><a href="#cb36-17"></a>            )</span>
<span id="cb36-18"><a href="#cb36-18"></a></span>
<span id="cb36-19"><a href="#cb36-19"></a>    <span class="co"># Update parameters</span></span>
<span id="cb36-20"><a href="#cb36-20"></a>    <span class="kw">def</span> update_params(<span class="va">self</span>, layer):</span>
<span id="cb36-21"><a href="#cb36-21"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">hasattr</span>(layer, <span class="st">"weight_cache"</span>):</span>
<span id="cb36-22"><a href="#cb36-22"></a>            layer.weight_cache <span class="op">=</span> np.zeros_like(layer.weights)</span>
<span id="cb36-23"><a href="#cb36-23"></a>            layer.bias_cache <span class="op">=</span> np.zeros_like(layer.biases)</span>
<span id="cb36-24"><a href="#cb36-24"></a></span>
<span id="cb36-25"><a href="#cb36-25"></a>        <span class="co"># Update cache with squared current gradients</span></span>
<span id="cb36-26"><a href="#cb36-26"></a>        layer.weight_cache <span class="op">+=</span> layer.dloss_dweights<span class="op">**</span><span class="dv">2</span></span>
<span id="cb36-27"><a href="#cb36-27"></a>        layer.bias_cache <span class="op">+=</span> layer.dloss_dbiases<span class="op">**</span><span class="dv">2</span></span>
<span id="cb36-28"><a href="#cb36-28"></a></span>
<span id="cb36-29"><a href="#cb36-29"></a>        <span class="co"># Vanilla SGD parameter update + normalization</span></span>
<span id="cb36-30"><a href="#cb36-30"></a>        <span class="co"># with square rooted cache</span></span>
<span id="cb36-31"><a href="#cb36-31"></a>        layer.weights <span class="op">+=</span> (</span>
<span id="cb36-32"><a href="#cb36-32"></a>            <span class="va">self</span>.current_learning_rate</span>
<span id="cb36-33"><a href="#cb36-33"></a>            <span class="op">*</span> layer.dloss_dweights</span>
<span id="cb36-34"><a href="#cb36-34"></a>            <span class="op">/</span> (np.sqrt(layer.weight_cache) <span class="op">+</span> <span class="va">self</span>.epsilon)</span>
<span id="cb36-35"><a href="#cb36-35"></a>        )</span>
<span id="cb36-36"><a href="#cb36-36"></a>        layer.biases <span class="op">+=</span> (</span>
<span id="cb36-37"><a href="#cb36-37"></a>            <span class="va">self</span>.current_learning_rate</span>
<span id="cb36-38"><a href="#cb36-38"></a>            <span class="op">*</span> layer.dloss_dbiases</span>
<span id="cb36-39"><a href="#cb36-39"></a>            <span class="op">/</span> (np.sqrt(layer.bias_cache) <span class="op">+</span> <span class="va">self</span>.epsilon)</span>
<span id="cb36-40"><a href="#cb36-40"></a>        )</span>
<span id="cb36-41"><a href="#cb36-41"></a></span>
<span id="cb36-42"><a href="#cb36-42"></a>    <span class="kw">def</span> post_update_params(<span class="va">self</span>):</span>
<span id="cb36-43"><a href="#cb36-43"></a>        <span class="va">self</span>.iterations <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="rmsprop">RMSProp</h2>
<p>One of the key issues of Adagrad is that the learning rate decreases at a predefined schedule essentially at a rate proportional <span class="math inline">\(\frac{1}{\sqrt{t}}\)</span>. While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.</p>
<p>Tieleman and Hinton(<a href="https://www.d2l.ai/chapter_references/zreferences.html#id284">2012</a>) have proposed the RMSProp algorithm as a simple fix to decouple the rate scheduling from coordinate adaptive learning rates. The issue is that the squares of the gradient <span class="math inline">\(\mathbf{g}_t\)</span> keeps accumulating into the state vector <span class="math inline">\(\mathbf{s}_t = \mathbf{s}_{t-1} + \mathbf{g}_t^2\)</span>. As a result, <span class="math inline">\(\mathbf{s}_t\)</span> keeps on growing without bounds, essentially linearly as the algorithm converges.</p>
<section id="the-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-algorithm">The Algorithm</h3>
<p>The update rule for the RMSProp algorithm is as follows:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{s}_t &amp;= \gamma \mathbf{s}_{t-1} + (1- \gamma)\mathbf{g}_t^2\\
\mathbf{x}_t &amp;= \mathbf{x}_{t-1} - \frac{\alpha}{\sqrt{\mathbf{s}_t + \epsilon}}\odot \mathbf{g}_t
\end{align*}\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>
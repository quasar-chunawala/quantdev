<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-04-27">

<title>quantdev.blog - Properties of Brownian Motion</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap')
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../.././symbol.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sell_side_quant_critical_path.html" rel="" target="">
 <span class="menu-text">Sell-side Quant</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../roadmap.html" rel="" target="">
 <span class="menu-text">C++ Roadmap</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://patreon.com/u59411143?utm_medium=unknown&amp;utm_source=join_link&amp;utm_campaign=creatorshare_creator&amp;utm_content=copyLink" rel="" target=""><i class="bi bi-patreon" role="img">
</i> 
 <span class="menu-text">Become a patreon</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/quasar-chunawala" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://linkedin.com/in/quasar-chunawala" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Properties of Brownian Motion</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Fin Math</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 27, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#properties-of-brownian-motion." id="toc-properties-of-brownian-motion." class="nav-link active" data-scroll-target="#properties-of-brownian-motion.">Properties of Brownian Motion.</a>
  <ul class="collapse">
  <li><a href="#properties-of-brownian-motion.-1" id="toc-properties-of-brownian-motion.-1" class="nav-link" data-scroll-target="#properties-of-brownian-motion.-1">Properties of Brownian Motion.</a></li>
  <li><a href="#properties-of-the-paths." id="toc-properties-of-the-paths." class="nav-link" data-scroll-target="#properties-of-the-paths.">Properties of the paths.</a>
  <ul class="collapse">
  <li><a href="#functions-considered-in-stochastic-calculus." id="toc-functions-considered-in-stochastic-calculus." class="nav-link" data-scroll-target="#functions-considered-in-stochastic-calculus.">Functions considered in Stochastic Calculus.</a></li>
  <li><a href="#variation-of-a-function." id="toc-variation-of-a-function." class="nav-link" data-scroll-target="#variation-of-a-function.">Variation of a function.</a></li>
  <li><a href="#jordan-decomposition." id="toc-jordan-decomposition." class="nav-link" data-scroll-target="#jordan-decomposition.">Jordan Decomposition.</a></li>
  <li><a href="#riemann-stieltjes-integral." id="toc-riemann-stieltjes-integral." class="nav-link" data-scroll-target="#riemann-stieltjes-integral.">Riemann-Stieltjes Integral.</a></li>
  <li><a href="#brownian-motion-as-the-limit-of-a-symmetric-random-walk." id="toc-brownian-motion-as-the-limit-of-a-symmetric-random-walk." class="nav-link" data-scroll-target="#brownian-motion-as-the-limit-of-a-symmetric-random-walk.">Brownian motion as the limit of a symmetric random walk.</a></li>
  </ul></li>
  <li><a href="#what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance" id="toc-what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance" class="nav-link" data-scroll-target="#what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance">What exactly is <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> in mathematical finance?</a></li>
  <li><a href="#continuity-and-regularity-of-paths." id="toc-continuity-and-regularity-of-paths." class="nav-link" data-scroll-target="#continuity-and-regularity-of-paths.">Continuity and Regularity of paths.</a></li>
  <li><a href="#a-point-of-comparison-the-poisson-process." id="toc-a-point-of-comparison-the-poisson-process." class="nav-link" data-scroll-target="#a-point-of-comparison-the-poisson-process.">A point of comparison: The Poisson Process.</a></li>
  </ul></li>
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="properties-of-brownian-motion." class="level1">
<h1>Properties of Brownian Motion.</h1>
<section id="properties-of-brownian-motion.-1" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-brownian-motion.-1">Properties of Brownian Motion.</h2>
<p>Let <span class="math inline">\(B(t)\)</span> be a fixed Brownian motion. We give below some simple properties that follow directly from the definition of the Brownian Motion.</p>
<div class="prop">
<p>For any <span class="math inline">\(t\geq0\)</span>, <span class="math inline">\(B(t)\)</span> is normally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t\)</span>. For any <span class="math inline">\(s,t\geq0\)</span> we have <span class="math inline">\(\mathbb{E}(B_{s}B_{t})=\min\{s,t\}\)</span>.</p>
</div>
<p><em>Proof.</em> From condition (1), we have that <span class="math inline">\(B_{0}=0\)</span>. From condition (2), <span class="math inline">\(B_{t}-B_{0}=B_{t}\)</span> is normally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t\)</span>.</p>
<p>Assume that <span class="math inline">\(s&lt;t\)</span>.</p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}(B_{s}B_{t}) &amp; =\mathbb{E}\left[B_{s}(B_{t}-B_{s}+B_{s})\right] &amp; \{\text{Write }B_{t}=B_{t}-B_{s}+B_{s}\}\\
&amp; =\mathbb{E}[B_{s}(B_{t}-B_{s})]+\mathbb{E}[B_{s}^{2}] &amp; \{\text{Linearity of expectations}\}\\
&amp; =\mathbb{E}[B_{s}]\mathbb{E}(B_{t}-B_{s})+s &amp; \{B_{s},(B_{t}-B_{s})\text{ are independent}\}\\
&amp; =0\cdot0+s\\
&amp; =s
\end{aligned}\]</span></p>
<p>This closes the proof. ◻ :::</p>
<div class="prop">
<p>(Translation Invariance) For fixed <span class="math inline">\(t_{0}\geq0\)</span>, the stochastic process <span class="math inline">\(\tilde{B}(t)=B(t+t_{0})-B(t_{0})\)</span> is also a Brownian motion.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Firstly, the stochastic process <span class="math inline">\(\tilde{B}(t)\)</span> is such that:</p>
<p>(1) <span class="math inline">\(\tilde{B}(0)=B(t_{0})-B(t_{0})=0\)</span>. Hence, it satisfies condition (1).</p>
<p>(2) Let <span class="math inline">\(s&lt;t\)</span>. We have: <span class="math inline">\(\tilde{B}(t)-\tilde{B}(s)=B(t+t_{0})-B(s+t_{0})\)</span> which a Gaussian random variable with mean 0 and variance <span class="math inline">\(t-s\)</span>. Hence, for <span class="math inline">\(a\leq b\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\{a\leq &amp; \tilde{B}(t)\leq b\}=\frac{1}{\sqrt{2\pi(t-s)}}\int_{a}^{b}e^{-\frac{x^{2}}{2(t-s)}}dx
\end{aligned}\]</span></p>
<p>Hence, it satisfies condition (2).</p>
<p>(3) To check condition (3) for <span class="math inline">\(\tilde{B}(t)\)</span>, we may assume <span class="math inline">\(t_{0}&gt;0\)</span>. Then, for any <span class="math inline">\(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}\)</span>, we have:</p>
<p><span class="math display">\[0&lt;t_{0}\leq t_{0}+t_{1}\leq t_{0}+t_{2}\leq\ldots\leq t_{0}+t_{n}\]</span></p>
<p>So, <span class="math inline">\(B(t_{1}+t_{0})-B(t_{0})\)</span>, <span class="math inline">\(B(t_{2}+t_{0})-B(t_{1}+t_{0})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(B(t_{k}+t_{0})-B(t_{k-1}+t_{0})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(B(t_{n}+t_{0})-B(t_{n-1}+t_{0})\)</span> are independent random variables. Consequently, <span class="math inline">\(\tilde{B}(t)\)</span> satisfies condition (3).</p>
<p>This closes the proof. ◻</p>
</div>
<p>The above translation invariance property says that a Brownian motion starts afresh at any moment as a new Brownian motion.</p>
<div class="prop">
<p>(Scaling Invariance) For any real number <span class="math inline">\(\lambda&gt;0\)</span>, the stochastic process <span class="math inline">\(\tilde{B}(t)=B(\lambda t)/\sqrt{\lambda}\)</span> is also a Brownian motion.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> The scaled stochastic process <span class="math inline">\(\tilde{B}(t)\)</span> is such that:</p>
<p>(1) <span class="math inline">\(\tilde{B}(0)=0\)</span>. Hence it satisfies condition (1).</p>
<p>(2) Let <span class="math inline">\(s&lt;t\)</span>. Then, <span class="math inline">\(\lambda s&lt;\lambda t\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{B}(t)-\tilde{B}(s) &amp; =\frac{1}{\sqrt{\lambda}}(B(\lambda t)-B(\lambda s))
\end{aligned}\]</span></p>
<p>Now, <span class="math inline">\(B(\lambda t)-B(\lambda s)\)</span> is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\lambda(t-s)\)</span>. We know that, if <span class="math inline">\(X\)</span> is a random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>, <span class="math inline">\(Z=\left(\frac{X-\mu}{\sigma}\right)\)</span> has mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. Consequently, <span class="math inline">\(\frac{B(\lambda t)-B(\lambda s)}{\sqrt{\lambda}}\)</span> is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\((t-s)\)</span>.</p>
<p>Hence, <span class="math inline">\(\tilde{B}(t)-\tilde{B}(s)\)</span> is normal distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t-s\)</span> and it satisfies condition (2).</p>
<p>(3) To check condition (3) for <span class="math inline">\(\tilde{B}(t)\)</span>, we may assume <span class="math inline">\(t_{0}&gt;0\)</span>. Then, for any <span class="math inline">\(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}\)</span>, we have:</p>
<p><span class="math display">\[0\leq\lambda t_{1}\leq\lambda t_{2}\leq\ldots\leq\lambda t_{n}\]</span></p>
<p>Consequently, the random variables <span class="math inline">\(B(\lambda t_{k})-B(\lambda t_{k-1})\)</span>, <span class="math inline">\(k=1,2,3,\ldots,n\)</span> are independent. Hence it follows that <span class="math inline">\(\frac{1}{\sqrt{\lambda}}[B(\lambda t_{k})-B(\lambda t_{k-1})]\)</span> for <span class="math inline">\(k=1,2,\ldots,n\)</span> are also independent random variables.</p>
<p>This closes the proof. ◻</p>
</div>
<p>It follows from the scaling invariance property that for any <span class="math inline">\(\lambda&gt;0\)</span> and <span class="math inline">\(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}\)</span>, the random vectors:</p>
<p><span class="math display">\[(B(\lambda t_{1}),B(\lambda t_{2}),\ldots,B(\lambda t_{n}))\quad(\sqrt{\lambda}B(t_{1}),\sqrt{\lambda}B(t_{1}),\ldots,\sqrt{\lambda}B(t_{n}))\]</span></p>
<p>have the same distribution.</p>
<p>The scaling property shows that Brownian motion is <em>self-similar</em>, much like a fractal. To see this, suppose we zoom into a Brownian motion path very close to zero, say on the interval <span class="math inline">\([0,10^{-6}]\)</span>. If the Brownian motion path were smooth and differentiable, the closer we zoom in around the origin, the flatter the function will look. In the limit, we would essentially see a straight line given by the derivative at <span class="math inline">\(0\)</span>. However, what we see with the Brownian motion is very different. The scaling property means that for <span class="math inline">\(a=10^{-6}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
(B_{10^{-6}t,}t\in[0,1]) &amp; \stackrel{\text{distrib.}}{=}(10^{-3}B_{t},t\in[0,1])
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\stackrel{\text{distrib.}}{=}\)</span> means equality of the distribution of the two processes. In other words, Brownian motion on <span class="math inline">\([0,10^{-6}]\)</span> looks like a Browian motion on <span class="math inline">\([0,1]\)</span>, but with its amplitude multiplied by a factor of <span class="math inline">\(10^{-3}\)</span>. In particular, it will remain rugged as we zoom in, unlike a smooth function.</p>
<div class="prop">
<p><span id="prop:brownian-motion-symmetry-of-reflection-at-time-s" label="prop:brownian-motion-symmetry-of-reflection-at-time-s"></span>(Reflection at time <span class="math inline">\(s\)</span>) The process <span class="math inline">\((-B_{t},t\geq0)\)</span> is a Brownian motion. More generally, for any <span class="math inline">\(s\geq0\)</span>, the process <span class="math inline">\((\tilde{B}(t),t\geq0)\)</span> defined by:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{B}(t) &amp; =\begin{cases}
B_{t} &amp; \text{if }t\leq s\\
B_{s}-(B_{t}-B_{s}) &amp; \text{if }t&gt;s
\end{cases}\label{eq:reflection-property}
\end{aligned}\]</span></p>
<p>is a Brownian motion.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (a) Consider the process <span class="math inline">\(\tilde{B}(t)=(-B_{t},t\geq0)\)</span>.</p>
<p>(1) <span class="math inline">\(\tilde{B}(0)=0\)</span>.</p>
<p>(2) If <span class="math inline">\(X\)</span> is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t-s\)</span>, <span class="math inline">\(-X\)</span> is also Gaussian with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t-s\)</span>. Thus, <span class="math inline">\(\tilde{B}(t)-\tilde{B}(s)=-(B(t)-B(s))\)</span> is also Gaussian with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\((t-s)\)</span>. Hence condition (2) is satisfied.</p>
<p>(3) Assume that <span class="math inline">\(0\leq t_{0}\leq t_{1}\leq\ldots\leq t_{n}\)</span>. Then, the random variables <span class="math inline">\(-(B(t_{k})-B(t_{k-1}))\)</span> are independent for <span class="math inline">\(k=1,2,3,\ldots,n\)</span>. Hence, condition (3) is satisfied.</p>
<p>(b) Consider the process <span class="math inline">\(\tilde{B}(t)\)</span> as defined in (<a href="#eq:reflection-property" data-reference-type="ref" data-reference="eq:reflection-property">[eq:reflection-property]</a>).</p>
<p>Fix an <span class="math inline">\(s\geq0\)</span>.</p>
<p>(1) Let <span class="math inline">\(t=0\)</span>. Then, <span class="math inline">\(t\leq s\)</span>. <span class="math inline">\(\tilde{B}(t)=\tilde{B}(0)=B(0)=0\)</span>.</p>
<p>(2) Let <span class="math inline">\(t_{1}&lt;t_{2}\leq s\)</span>. Then, <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})=B(t_{2})-B(t_{1})\)</span>. This is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t_{2}-t_{1}\)</span>.</p>
<p>Let <span class="math inline">\(t_{1}&lt;s&lt;t_{2}\)</span>. Then, <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})=B(s)-(B(t_{2})-B(s))-B(t_{1})=(B(s)-B(t_{1}))-(B(t_{2})-B(s))\)</span>. Since, <span class="math inline">\(B(s)-B(t_{1})\)</span> and <span class="math inline">\(B(t_{2})-B(s)\)</span> are independent Gaussian random variables, any linear combination of these is Gaussian. Moreover, its mean is zero. The variance is given by:</p>
<p><span class="math display">\[\begin{aligned}
Var[\tilde{B}(t_{2})-\tilde{B}(t_{1})] &amp; =Var[B(s)-B(t_{1})]+Var[B(t_{2})-B(s)]\\
&amp; =(s-t_{1})+(t_{2}-s)\\
&amp; =t_{2}-t_{1}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(s&lt;t_{1}&lt;t_{2}\)</span>. Then, <span class="math display">\[\begin{aligned}
\tilde{B}(t_{2})-\tilde{B}(t_{1}) &amp; =B_{s}-(B_{t_{2}}-B_{s})-(B_{s}-(B_{t_{1}}-B_{s}))\\
&amp; =\cancel{B_{s}}-(B_{t_{2}}-\cancel{B_{s}})-(\cancel{B_{s}}-(B_{t_{1}}-\cancel{B_{s}}))\\
&amp; =-(B_{t_{2}}-B_{t_{1}})
\end{aligned}\]</span></p>
<p>Hence, <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})\)</span> is again a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t_{2}-t_{1}\)</span>. Hence, condition (3) is satisfied.</p>
<p>(3) Assume that <span class="math inline">\(0\leq t_{1}\leq\ldots\leq t_{k-1}\leq s\leq t_{k}\leq\ldots\leq t_{n}\)</span>. From the above discussion, the increments <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\tilde{B}(s)-\tilde{B}(t_{k-1})\)</span>, <span class="math inline">\(\tilde{B}(t_{k})-\tilde{B}(s)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\tilde{B}(t_{n})-\tilde{B}(t_{n-1})\)</span> are independent increments. The increment <span class="math inline">\(\tilde{B}(t_{k})-\tilde{B}(t_{k-1})\)</span> only depends on the random variables <span class="math inline">\(\tilde{B}(s)-\tilde{B}(t_{k-1})\)</span> and <span class="math inline">\(\tilde{B}(t_{k})-\tilde{B}(s)\)</span>. Thus, <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\tilde{B}(t_{k})-\tilde{B}(t_{k-1})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\tilde{B}(t_{n})-\tilde{B}(t_{n-1})\)</span> are independent. ◻</p>
</div>
<div class="prop">
<p>(Time Reversal). Let <span class="math inline">\((B_{t},t\geq0)\)</span> be a Brownian motion. Show that the process <span class="math inline">\((B_{1}-B_{1-t},t\in[0,1])\)</span> has the distribution of a standard brownian motion on <span class="math inline">\([0,1]\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (1) At <span class="math inline">\(t=0\)</span>, <span class="math inline">\(B(1)-B(1-t)=B(1)-B(1)=0\)</span>.</p>
<p>(2) Let <span class="math inline">\(s&lt;t\)</span>. Then, <span class="math inline">\(1-t&lt;1-s\)</span>. So, the increment :</p>
<p><span class="math display">\[\begin{aligned}
(B(1)-B(1-t))-(B(1)-B(1-s)) &amp; =B(1-s)-B(1-t)
\end{aligned}\]</span></p>
<p>has a Gaussian distribution. It’s mean is <span class="math inline">\(0\)</span> and variance is <span class="math inline">\((1-s)-(1-t)=t-s\)</span>.</p>
<p>(3) Let <span class="math inline">\(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}\)</span>. Then:</p>
<p><span class="math display">\[1-t_{n}\leq\ldots\leq1-t_{k}\leq1-t_{k-1}\leq\ldots\leq1-t_{2}\leq1-t_{1}\]</span></p>
<p>Consider the increments of the process for <span class="math inline">\(k=1,2,\ldots,n\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
(B(1)-B(1-t_{k}))-(B(1)-B(1-t_{k-1})) &amp; =B(1-t_{k-1})-B(1-t_{k})
\end{aligned}\]</span></p>
<p>They are independent random variables. Hence, condition (3) is satisfied. ◻</p>
</div>
<div class="example">
<p>(Evaluating Brownian Probabilities). Let’s compute the probability that <span class="math inline">\(B_{1}&gt;0\)</span> and <span class="math inline">\(B_{2}&gt;0\)</span>. We know from the definition that <span class="math inline">\((B_{1},B_{2})\)</span> is a Gaussian vector with mean <span class="math inline">\(0\)</span> and covariance matrix:</p>
<p><span class="math display">\[\begin{aligned}
C &amp; =\left[\begin{array}{cc}
1 &amp; 1\\
1 &amp; 2
\end{array}\right]
\end{aligned}\]</span></p>
<p>The determinant of <span class="math inline">\(C\)</span> is <span class="math inline">\(1\)</span>. By performing row operations on the augmented matrix <span class="math inline">\([C|I]\)</span> we find that:</p>
<p><span class="math display">\[\begin{aligned}
C^{-1} &amp; =\left[\begin{array}{cc}
2 &amp; -1\\
-1 &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>Thus, the probability <span class="math inline">\(\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0)\)</span> can be expressed as:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0) &amp; =\frac{1}{\sqrt{(2\pi)^{2}}}\int_{0}^{\infty}\int_{0}^{\infty}\exp\left[-\frac{1}{2}(2x_{1}^{2}-2x_{1}x_{2}+x_{2}^{2}\right]dx_{2}dx_{1}
\end{aligned}\]</span></p>
<p>This integral can be evaluated using a calculator or software and is equal to <span class="math inline">\(3/8\)</span>. The probability can also be computed using the independence of increments. The increments <span class="math inline">\((B_{1},B_{2}-B_{1})\)</span> are IID standard Gaussians. We know their joint PDF. It remains to integrate over the correct region of <span class="math inline">\(\mathbf{R}^{2}\)</span> which in this case will be:</p>
<p><span class="math display">\[\begin{aligned}
D^{*} &amp; =\{(z_{1},z_{2}):(z_{1}&gt;0,z_{1}+z_{2}&gt;0)\}
\end{aligned}\]</span></p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0) &amp; =\frac{1}{2\pi}\int_{0}^{\infty}\int_{z_{2}=-z_{1}}^{z_{2}=\infty}e^{-(z_{1}^{2}+z_{2}^{2})/2}dz_{2}dz_{1}
\end{aligned}\]</span></p>
<p>It turns out that this integral can be evaluated exactly. Indeed by writing <span class="math inline">\(B_{1}=Z_{1}\)</span> and <span class="math inline">\(Z_{2}=B_{2}-B_{1}\)</span> and splitting the probability on the event <span class="math inline">\(\{Z_{2}\geq0\}\)</span> and its complement, we have that <span class="math inline">\(\mathbb{P}(B_{1}\geq0,B_{2}\geq0)\)</span> equals:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(B_{1}\geq0,B_{2}\geq0) &amp; =\mathbb{P}(Z_{1}\geq0,Z_{1}+Z_{2}&gt;0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}+Z_{2}&gt;0,Z_{2}&lt;0)\\
&amp; =\mathbb{P}(Z_{1}\geq0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;-Z_{2},-Z_{2}&gt;0)\\
&amp; =\mathbb{P}(Z_{1}\geq0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;Z_{2},Z_{2}&gt;0)\\
&amp; =\frac{1}{4}+\frac{1}{8}\\
&amp; =\frac{3}{8}
\end{aligned}\]</span></p>
<p>Note that, by symmetry, <span class="math inline">\(\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;Z_{2},Z_{2}&gt;0)=\mathbb{P}(Z_{1}\geq0,Z_{1}\leq Z_{2},Z_{2}&gt;0)=\frac{1}{8}\)</span>.</p>
</div>
<div class="example">
<p>(Another look at Ornstein Uhlenbeck process.) Consider the process <span class="math inline">\((X_{t},t\in\mathbf{R})\)</span> defined by :</p>
<p><span class="math display">\[\begin{aligned}
X_{t} &amp; =\frac{e^{-2t}}{\sqrt{2}}B(e^{4t}),\quad t\in\mathbf{R}
\end{aligned}\]</span></p>
<p>Here the process <span class="math inline">\((B_{e^{4t}},t\ge0)\)</span> is called a time change of Brownian motion, since the time is now quantitfied by an increasing function of <span class="math inline">\(t\)</span> namely <span class="math inline">\(e^{4t}\)</span>. The example <span class="math inline">\((B(\lambda t),t\geq0)\)</span> in the scaling property is another example of time change.</p>
</div>
<p>It turns out that <span class="math inline">\((X_{t},t\in\mathbf{R})\)</span> is a stationary Ornstein-Uhlenbeck process. (Here the index of time is <span class="math inline">\(\mathbf{R}\)</span> instead of <span class="math inline">\([0,\infty)\)</span>, but the definition also applies as the process is stationary. Since the original brownian motion <span class="math inline">\(B(t)\)</span> is a Gaussian process, any finite dimensional vector <span class="math inline">\((B(t_{1}),\ldots,B(t_{n}))\)</span> is Gaussian. It follows that:</p>
<p><span class="math display">\[(B(T_{1}),\ldots,B(T_{n}))=\frac{1}{\sqrt{2}}(e^{-2t_{1}}B(e^{4t_{1}}),\ldots,e^{-2t_{n}}B(e^{4t_{n}}))\]</span></p>
<p>is also a Gaussian vector. (Note, once we fix <span class="math inline">\(t_{1},t_{2},\ldots,t_{n}\)</span>, <span class="math inline">\(e^{-4t_{1}},\ldots,e^{-4t_{n}}\)</span> are constants.) Hence, <span class="math inline">\((X_{t},t\in\mathbf{R})\)</span> is a Gaussian process.</p>
<p>The mean of <span class="math inline">\((X_{t},t\in\mathbf{R})\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[X_{t}] &amp; =\frac{e^{-2t}}{\sqrt{2}}\mathbb{E}[B(e^{4t})]=0
\end{aligned}\]</span></p>
<p>And if <span class="math inline">\(s&lt;t\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[X_{s}X_{t}] &amp; =\frac{e^{-2(s+t)}}{2}\mathbb{E}[B(e^{4s})B(e^{4t})]\\
&amp; =\frac{e^{-2(s+t)}}{2}e^{4s}\\
&amp; =\frac{e^{-2(t-s)}}{2}
\end{aligned}\]</span></p>
<p>Two Gaussian processes having the same mean and covariance have the same distribution. Hence, it proves the claim that <span class="math inline">\((X_{t})\)</span> is a stationary OU process.</p>
</section>
<section id="properties-of-the-paths." class="level2">
<h2 class="anchored" data-anchor-id="properties-of-the-paths.">Properties of the paths.</h2>
<p>First we review the definitions of the Riemann integral and the Riemann-Stieljtes integral in Calculus.</p>
<div class="defn">
<p>A partition <span class="math inline">\(P\)</span> of <span class="math inline">\([a,b]\)</span> is a <em>finite</em> set of points from <span class="math inline">\([a,b]\)</span> that includes both <span class="math inline">\([a,b].\)</span>The notational convention is to always list the points of a partition <span class="math inline">\(P=\{a=x_{0},x_{1},x_{2},\ldots,x_{n}=b\}\)</span> in increasing order. Thus:</p>
<p><span class="math display">\[a=x_{0}&lt;x_{1}&lt;\ldots&lt;x_{k-1}&lt;x_{k}&lt;\ldots&lt;x_{n}=b\]</span></p>
</div>
<p>For each subinterval <span class="math inline">\([x_{k-1},x_{k}]\)</span> of <span class="math inline">\(P\)</span>, let</p>
<p><span class="math display">\[\begin{aligned}
m_{k} &amp; =\inf\{f(x):x\in[x_{k-1},x_{k}]\}\\
M_{k} &amp; =\sup\{f(x):x\in[x_{k-1},x_{k}]\}
\end{aligned}\]</span></p>
<p>The lower sum of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(P\)</span> is given by :</p>
<p><span class="math display">\[\begin{aligned}
L(f,P) &amp; =\sum_{k=1}^{n}m_{k}(x_{k}-x_{k-1})
\end{aligned}\]</span></p>
<p>The upper sum of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(P\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
U(f,P) &amp; =\sum_{k=1}^{n}M_{k}(x_{k}-x_{k-1})
\end{aligned}\]</span></p>
<p>For a particular partition <span class="math inline">\(P\)</span>, it is clear that <span class="math inline">\(U(f,P)\geq L(f,P)\)</span> because <span class="math inline">\(M_{k}\geq m_{k}\)</span> for all <span class="math inline">\(k=0,1,2,\ldots,n\)</span>.</p>
<div class="defn">
<p>A partition <span class="math inline">\(Q\)</span> is called a <em>refinement</em> of <span class="math inline">\(P\)</span> if <span class="math inline">\(Q\)</span> contains all of the points of <span class="math inline">\(P\)</span>; that is <span class="math inline">\(Q\subseteq P\)</span>.</p>
</div>
<div class="lem">
<p>If <span class="math inline">\(P\subseteq Q\)</span>, then <span class="math inline">\(L(f,P)\leq L(f,Q)\)</span> and <span class="math inline">\(U(f,Q)\leq U(f,P)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Consider what happens when we refine <span class="math inline">\(P\)</span> by adding a single point <span class="math inline">\(z\)</span> to some subinterval <span class="math inline">\([x_{k-1},x_{k}]\)</span> of <span class="math inline">\(P\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
m_{k}(x_{k}-x_{k-1}) &amp; =m_{k}(x_{k}-z)+m_{k}(z-x_{k-1})\\
&amp; \leq m_{k}'(x_{k}-z)+m_{k}''(z-x_{k-1})
\end{aligned}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{aligned}
m_{k}' &amp; =\inf\{f(x):x\in[z,x_{k}]\}\\
m_{k}'' &amp; =\inf\{f(x):x\in[x_{k-1},z]\}
\end{aligned}\]</span></p>
<p>By induction we have:</p>
<p><span class="math display">\[\begin{aligned}
L(f,P) &amp; \leq L(f,Q)\\
U(f,Q) &amp; \leq U(f,P)
\end{aligned}\]</span> ◻</p>
</div>
<div class="lem">
<p>If <span class="math inline">\(P_{1}\)</span> and <span class="math inline">\(P_{2}\)</span> are any two partitions of <span class="math inline">\([a,b]\)</span>, then <span class="math inline">\(L(f,P_{1})\leq U(f,P_{2})\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Let <span class="math inline">\(Q=P_{1}\cup P_{2}\)</span>. Then, <span class="math inline">\(P_{1}\subseteq Q\)</span> and <span class="math inline">\(P_{2}\subseteq Q\)</span>. Thus, <span class="math inline">\(L(f,P_{1})\leq L(f,Q)\leq U(f,Q)\leq L(f,P_{2})\)</span>. ◻</p>
</div>
<div class="defn">
<p>Let <span class="math inline">\(\mathcal{P}\)</span> be the collection of all possible partitions of the interval <span class="math inline">\([a,b]\)</span>. The upper integral of <span class="math inline">\(f\)</span> is defined to be:</p>
<p><span class="math display">\[\begin{aligned}
U(f) &amp; =\inf\{U(f,P):P\in\mathcal{P}\}
\end{aligned}\]</span></p>
<p>The lower integral of <span class="math inline">\(f\)</span> is defined by:</p>
<p><span class="math display">\[\begin{aligned}
L(f) &amp; =\sup\{L(f,P):P\in\mathcal{P}\}
\end{aligned}\]</span></p>
</div>
<p>Consider the set of all upper sums of <span class="math inline">\(f\)</span> - <span class="math inline">\(\{U(f,P):P\in\mathcal{P}\}\)</span>. Take an arbitrary partition <span class="math inline">\(P'\in\mathcal{P}\)</span>. Since <span class="math inline">\(L(f,P')\leq U(f,P)\)</span> for all <span class="math inline">\(P\in\mathcal{P}\)</span>, by the Axiom of Completeness(AoC), <span class="math inline">\(\inf\{U(f,P):P\in\mathcal{P}\}\)</span> exists.We can similarly argue for the supremum of all lower Riemann sums.</p>
<div class="lem">
<p>For any bounded function <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>, it is always the case that <span class="math inline">\(U(f)\geq L(f)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> By the properties of the infimum of a set, <span class="math inline">\((\forall\epsilon&gt;0)\)</span>, <span class="math inline">\(\exists P(\epsilon)\)</span> such that <span class="math inline">\(U(f)&lt;U(f,P(\epsilon))&lt;U(f)+\epsilon\)</span>. Pick <span class="math inline">\(\epsilon=1,\frac{1}{2},\frac{1}{3}\ldots,\frac{1}{n},\ldots\)</span>. Thus, we can produce a sequence of partitions <span class="math inline">\(P_{n}\)</span> such that :</p>
<p><span class="math display">\[U(f)&lt;\ldots&lt;U(f,P_{n})&lt;U(f)+\frac{1}{n}\]</span></p>
<p>Consequently, <span class="math inline">\(\lim U(f,P_{n})=U(f)\)</span>. Similarly, we can produce a sequence of partitions <span class="math inline">\((Q_{m})\)</span> such that :</p>
<p><span class="math display">\[L(f)-\frac{1}{m}&lt;\ldots&lt;L(f,Q_{m})&lt;L(f)\]</span></p>
<p>We know that:</p>
<p><span class="math display">\[\begin{aligned}
L(f,Q_{m}) &amp; \leq U(f,P_{n})
\end{aligned}\]</span></p>
<p>Keeping <span class="math inline">\(m\)</span> fixed and passing to the limit, as <span class="math inline">\(n\to\infty\)</span> on both sides, we have:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}L(f,Q_{m}) &amp; \leq\lim_{n\to\infty}U(f,P_{n})\quad\left\{ \text{Order Limit Theorem}\right\} \\
L(f,Q_{m}) &amp; \leq U(f)
\end{aligned}\]</span></p>
<p>Now, passing to the limit, as <span class="math inline">\(m\to\infty\)</span> on both sides, we have:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{m\to\infty}L(f,Q_{m}) &amp; \leq\lim_{m\to\infty}U(f)\quad\left\{ \text{Order Limit Theorem}\right\} \\
L(f) &amp; \leq U(f)
\end{aligned}\]</span> ◻</p>
</div>
<div class="defn">
<p>(Riemann Integrability). A bounded function <span class="math inline">\(f\)</span> on the interval <span class="math inline">\([a,b]\)</span> is said to be Riemann integrable if <span class="math inline">\(U(f)=L(f)\)</span>. In this case, we define <span class="math inline">\(\int_{a}^{b}f\)</span> or <span class="math inline">\(\int_{a}^{b}f(x)dx\)</span> to be the common value:</p>
<p><span class="math display">\[\begin{aligned}
\int_{a}^{b}f(x)dx &amp; =U(f)=L(f)
\end{aligned}\]</span></p>
</div>
<div class="thm">
<p>(Integrability Criterion) A bounded function <span class="math inline">\(f\)</span> is integrable on <span class="math inline">\([a,b]\)</span> if and only if, for every <span class="math inline">\(\epsilon&gt;0\)</span>, there exists a partition <span class="math inline">\(P_{\epsilon}\)</span> of <span class="math inline">\([a,b]\)</span> such that:</p>
<p><span class="math display">\[\begin{aligned}
U(f,P_{\epsilon})-L(f,P_{\epsilon}) &amp; &lt;\epsilon
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (<span class="math inline">\(\Longleftarrow\)</span> direction.) Let <span class="math inline">\(\epsilon&gt;0\)</span>. If such a partition <span class="math inline">\(P_{\epsilon}\)</span> exists, then:</p>
<p><span class="math display">\[U(f)-L(f)\leq U(f,P_{\epsilon})-L(f,P_{\epsilon})&lt;\epsilon\]</span></p>
<p>Because <span class="math inline">\(\epsilon\)</span> is arbitrary, it follows that <span class="math inline">\(U(f)=L(f)\)</span> and hence <span class="math inline">\(f\)</span> is Riemann integrable.</p>
<p>(<span class="math inline">\(\Longrightarrow\)</span> direction.) Let <span class="math inline">\(f\)</span> be a bounded function on <span class="math inline">\([a,b]\)</span> such that <span class="math inline">\(f\)</span> is Riemann integrable.</p>
<p>Pick an arbitrary <span class="math inline">\(\epsilon&gt;0\)</span>.</p>
<p>Then, since <span class="math inline">\(U(f)=\inf\{U(f,P):P\in\mathcal{P}\}\)</span>, there exists <span class="math inline">\(P_{\epsilon}\in\mathcal{P}\)</span>, such that <span class="math inline">\(U(f)&lt;U(f,P_{\epsilon})&lt;U(f)+\frac{\epsilon}{2}\)</span>. Since <span class="math inline">\(L(f)=\sup\{L(f,P):P\in\mathcal{P}\}\)</span>, there exists <span class="math inline">\(P_{\epsilon}\in\mathcal{P}\)</span>, such that <span class="math inline">\(L(f)-\frac{\epsilon}{2}&lt;L(f,P_{\epsilon})&lt;L(f)\)</span>. Consequently,</p>
<p><span class="math display">\[\begin{aligned}
U(f,P_{\epsilon})-L(f,P_{\epsilon}) &amp; &lt;U(f)+\frac{\epsilon}{2}-\left(L(f)-\frac{\epsilon}{2}\right)\\
&amp; =U(f)-L(f)+\epsilon\\
&amp; =\epsilon
\end{aligned}\]</span> ◻</p>
</div>
<section id="functions-considered-in-stochastic-calculus." class="level3">
<h3 class="anchored" data-anchor-id="functions-considered-in-stochastic-calculus.">Functions considered in Stochastic Calculus.</h3>
<div class="defn">
<p>A point <span class="math inline">\(c\)</span> is called a discontinuity of the first kind or jump point if both limits <span class="math inline">\(g(c+)=\lim_{t\uparrow c}g(t)\)</span> and <span class="math inline">\(g(c-)=\lim_{t\downarrow c}g(t)\)</span> exist and are not equal. The jump at <span class="math inline">\(c\)</span> is defined as <span class="math inline">\(\Delta g(c)=g(c+)-g(c-)\)</span>. Any other discontinuity is said to be of the second kind.</p>
</div>
<div class="example">
<p>Consider the function</p>
<p><span class="math display">\[\begin{aligned}
f(x) &amp; =\sin\left(\frac{1}{x}\right)
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(x_{n}=\frac{1}{2n\pi}\)</span>. Then, <span class="math inline">\(f(x_{n})=(0,0,0,\ldots)\)</span>. Next, consider <span class="math inline">\(y_{n}=\frac{1}{\pi/2+2n\pi}\)</span>. Then, <span class="math inline">\(f(y_{n})=(1,1,1,\ldots)\)</span>. Consequently, <span class="math inline">\(f\)</span> is not continuous at <span class="math inline">\(0\)</span>. Hence, limits from the left or right don’t exist. Consequently, this is a discontinuity of the second kind.</p>
</div>
<p>Functions in stochastic calculus are functions without discontinuities of the second kind, that is functions have both left and right hand limits at any point of the domain and have one-sided limits at the boundary. These functions are called <em>regular</em> functions. It is often agreed to identify functions if they have the same right and left limits at any point.</p>
<p>The class <span class="math inline">\(D=D[0,T]\)</span> of right-continuous functions on <span class="math inline">\([0,T]\)</span> with left limits has a special name, <em>cadlag</em> functions (which is the abbreviation of right continuous with left limits in French). Sometimes these processes are called R.R.C. for regular right continuous. Notice that this class of processes includes <span class="math inline">\(C\)</span>, the class of continuous functions.</p>
<p>Let <span class="math inline">\(g\in D\)</span> be a cadlag function, then, by definition, all the discontinuities of <span class="math inline">\(g\)</span> are jumps. An important result in analysis is that, a function can have no more than a countable number of discontinuities.</p>
</section>
<section id="variation-of-a-function." class="level3">
<h3 class="anchored" data-anchor-id="variation-of-a-function.">Variation of a function.</h3>
<p>If <span class="math inline">\(g\)</span> is a function of a real variable, its variation over the interval <span class="math inline">\([a,b]\)</span> is defined as:</p>
<p><span class="math display">\[\begin{aligned}
V_{g}([a,b]) &amp; =\sup\left\{ \sum_{i=1}^{n}\left|g(t_{i})-g(t_{i-1})\right|\right\} \label{eq:total-variation-of-a-function}
\end{aligned}\]</span></p>
<p>where the supremum is taken over all partitions <span class="math inline">\(P\in\mathcal{P}\)</span>.</p>
<p>Clearly, by the Triangle Inequality, the sums in (<a href="#eq:total-variation-of-a-function" data-reference-type="ref" data-reference="eq:total-variation-of-a-function">[eq:total-variation-of-a-function]</a>) increase as new points are added to the partitions. Therefore, the variation of <span class="math inline">\(g\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
V_{g}([a,b]) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}\left|g(t_{i})-g(t_{i-1})\right|
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(||\Delta_{n}||=\max_{1\leq i\leq n}(t_{i}-t_{i-1})\)</span>. If <span class="math inline">\(V_{g}([a,b])\)</span> is finite, then <span class="math inline">\(g\)</span> is said to be a function of finite variation on <span class="math inline">\([a,b]\)</span>. If <span class="math inline">\(g\)</span> is a function of <span class="math inline">\(t\geq0\)</span>, then the variation of <span class="math inline">\(g\)</span> as a function of <span class="math inline">\(t\)</span> is defined by:</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =V_{g}([0,t])
\end{aligned}\]</span></p>
<p>Clearly, <span class="math inline">\(V_{g}(t)\)</span> is an increasing function of <span class="math inline">\(t\)</span>.</p>
<div class="defn">
<p><span class="math inline">\(g\)</span> is a function of finite variation if <span class="math inline">\(V_{g}(t)&lt;\infty\)</span> for all <span class="math inline">\(t\in[0,\infty)\)</span>. <span class="math inline">\(g\)</span> is of bounded variation if <span class="math inline">\(\sup_{t}V_{g}(t)&lt;\infty\)</span>, in other words there exists <span class="math inline">\(C\)</span>, for all <span class="math inline">\(t\)</span>, such that <span class="math inline">\(V_{g}(t)&lt;C\)</span>. Here <span class="math inline">\(C\)</span> is independent of <span class="math inline">\(t\)</span>.</p>
</div>
<div class="example">
<p>(1) If <span class="math inline">\(g(t)\)</span> is increasing then for any <span class="math inline">\(i\)</span>, <span class="math inline">\(g(t_{i})\geq g(t_{i-1})\)</span>, resulting in a telescopic sum, where all terms excluding the first and the last cancel out, leaving</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =g(t)-g(0)
\end{aligned}\]</span></p>
<p>(2) If <span class="math inline">\(g(t)\)</span> is decreasing, then similarly,</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =g(0)-g(t)
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>If <span class="math inline">\(g(t)\)</span> is differentiable with continuous derivative <span class="math inline">\(g'(t)\)</span>, <span class="math inline">\(g(t)=\int_{0}^{t}g'(s)ds\)</span> then</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =\int_{0}^{t}|g'(s)|ds
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> By definition,</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =\lim_{||\Delta_{n}\to0||}\sum_{i=1}^{n}|g(t_{i})-g(t_{i-1})|
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(g\)</span> is continuous and differentiable on <span class="math inline">\([t_{i-1},t_{i}]\)</span>, there exists <span class="math inline">\(z_{i}\in(t_{i-1},t_{i})\)</span> such, that <span class="math inline">\(g(t_{i})-g(t_{i-1})=g'(z_{i})(t_{i}-t_{i-1})\)</span>. Therefore, we can write:</p>
<p><span class="math display">\[\begin{aligned}
{1}
V_{g}(t) &amp; =\lim_{||\Delta_{n}\to0||}\sum_{i=1}^{n}|g'(z_{i})|(t_{i}-t_{i-1})\\
&amp; =\int_{0}^{t}|g'(s)|ds
\end{aligned}\]</span> ◻</p>
</div>
<div class="thm">
<p>If <span class="math inline">\(g\)</span> is continuous, <span class="math inline">\(g'\)</span> exists and <span class="math inline">\(\int_{0}^{t}|g'(s)|ds\)</span> is finite, then <span class="math inline">\(g\)</span> is of finite variation.</p>
</div>
<div class="example">
<p>The function <span class="math inline">\(g(t)=t\sin(1/t)\)</span> for <span class="math inline">\(t&gt;0\)</span> and <span class="math inline">\(g(0)=0\)</span> is continuous on <span class="math inline">\([0,1]\)</span> and differentiable at all points except zero, but is not of bounded variation on any interval that includes <span class="math inline">\(0\)</span>. Consider the partition <span class="math inline">\(\{x_{n}\}=\left\{ \frac{1}{\pi/2+n\pi}\right\}\)</span>. Thus,</p>
<p><span class="math display">\[\begin{aligned}
\sin(\frac{1}{x_{n}}) &amp; =\begin{cases}
1 &amp; \text{if }n\text{ is even}\\
-1 &amp; \text{if }n\text{ is odd}
\end{cases}
\end{aligned}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
f(x_{n}) &amp; =\begin{cases}
x_{n} &amp; n\text{ is even}\\
-x_{n} &amp; n\text{ is odd}
\end{cases}
\end{aligned}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{aligned}
\sum_{n=1}^{m}|f(x_{n})-f(x_{n-1})| &amp; =\sum_{n=1}^{m}(x_{n}+x_{n-1})\\
&amp; =x_{0}+x_{n}+2\sum_{n=1}^{m-1}x_{n}\\
&amp; \geq\sum_{n=1}^{m-1}x_{n}
\end{aligned}\]</span></p>
<p>This is the lower bound on the variation of <span class="math inline">\(g\)</span> on the partition <span class="math inline">\(\{0,x_{m},\ldots,x_{1},x_{0},1\}\)</span>. Now, passing to the limit as <span class="math inline">\(m\)</span> approaches infinity, <span class="math inline">\(\sum\frac{1}{\pi/2+n\pi}\)</span> is a divergent series. Consequently, <span class="math inline">\(V_{g}([0,1])\)</span> has unbounded variation.</p>
</div>
</section>
<section id="jordan-decomposition." class="level3">
<h3 class="anchored" data-anchor-id="jordan-decomposition.">Jordan Decomposition.</h3>
<div class="thm">
<p>Any function <span class="math inline">\(g:[0,\infty)\to\mathbf{R}\)</span> is of bounded variation if and only if it can be expressed as the difference of two increasing functions:</p>
<p><span class="math display">\[\begin{aligned}
g(t) &amp; =a(t)-b(t)
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (<span class="math inline">\(\Longrightarrow\)</span>direction). If <span class="math inline">\(g\)</span> is of finite variation, <span class="math inline">\(V_{g}(t)&lt;\infty\)</span> for all <span class="math inline">\(t\)</span>, and we can write:</p>
<p><span class="math display">\[\begin{aligned}
g(t) &amp; =V_{g}(t)-(V_{g}(t)-g(t))
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(a(t)=V_{g}(t)\)</span> and <span class="math inline">\(b(t)=V_{g}(t)-g(t)\)</span>. Clearly, both <span class="math inline">\(a(t)\)</span> and <span class="math inline">\(b(t)\)</span> are increasing functions.</p>
<p>(<span class="math inline">\(\Longleftarrow\)</span>direction). Suppose a function <span class="math inline">\(g\)</span> can be expressed as a difference of two bounded increasing functions. Then,</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}|(a(t_{i})-b(t_{i}))-(a(t_{i-1})-b(t_{i-1})|\\
&amp; \quad\{\text{ Telescoping sum }\}\\
&amp; =a(t)-b(t)-(a(0)-b(0))
\end{aligned}\]</span></p>
<p>Since both <span class="math inline">\(a(t)\)</span> and <span class="math inline">\(b(t)\)</span> are bounded, <span class="math inline">\(g\)</span> has bounded variation. ◻</p>
</div>
</section>
<section id="riemann-stieltjes-integral." class="level3">
<h3 class="anchored" data-anchor-id="riemann-stieltjes-integral.">Riemann-Stieltjes Integral.</h3>
<p>Let <span class="math inline">\(g\)</span> be a montonically increasing function on a finite closed interval <span class="math inline">\([a,b]\)</span>. A bounded function <span class="math inline">\(f\)</span> defined on <span class="math inline">\([a,b]\)</span> is said to <em>Riemann-Stieltjes integrable</em> with respect to <span class="math inline">\(g\)</span> if the following limit exists:</p>
<p><span class="math display">\[\begin{aligned}
\int_{a}^{b}f(t)dg(t) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}f(\tau_{i})(g(t_{i})-g(t_{i-1}))\label{eq:riemann-stieltjes-integral}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tau_{i}\)</span> is an evaluation point in the interval <span class="math inline">\([t_{i-1},t_{i}]\)</span>. It is a well-known fact that continuous functions are Riemann integrable and Riemann-Stieltjes integrable with respect to any monotonically increasing function on <span class="math inline">\([a,b]\)</span>.</p>
<p>We ask the following question. For any continuous functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> on <span class="math inline">\([a,b]\)</span>, can we define the integral <span class="math inline">\(\int_{a}^{b}f(t)dg(t)\)</span> by Equation (<a href="#eq:riemann-stieltjes-integral" data-reference-type="ref" data-reference="eq:riemann-stieltjes-integral">[eq:riemann-stieltjes-integral]</a>)?</p>
<p>Consider the special case <span class="math inline">\(f=g\)</span>, namely, the integral:</p>
<p><span class="math display">\[\int_{a}^{b}f(t)df(t)\]</span></p>
<p>Let <span class="math inline">\(\Delta_{n}=\{a=t_{0},t_{1},\ldots,t_{n}=b\}\)</span> be a partition of <span class="math inline">\([a,b]\)</span>. Let <span class="math inline">\(L_{n}\)</span> and <span class="math inline">\(R_{n}\)</span> denote the corresponding Riemann sums with the evaluation points <span class="math inline">\(\tau_{i}=t_{i-1}\)</span> and <span class="math inline">\(\tau_{i}=t_{i}\)</span>, respectively, namely,</p>
<p><span class="math display">\[\begin{aligned}
L_{n} &amp; =\sum_{i=1}^{n}f(t_{i-1})(f(t_{i})-f(t_{i-1}))\label{eq:left-riemann-sum}\\
R_{n} &amp; =\sum_{i=1}^{n}f(t_{i})(f(t_{i})-f(t_{i-1}))\label{eq:right-riemann-sum}
\end{aligned}\]</span></p>
<p>Is it true that, <span class="math inline">\(\lim L_{n}=\lim R_{n}\)</span> as <span class="math inline">\(||\Delta_{n}||\to0\)</span>? Observe that:</p>
<p><span class="math display">\[R_{n}-L_{n}=\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\label{eq:quadratic-variation}\]</span></p>
<p><span class="math display">\[R_{n}+L_{n}=\sum_{i=1}^{n}(f(t_{i})^{2}-f(t_{i-1})^{2})=f(b)^{2}-f(a)^{2}\label{eq:sum-of-left-and-right-riemann-sums}\]</span></p>
<p>Therefore, <span class="math inline">\(R_{n}\)</span> and <span class="math inline">\(L_{n}\)</span> are given by:</p>
<p><span class="math display">\[R_{n}=\frac{1}{2}\left(f(b)^{2}-f(a)^{2}+\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\right)\]</span></p>
<p><span class="math display">\[L_{n}=\frac{1}{2}\left(f(b)^{2}-f(a)^{2}-\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\right)\]</span></p>
<p>The limit of the right-hand side of equation (<a href="#eq:quadratic-variation" data-reference-type="ref" data-reference="eq:quadratic-variation">[eq:quadratic-variation]</a>) is called the <em>quadratic variation</em> of the function <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>. Obviously, <span class="math inline">\(\lim_{||\Delta_{n}||\to0}R_{n}\neq\lim_{||\Delta_{n}||\to0}L_{n}\)</span> if and only the quadratic variation of the function <span class="math inline">\(f\)</span> is non-zero.</p>
<div class="example">
<p>Let <span class="math inline">\(f\)</span> be a <span class="math inline">\(C^{1}\)</span>-function that is <span class="math inline">\(f'(t)\)</span> is a continuous function. Then, by the mean value theorem:</p>
<p><span class="math display">\[\begin{aligned}
|R_{n}-L_{n}| &amp; =\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\\
&amp; =\sum_{i=1}^{n}(f'(t_{i}^{*})(t_{i}-t_{i-1}))^{2}\\
&amp; \quad\{\text{Mean Value Theorem}\}\\
&amp; \leq\sum_{i=1}^{n}\left\Vert f'\right\Vert _{\infty}^{2}(t_{i}-t_{i-1})^{2}\\
&amp; \quad\{\text{ Interior Extremum Theorem }\}\\
&amp; \leq\left\Vert f'\right\Vert _{\infty}^{2}\left\Vert \Delta_{n}\right\Vert \sum_{i=1}^{n}(t_{i}-t_{i-1})\\
&amp; =\left\Vert f'\right\Vert _{\infty}^{2}\left\Vert \Delta_{n}\right\Vert (b-a)
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\left\Vert f'\right\Vert _{\infty}=\sup_{x\in[a,b]}f(x)\)</span>. Thus, the limit as <span class="math inline">\(\left\Vert \Delta_{n}\right\Vert \to0\)</span> of the distance <span class="math inline">\(|R_{n}-L_{n}|\)</span> also approaches zero. Thus, <span class="math inline">\(\lim L_{n}=\lim R_{n}\)</span> as <span class="math inline">\(\left\Vert \Delta_{n}\right\Vert \to0\)</span> and the Riemann-Stieltjes integral exists. By equation (<a href="#eq:sum-of-left-and-right-riemann-sums" data-reference-type="ref" data-reference="eq:sum-of-left-and-right-riemann-sums">[eq:sum-of-left-and-right-riemann-sums]</a>), we have:</p>
<p><span class="math display">\[\lim_{\left\Vert \Delta_{n}\right\Vert \to0}L_{n}=\lim_{\left\Vert \Delta_{n}\right\Vert \to0}R_{n}=\frac{1}{2}(f(b)^{2}-f(a)^{2})\]</span></p>
<p>On the other hand, for such a <span class="math inline">\(C^{1}\)</span>-function <span class="math inline">\(f\)</span>, we may simply define the integral <span class="math inline">\(\int_{a}^{b}f(t)df(t)\)</span> by:</p>
<p><span class="math display">\[\begin{aligned}
\int_{a}^{b}f(t)df(t) &amp; =\int_{a}^{b}f(t)f'(t)dt
\end{aligned}\]</span></p>
<p>Then, by the fundamental theorem of Calculus:</p>
<p><span class="math display">\[\int_{a}^{b}f(t)df(t)=\int_{a}^{b}f(t)f'(t)dt=\frac{1}{2}f(t)^{2}|_{a}^{b}=\frac{1}{2}(f(b)^{2}-f(a)^{2})\]</span></p>
</div>
<div class="rem*">
<p>There is a very close relationship between functions with bounded variation and functions for which the classical integral makes sense. For the Ito integral, the quadratic variation plays a similar role. The quadratic variation of a smooth fuction <span class="math inline">\(f\in C^{1}([0,t])\)</span> is zero.</p>
</div>
<div class="example">
<p><span id="ex:non-zero-quadratic-variation-example" label="ex:non-zero-quadratic-variation-example"></span>Suppose <span class="math inline">\(f\)</span> is a continuous function satisfying the condition</p>
<p><span class="math display">\[\begin{aligned}
|f(t)-f(s)| &amp; \leq C|t-s|^{1/2}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(0&lt;C&lt;1\)</span>.</p>
<p>In this case we have:</p>
<p><span class="math display">\[0\leq|R_{n}-L_{n}|\leq C^{2}\sum_{i=1}^{n}(t_{i}-t_{i-1})=C^{2}(b-a)\]</span></p>
<p>Hence, <span class="math inline">\(\lim R_{n}\neq\lim L_{n}\)</span> as <span class="math inline">\(\left\Vert \Delta_{n}\right\Vert \to0\)</span> when <span class="math inline">\(a\neq b\)</span>. Consequently, the integral <span class="math inline">\(\int_{a}^{b}f(t)df(t)\)</span> cannot be defined for such a function <span class="math inline">\(f\)</span>. Observe that the quandratic variation of the function is <span class="math inline">\(b-a\)</span> (non-zero).</p>
</div>
<p>We see from the above examples, that definining the integral <span class="math inline">\(\int_{a}^{b}f(t)dg(t)\)</span> even when <span class="math inline">\(f=g\)</span> is a non-trivial problem. Consider the question posed earlier - if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are continuous functions on <span class="math inline">\([a,b]\)</span>, can we define the integral <span class="math inline">\(\int_{a}^{b}f(t)dg(t)\)</span>? There is no simple answer to this question. But then in view of example (<a href="#ex:non-zero-quadratic-variation-example" data-reference-type="ref" data-reference="ex:non-zero-quadratic-variation-example">[ex:non-zero-quadratic-variation-example]</a>), we can ask another question:</p>
<p><em>Question</em>. Are there continuous functions <span class="math inline">\(f\)</span> satisfying the condition</p>
<p><span class="math display">\[\begin{aligned}
|f(t)-f(s)| &amp; \leq C|t-s|^{1/2}
\end{aligned}\]</span></p>
</section>
<section id="brownian-motion-as-the-limit-of-a-symmetric-random-walk." class="level3">
<h3 class="anchored" data-anchor-id="brownian-motion-as-the-limit-of-a-symmetric-random-walk.">Brownian motion as the limit of a symmetric random walk.</h3>
<p>Consider a random walk starting at <span class="math inline">\(0\)</span> with jumps <span class="math inline">\(h\)</span> and <span class="math inline">\(-h\)</span> equally at times <span class="math inline">\(\delta\)</span>, <span class="math inline">\(2\delta\)</span>, <span class="math inline">\(\ldots\)</span> where <span class="math inline">\(h\)</span> and <span class="math inline">\(\delta\)</span> are positive numbers. More precisely, let <span class="math inline">\(\{X_{n}\}_{n=1}^{\infty}\)</span> be a sequence of independent and identically distributed random variables with :</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\{X_{j}=h\} &amp; =\mathbb{P}\{X_{j}=-h\}=\frac{1}{2}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(Y_{\delta,h}(0)=0\)</span> and put:</p>
<p><span class="math display">\[\begin{aligned}
Y_{\delta,h}(n\delta) &amp; =X_{1}+X_{2}+\ldots+X_{n}
\end{aligned}\]</span></p>
<p>For <span class="math inline">\(t&gt;0\)</span>, define <span class="math inline">\(Y_{\delta,h}(t)\)</span> by linearization that is, for <span class="math inline">\(n\delta&lt;t&lt;(n+1)\delta\)</span>, define:</p>
<p><span class="math display">\[\begin{aligned}
Y_{\delta,h}(t) &amp; =\frac{(n+1)\delta-t}{\delta}Y_{\delta,h}(n\delta)+\frac{t-n\delta}{\delta}Y_{\delta,h}((n+1)\delta)
\end{aligned}\]</span></p>
<p>We can think of <span class="math inline">\(Y_{\delta,h}(t)\)</span> as the position of the random walk at time <span class="math inline">\(t\)</span>. In particular, <span class="math inline">\(X_{1}+X_{2}+\ldots+X_{n}\)</span> is the position of this random walk at time <span class="math inline">\(n\delta\)</span>.</p>
<p><em>Question</em>. What is the limit of the random walk <span class="math inline">\(Y_{\delta,h}\)</span> as <span class="math inline">\(\delta,h\to0\)</span>?</p>
<p>Recall that the characteristic function of a random variable <span class="math inline">\(X\)</span> is <span class="math inline">\(\phi_{X}(\lambda)=\mathbb{E}\exp[i\lambda X]\)</span>. In order to find out the answer, let us compute the following limit of the characteristic function of <span class="math inline">\(Y_{\delta,h}(t)\)</span>:</p>
<p><span class="math display">\[\lim_{\delta,h\to0}\mathbb{E}\exp\left[i\lambda Y_{\delta,h}(t)\right]\]</span></p>
<p>where <span class="math inline">\(\lambda\in\mathbf{R}\)</span>is fixed. For heuristic derivation, let <span class="math inline">\(t=n\delta\)</span> and so <span class="math inline">\(n=t/\delta\)</span>. Then we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\exp\left[i\lambda Y_{\delta,h}(t)\right] &amp; =\prod_{j=1}^{n}\mathbb{E}e^{i\lambda X_{j}}\\
&amp; =\prod_{j=1}^{n}\left(\frac{1}{2}e^{i\lambda h}+\frac{1}{2}e^{-i\lambda h}\right)\\
&amp; =\left(\frac{1}{2}e^{i\lambda h}+\frac{1}{2}e^{-i\lambda h}\right)^{n}\\
&amp; =\left(\cos\lambda h\right)^{n}\\
&amp; =\left(\cos\lambda h\right)^{t/\delta}
\end{aligned}\]</span></p>
<p>For fixed <span class="math inline">\(t\)</span> and <span class="math inline">\(\lambda\)</span>, when <span class="math inline">\(\delta\)</span> and <span class="math inline">\(h\)</span> independently approach <span class="math inline">\(0\)</span>, the limit of <span class="math inline">\(\mathbb{E}\exp\left[i\lambda Y_{\delta,h}(t)\right]\)</span> may not exist. For example, holding <span class="math inline">\(h\)</span> constant, letting <span class="math inline">\(\delta\to0\)</span>, since <span class="math inline">\(-1\leq\cos\theta\leq1\)</span>, the function <span class="math inline">\(\left(\cos\lambda h\right)^{t/\delta}\to0\)</span>. Holding <span class="math inline">\(\delta\)</span> constant, letting <span class="math inline">\(h\to0\)</span>, the function <span class="math inline">\(\left(\cos\lambda h\right)^{t/\delta}\to1\)</span>. In order for the limit to exist, we impose a certain relationship between <span class="math inline">\(\delta\)</span> and <span class="math inline">\(h\)</span>. However, depending on the relationship, we may obtain different limits.</p>
<p>Let <span class="math inline">\(u=\cos(\lambda h)^{1/\delta}\)</span>. Then <span class="math inline">\(\ln u=\frac{1}{\delta}\ln\cos(\lambda h)\)</span>. Note that:</p>
<p><span class="math display">\[\begin{aligned}
\cos(\lambda h) &amp; \approx1-\frac{1}{2}\lambda^{2}h^{2}
\end{aligned}\]</span></p>
<p>And <span class="math inline">\(\ln(1+x)\approx x\)</span>. Hence,</p>
<p><span class="math display">\[\ln\cos(\lambda h)\approx\ln\left(1-\frac{1}{2}\lambda^{2}h^{2}\right)\approx-\frac{1}{2}\lambda^{2}h^{2}\]</span></p>
<p>Therefore for small <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(h\)</span>, we have <span class="math inline">\(\ln u\approx-\frac{1}{2\delta}\lambda^{2}h^{2}\)</span> and so:</p>
<p><span class="math display">\[\begin{aligned}
u &amp; \approx\exp\left[-\frac{1}{2\delta}\lambda^{2}h^{2}\right]
\end{aligned}\]</span></p>
<p>In particular, if <span class="math inline">\(\delta\)</span> and <span class="math inline">\(h\)</span> are related by <span class="math inline">\(h^{2}=\delta\)</span>, then</p>
<p><span class="math display">\[\begin{aligned}
\lim_{\delta\to0}\mathbb{E}\exp\left[i\lambda Y_{\delta,h}(t)\right] &amp; =e^{-\frac{1}{2}\lambda^{2}t}
\end{aligned}\]</span></p>
<p>But, <span class="math inline">\(e^{-\frac{1}{2}\lambda^{2}t}\)</span> is the characteristic function of a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t\)</span>. Thus, we have derived the following theorem about the limit of the random walk <span class="math inline">\(Y_{\delta,h}\)</span> as <span class="math inline">\(\delta,h\to0\)</span> in such a way that <span class="math inline">\(h^{2}=\delta\)</span>.</p>
<div class="thm">
<p>Let <span class="math inline">\(Y_{\delta,h}(t)\)</span> be the random walk starting at <span class="math inline">\(0\)</span> with jumps <span class="math inline">\(h\)</span> and <span class="math inline">\(-h\)</span> equally likely at times <span class="math inline">\(\delta\)</span>, <span class="math inline">\(2\delta\)</span>, <span class="math inline">\(3\delta\)</span>, <span class="math inline">\(\ldots\)</span>. Assume that <span class="math inline">\(h^{2}=\delta\)</span>. Then, for each <span class="math inline">\(t\geq0\)</span>, the limit:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{\delta\to0}Y_{\delta,h}(t) &amp; =B(t)
\end{aligned}\]</span> exists in distribution. Moreover, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}e^{i\lambda B(t)} &amp; =e^{-\frac{1}{2}\lambda^{2}t}
\end{aligned}\]</span></p>
</div>
<div class="thm">
<p><span id="th:quadratic-variation-of-bm-approaches-t-in-mean-square" label="th:quadratic-variation-of-bm-approaches-t-in-mean-square"></span>(Quadratic Variation of a Brownian motion). Let <span class="math inline">\((B_{t},t\ge0)\)</span> be a standard brownian motion. Then, for any sequence of partitions <span class="math inline">\((t_{j},j\leq n)\)</span> of <span class="math inline">\([0,t]\)</span> we have:</p>
<p><span class="math display">\[\begin{aligned}
\left\langle B\right\rangle _{t} &amp; =\sum_{j=1}^{n}(B_{t_{j+1}}-B_{t_{j}})^{2}\stackrel{L^{2}}{\to}t
\end{aligned}\]</span></p>
<p>where the convergence is in the <span class="math inline">\(L^{2}\)</span> sense.</p>
</div>
<div class="rem*">
<p>It is reasonable to have some sort of convergence as we are dealing with a sum of independent random variables. However, the conclusion would not hold if the increments were not squared. So there is something more at play here.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-\sum_{j=0}^{n-1}(t_{j+1}-t_{j})\right)^{2}\right]\\
&amp; =\mathbb{E}\left[\left(\sum_{j=0}^{n-1}\left\{ (B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\right\} \right)^{2}\right]
\end{aligned}\]</span></p>
<p>For simplicity, we define the variables <span class="math inline">\(X_{j}=(B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\)</span>. Then, we may write:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =\mathbb{E}\left[\left(\sum_{j=0}^{n-1}X_{j}\right)^{2}\right]\\
&amp; =\mathbb{E}\left[\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}X_{i}X_{j}\right]\\
&amp; =\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}\mathbb{E}[X_{i}X_{j}]
\end{aligned}\]</span></p>
<p>Now, the random variables <span class="math inline">\(X_{j}\)</span> are independent.</p>
<p>The expectation of <span class="math inline">\(X_{j}\)</span> is <span class="math inline">\(\mathbb{E}[X_{j}]=\mathbb{E}(B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})=0\)</span>.</p>
<p>Since, <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span> are independent, for <span class="math inline">\(i\neq j\)</span>, <span class="math inline">\(\mathbb{E}[X_{i}X_{j}]=\mathbb{E}X_{i}\cdot\mathbb{E}X_{j}=0\)</span>.</p>
<p>Hence, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =\sum_{i=0}^{n-1}\mathbb{E}[X_{i}^{2}]
\end{aligned}\]</span></p>
<p>We now develop the expectation of the square of <span class="math inline">\(X_{i}\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[X_{i}^{2}] &amp; =\mathbb{E}\left[\left((B(t_{i+1})-B(t_{i}))^{2}-(t_{i+1}-t_{i})\right)^{2}\right]\\
&amp; =\mathbb{E}\left[((B(t_{i+1})-B(t_{i}))^{4}-2(B(t_{i+1})-B(t_{i}))^{2}(t_{i+1}-t_{i})+(t_{i+1}-t_{i})^{2}\right]
\end{aligned}\]</span></p>
<p>The MGF of the random variable <span class="math inline">\(B(t_{i+1})-B(t_{i})\)</span> is :</p>
<p><span class="math display">\[\begin{aligned}
\phi(\lambda) &amp; =\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]\\
\phi'(\lambda) &amp; =\lambda(t_{i+1}-t_{i})\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]\\
\phi''(\lambda) &amp; =\left[(t_{i+1}-t_{i})+\lambda^{2}(t_{i+1}-t_{i})^{2}\right]\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]\\
\phi^{(3)}(\lambda) &amp; =\left[3\lambda(t_{i+1}-t_{i})^{2}+\lambda^{3}(t_{i+1}-t_{i})^{3}\right]\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]\\
\phi^{(4)}(\lambda) &amp; =\left[3(t_{i+1}-t_{i})^{2}+6\lambda^{2}(t_{i+1}-t_{i})^{3}+\lambda^{4}(t_{i+1}-t_{i})^{4}\right]\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(\mathbb{E}[(B(t_{i+1})-B(t_{i}))^{4}]=3(t_{i+1}-t_{i})^{2}\)</span>. Consequently,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[X_{i}^{2}] &amp; =\mathbb{E}[(B(t_{i+1})-B(t_{i}))^{4}]-2(t_{i+1}-t_{i})\mathbb{E}[(B(t_{i+1})-B(t_{i}))^{2}]+(t_{i+1}-t_{i})^{2}\\
&amp; =3(t_{i+1}-t_{i})^{2}-2(t_{i+1}-t_{i})^{2}+(t_{i+1}-t_{i})^{2}\\
&amp; =2(t_{i+1}-t_{i})^{2}
\end{aligned}\]</span></p>
<p>Putting all this together, we finally have that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =2\sum_{i=0}^{n-1}(t_{i+1}-t_{i})^{2}\label{eq:second-moment-of-qv}\\
&amp; \leq2\left\Vert \Delta_{n}\right\Vert \sum_{i=0}^{n-1}(t_{i+1}-t_{i})\nonumber \\
&amp; =2\left\Vert \Delta_{n}\right\Vert \cdot t\nonumber
\end{aligned}\]</span></p>
<p>As <span class="math inline">\(n\to\infty\)</span>, <span class="math inline">\(\left\Vert \Delta_{n}\right\Vert \to0\)</span>. Hence,</p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =0
\end{aligned}\]</span></p>
<p>Hence, the sequence of random variables</p>
<p><span class="math display">\[\begin{aligned}
\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2} &amp; \stackrel{L^{2}}{\to}t
\end{aligned}\]</span> ◻</p>
</div>
<div class="cor">
<p>(Quadratic Variation of a Brownian Motion Path). Let <span class="math inline">\((B_{s},s\geq0)\)</span> be a Brownian motion. For every <span class="math inline">\(n\in\mathbf{N}\)</span>, consider the dyadic partition <span class="math inline">\((t_{j},j\leq2^{n})\)</span> of <span class="math inline">\([0,t]\)</span> where <span class="math inline">\(t_{j}=\frac{j}{2^{n}}t\)</span>. Then we have that:</p>
<p><span class="math display">\[\begin{aligned}
\left\langle B\right\rangle _{t} &amp; =\sum_{j=1}^{2^{n}-1}(B_{t_{j+1}}-B_{t_{j}})^{2}\stackrel{a.s.}{\to}t
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> We have <span class="math inline">\((t_{i+1}-t_{i})=\frac{t}{2^{n}}.\)</span> Borrowing equation (<a href="#eq:second-moment-of-qv" data-reference-type="ref" data-reference="eq:second-moment-of-qv">[eq:second-moment-of-qv]</a>) from the proof of theorem (<a href="#th:quadratic-variation-of-bm-approaches-t-in-mean-square" data-reference-type="ref" data-reference="th:quadratic-variation-of-bm-approaches-t-in-mean-square">[th:quadratic-variation-of-bm-approaches-t-in-mean-square]</a>), we have that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =2\sum_{i=0}^{2^{n}-1}\left(\frac{t}{2^{n}}\right)^{2}\\
&amp; =2\cdot(2^{n})\cdot\frac{t^{2}}{2^{2n}}\\
&amp; =\frac{2t^{2}}{2^{n}}
\end{aligned}\]</span></p>
<p>By Chebyshev’s inequality,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\left|\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right|&gt;\epsilon\right) &amp; \leq\frac{1}{\epsilon^{2}}\mathbb{E}\left[\left(\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right]\\
&amp; \leq\frac{1}{\epsilon^{2}}\cdot\frac{2t^{2}}{2^{n}}
\end{aligned}\]</span></p>
<p>Define <span class="math inline">\(A_{n}:=\left\{ \left|\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right|&gt;\epsilon\right\}\)</span>. Since, <span class="math inline">\(\sum\frac{1}{2^{n}}\)</span> is a convergent series, any multiple of it, <span class="math inline">\((2t^{2}/\epsilon^{2})\sum\frac{1}{2^{n}}\)</span> also converges. Now, <span class="math inline">\(0\leq\mathbb{P}(A_{n})\leq\frac{(2t^{2}/\epsilon^{2})}{2^{n}}\)</span>. By the comparison test, <span class="math inline">\(\sum\mathbb{P}(A_{n})\)</span> converges to a finite value. By Theorem (<a href="#th:sufficient-condition-for-almost-sure-convergence" data-reference-type="ref" data-reference="th:sufficient-condition-for-almost-sure-convergence">[th:sufficient-condition-for-almost-sure-convergence]</a>),</p>
<p><span class="math display">\[\begin{aligned}
\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2} &amp; \stackrel{a.s.}{\to}t
\end{aligned}\]</span> ◻</p>
</div>
<p>We are now ready to show that every Brownian motion path has infinite variation.</p>
<p>If <span class="math inline">\(g\)</span> is a <span class="math inline">\(C^{1}\)</span> function,</p>
<p><span class="math display">\[\begin{aligned}
\int_{0}^{t}|g'(t)|dt &amp; =\int_{0}^{t}\sqrt{g'(t)^{2}}dt\\
&amp; \leq\int_{0}^{t}\sqrt{1+g'(t)^{2}}dt\\
&amp; =l_{g}(t)
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(l_{g}(t)\)</span> is the arclength of the function <span class="math inline">\(g\)</span> between <span class="math inline">\([0,t]\)</span>. So, <span class="math inline">\(V_{g}(t)\leq l_{g}(t)\)</span> and further:</p>
<p><span class="math display">\[\begin{aligned}
l_{g}(t) &amp; =\int_{0}^{t}\sqrt{1+g'(t)^{2}}dt\\
&amp; \leq\int_{0}^{t}\left(1+\sqrt{g'(t)^{2}}\right)dt\\
&amp; \leq t+V_{g}(t)
\end{aligned}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; \leq l_{g}(t)\leq t+V_{g}(t)
\end{aligned}\]</span></p>
<p>The total variation of the function is finite if and only if it’s arclength is.</p>
<p>Hence, intuitively, our claim is that a Brownian motion path on <span class="math inline">\([0,T]\)</span> has infinite arc-length. Since <span class="math inline">\(g\in C^{1}([a,b])\Longrightarrow(V_{g}(t)&lt;\infty)\)</span>, it follows that <span class="math inline">\((V_{g}(t)\to\infty)\Longrightarrow g\notin C^{1}\)</span>.</p>
<div class="cor">
<p>(Brownian Motion paths have unbounded total variation.) <span id="th:bm-paths-have-unbounded-total-variation" label="th:bm-paths-have-unbounded-total-variation"></span> Let <span class="math inline">\((B_{s},s\geq0)\)</span> be a Brownian motion. Then, the random functions <span class="math inline">\(B(s,\omega)\)</span> on the interval <span class="math inline">\([0,t]\)</span> have unbounded variation almost surely.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Take the sequence of dyadic partitions of <span class="math inline">\([0,t]\)</span>: <span class="math inline">\(t_{j}=\frac{j}{2^{n}}t\)</span>, <span class="math inline">\(n\in\mathbf{N}\)</span>, <span class="math inline">\(j\leq2^{n}\)</span>. By pulling out the worst increment, we have the trivial bound for every <span class="math inline">\(\omega\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\sum_{j=0}^{2^{n}-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))^{2} &amp; \leq\max_{0\leq j\leq2^{n}}\left|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)\right|\cdot\sum_{j=0}^{2^{n}-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))\label{eq:trivial-upper-bound-on-quadratic-variation}
\end{aligned}\]</span></p>
<p>We proceed by contradiction. Let <span class="math inline">\(A'\)</span> be the set of all <span class="math inline">\(\omega\)</span>, for which the Brownian motion paths have bounded total variation. Let <span class="math inline">\(A\)</span> be event that the Brownian motion paths have unbounded variation.</p>
<p>By the definition of total variation, that would imply, <span class="math inline">\(\exists M\in\mathbf{N}\)</span> :</p>
<p><span class="math display">\[\begin{aligned}
(\forall\omega\in A')\quad\lim_{n\to\infty}\sum_{j=0}^{2^{n}-1}\left|(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))\right| &amp; &lt;M
\end{aligned}\]</span></p>
<p>Since Brownian Motion paths are continuous on the compact set <span class="math inline">\([\frac{j}{2^{n}}t,\frac{j+1}{2^{n}}t]\)</span>, they are uniformly continuous. So, as <span class="math inline">\(n\to\infty\)</span>, <span class="math inline">\(|t_{j+1}-t_{j}|\to0\)</span> and therefore <span class="math inline">\(|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)|\to0\)</span>. And consequently, <span class="math inline">\(\max_{0\leq j\leq2^{n}}\left|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)\right|\to0\)</span>.</p>
<p>Thus, for every <span class="math inline">\(\omega\in A'\)</span>, the right hand side of the inequality (<a href="#eq:trivial-upper-bound-on-quadratic-variation" data-reference-type="ref" data-reference="eq:trivial-upper-bound-on-quadratic-variation">[eq:trivial-upper-bound-on-quadratic-variation]</a>), converges to <span class="math inline">\(0\)</span> and therefore the left hand side converges to <span class="math inline">\(0\)</span>. But, this contradicts the fact that <span class="math inline">\(\left\langle B\right\rangle _{t}\stackrel{a.s.}{\to}t\)</span>. So, <span class="math inline">\(A'\)</span> is a null set, and <span class="math inline">\(\mathbb{P}(A')=0\)</span> and <span class="math inline">\(\mathbb{P}(A)=1\)</span>. This closes the proof. ◻</p>
</div>
</section>
</section>
<section id="what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance" class="level2">
<h2 class="anchored" data-anchor-id="what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance">What exactly is <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> in mathematical finance?</h2>
<p>If we make the simplifying assumption that the process paths are continuous, we obtain the set of all continuous functions on <span class="math inline">\([0,T]\)</span>, denoted by <span class="math inline">\(C[0,T]\)</span>. This is a very rich space. In a more general model, it is assumed that the process paths are right continuous with left limits (regular right-continuous RRC, cadlag) functions.</p>
<p>Let the sample space <span class="math inline">\(\Omega=D[0,T]\)</span> be the set of all RRC functions on <span class="math inline">\([0,T]\)</span>. An element of this set is a RRC function from <span class="math inline">\([0,T]\)</span> into <span class="math inline">\(\mathbf{R}\)</span>. First we must decide what kind of sets of these functions are measurable? The simplest set for which we would like to calculate the probabilities are sets of the form <span class="math inline">\(\{a\leq S(t_{1})\leq b\}\)</span> for some <span class="math inline">\(t_{1}\)</span>. If <span class="math inline">\(S(t)\)</span> represents the price of a stock at time <span class="math inline">\(t\)</span>, then the probability of such a set gives the probability that the stock price at time <span class="math inline">\(t_{1}\)</span> is between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. We are also interested in how the price of the stock at time <span class="math inline">\(t_{1}\)</span> affects the price at another time <span class="math inline">\(t_{2}\)</span>. Thus, we need to talk about the joint distribution of stock prices <span class="math inline">\(S(t_{1})\)</span> and <span class="math inline">\(S(t_{2})\)</span>. This means that we need to define probability on the sets of the form <span class="math inline">\(\{S(t_{1})\in B_{1},S(t_{2})\in B_{2}\}\)</span> where <span class="math inline">\(B_{1}\)</span> and <span class="math inline">\(B_{2}\)</span> are intervals on the line. More generally, we would like to have all the finite-dimensional distributions of the process <span class="math inline">\(S(t)\)</span>, that is, the probabilities of the sets: <span class="math inline">\(\{S(t_{1})\in B_{1},S(t_{2})\in B_{2},\ldots,S(t_{n})\in B_{n}\}\)</span> for any choice of <span class="math inline">\(0\leq t_{1}\leq\ldots\leq t_{n}\leq T\)</span>.</p>
<p>The sets of the form <span class="math inline">\(A=\{\omega(\cdot)\in D[0,T]:\omega(t_{1})\in B_{1},\ldots,\omega(t_{n})\in B_{n}\}\)</span>, where <span class="math inline">\(B_{i}\)</span>’s are borel subsets of <span class="math inline">\(\mathbf{R}\)</span>, are called cylinder sets or finite-dimensional rectangles.</p>
<p>The stochastic process <span class="math inline">\(S(t)\)</span> is just a (function-valued) random variable on this sample space, which takes some value <span class="math inline">\(\omega(t)\)</span> - the value of the function <span class="math inline">\(\omega\)</span> at <span class="math inline">\(t\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{R}\)</span> be the colllection of all cylindrical subsets of <span class="math inline">\(D[0,1]\)</span>. Obviously <span class="math inline">\(\mathcal{R}\)</span> is not a <span class="math inline">\(\sigma\)</span>-field.</p>
<p>Probability is first defined by on the elements of <span class="math inline">\(\mathcal{R}\)</span>. Let <span class="math inline">\(A\subseteq\mathcal{R}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(A) &amp; =\int_{B_{1}}\cdots\int_{B_{n}}\prod_{i=1}^{n}\frac{1}{\sqrt{(2\pi)(t_{i}-t_{i-1})}}\exp\left[-\frac{(u_{i}-u_{i-1})^{2}}{2(t_{i}-t_{i-1})}\right]du_{1}\cdots du_{n}
\end{aligned}\]</span></p>
<p>and then extended to the <span class="math inline">\(\sigma\)</span>-field generated by taking unions, complements and intersections of cylinders. We take the smallest <span class="math inline">\(\sigma\)</span>-algebra containing all the cylindrical subsets of <span class="math inline">\(D[0,1]\)</span>. Thus, <span class="math inline">\(\mathcal{F}=\mathcal{B}(D[0,1])\)</span>.</p>
<p>Hence, <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})=(D[0,1],\mathcal{B}(D[0,1]),\mathbb{P})\)</span> is a probability space. It is called the <em>Wiener space</em> and <span class="math inline">\(\mathbb{P}\)</span> here is called the <em>Wiener measure</em>.</p>
</section>
<section id="continuity-and-regularity-of-paths." class="level2">
<h2 class="anchored" data-anchor-id="continuity-and-regularity-of-paths.">Continuity and Regularity of paths.</h2>
<p>As discussed in the previous section, a stochastic process is determined by its finite-dimensional distribution. In studying stochastic processes, it is often natural to think of them as function-valued random variables in <span class="math inline">\(t\)</span>. Let <span class="math inline">\(S(t)\)</span> be defined for <span class="math inline">\(0\leq t\leq T\)</span>, then for a fixed <span class="math inline">\(\omega\)</span>, it is a function in <span class="math inline">\(t\)</span>, called the sample path or a realization of <span class="math inline">\(S\)</span>. Finite-dimensional distributions do not determine the continuity property of sample paths. The following example illustrates this.</p>
<div class="example">
<p><span id="ex:modifications-of-a-stochastic-process" label="ex:modifications-of-a-stochastic-process"></span>Let <span class="math inline">\(X(t)=0\)</span> for all <span class="math inline">\(t\)</span>, <span class="math inline">\(0\leq t\leq1\)</span> and <span class="math inline">\(\tau\)</span> be a uniformly distributed random variable on <span class="math inline">\([0,1]\)</span>. Let <span class="math inline">\(Y(t)=0\)</span> for <span class="math inline">\(t\neq\tau\)</span> and <span class="math inline">\(Y(t)=1\)</span> if <span class="math inline">\(t=\tau.\)</span> Then, for any fixed <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbb{P}(Y(t)\neq0)=\mathbb{P}(\tau=t)=0\)</span>, and hence <span class="math inline">\(\mathbb{P}(Y(t)=0)=1\)</span>. So, that all one-dimensional distributions of <span class="math inline">\(X(t)\)</span> and <span class="math inline">\(Y(t)\)</span> are the same. Similarly, all finite-dimensional distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the same. However, the sample paths of the process <span class="math inline">\(X\)</span>, that is, the functions <span class="math inline">\(X(t)_{0\leq t\leq1}\)</span> are continuous in <span class="math inline">\(t\)</span>, whereas every sample path <span class="math inline">\(Y(t)_{0\leq t\leq1}\)</span> has a jump at the (random) point <span class="math inline">\(\tau\)</span>. Notice that, <span class="math inline">\(\mathbb{P}(X(t)=Y(t))=1\)</span> for all <span class="math inline">\(t\)</span>, <span class="math inline">\(0\leq t\leq1\)</span>.</p>
</div>
<div class="defn">
<p>Two stochastic processes are called <em>versions</em> (modifications) of one another if</p>
<p><span class="math display">\[\mathbb{P}(X(t)=Y(t))=1\quad\text{for all }0\leq t\leq T\]</span></p>
</div>
<p>Thus, the two processes in the example (<a href="#ex:modifications-of-a-stochastic-process" data-reference-type="ref" data-reference="ex:modifications-of-a-stochastic-process">[ex:modifications-of-a-stochastic-process]</a>) are versions of one another, one has continuous sample paths, the other does not. If we agree to pick any version of the process we want, then we can pick the continous version when it exists. In general, we choose the smoothest possible version of the process.</p>
<p>For two processes, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, denote by <span class="math inline">\(N_{t}=\{X(t)\neq Y(t)\}\)</span>, <span class="math inline">\(0\leq t\leq T\)</span>. In the above example, <span class="math inline">\(\mathbb{P}(N_{t})=\mathbb{P}(\tau=t)=0\)</span> for any <span class="math inline">\(t\)</span>, <span class="math inline">\(0\leq t\leq1\)</span>. However, <span class="math inline">\(\mathbb{P}(\bigcup_{0\leq t\leq1}N_{t})=\mathbb{P}(\tau=t\:\text{for some }t\:\text{in }[0,1])=1\)</span>. Although, each of <span class="math inline">\(N_{t}\)</span> is a <span class="math inline">\(\mathbb{P}\)</span>-null set, the union <span class="math inline">\(N=\bigcup_{0\leq t\leq1}N_{t}\)</span> contains uncountably many null sets, and in this particular case it is a set of of probability one.</p>
<p>If it happens that <span class="math inline">\(\mathbb{P}(N)=0\)</span>, then <span class="math inline">\(N\)</span> is called an <em>evanescent set</em>, and the processes <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are called <em>indistinguishable</em>. Note that in this case, <span class="math inline">\(\mathbb{P}(\{\omega:\exists t:X(t)\neq Y(t)\})=\mathbb{P}(\bigcup_{0\leq t\leq1}\{X(t)\neq Y(t))=0\)</span> and <span class="math inline">\(\mathbb{P}(\bigcap_{0\leq t\leq1}\{X(t)=Y(t)\})=1\)</span>. It is clear, that if the time is discrete, then any two versions of the process are indistinguishable. It is also not hard to see, that if <span class="math inline">\(X(t)\)</span> and <span class="math inline">\(Y(t)\)</span> are versions of one another and they are both right-continuous, they are indistinguishable.</p>
<div class="thm">
<p>(Paul Levy’s construction of Brownian Motion). Standard Brownian motion exists.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> I reproduce the standard proof as present in <em>Brownian Motion</em> by Morters and Peres. I added some remarks for greater clarity.</p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{D}_{n} &amp; =\left\{ \frac{k}{2^{n}}:k=0,1,2,\ldots,2^{n}\right\}
\end{aligned}\]</span></p>
<p>be a finite set of dyadic points.</p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{D} &amp; =\bigcup_{n=0}^{\infty}\mathcal{D}_{n}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(\{Z_{t}:t\in\mathcal{D}\}\)</span> be a collection of independent, standard normally distributed random variables. This is a countable set of random variables.</p>
<p>Let <span class="math inline">\(B(0):=0\)</span> and <span class="math inline">\(B(1):=Z_{1}\)</span>.</p>
<p>For each <span class="math inline">\(n\in\mathbf{N}\)</span>, we define the random variables <span class="math inline">\(B(d)\)</span>, <span class="math inline">\(d\in\mathcal{D}_{n}\)</span> such that, the following invariant holds:</p>
<p>(1) for all <span class="math inline">\(r&lt;s&lt;t\)</span> in <span class="math inline">\(\mathcal{D}_{n}\)</span> the random variable <span class="math inline">\(B(t)-B(s)\)</span> is normally distributed with mean zero and variance <span class="math inline">\(t-s\)</span> and is independent of <span class="math inline">\(B(s)-B(r)\)</span>.</p>
<p>(2) the vectors <span class="math inline">\((B(d):d\in\mathcal{D}_{n})\)</span> and <span class="math inline">\((Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n})\)</span> are independent.</p>
<p>Note that we have already done this for <span class="math inline">\(\mathcal{D}_{0}=\{0,1\}\)</span>. Proceeding inductively, let’s assume that the above holds for some <span class="math inline">\(n-1\)</span>. We are interested to prove that the invariant also holds for <span class="math inline">\(n\)</span>.</p>
<p>We define <span class="math inline">\(B(d)\)</span> for <span class="math inline">\(d\in\mathcal{D}_{n}\backslash\mathcal{D}_{n-1}\)</span> by:</p>
<p><span class="math display">\[\begin{aligned}
B(d) &amp; =\frac{B(d-2^{-n})+B(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>Note that, the points <span class="math inline">\(0,\frac{1}{2^{n-1}},\ldots,\frac{k}{2^{n-1}},\frac{k+1}{2^{n-1}},\ldots,1\)</span> belong to <span class="math inline">\(\mathcal{D}_{n-1}\)</span>. The first summand is the linear interpolation of the values of <span class="math inline">\(B\)</span> at the neighbouring points of <span class="math inline">\(d\)</span> in <span class="math inline">\(\mathcal{D}_{n-1}\)</span>. That is,</p>
<p><span class="math display">\[\begin{aligned}
B\left(\frac{2k+1}{2^{n}}\right) &amp; =\frac{B\left(\frac{k}{2^{n-1}}\right)+B\left(\frac{k+1}{2^{n-1}}\right)}{2}+\frac{Z_{d}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(P(n-1)\)</span> holds, <span class="math inline">\(B(d-2^{-n})\)</span> and <span class="math inline">\(B(d+2^{-n})\)</span> are have no dependence on <span class="math inline">\((Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n-1})\)</span>. Consequently, <span class="math inline">\(B(d)\)</span> has no dependence on <span class="math inline">\((Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n})\)</span> and the second property is fulfilled.</p>
<p>Moreover, as <span class="math inline">\(\frac{1}{2}[B(d+2^{-n})-B(d-2^{-n})]\)</span> depends only on <span class="math inline">\((Z_{t}:t\in\mathcal{D}_{n-1})\)</span>, it is independent of <span class="math inline">\(\frac{Z_{d}}{2^{(n+1)/2}}\)</span>. By our induction assumptions, they are both nromally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\frac{1}{2^{(n+1)}}\)</span>.</p>
<p>So, their sum and difference random variables</p>
<p><span class="math display">\[\begin{aligned}
B(d)-B(d-2^{-n}) &amp; =\frac{B(d+2^{-n})-B(d-2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\\
B(d+2^{-n})-B(d) &amp; =\frac{B(d+2^{-n})-B(d-2^{-n})}{2}-\frac{Z_{d}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>are also independent, with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\frac{1}{2^{n}}\)</span> (the variance of independent random variables is the sum of the variances).</p>
<p>Indeed all increments <span class="math inline">\(B(d)-B(d-2^{-n})\)</span> for <span class="math inline">\(d\in\mathcal{D}_{n}\setminus\{0\}\)</span> are independent. To see this, it suffices to show that they are pairwise independent. We have seen in the previous paragraph that the pairs <span class="math inline">\(B(d)-B(d-2^{-n})\)</span> and <span class="math inline">\(B(d+2^{-n})-B(d)\)</span> with <span class="math inline">\(d\in\mathcal{D}_{n}\setminus\mathcal{D}_{n-1}\)</span> are independent. The other possibility is that the increments are over the intervals separated by some <span class="math inline">\(d\in\mathcal{D}_{n-1}\)</span>. For concreteness, if <span class="math inline">\(n\)</span> were <span class="math inline">\(3\)</span>, then the increments, <span class="math inline">\(B_{7/8}-B_{6/8}\)</span> and <span class="math inline">\(B_{5/8}-B_{4/8}\)</span> are seperated by <span class="math inline">\(d=\frac{3}{4}\in\mathcal{D}_{2}\)</span>. Choose <span class="math inline">\(d\in\mathcal{D}_{j}\)</span> with this property and minimal <span class="math inline">\(j\)</span>, so, the two intervals are contained in <span class="math inline">\([d-2^{-j},d]\)</span> and <span class="math inline">\([d,d+2^{-j}]\)</span> respectively. By induction, the increments over these two intervals of length <span class="math inline">\(2^{-j}\)</span> are independent and the increments over the intervals of length <span class="math inline">\(2^{-n}\)</span> are constructed from the independent increments <span class="math inline">\(B(d)-B(d-2^{-j})\)</span> and <span class="math inline">\(B(d+2^{-j})-B(d)\)</span> using a disjoint set of variables <span class="math inline">\((Z_{t}:t\in\mathcal{D}_{n})\)</span>. Hence, they are independent and this implies pairwise independence. This implies the first property. Consequently, the vector of increments <span class="math inline">\((B(d)-B(d-2^{-n})\)</span> for all <span class="math inline">\(d\in\mathcal{D}_{n}\)</span> is Gaussian.</p>
<p>Having thus chosen the value of the process on all the dyadic points, we interpolate between them. Formally, we define:</p>
<p><span class="math display">\[\begin{aligned}
F_{0}(t) &amp; =\begin{cases}
Z_{1} &amp; \text{for }t=1\\
0 &amp; \text{for }t=0\\
\text{\text{linear in between}}
\end{cases}
\end{aligned}\]</span></p>
<p>and for each <span class="math inline">\(n\geq1\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
F_{n}(t) &amp; =\begin{cases}
\frac{Z_{t}}{2^{(n+1)/2}} &amp; \text{for }t\in\mathcal{D}\setminus\mathcal{D}_{n-1}\\
0 &amp; \text{for }t\in\mathcal{D}_{n-1}\\
\text{\text{linear between consecutive points in }\ensuremath{\mathcal{D}_{n}}}
\end{cases}
\end{aligned}\]</span></p>
<p>These functions are continuous on <span class="math inline">\([0,1]\)</span> and for all <span class="math inline">\(n\)</span> and <span class="math inline">\(d\in\mathcal{D}_{n}\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
B(d) &amp; =\sum_{i=0}^{n}F_{i}(d)=\sum_{i=0}^{\infty}F_{i}(d)\label{eq:claim-of-induction-for-bd}
\end{aligned}\]</span></p>
<p>To see this, assume that above equation holds for all <span class="math inline">\(d\in\mathcal{D}_{n-1}\)</span>.</p>
<p>Let’s consider the point <span class="math inline">\(d\in\mathcal{D}_{n}\setminus\mathcal{D}_{n-1}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
B(d) &amp; =\frac{B(d-2^{-n})+B(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\nonumber \\
&amp; =\sum_{i=0}^{n-1}\frac{F_{i}(d-2^{-n})+F_{i}(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\label{eq:expression-for-bd}
\end{aligned}\]</span></p>
<p>Now, <span class="math inline">\(d-2^{-n}\)</span> and <span class="math inline">\(d+2^{-n}\)</span> belong to <span class="math inline">\(\mathcal{D}_{n-1}\)</span> and are not in <span class="math inline">\(\bigcup_{i&lt;n-1}\mathcal{D}_{i}\)</span>. Therefore, for <span class="math inline">\(i=0,1,\ldots,n-2\)</span>, the points <span class="math inline">\((d-2^{-n},F_{i}(d-2^{-n}))\)</span> and <span class="math inline">\((d+2^{-n},F_{i}(d+2^{-n})\)</span> lie on some straight line and have <span class="math inline">\((d,F_{i}(d))\)</span> as their midpoint. Moreover, <span class="math inline">\(d-2^{-n}\)</span> and <span class="math inline">\(d+2^{-n}\)</span> are vertices in <span class="math inline">\(\mathcal{D}_{n-1}\)</span>. So, by definition of <span class="math inline">\(F_{n-1}(d)\)</span>, we have <span class="math inline">\(F_{n-1}(d)=[F_{n-1}(d-2^{-n})+F_{n-1}(d+2^{-n})]/2\)</span>.</p>
<p>To summarize, the first term on the right hand side of expression (<a href="#eq:expression-for-bd" data-reference-type="ref" data-reference="eq:expression-for-bd">[eq:expression-for-bd]</a>) is equal to <span class="math inline">\(\sum_{i=0}^{n-1}F_{i}(d)\)</span>. By mathematical induction, it follows that the claim (<a href="#eq:claim-of-induction-for-bd" data-reference-type="ref" data-reference="eq:claim-of-induction-for-bd">[eq:claim-of-induction-for-bd]</a>) is true for all <span class="math inline">\(n\in\mathbf{N}\)</span>.</p>
<p>It’s extremely easy to find an upper bound on the probability contained in the Gaussian tails. Suppose <span class="math inline">\(X\sim N(0,1)\)</span> and let <span class="math inline">\(x&gt;0\)</span>. We are interested in the tail probability <span class="math inline">\(\mathbb{P}(X&gt;x)\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(X&gt;x) &amp; =\int_{x}^{\infty}e^{-x^{2}/2}dx=\int_{x}^{\infty}\frac{xe^{-x^{2}/2}dx}{x}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(u=\frac{1}{x}\)</span> and <span class="math inline">\(dv=xe^{-x^{2}/2}dx\)</span>. We have:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(u=\frac{1}{x}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(dv=xe^{-x^{2}/2}dx\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(du=-\frac{1}{x^{2}}dx\)</span></td>
<td style="text-align: center;"><span class="math inline">\(v=-e^{-x^{2}/2}\)</span></td>
</tr>
</tbody>
</table>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(X&gt;x) &amp; =-\left.\frac{1}{x}e^{-x^{2}/2}\right|_{x}^{\infty}-\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}dx\\
&amp; =\frac{e^{-x^{2}/2}}{x}-\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}dx\\
&amp; \quad\left\{ I(x)=\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}\geq0\right\} \\
&amp; \leq\frac{e^{-x^{2}/2}}{x}
\end{aligned}\]</span></p>
<p>Thus, for <span class="math inline">\(c&gt;1\)</span> and large <span class="math inline">\(n\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(|Z_{d}|\geq c\sqrt{n}) &amp; \leq\frac{1}{c\sqrt{n}}e^{-c^{2}n/2}\leq\exp\left(-\frac{c^{2}n}{2}\right)
\end{aligned}\]</span></p>
<p>So, the series:</p>
<p><span class="math display">\[\begin{aligned}
\sum_{n=0}^{\infty}\mathbb{P}\left\{ \text{There exists atleast one }d\in\mathcal{D}_{n}\text{ with }|Z_{d}|\geq c\sqrt{n}\right\}  &amp; \leq\sum_{n=0}^{\infty}\sum_{d\in\mathcal{D}_{n}}\mathbb{P}\left\{ |Z_{d}|\geq c\sqrt{n}\right\} \\
&amp; \leq\sum_{n=0}^{\infty}(2^{n}+1)\exp\left(-\frac{c^{2}n}{2}\right)
\end{aligned}\]</span></p>
<p>Now, the series <span class="math inline">\((a_{n})\)</span> given by, <span class="math inline">\(a_{n}:=(2^{n}+1)e^{-c^{2}n/2}\)</span> has the ratio between successive terms:</p>
<p><span class="math display">\[\begin{aligned}
\lim\left|\frac{a_{n+1}}{a_{n}}\right| &amp; =\lim_{n\to\infty}\frac{2^{n+1}+1}{2^{n}+1}\cdot\frac{e^{(c^{2}n)/2}}{e^{c^{2}(n+1)/2}}\\
&amp; =\lim_{n\to\infty}\frac{\frac{1}{2}+\frac{1}{2^{n}}}{1+\frac{1}{2^{n}}}\cdot\frac{1}{e^{c^{2}/2}}\\
&amp; =\frac{1}{2e^{c^{2}/2}}
\end{aligned}\]</span></p>
<p>If this ratio is less than unity, that is <span class="math inline">\(c&gt;\sqrt{2\log2}\)</span>, than by the ratio test, <span class="math inline">\(\sum(2^{n}+1)e^{-c^{2}n/2}\)</span> converges to a finite value. Fix such a <span class="math inline">\(c\)</span>.</p>
<p>By BCL1(Borel-Cantelli Lemma), if <span class="math inline">\(A_{n}:=\left\{ \text{There exists atleast one }d\in\mathcal{D}_{n}\text{ with }|Z_{d}|\geq c\sqrt{n}\right\}\)</span> and <span class="math inline">\(\sum_{n=0}^{\infty}\mathbb{P}(A_{n})\)</span> converges to a finite value, then the event <span class="math inline">\(A_{n}\)</span> occurs finitely many times with probability <span class="math inline">\(1\)</span>. There exists <span class="math inline">\(N\in\mathbf{N}\)</span>, such that for all <span class="math inline">\(n\geq N\)</span>, <span class="math inline">\(A_{n}\)</span> fails to occur with probability <span class="math inline">\(1\)</span>. Thus, for all <span class="math inline">\(n\geq N\)</span>, <span class="math inline">\(\{Z_{d}\leq c\sqrt{n}\}\)</span> occurs with probability <span class="math inline">\(1\)</span>. It follows that:</p>
<p><span class="math display">\[\begin{aligned}
\sup_{t\in[0,1]}F_{n}(t) &amp; \leq\frac{c\sqrt{n}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>Define</p>
<p><span class="math display">\[\begin{aligned}
M_{n} &amp; =\frac{c\sqrt{n}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(\sum M_{n}\)</span> converges, by the Weierstrass <span class="math inline">\(M\)</span>-test, the infinite series of functions <span class="math inline">\(\sum_{n=0}^{\infty}F_{n}(t)\)</span> converges uniformly on <span class="math inline">\([0,1].\)</span> Since, each <span class="math inline">\(F_{n}(t)\)</span> is piecewise linear and continuous, by the Term-by-Term continuity theorem, <span class="math inline">\(\sum_{n=0}^{\infty}F_{n}(t)\)</span> is continuous on <span class="math inline">\([0,1]\)</span>. ◻</p>
</div>
</section>
<section id="a-point-of-comparison-the-poisson-process." class="level2">
<h2 class="anchored" data-anchor-id="a-point-of-comparison-the-poisson-process.">A point of comparison: The Poisson Process.</h2>
<p>Like the Brownian motion, the Poisson process is defined as a process with stationary and independent increments.</p>
<div class="defn">
<p><span id="def:poisson-process" label="def:poisson-process"></span> A process <span class="math inline">\((N_{t},t\geq0)\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> has the distribution of the Poisson process with rate <span class="math inline">\(\lambda&gt;0\)</span>, if and only if the following hold:</p>
<p>(1) <span class="math inline">\(N_{0}=0\)</span>.</p>
<p>(2) For any <span class="math inline">\(s&lt;t\)</span>, the increment <span class="math inline">\(N_{t}-N_{s}\)</span> is a Poisson random variable with parameter <span class="math inline">\(\lambda(t-s).\)</span></p>
<p>(3) For any <span class="math inline">\(n\in\mathbf{N}\)</span> and any choice <span class="math inline">\(0&lt;t_{1}&lt;t_{2}&lt;\ldots&lt;t_{n}&lt;\infty\)</span>, the increments <span class="math inline">\(N_{t_{2}}-N_{t_{1}},N_{t_{3}}-N_{t_{2}},\ldots,N_{t_{n}}-N_{t_{n-1}}\)</span> are independent.</p>
</div>
<p>Poisson paths can be sampled using this definition. By construction, it is not hard to see that the paths of Poisson processes are piecewise, constant, integer-valued and non-decreasing. In particular, the paths of Poisson processes have finite variation. Poisson paths are much simpler than the ones of Brownian motion in many ways!</p>
<div class="example">
<p>(Simulating the Poisson Process.) Use the definition (<a href="#def:poisson-process" data-reference-type="ref" data-reference="def:poisson-process">[def:poisson-process]</a>) to generate <span class="math inline">\(10\)</span> paths of the Poisson process with rate <span class="math inline">\(1\)</span> on the interval <span class="math inline">\([0,10]\)</span> with step-size <span class="math inline">\(0.01\)</span>.</p>
</div>
<pre data-caption="Generating 10 paths of a Poisson process"><code>def generatePoissonProcess(lam,T,stepSize):
    N = int(T/stepSize)
    x = np.random.poisson(lam=lam,size=N)
    y = np.cumsum(x)
    y = np.concatenate([[0.0],y])
    return y</code></pre>
<p>We can construct a Poisson process as follows. Consider <span class="math inline">\((\tau_{j},j\in\mathbf{N})\)</span> IID exponential random variables with parameter <span class="math inline">\(1/\lambda\)</span>. One should think of <span class="math inline">\(\tau_{j}\)</span> as the waiting time from the <span class="math inline">\((j-1)\)</span>st to the <span class="math inline">\(j\)</span>th jump. Then, one defines :</p>
<p><span class="math display">\[\begin{aligned}
N_{t} &amp; =\#\{k:\tau_{1}+\tau_{2}+\ldots+\tau_{k}\leq t\}\\
&amp; =\text{Number of jumps upto and including time }t
\end{aligned}\]</span></p>
<p>Now, here is an idea! What about defining a new process with stationary and independent increments using a given distribution other than Poisson and Gaussian? Is this even possible? The answer is yes, but only if the distribution satisfies the property of being <em>infinitely divisible</em>. To see this, consider the value of the process at time <span class="math inline">\(1\)</span>, <span class="math inline">\(N_{1}\)</span>. Then, no matter how many subintervals we chop the interval <span class="math inline">\([0,1]\)</span> into, we must have the increments add up to <span class="math inline">\(N_{1}\)</span>. In other words, we must be able to write <span class="math inline">\(N_{1}\)</span> as a sum of <span class="math inline">\(n\)</span> IID random variables for every possible <span class="math inline">\(n\)</span>. This is certainly true for Poisson random variables and Gaussian random variables. Another example is the Cauchy distribution. In general, processes that can be constructed using independent, stationary increments are called Levy processes.</p>
<div class="example">
<p><strong>Time Inversion.</strong> Let <span class="math inline">\((B_{t},t\geq0)\)</span> be a standard brownian motion. We consider the process:</p>
<p><span class="math display">\[\begin{aligned}
X_{t} &amp; =tB_{1/t}\quad\text{for }t&gt;0
\end{aligned}\]</span></p>
<p><em>This property relates the behavior of <span class="math inline">\(t\)</span> large to the behavior of <span class="math inline">\(t\)</span> small.</em></p>
</div>
<p>(a) Show that <span class="math inline">\((X_{t},t&gt;0)\)</span> has the distribution of Brownian motion on <span class="math inline">\(t&gt;0\)</span>.</p>
<p><em>Proof.</em></p>
<p>Like <span class="math inline">\(B(t)\)</span>, it is an easy exercise to prove that <span class="math inline">\(X(t)\)</span> is also a Gaussian process.</p>
<p>We have, <span class="math inline">\(\mathbb{E}[X_{s}]=0\)</span>.</p>
<p>Let <span class="math inline">\(s&lt;t\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
Cov(X_{s},X_{t}) &amp; =\mathbb{E}[sB(1/s)\cdot tB(1/t)]\\
&amp; =st\mathbb{E}[B(1/s)\cdot B(1/t)]\\
&amp; =st\cdot\frac{1}{t}\\
&amp; \quad\left\{ \because\frac{1}{t}&lt;\frac{1}{s}\right\} \\
&amp; =s
\end{aligned}\]</span></p>
<p>Consequently, <span class="math inline">\(X(t)\)</span> has the distribution of a Brownian motion.</p>
<p>(b) Argue that <span class="math inline">\(X(t)\)</span> converges to <span class="math inline">\(0\)</span> as <span class="math inline">\(t\to0\)</span> in the sense of <span class="math inline">\(L^{2}\)</span>-convergence. It is possible to show convergence almost surely so that <span class="math inline">\((X_{t},t\geq0)\)</span> is really a Brownian motion for <span class="math inline">\(t\geq0\)</span>.</p>
<p><em>Solution</em>.</p>
<p>Let <span class="math inline">\((t_{n})\)</span> be any arbitrary sequence of positive real numbers approaching <span class="math inline">\(0\)</span> and consider the sequence of random variables <span class="math inline">\((X(t_{n}))_{n=1}^{\infty}\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[X(t_{n})^{2}\right] &amp; =\mathbb{E}\left[t_{n}^{2}B(1/t_{n})^{2}\right]\\
&amp; =t_{n}^{2}\mathbb{E}\left[B(1/t_{n})^{2}\right]\\
&amp; =t_{n}^{2}\cdot\frac{1}{t_{n}}\\
&amp; =t_{n}
\end{aligned}\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[\begin{aligned}
\lim\mathbb{E}\left[X(t_{n})^{2}\right] &amp; =\lim t_{n}=0
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\((t_{n})\)</span> was an arbitrary sequence, it follows that <span class="math inline">\(\lim_{t\to0}\mathbb{E}[(X(t))^{2}]=0\)</span>.</p>
<p>(c) Use this property of Brownian motion to show the law of large numbers for Brownian motion:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{t\to\infty}\frac{X(t)}{t} &amp; =0\quad\text{almost surely}
\end{aligned}\]</span></p>
<p><em>Solution.</em></p>
<p>What we need to do is to show that <span class="math inline">\(X(t)\to0\)</span> as <span class="math inline">\(t\to0\)</span> almost surely. That would show that <span class="math inline">\(\frac{B(1/t)}{1/t}\to0\)</span> as <span class="math inline">\(t\to0\)</span> almost surely, which is the same as showing <span class="math inline">\(\frac{B(t)}{t}\to0\)</span> as <span class="math inline">\(t\to\infty\)</span>, which is the law of large numbers for Brownian motion.</p>
<p>What we have done in part (b), is to prove the claim that <span class="math inline">\(\mathbb{E}[X(t)^{2}]\to0\)</span> as <span class="math inline">\(t\to0\)</span>, which shows convergence in the <span class="math inline">\(L^{2}\)</span> sense and hence convergence in probability. This is infact the weak law of large numbers. <span class="math inline">\(\frac{B(t)}{t}\stackrel{\mathbb{\mathbf{P}}}{\to}0\)</span> as <span class="math inline">\(t\to\infty\)</span>.</p>
<p>For <span class="math inline">\(t&gt;0\)</span>, continuity is clear. However, it is the proof that as <span class="math inline">\(t\to0\)</span>, <span class="math inline">\(X(t)\to0\)</span> almost surely which we have not done.</p>
<p>Note that, the limit <span class="math inline">\(X(t)\to0\)</span> as <span class="math inline">\(t\to0\)</span> if and only if <span class="math inline">\((\forall n\geq1)\)</span>, <span class="math inline">\((\exists m\geq1)\)</span>, such that <span class="math inline">\(\forall r\in\mathbb{Q}\cap(0,\frac{1}{m}]\)</span>, we have <span class="math inline">\(|X(r)|=\left|rB\left(\frac{1}{r}\right)\right|\leq\frac{1}{n}\)</span>.</p>
<p>To understand the above, we just recall the <span class="math inline">\(\epsilon-\delta\)</span> definition of continuity. Note that <span class="math inline">\(\frac{1}{n}\)</span> plays the role of <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\frac{1}{m}\)</span> works as <span class="math inline">\(\delta\)</span>.</p>
<p>That is,</p>
<p><span class="math display">\[\begin{aligned}
\Omega^{X}:=\left\{ \lim_{t\to0}X(t)=0\right\}  &amp; =\bigcap_{n\geq1}\bigcup_{m\geq1}\bigcap_{r\in\mathbb{Q}\cap(0,\frac{1}{m}]}\left\{ \left|X(r)\right|\leq\frac{1}{n}\right\}
\end{aligned}\]</span></p>
<p>Also, note that <span class="math inline">\(X(t)\)</span> is continuous on all <span class="math inline">\([a,1]\)</span> for all <span class="math inline">\(a&gt;0\)</span>, thus, uniformly continuous on <span class="math inline">\([a,1]\)</span>, and hence uniformly continuous on <span class="math inline">\(\mathbb{Q}\cap(0,1]\)</span>. So, there exists a continuous extension of <span class="math inline">\(X(t)\)</span> on <span class="math inline">\([0,1]\)</span>. We already know from part (a), that <span class="math inline">\((X(t))_{t&gt;0}\)</span> and <span class="math inline">\((B(t))_{t&gt;0}\)</span> have the same finite dimensional distributions. Therefore, the RHS event has the same probability as <span class="math inline">\(\Omega^{B}:=\bigcap_{n\geq1}\bigcup_{m\geq1}\bigcap_{r\in\mathbb{Q}\cap(0,\frac{1}{m}]}\left\{ \left|B(r)\right|\leq\frac{1}{n}\right\}\)</span>. Since <span class="math inline">\(B(t)\to0\)</span> as <span class="math inline">\(t\to0\)</span> almost surely, the event <span class="math inline">\(\Omega^{B}\)</span> has probability <span class="math inline">\(1\)</span>. Thus, <span class="math inline">\(\mathbb{P}\left\{ \lim_{t\to0}X(t)=0\right\} =1\)</span>.</p>
<p>This actually shows that <span class="math inline">\(X(t)\)</span> is a bonafide standard brownian motion, as we have established continuity as well.</p>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="references" class="level1 appendix"><h2 class="anchored quarto-appendix-heading">References</h2><div class="quarto-appendix-contents">

<ul>
<li><em><a href="https://www.amazon.co.uk/Introduction-Stochastic-Calculus-Applications-3Rd/dp/1848168322">Introduction to Stochastic Calculus with Applications</a>, Fima C Klebaner</em></li>
<li><em><a href="https://www.amazon.co.uk/Brownian-Motion-Calculus-Ubbo-Wiersema/dp/0470021705">Brownian Motion Calculus</a>, Ubbo Wiersema</em></li>
</ul>


</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>
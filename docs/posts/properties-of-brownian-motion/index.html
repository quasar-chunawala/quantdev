<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Quasar">
<meta name="dcterms.date" content="2024-04-27">

<title>Properties of Brownian Motion – quantdev.blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-f2a1071e85750ec973bbb8a8f120da0f.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-b0693a1090cce3f71fbb13c85da23e1f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap');
  @import url('https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap');
</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9993009899870547" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">quantdev.blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Properties of Brownian Motion</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">Stochastic Calculus</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Quasar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 27, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#properties-of-brownian-motion." id="toc-properties-of-brownian-motion." class="nav-link active" data-scroll-target="#properties-of-brownian-motion.">Properties of Brownian Motion.</a>
  <ul class="collapse">
  <li><a href="#properties-of-brownian-motion.-1" id="toc-properties-of-brownian-motion.-1" class="nav-link" data-scroll-target="#properties-of-brownian-motion.-1">Properties of Brownian Motion.</a></li>
  <li><a href="#properties-of-the-paths." id="toc-properties-of-the-paths." class="nav-link" data-scroll-target="#properties-of-the-paths.">Properties of the paths.</a>
  <ul class="collapse">
  <li><a href="#functions-considered-in-stochastic-calculus." id="toc-functions-considered-in-stochastic-calculus." class="nav-link" data-scroll-target="#functions-considered-in-stochastic-calculus.">Functions considered in Stochastic Calculus.</a></li>
  <li><a href="#variation-of-a-function." id="toc-variation-of-a-function." class="nav-link" data-scroll-target="#variation-of-a-function.">Variation of a function.</a></li>
  <li><a href="#jordan-decomposition." id="toc-jordan-decomposition." class="nav-link" data-scroll-target="#jordan-decomposition.">Jordan Decomposition.</a></li>
  <li><a href="#riemann-stieltjes-integral." id="toc-riemann-stieltjes-integral." class="nav-link" data-scroll-target="#riemann-stieltjes-integral.">Riemann-Stieltjes Integral.</a></li>
  <li><a href="#brownian-motion-as-the-limit-of-a-symmetric-random-walk." id="toc-brownian-motion-as-the-limit-of-a-symmetric-random-walk." class="nav-link" data-scroll-target="#brownian-motion-as-the-limit-of-a-symmetric-random-walk.">Brownian motion as the limit of a symmetric random walk.</a></li>
  </ul></li>
  <li><a href="#what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance" id="toc-what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance" class="nav-link" data-scroll-target="#what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance">What exactly is <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> in mathematical finance?</a></li>
  <li><a href="#continuity-and-regularity-of-paths." id="toc-continuity-and-regularity-of-paths." class="nav-link" data-scroll-target="#continuity-and-regularity-of-paths.">Continuity and Regularity of paths.</a></li>
  <li><a href="#a-point-of-comparison-the-poisson-process." id="toc-a-point-of-comparison-the-poisson-process." class="nav-link" data-scroll-target="#a-point-of-comparison-the-poisson-process.">A point of comparison: The Poisson Process.</a></li>
  </ul></li>
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="properties-of-brownian-motion." class="level1">
<h1>Properties of Brownian Motion.</h1>
<section id="properties-of-brownian-motion.-1" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-brownian-motion.-1">Properties of Brownian Motion.</h2>
<p>Let <span class="math inline">\(B(t)\)</span> be a fixed Brownian motion. We give below some simple properties that follow directly from the definition of the Brownian Motion.</p>
<div class="prop">
<p>For any <span class="math inline">\(t\geq0\)</span>, <span class="math inline">\(B(t)\)</span> is normally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t\)</span>. For any <span class="math inline">\(s,t\geq0\)</span> we have <span class="math inline">\(\mathbb{E}(B_{s}B_{t})=\min\{s,t\}\)</span>.</p>
</div>
<p><em>Proof.</em> From condition (1), we have that <span class="math inline">\(B_{0}=0\)</span>. From condition (2), <span class="math inline">\(B_{t}-B_{0}=B_{t}\)</span> is normally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t\)</span>.</p>
<p>Assume that <span class="math inline">\(s&lt;t\)</span>.</p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}(B_{s}B_{t}) &amp; =\mathbb{E}\left[B_{s}(B_{t}-B_{s}+B_{s})\right] &amp; \{\text{Write }B_{t}=B_{t}-B_{s}+B_{s}\}\\
&amp; =\mathbb{E}[B_{s}(B_{t}-B_{s})]+\mathbb{E}[B_{s}^{2}] &amp; \{\text{Linearity of expectations}\}\\
&amp; =\mathbb{E}[B_{s}]\mathbb{E}(B_{t}-B_{s})+s &amp; \{B_{s},(B_{t}-B_{s})\text{ are independent}\}\\
&amp; =0\cdot0+s\\
&amp; =s
\end{aligned}\]</span></p>
<p>This closes the proof. ◻ :::</p>
<div class="prop">
<p>(Translation Invariance) For fixed <span class="math inline">\(t_{0}\geq0\)</span>, the stochastic process <span class="math inline">\(\tilde{B}(t)=B(t+t_{0})-B(t_{0})\)</span> is also a Brownian motion.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Firstly, the stochastic process <span class="math inline">\(\tilde{B}(t)\)</span> is such that:</p>
<p>(1) <span class="math inline">\(\tilde{B}(0)=B(t_{0})-B(t_{0})=0\)</span>. Hence, it satisfies condition (1).</p>
<p>(2) Let <span class="math inline">\(s&lt;t\)</span>. We have: <span class="math inline">\(\tilde{B}(t)-\tilde{B}(s)=B(t+t_{0})-B(s+t_{0})\)</span> which a Gaussian random variable with mean 0 and variance <span class="math inline">\(t-s\)</span>. Hence, for <span class="math inline">\(a\leq b\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\{a\leq &amp; \tilde{B}(t)\leq b\}=\frac{1}{\sqrt{2\pi(t-s)}}\int_{a}^{b}e^{-\frac{x^{2}}{2(t-s)}}dx
\end{aligned}\]</span></p>
<p>Hence, it satisfies condition (2).</p>
<p>(3) To check condition (3) for <span class="math inline">\(\tilde{B}(t)\)</span>, we may assume <span class="math inline">\(t_{0}&gt;0\)</span>. Then, for any <span class="math inline">\(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}\)</span>, we have:</p>
<p><span class="math display">\[0&lt;t_{0}\leq t_{0}+t_{1}\leq t_{0}+t_{2}\leq\ldots\leq t_{0}+t_{n}\]</span></p>
<p>So, <span class="math inline">\(B(t_{1}+t_{0})-B(t_{0})\)</span>, <span class="math inline">\(B(t_{2}+t_{0})-B(t_{1}+t_{0})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(B(t_{k}+t_{0})-B(t_{k-1}+t_{0})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(B(t_{n}+t_{0})-B(t_{n-1}+t_{0})\)</span> are independent random variables. Consequently, <span class="math inline">\(\tilde{B}(t)\)</span> satisfies condition (3).</p>
<p>This closes the proof. ◻</p>
</div>
<p>The above translation invariance property says that a Brownian motion starts afresh at any moment as a new Brownian motion.</p>
<div class="prop">
<p>(Scaling Invariance) For any real number <span class="math inline">\(\lambda&gt;0\)</span>, the stochastic process <span class="math inline">\(\tilde{B}(t)=B(\lambda t)/\sqrt{\lambda}\)</span> is also a Brownian motion.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> The scaled stochastic process <span class="math inline">\(\tilde{B}(t)\)</span> is such that:</p>
<p>(1) <span class="math inline">\(\tilde{B}(0)=0\)</span>. Hence it satisfies condition (1).</p>
<p>(2) Let <span class="math inline">\(s&lt;t\)</span>. Then, <span class="math inline">\(\lambda s&lt;\lambda t\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{B}(t)-\tilde{B}(s) &amp; =\frac{1}{\sqrt{\lambda}}(B(\lambda t)-B(\lambda s))
\end{aligned}\]</span></p>
<p>Now, <span class="math inline">\(B(\lambda t)-B(\lambda s)\)</span> is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\lambda(t-s)\)</span>. We know that, if <span class="math inline">\(X\)</span> is a random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>, <span class="math inline">\(Z=\left(\frac{X-\mu}{\sigma}\right)\)</span> has mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. Consequently, <span class="math inline">\(\frac{B(\lambda t)-B(\lambda s)}{\sqrt{\lambda}}\)</span> is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\((t-s)\)</span>.</p>
<p>Hence, <span class="math inline">\(\tilde{B}(t)-\tilde{B}(s)\)</span> is normal distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t-s\)</span> and it satisfies condition (2).</p>
<p>(3) To check condition (3) for <span class="math inline">\(\tilde{B}(t)\)</span>, we may assume <span class="math inline">\(t_{0}&gt;0\)</span>. Then, for any <span class="math inline">\(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}\)</span>, we have:</p>
<p><span class="math display">\[0\leq\lambda t_{1}\leq\lambda t_{2}\leq\ldots\leq\lambda t_{n}\]</span></p>
<p>Consequently, the random variables <span class="math inline">\(B(\lambda t_{k})-B(\lambda t_{k-1})\)</span>, <span class="math inline">\(k=1,2,3,\ldots,n\)</span> are independent. Hence it follows that <span class="math inline">\(\frac{1}{\sqrt{\lambda}}[B(\lambda t_{k})-B(\lambda t_{k-1})]\)</span> for <span class="math inline">\(k=1,2,\ldots,n\)</span> are also independent random variables.</p>
<p>This closes the proof. ◻</p>
</div>
<p>It follows from the scaling invariance property that for any <span class="math inline">\(\lambda&gt;0\)</span> and <span class="math inline">\(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}\)</span>, the random vectors:</p>
<p><span class="math display">\[(B(\lambda t_{1}),B(\lambda t_{2}),\ldots,B(\lambda t_{n}))\quad(\sqrt{\lambda}B(t_{1}),\sqrt{\lambda}B(t_{1}),\ldots,\sqrt{\lambda}B(t_{n}))\]</span></p>
<p>have the same distribution.</p>
<p>The scaling property shows that Brownian motion is <em>self-similar</em>, much like a fractal. To see this, suppose we zoom into a Brownian motion path very close to zero, say on the interval <span class="math inline">\([0,10^{-6}]\)</span>. If the Brownian motion path were smooth and differentiable, the closer we zoom in around the origin, the flatter the function will look. In the limit, we would essentially see a straight line given by the derivative at <span class="math inline">\(0\)</span>. However, what we see with the Brownian motion is very different. The scaling property means that for <span class="math inline">\(a=10^{-6}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
(B_{10^{-6}t,}t\in[0,1]) &amp; \stackrel{\text{distrib.}}{=}(10^{-3}B_{t},t\in[0,1])
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\stackrel{\text{distrib.}}{=}\)</span> means equality of the distribution of the two processes. In other words, Brownian motion on <span class="math inline">\([0,10^{-6}]\)</span> looks like a Browian motion on <span class="math inline">\([0,1]\)</span>, but with its amplitude multiplied by a factor of <span class="math inline">\(10^{-3}\)</span>. In particular, it will remain rugged as we zoom in, unlike a smooth function.</p>
<div class="prop">
<p><span id="prop:brownian-motion-symmetry-of-reflection-at-time-s" data-label="prop:brownian-motion-symmetry-of-reflection-at-time-s"></span>(Reflection at time <span class="math inline">\(s\)</span>) The process <span class="math inline">\((-B_{t},t\geq0)\)</span> is a Brownian motion. More generally, for any <span class="math inline">\(s\geq0\)</span>, the process <span class="math inline">\((\tilde{B}(t),t\geq0)\)</span> defined by:</p>
<p><span class="math display">\[\begin{aligned}
\tilde{B}(t) &amp; =\begin{cases}
B_{t} &amp; \text{if }t\leq s\\
B_{s}-(B_{t}-B_{s}) &amp; \text{if }t&gt;s
\end{cases}\label{eq:reflection-property}
\end{aligned}\]</span></p>
<p>is a Brownian motion.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (a) Consider the process <span class="math inline">\(\tilde{B}(t)=(-B_{t},t\geq0)\)</span>.</p>
<p>(1) <span class="math inline">\(\tilde{B}(0)=0\)</span>.</p>
<p>(2) If <span class="math inline">\(X\)</span> is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t-s\)</span>, <span class="math inline">\(-X\)</span> is also Gaussian with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t-s\)</span>. Thus, <span class="math inline">\(\tilde{B}(t)-\tilde{B}(s)=-(B(t)-B(s))\)</span> is also Gaussian with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\((t-s)\)</span>. Hence condition (2) is satisfied.</p>
<p>(3) Assume that <span class="math inline">\(0\leq t_{0}\leq t_{1}\leq\ldots\leq t_{n}\)</span>. Then, the random variables <span class="math inline">\(-(B(t_{k})-B(t_{k-1}))\)</span> are independent for <span class="math inline">\(k=1,2,3,\ldots,n\)</span>. Hence, condition (3) is satisfied.</p>
<p>(b) Consider the process <span class="math inline">\(\tilde{B}(t)\)</span> as defined in (<a href="#eq:reflection-property" data-reference-type="ref" data-reference="eq:reflection-property">[eq:reflection-property]</a>).</p>
<p>Fix an <span class="math inline">\(s\geq0\)</span>.</p>
<p>(1) Let <span class="math inline">\(t=0\)</span>. Then, <span class="math inline">\(t\leq s\)</span>. <span class="math inline">\(\tilde{B}(t)=\tilde{B}(0)=B(0)=0\)</span>.</p>
<p>(2) Let <span class="math inline">\(t_{1}&lt;t_{2}\leq s\)</span>. Then, <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})=B(t_{2})-B(t_{1})\)</span>. This is a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t_{2}-t_{1}\)</span>.</p>
<p>Let <span class="math inline">\(t_{1}&lt;s&lt;t_{2}\)</span>. Then, <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})=B(s)-(B(t_{2})-B(s))-B(t_{1})=(B(s)-B(t_{1}))-(B(t_{2})-B(s))\)</span>. Since, <span class="math inline">\(B(s)-B(t_{1})\)</span> and <span class="math inline">\(B(t_{2})-B(s)\)</span> are independent Gaussian random variables, any linear combination of these is Gaussian. Moreover, its mean is zero. The variance is given by:</p>
<p><span class="math display">\[\begin{aligned}
Var[\tilde{B}(t_{2})-\tilde{B}(t_{1})] &amp; =Var[B(s)-B(t_{1})]+Var[B(t_{2})-B(s)]\\
&amp; =(s-t_{1})+(t_{2}-s)\\
&amp; =t_{2}-t_{1}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(s&lt;t_{1}&lt;t_{2}\)</span>. Then, <span class="math display">\[\begin{aligned}
\tilde{B}(t_{2})-\tilde{B}(t_{1}) &amp; =B_{s}-(B_{t_{2}}-B_{s})-(B_{s}-(B_{t_{1}}-B_{s}))\\
&amp; =\cancel{B_{s}}-(B_{t_{2}}-\cancel{B_{s}})-(\cancel{B_{s}}-(B_{t_{1}}-\cancel{B_{s}}))\\
&amp; =-(B_{t_{2}}-B_{t_{1}})
\end{aligned}\]</span></p>
<p>Hence, <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})\)</span> is again a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t_{2}-t_{1}\)</span>. Hence, condition (3) is satisfied.</p>
<p>(3) Assume that <span class="math inline">\(0\leq t_{1}\leq\ldots\leq t_{k-1}\leq s\leq t_{k}\leq\ldots\leq t_{n}\)</span>. From the above discussion, the increments <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\tilde{B}(s)-\tilde{B}(t_{k-1})\)</span>, <span class="math inline">\(\tilde{B}(t_{k})-\tilde{B}(s)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\tilde{B}(t_{n})-\tilde{B}(t_{n-1})\)</span> are independent increments. The increment <span class="math inline">\(\tilde{B}(t_{k})-\tilde{B}(t_{k-1})\)</span> only depends on the random variables <span class="math inline">\(\tilde{B}(s)-\tilde{B}(t_{k-1})\)</span> and <span class="math inline">\(\tilde{B}(t_{k})-\tilde{B}(s)\)</span>. Thus, <span class="math inline">\(\tilde{B}(t_{2})-\tilde{B}(t_{1})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\tilde{B}(t_{k})-\tilde{B}(t_{k-1})\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\tilde{B}(t_{n})-\tilde{B}(t_{n-1})\)</span> are independent. ◻</p>
</div>
<div class="prop">
<p>(Time Reversal). Let <span class="math inline">\((B_{t},t\geq0)\)</span> be a Brownian motion. Show that the process <span class="math inline">\((B_{1}-B_{1-t},t\in[0,1])\)</span> has the distribution of a standard brownian motion on <span class="math inline">\([0,1]\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (1) At <span class="math inline">\(t=0\)</span>, <span class="math inline">\(B(1)-B(1-t)=B(1)-B(1)=0\)</span>.</p>
<p>(2) Let <span class="math inline">\(s&lt;t\)</span>. Then, <span class="math inline">\(1-t&lt;1-s\)</span>. So, the increment :</p>
<p><span class="math display">\[\begin{aligned}
(B(1)-B(1-t))-(B(1)-B(1-s)) &amp; =B(1-s)-B(1-t)
\end{aligned}\]</span></p>
<p>has a Gaussian distribution. It’s mean is <span class="math inline">\(0\)</span> and variance is <span class="math inline">\((1-s)-(1-t)=t-s\)</span>.</p>
<p>(3) Let <span class="math inline">\(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}\)</span>. Then:</p>
<p><span class="math display">\[1-t_{n}\leq\ldots\leq1-t_{k}\leq1-t_{k-1}\leq\ldots\leq1-t_{2}\leq1-t_{1}\]</span></p>
<p>Consider the increments of the process for <span class="math inline">\(k=1,2,\ldots,n\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
(B(1)-B(1-t_{k}))-(B(1)-B(1-t_{k-1})) &amp; =B(1-t_{k-1})-B(1-t_{k})
\end{aligned}\]</span></p>
<p>They are independent random variables. Hence, condition (3) is satisfied. ◻</p>
</div>
<div class="example">
<p>(Evaluating Brownian Probabilities). Let’s compute the probability that <span class="math inline">\(B_{1}&gt;0\)</span> and <span class="math inline">\(B_{2}&gt;0\)</span>. We know from the definition that <span class="math inline">\((B_{1},B_{2})\)</span> is a Gaussian vector with mean <span class="math inline">\(0\)</span> and covariance matrix:</p>
<p><span class="math display">\[\begin{aligned}
C &amp; =\left[\begin{array}{cc}
1 &amp; 1\\
1 &amp; 2
\end{array}\right]
\end{aligned}\]</span></p>
<p>The determinant of <span class="math inline">\(C\)</span> is <span class="math inline">\(1\)</span>. By performing row operations on the augmented matrix <span class="math inline">\([C|I]\)</span> we find that:</p>
<p><span class="math display">\[\begin{aligned}
C^{-1} &amp; =\left[\begin{array}{cc}
2 &amp; -1\\
-1 &amp; 1
\end{array}\right]
\end{aligned}\]</span></p>
<p>Thus, the probability <span class="math inline">\(\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0)\)</span> can be expressed as:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0) &amp; =\frac{1}{\sqrt{(2\pi)^{2}}}\int_{0}^{\infty}\int_{0}^{\infty}\exp\left[-\frac{1}{2}(2x_{1}^{2}-2x_{1}x_{2}+x_{2}^{2}\right]dx_{2}dx_{1}
\end{aligned}\]</span></p>
<p>This integral can be evaluated using a calculator or software and is equal to <span class="math inline">\(3/8\)</span>. The probability can also be computed using the independence of increments. The increments <span class="math inline">\((B_{1},B_{2}-B_{1})\)</span> are IID standard Gaussians. We know their joint PDF. It remains to integrate over the correct region of <span class="math inline">\(\mathbf{R}^{2}\)</span> which in this case will be:</p>
<p><span class="math display">\[\begin{aligned}
D^{*} &amp; =\{(z_{1},z_{2}):(z_{1}&gt;0,z_{1}+z_{2}&gt;0)\}
\end{aligned}\]</span></p>
<p>We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0) &amp; =\frac{1}{2\pi}\int_{0}^{\infty}\int_{z_{2}=-z_{1}}^{z_{2}=\infty}e^{-(z_{1}^{2}+z_{2}^{2})/2}dz_{2}dz_{1}
\end{aligned}\]</span></p>
<p>It turns out that this integral can be evaluated exactly. Indeed by writing <span class="math inline">\(B_{1}=Z_{1}\)</span> and <span class="math inline">\(Z_{2}=B_{2}-B_{1}\)</span> and splitting the probability on the event <span class="math inline">\(\{Z_{2}\geq0\}\)</span> and its complement, we have that <span class="math inline">\(\mathbb{P}(B_{1}\geq0,B_{2}\geq0)\)</span> equals:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(B_{1}\geq0,B_{2}\geq0) &amp; =\mathbb{P}(Z_{1}\geq0,Z_{1}+Z_{2}&gt;0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}+Z_{2}&gt;0,Z_{2}&lt;0)\\
&amp; =\mathbb{P}(Z_{1}\geq0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;-Z_{2},-Z_{2}&gt;0)\\
&amp; =\mathbb{P}(Z_{1}\geq0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;Z_{2},Z_{2}&gt;0)\\
&amp; =\frac{1}{4}+\frac{1}{8}\\
&amp; =\frac{3}{8}
\end{aligned}\]</span></p>
<p>Note that, by symmetry, <span class="math inline">\(\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;Z_{2},Z_{2}&gt;0)=\mathbb{P}(Z_{1}\geq0,Z_{1}\leq Z_{2},Z_{2}&gt;0)=\frac{1}{8}\)</span>.</p>
</div>
<div class="example">
<p>(Another look at Ornstein Uhlenbeck process.) Consider the process <span class="math inline">\((X_{t},t\in\mathbf{R})\)</span> defined by :</p>
<p><span class="math display">\[\begin{aligned}
X_{t} &amp; =\frac{e^{-2t}}{\sqrt{2}}B(e^{4t}),\quad t\in\mathbf{R}
\end{aligned}\]</span></p>
<p>Here the process <span class="math inline">\((B_{e^{4t}},t\ge0)\)</span> is called a time change of Brownian motion, since the time is now quantitfied by an increasing function of <span class="math inline">\(t\)</span> namely <span class="math inline">\(e^{4t}\)</span>. The example <span class="math inline">\((B(\lambda t),t\geq0)\)</span> in the scaling property is another example of time change.</p>
</div>
<p>It turns out that <span class="math inline">\((X_{t},t\in\mathbf{R})\)</span> is a stationary Ornstein-Uhlenbeck process. (Here the index of time is <span class="math inline">\(\mathbf{R}\)</span> instead of <span class="math inline">\([0,\infty)\)</span>, but the definition also applies as the process is stationary. Since the original brownian motion <span class="math inline">\(B(t)\)</span> is a Gaussian process, any finite dimensional vector <span class="math inline">\((B(t_{1}),\ldots,B(t_{n}))\)</span> is Gaussian. It follows that:</p>
<p><span class="math display">\[(B(T_{1}),\ldots,B(T_{n}))=\frac{1}{\sqrt{2}}(e^{-2t_{1}}B(e^{4t_{1}}),\ldots,e^{-2t_{n}}B(e^{4t_{n}}))\]</span></p>
<p>is also a Gaussian vector. (Note, once we fix <span class="math inline">\(t_{1},t_{2},\ldots,t_{n}\)</span>, <span class="math inline">\(e^{-4t_{1}},\ldots,e^{-4t_{n}}\)</span> are constants.) Hence, <span class="math inline">\((X_{t},t\in\mathbf{R})\)</span> is a Gaussian process.</p>
<p>The mean of <span class="math inline">\((X_{t},t\in\mathbf{R})\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[X_{t}] &amp; =\frac{e^{-2t}}{\sqrt{2}}\mathbb{E}[B(e^{4t})]=0
\end{aligned}\]</span></p>
<p>And if <span class="math inline">\(s&lt;t\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[X_{s}X_{t}] &amp; =\frac{e^{-2(s+t)}}{2}\mathbb{E}[B(e^{4s})B(e^{4t})]\\
&amp; =\frac{e^{-2(s+t)}}{2}e^{4s}\\
&amp; =\frac{e^{-2(t-s)}}{2}
\end{aligned}\]</span></p>
<p>Two Gaussian processes having the same mean and covariance have the same distribution. Hence, it proves the claim that <span class="math inline">\((X_{t})\)</span> is a stationary OU process.</p>
</section>
<section id="properties-of-the-paths." class="level2">
<h2 class="anchored" data-anchor-id="properties-of-the-paths.">Properties of the paths.</h2>
<p>First we review the definitions of the Riemann integral and the Riemann-Stieljtes integral in Calculus.</p>
<div class="defn">
<p>A partition <span class="math inline">\(P\)</span> of <span class="math inline">\([a,b]\)</span> is a <em>finite</em> set of points from <span class="math inline">\([a,b]\)</span> that includes both <span class="math inline">\([a,b].\)</span>The notational convention is to always list the points of a partition <span class="math inline">\(P=\{a=x_{0},x_{1},x_{2},\ldots,x_{n}=b\}\)</span> in increasing order. Thus:</p>
<p><span class="math display">\[a=x_{0}&lt;x_{1}&lt;\ldots&lt;x_{k-1}&lt;x_{k}&lt;\ldots&lt;x_{n}=b\]</span></p>
</div>
<p>For each subinterval <span class="math inline">\([x_{k-1},x_{k}]\)</span> of <span class="math inline">\(P\)</span>, let</p>
<p><span class="math display">\[\begin{aligned}
m_{k} &amp; =\inf\{f(x):x\in[x_{k-1},x_{k}]\}\\
M_{k} &amp; =\sup\{f(x):x\in[x_{k-1},x_{k}]\}
\end{aligned}\]</span></p>
<p>The lower sum of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(P\)</span> is given by :</p>
<p><span class="math display">\[\begin{aligned}
L(f,P) &amp; =\sum_{k=1}^{n}m_{k}(x_{k}-x_{k-1})
\end{aligned}\]</span></p>
<p>The upper sum of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(P\)</span> is given by:</p>
<p><span class="math display">\[\begin{aligned}
U(f,P) &amp; =\sum_{k=1}^{n}M_{k}(x_{k}-x_{k-1})
\end{aligned}\]</span></p>
<p>For a particular partition <span class="math inline">\(P\)</span>, it is clear that <span class="math inline">\(U(f,P)\geq L(f,P)\)</span> because <span class="math inline">\(M_{k}\geq m_{k}\)</span> for all <span class="math inline">\(k=0,1,2,\ldots,n\)</span>.</p>
<div class="defn">
<p>A partition <span class="math inline">\(Q\)</span> is called a <em>refinement</em> of <span class="math inline">\(P\)</span> if <span class="math inline">\(Q\)</span> contains all of the points of <span class="math inline">\(P\)</span>; that is <span class="math inline">\(Q\subseteq P\)</span>.</p>
</div>
<div class="lem">
<p>If <span class="math inline">\(P\subseteq Q\)</span>, then <span class="math inline">\(L(f,P)\leq L(f,Q)\)</span> and <span class="math inline">\(U(f,Q)\leq U(f,P)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Consider what happens when we refine <span class="math inline">\(P\)</span> by adding a single point <span class="math inline">\(z\)</span> to some subinterval <span class="math inline">\([x_{k-1},x_{k}]\)</span> of <span class="math inline">\(P\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
m_{k}(x_{k}-x_{k-1}) &amp; =m_{k}(x_{k}-z)+m_{k}(z-x_{k-1})\\
&amp; \leq m_{k}'(x_{k}-z)+m_{k}''(z-x_{k-1})
\end{aligned}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{aligned}
m_{k}' &amp; =\inf\{f(x):x\in[z,x_{k}]\}\\
m_{k}'' &amp; =\inf\{f(x):x\in[x_{k-1},z]\}
\end{aligned}\]</span></p>
<p>By induction we have:</p>
<p><span class="math display">\[\begin{aligned}
L(f,P) &amp; \leq L(f,Q)\\
U(f,Q) &amp; \leq U(f,P)
\end{aligned}\]</span> ◻</p>
</div>
<div class="lem">
<p>If <span class="math inline">\(P_{1}\)</span> and <span class="math inline">\(P_{2}\)</span> are any two partitions of <span class="math inline">\([a,b]\)</span>, then <span class="math inline">\(L(f,P_{1})\leq U(f,P_{2})\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Let <span class="math inline">\(Q=P_{1}\cup P_{2}\)</span>. Then, <span class="math inline">\(P_{1}\subseteq Q\)</span> and <span class="math inline">\(P_{2}\subseteq Q\)</span>. Thus, <span class="math inline">\(L(f,P_{1})\leq L(f,Q)\leq U(f,Q)\leq L(f,P_{2})\)</span>. ◻</p>
</div>
<div class="defn">
<p>Let <span class="math inline">\(\mathcal{P}\)</span> be the collection of all possible partitions of the interval <span class="math inline">\([a,b]\)</span>. The upper integral of <span class="math inline">\(f\)</span> is defined to be:</p>
<p><span class="math display">\[\begin{aligned}
U(f) &amp; =\inf\{U(f,P):P\in\mathcal{P}\}
\end{aligned}\]</span></p>
<p>The lower integral of <span class="math inline">\(f\)</span> is defined by:</p>
<p><span class="math display">\[\begin{aligned}
L(f) &amp; =\sup\{L(f,P):P\in\mathcal{P}\}
\end{aligned}\]</span></p>
</div>
<p>Consider the set of all upper sums of <span class="math inline">\(f\)</span> - <span class="math inline">\(\{U(f,P):P\in\mathcal{P}\}\)</span>. Take an arbitrary partition <span class="math inline">\(P'\in\mathcal{P}\)</span>. Since <span class="math inline">\(L(f,P')\leq U(f,P)\)</span> for all <span class="math inline">\(P\in\mathcal{P}\)</span>, by the Axiom of Completeness(AoC), <span class="math inline">\(\inf\{U(f,P):P\in\mathcal{P}\}\)</span> exists.We can similarly argue for the supremum of all lower Riemann sums.</p>
<div class="lem">
<p>For any bounded function <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>, it is always the case that <span class="math inline">\(U(f)\geq L(f)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> By the properties of the infimum of a set, <span class="math inline">\((\forall\epsilon&gt;0)\)</span>, <span class="math inline">\(\exists P(\epsilon)\)</span> such that <span class="math inline">\(U(f)&lt;U(f,P(\epsilon))&lt;U(f)+\epsilon\)</span>. Pick <span class="math inline">\(\epsilon=1,\frac{1}{2},\frac{1}{3}\ldots,\frac{1}{n},\ldots\)</span>. Thus, we can produce a sequence of partitions <span class="math inline">\(P_{n}\)</span> such that :</p>
<p><span class="math display">\[U(f)&lt;\ldots&lt;U(f,P_{n})&lt;U(f)+\frac{1}{n}\]</span></p>
<p>Consequently, <span class="math inline">\(\lim U(f,P_{n})=U(f)\)</span>. Similarly, we can produce a sequence of partitions <span class="math inline">\((Q_{m})\)</span> such that :</p>
<p><span class="math display">\[L(f)-\frac{1}{m}&lt;\ldots&lt;L(f,Q_{m})&lt;L(f)\]</span></p>
<p>We know that:</p>
<p><span class="math display">\[\begin{aligned}
L(f,Q_{m}) &amp; \leq U(f,P_{n})
\end{aligned}\]</span></p>
<p>Keeping <span class="math inline">\(m\)</span> fixed and passing to the limit, as <span class="math inline">\(n\to\infty\)</span> on both sides, we have:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}L(f,Q_{m}) &amp; \leq\lim_{n\to\infty}U(f,P_{n})\quad\left\{ \text{Order Limit Theorem}\right\} \\
L(f,Q_{m}) &amp; \leq U(f)
\end{aligned}\]</span></p>
<p>Now, passing to the limit, as <span class="math inline">\(m\to\infty\)</span> on both sides, we have:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{m\to\infty}L(f,Q_{m}) &amp; \leq\lim_{m\to\infty}U(f)\quad\left\{ \text{Order Limit Theorem}\right\} \\
L(f) &amp; \leq U(f)
\end{aligned}\]</span> ◻</p>
</div>
<div class="defn">
<p>(Riemann Integrability). A bounded function <span class="math inline">\(f\)</span> on the interval <span class="math inline">\([a,b]\)</span> is said to be Riemann integrable if <span class="math inline">\(U(f)=L(f)\)</span>. In this case, we define <span class="math inline">\(\int_{a}^{b}f\)</span> or <span class="math inline">\(\int_{a}^{b}f(x)dx\)</span> to be the common value:</p>
<p><span class="math display">\[\begin{aligned}
\int_{a}^{b}f(x)dx &amp; =U(f)=L(f)
\end{aligned}\]</span></p>
</div>
<div class="thm">
<p>(Integrability Criterion) A bounded function <span class="math inline">\(f\)</span> is integrable on <span class="math inline">\([a,b]\)</span> if and only if, for every <span class="math inline">\(\epsilon&gt;0\)</span>, there exists a partition <span class="math inline">\(P_{\epsilon}\)</span> of <span class="math inline">\([a,b]\)</span> such that:</p>
<p><span class="math display">\[\begin{aligned}
U(f,P_{\epsilon})-L(f,P_{\epsilon}) &amp; &lt;\epsilon
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (<span class="math inline">\(\Longleftarrow\)</span> direction.) Let <span class="math inline">\(\epsilon&gt;0\)</span>. If such a partition <span class="math inline">\(P_{\epsilon}\)</span> exists, then:</p>
<p><span class="math display">\[U(f)-L(f)\leq U(f,P_{\epsilon})-L(f,P_{\epsilon})&lt;\epsilon\]</span></p>
<p>Because <span class="math inline">\(\epsilon\)</span> is arbitrary, it follows that <span class="math inline">\(U(f)=L(f)\)</span> and hence <span class="math inline">\(f\)</span> is Riemann integrable.</p>
<p>(<span class="math inline">\(\Longrightarrow\)</span> direction.) Let <span class="math inline">\(f\)</span> be a bounded function on <span class="math inline">\([a,b]\)</span> such that <span class="math inline">\(f\)</span> is Riemann integrable.</p>
<p>Pick an arbitrary <span class="math inline">\(\epsilon&gt;0\)</span>.</p>
<p>Then, since <span class="math inline">\(U(f)=\inf\{U(f,P):P\in\mathcal{P}\}\)</span>, there exists <span class="math inline">\(P_{\epsilon}\in\mathcal{P}\)</span>, such that <span class="math inline">\(U(f)&lt;U(f,P_{\epsilon})&lt;U(f)+\frac{\epsilon}{2}\)</span>. Since <span class="math inline">\(L(f)=\sup\{L(f,P):P\in\mathcal{P}\}\)</span>, there exists <span class="math inline">\(P_{\epsilon}\in\mathcal{P}\)</span>, such that <span class="math inline">\(L(f)-\frac{\epsilon}{2}&lt;L(f,P_{\epsilon})&lt;L(f)\)</span>. Consequently,</p>
<p><span class="math display">\[\begin{aligned}
U(f,P_{\epsilon})-L(f,P_{\epsilon}) &amp; &lt;U(f)+\frac{\epsilon}{2}-\left(L(f)-\frac{\epsilon}{2}\right)\\
&amp; =U(f)-L(f)+\epsilon\\
&amp; =\epsilon
\end{aligned}\]</span> ◻</p>
</div>
<section id="functions-considered-in-stochastic-calculus." class="level3">
<h3 class="anchored" data-anchor-id="functions-considered-in-stochastic-calculus.">Functions considered in Stochastic Calculus.</h3>
<div class="defn">
<p>A point <span class="math inline">\(c\)</span> is called a discontinuity of the first kind or jump point if both limits <span class="math inline">\(g(c+)=\lim_{t\uparrow c}g(t)\)</span> and <span class="math inline">\(g(c-)=\lim_{t\downarrow c}g(t)\)</span> exist and are not equal. The jump at <span class="math inline">\(c\)</span> is defined as <span class="math inline">\(\Delta g(c)=g(c+)-g(c-)\)</span>. Any other discontinuity is said to be of the second kind.</p>
</div>
<div class="example">
<p>Consider the function</p>
<p><span class="math display">\[\begin{aligned}
f(x) &amp; =\sin\left(\frac{1}{x}\right)
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(x_{n}=\frac{1}{2n\pi}\)</span>. Then, <span class="math inline">\(f(x_{n})=(0,0,0,\ldots)\)</span>. Next, consider <span class="math inline">\(y_{n}=\frac{1}{\pi/2+2n\pi}\)</span>. Then, <span class="math inline">\(f(y_{n})=(1,1,1,\ldots)\)</span>. Consequently, <span class="math inline">\(f\)</span> is not continuous at <span class="math inline">\(0\)</span>. Hence, limits from the left or right don’t exist. Consequently, this is a discontinuity of the second kind.</p>
</div>
<p>Functions in stochastic calculus are functions without discontinuities of the second kind, that is functions have both left and right hand limits at any point of the domain and have one-sided limits at the boundary. These functions are called <em>regular</em> functions. It is often agreed to identify functions if they have the same right and left limits at any point.</p>
<p>The class <span class="math inline">\(D=D[0,T]\)</span> of right-continuous functions on <span class="math inline">\([0,T]\)</span> with left limits has a special name, <em>cadlag</em> functions (which is the abbreviation of right continuous with left limits in French). Sometimes these processes are called R.R.C. for regular right continuous. Notice that this class of processes includes <span class="math inline">\(C\)</span>, the class of continuous functions.</p>
<p>Let <span class="math inline">\(g\in D\)</span> be a cadlag function, then, by definition, all the discontinuities of <span class="math inline">\(g\)</span> are jumps. An important result in analysis is that, a function can have no more than a countable number of discontinuities.</p>
</section>
<section id="variation-of-a-function." class="level3">
<h3 class="anchored" data-anchor-id="variation-of-a-function.">Variation of a function.</h3>
<p>If <span class="math inline">\(g\)</span> is a function of a real variable, its variation over the interval <span class="math inline">\([a,b]\)</span> is defined as:</p>
<p><span class="math display">\[\begin{aligned}
V_{g}([a,b]) &amp; =\sup\left\{ \sum_{i=1}^{n}\left|g(t_{i})-g(t_{i-1})\right|\right\} \label{eq:total-variation-of-a-function}
\end{aligned}\]</span></p>
<p>where the supremum is taken over all partitions <span class="math inline">\(P\in\mathcal{P}\)</span>.</p>
<p>Clearly, by the Triangle Inequality, the sums in (<a href="#eq:total-variation-of-a-function" data-reference-type="ref" data-reference="eq:total-variation-of-a-function">[eq:total-variation-of-a-function]</a>) increase as new points are added to the partitions. Therefore, the variation of <span class="math inline">\(g\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
V_{g}([a,b]) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}\left|g(t_{i})-g(t_{i-1})\right|
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(||\Delta_{n}||=\max_{1\leq i\leq n}(t_{i}-t_{i-1})\)</span>. If <span class="math inline">\(V_{g}([a,b])\)</span> is finite, then <span class="math inline">\(g\)</span> is said to be a function of finite variation on <span class="math inline">\([a,b]\)</span>. If <span class="math inline">\(g\)</span> is a function of <span class="math inline">\(t\geq0\)</span>, then the variation of <span class="math inline">\(g\)</span> as a function of <span class="math inline">\(t\)</span> is defined by:</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =V_{g}([0,t])
\end{aligned}\]</span></p>
<p>Clearly, <span class="math inline">\(V_{g}(t)\)</span> is an increasing function of <span class="math inline">\(t\)</span>.</p>
<div class="defn">
<p><span class="math inline">\(g\)</span> is a function of finite variation if <span class="math inline">\(V_{g}(t)&lt;\infty\)</span> for all <span class="math inline">\(t\in[0,\infty)\)</span>. <span class="math inline">\(g\)</span> is of bounded variation if <span class="math inline">\(\sup_{t}V_{g}(t)&lt;\infty\)</span>, in other words there exists <span class="math inline">\(C\)</span>, for all <span class="math inline">\(t\)</span>, such that <span class="math inline">\(V_{g}(t)&lt;C\)</span>. Here <span class="math inline">\(C\)</span> is independent of <span class="math inline">\(t\)</span>.</p>
</div>
<div class="example">
<p>(1) If <span class="math inline">\(g(t)\)</span> is increasing then for any <span class="math inline">\(i\)</span>, <span class="math inline">\(g(t_{i})\geq g(t_{i-1})\)</span>, resulting in a telescopic sum, where all terms excluding the first and the last cancel out, leaving</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =g(t)-g(0)
\end{aligned}\]</span></p>
<p>(2) If <span class="math inline">\(g(t)\)</span> is decreasing, then similarly,</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =g(0)-g(t)
\end{aligned}\]</span></p>
</div>
<div class="example">
<p>If <span class="math inline">\(g(t)\)</span> is differentiable with continuous derivative <span class="math inline">\(g'(t)\)</span>, <span class="math inline">\(g(t)=\int_{0}^{t}g'(s)ds\)</span> then</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =\int_{0}^{t}|g'(s)|ds
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> By definition,</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =\lim_{||\Delta_{n}\to0||}\sum_{i=1}^{n}|g(t_{i})-g(t_{i-1})|
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(g\)</span> is continuous and differentiable on <span class="math inline">\([t_{i-1},t_{i}]\)</span>, there exists <span class="math inline">\(z_{i}\in(t_{i-1},t_{i})\)</span> such, that <span class="math inline">\(g(t_{i})-g(t_{i-1})=g'(z_{i})(t_{i}-t_{i-1})\)</span>. Therefore, we can write:</p>
<p><span class="math display">\[\begin{aligned}
{1}
V_{g}(t) &amp; =\lim_{||\Delta_{n}\to0||}\sum_{i=1}^{n}|g'(z_{i})|(t_{i}-t_{i-1})\\
&amp; =\int_{0}^{t}|g'(s)|ds
\end{aligned}\]</span> ◻</p>
</div>
<div class="thm">
<p>If <span class="math inline">\(g\)</span> is continuous, <span class="math inline">\(g'\)</span> exists and <span class="math inline">\(\int_{0}^{t}|g'(s)|ds\)</span> is finite, then <span class="math inline">\(g\)</span> is of finite variation.</p>
</div>
<div class="example">
<p>The function <span class="math inline">\(g(t)=t\sin(1/t)\)</span> for <span class="math inline">\(t&gt;0\)</span> and <span class="math inline">\(g(0)=0\)</span> is continuous on <span class="math inline">\([0,1]\)</span> and differentiable at all points except zero, but is not of bounded variation on any interval that includes <span class="math inline">\(0\)</span>. Consider the partition <span class="math inline">\(\{x_{n}\}=\left\{ \frac{1}{\pi/2+n\pi}\right\}\)</span>. Thus,</p>
<p><span class="math display">\[\begin{aligned}
\sin(\frac{1}{x_{n}}) &amp; =\begin{cases}
1 &amp; \text{if }n\text{ is even}\\
-1 &amp; \text{if }n\text{ is odd}
\end{cases}
\end{aligned}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
f(x_{n}) &amp; =\begin{cases}
x_{n} &amp; n\text{ is even}\\
-x_{n} &amp; n\text{ is odd}
\end{cases}
\end{aligned}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{aligned}
\sum_{n=1}^{m}|f(x_{n})-f(x_{n-1})| &amp; =\sum_{n=1}^{m}(x_{n}+x_{n-1})\\
&amp; =x_{0}+x_{n}+2\sum_{n=1}^{m-1}x_{n}\\
&amp; \geq\sum_{n=1}^{m-1}x_{n}
\end{aligned}\]</span></p>
<p>This is the lower bound on the variation of <span class="math inline">\(g\)</span> on the partition <span class="math inline">\(\{0,x_{m},\ldots,x_{1},x_{0},1\}\)</span>. Now, passing to the limit as <span class="math inline">\(m\)</span> approaches infinity, <span class="math inline">\(\sum\frac{1}{\pi/2+n\pi}\)</span> is a divergent series. Consequently, <span class="math inline">\(V_{g}([0,1])\)</span> has unbounded variation.</p>
</div>
</section>
<section id="jordan-decomposition." class="level3">
<h3 class="anchored" data-anchor-id="jordan-decomposition.">Jordan Decomposition.</h3>
<div class="thm">
<p>Any function <span class="math inline">\(g:[0,\infty)\to\mathbf{R}\)</span> is of bounded variation if and only if it can be expressed as the difference of two increasing functions:</p>
<p><span class="math display">\[\begin{aligned}
g(t) &amp; =a(t)-b(t)
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> (<span class="math inline">\(\Longrightarrow\)</span>direction). If <span class="math inline">\(g\)</span> is of finite variation, <span class="math inline">\(V_{g}(t)&lt;\infty\)</span> for all <span class="math inline">\(t\)</span>, and we can write:</p>
<p><span class="math display">\[\begin{aligned}
g(t) &amp; =V_{g}(t)-(V_{g}(t)-g(t))
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(a(t)=V_{g}(t)\)</span> and <span class="math inline">\(b(t)=V_{g}(t)-g(t)\)</span>. Clearly, both <span class="math inline">\(a(t)\)</span> and <span class="math inline">\(b(t)\)</span> are increasing functions.</p>
<p>(<span class="math inline">\(\Longleftarrow\)</span>direction). Suppose a function <span class="math inline">\(g\)</span> can be expressed as a difference of two bounded increasing functions. Then,</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}|(a(t_{i})-b(t_{i}))-(a(t_{i-1})-b(t_{i-1})|\\
&amp; \quad\{\text{ Telescoping sum }\}\\
&amp; =a(t)-b(t)-(a(0)-b(0))
\end{aligned}\]</span></p>
<p>Since both <span class="math inline">\(a(t)\)</span> and <span class="math inline">\(b(t)\)</span> are bounded, <span class="math inline">\(g\)</span> has bounded variation. ◻</p>
</div>
</section>
<section id="riemann-stieltjes-integral." class="level3">
<h3 class="anchored" data-anchor-id="riemann-stieltjes-integral.">Riemann-Stieltjes Integral.</h3>
<p>Let <span class="math inline">\(g\)</span> be a montonically increasing function on a finite closed interval <span class="math inline">\([a,b]\)</span>. A bounded function <span class="math inline">\(f\)</span> defined on <span class="math inline">\([a,b]\)</span> is said to <em>Riemann-Stieltjes integrable</em> with respect to <span class="math inline">\(g\)</span> if the following limit exists:</p>
<p><span class="math display">\[\begin{aligned}
\int_{a}^{b}f(t)dg(t) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}f(\tau_{i})(g(t_{i})-g(t_{i-1}))\label{eq:riemann-stieltjes-integral}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tau_{i}\)</span> is an evaluation point in the interval <span class="math inline">\([t_{i-1},t_{i}]\)</span>. It is a well-known fact that continuous functions are Riemann integrable and Riemann-Stieltjes integrable with respect to any monotonically increasing function on <span class="math inline">\([a,b]\)</span>.</p>
<p>We ask the following question. For any continuous functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> on <span class="math inline">\([a,b]\)</span>, can we define the integral <span class="math inline">\(\int_{a}^{b}f(t)dg(t)\)</span> by Equation (<a href="#eq:riemann-stieltjes-integral" data-reference-type="ref" data-reference="eq:riemann-stieltjes-integral">[eq:riemann-stieltjes-integral]</a>)?</p>
<p>Consider the special case <span class="math inline">\(f=g\)</span>, namely, the integral:</p>
<p><span class="math display">\[\int_{a}^{b}f(t)df(t)\]</span></p>
<p>Let <span class="math inline">\(\Delta_{n}=\{a=t_{0},t_{1},\ldots,t_{n}=b\}\)</span> be a partition of <span class="math inline">\([a,b]\)</span>. Let <span class="math inline">\(L_{n}\)</span> and <span class="math inline">\(R_{n}\)</span> denote the corresponding Riemann sums with the evaluation points <span class="math inline">\(\tau_{i}=t_{i-1}\)</span> and <span class="math inline">\(\tau_{i}=t_{i}\)</span>, respectively, namely,</p>
<p><span class="math display">\[\begin{aligned}
L_{n} &amp; =\sum_{i=1}^{n}f(t_{i-1})(f(t_{i})-f(t_{i-1}))\label{eq:left-riemann-sum}\\
R_{n} &amp; =\sum_{i=1}^{n}f(t_{i})(f(t_{i})-f(t_{i-1}))\label{eq:right-riemann-sum}
\end{aligned}\]</span></p>
<p>Is it true that, <span class="math inline">\(\lim L_{n}=\lim R_{n}\)</span> as <span class="math inline">\(||\Delta_{n}||\to0\)</span>? Observe that:</p>
<p><span class="math display">\[R_{n}-L_{n}=\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\label{eq:quadratic-variation}\]</span></p>
<p><span class="math display">\[R_{n}+L_{n}=\sum_{i=1}^{n}(f(t_{i})^{2}-f(t_{i-1})^{2})=f(b)^{2}-f(a)^{2}\label{eq:sum-of-left-and-right-riemann-sums}\]</span></p>
<p>Therefore, <span class="math inline">\(R_{n}\)</span> and <span class="math inline">\(L_{n}\)</span> are given by:</p>
<p><span class="math display">\[R_{n}=\frac{1}{2}\left(f(b)^{2}-f(a)^{2}+\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\right)\]</span></p>
<p><span class="math display">\[L_{n}=\frac{1}{2}\left(f(b)^{2}-f(a)^{2}-\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\right)\]</span></p>
<p>The limit of the right-hand side of equation (<a href="#eq:quadratic-variation" data-reference-type="ref" data-reference="eq:quadratic-variation">[eq:quadratic-variation]</a>) is called the <em>quadratic variation</em> of the function <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>. Obviously, <span class="math inline">\(\lim_{||\Delta_{n}||\to0}R_{n}\neq\lim_{||\Delta_{n}||\to0}L_{n}\)</span> if and only the quadratic variation of the function <span class="math inline">\(f\)</span> is non-zero.</p>
<div class="example">
<p>Let <span class="math inline">\(f\)</span> be a <span class="math inline">\(C^{1}\)</span>-function that is <span class="math inline">\(f'(t)\)</span> is a continuous function. Then, by the mean value theorem:</p>
<p><span class="math display">\[\begin{aligned}
|R_{n}-L_{n}| &amp; =\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\\
&amp; =\sum_{i=1}^{n}(f'(t_{i}^{*})(t_{i}-t_{i-1}))^{2}\\
&amp; \quad\{\text{Mean Value Theorem}\}\\
&amp; \leq\sum_{i=1}^{n}\left\Vert f'\right\Vert _{\infty}^{2}(t_{i}-t_{i-1})^{2}\\
&amp; \quad\{\text{ Interior Extremum Theorem }\}\\
&amp; \leq\left\Vert f'\right\Vert _{\infty}^{2}\left\Vert \Delta_{n}\right\Vert \sum_{i=1}^{n}(t_{i}-t_{i-1})\\
&amp; =\left\Vert f'\right\Vert _{\infty}^{2}\left\Vert \Delta_{n}\right\Vert (b-a)
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\left\Vert f'\right\Vert _{\infty}=\sup_{x\in[a,b]}f(x)\)</span>. Thus, the limit as <span class="math inline">\(\left\Vert \Delta_{n}\right\Vert \to0\)</span> of the distance <span class="math inline">\(|R_{n}-L_{n}|\)</span> also approaches zero. Thus, <span class="math inline">\(\lim L_{n}=\lim R_{n}\)</span> as <span class="math inline">\(\left\Vert \Delta_{n}\right\Vert \to0\)</span> and the Riemann-Stieltjes integral exists. By equation (<a href="#eq:sum-of-left-and-right-riemann-sums" data-reference-type="ref" data-reference="eq:sum-of-left-and-right-riemann-sums">[eq:sum-of-left-and-right-riemann-sums]</a>), we have:</p>
<p><span class="math display">\[\lim_{\left\Vert \Delta_{n}\right\Vert \to0}L_{n}=\lim_{\left\Vert \Delta_{n}\right\Vert \to0}R_{n}=\frac{1}{2}(f(b)^{2}-f(a)^{2})\]</span></p>
<p>On the other hand, for such a <span class="math inline">\(C^{1}\)</span>-function <span class="math inline">\(f\)</span>, we may simply define the integral <span class="math inline">\(\int_{a}^{b}f(t)df(t)\)</span> by:</p>
<p><span class="math display">\[\begin{aligned}
\int_{a}^{b}f(t)df(t) &amp; =\int_{a}^{b}f(t)f'(t)dt
\end{aligned}\]</span></p>
<p>Then, by the fundamental theorem of Calculus:</p>
<p><span class="math display">\[\int_{a}^{b}f(t)df(t)=\int_{a}^{b}f(t)f'(t)dt=\frac{1}{2}f(t)^{2}|_{a}^{b}=\frac{1}{2}(f(b)^{2}-f(a)^{2})\]</span></p>
</div>
<div class="rem*">
<p>There is a very close relationship between functions with bounded variation and functions for which the classical integral makes sense. For the Ito integral, the quadratic variation plays a similar role. The quadratic variation of a smooth fuction <span class="math inline">\(f\in C^{1}([0,t])\)</span> is zero.</p>
</div>
<div class="example">
<p><span id="ex:non-zero-quadratic-variation-example" data-label="ex:non-zero-quadratic-variation-example"></span>Suppose <span class="math inline">\(f\)</span> is a continuous function satisfying the condition</p>
<p><span class="math display">\[\begin{aligned}
|f(t)-f(s)| &amp; \leq C|t-s|^{1/2}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(0&lt;C&lt;1\)</span>.</p>
<p>In this case we have:</p>
<p><span class="math display">\[0\leq|R_{n}-L_{n}|\leq C^{2}\sum_{i=1}^{n}(t_{i}-t_{i-1})=C^{2}(b-a)\]</span></p>
<p>Hence, <span class="math inline">\(\lim R_{n}\neq\lim L_{n}\)</span> as <span class="math inline">\(\left\Vert \Delta_{n}\right\Vert \to0\)</span> when <span class="math inline">\(a\neq b\)</span>. Consequently, the integral <span class="math inline">\(\int_{a}^{b}f(t)df(t)\)</span> cannot be defined for such a function <span class="math inline">\(f\)</span>. Observe that the quandratic variation of the function is <span class="math inline">\(b-a\)</span> (non-zero).</p>
</div>
<p>We see from the above examples, that definining the integral <span class="math inline">\(\int_{a}^{b}f(t)dg(t)\)</span> even when <span class="math inline">\(f=g\)</span> is a non-trivial problem. Consider the question posed earlier - if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are continuous functions on <span class="math inline">\([a,b]\)</span>, can we define the integral <span class="math inline">\(\int_{a}^{b}f(t)dg(t)\)</span>? There is no simple answer to this question. But then in view of example (<a href="#ex:non-zero-quadratic-variation-example" data-reference-type="ref" data-reference="ex:non-zero-quadratic-variation-example">[ex:non-zero-quadratic-variation-example]</a>), we can ask another question:</p>
<p><em>Question</em>. Are there continuous functions <span class="math inline">\(f\)</span> satisfying the condition</p>
<p><span class="math display">\[\begin{aligned}
|f(t)-f(s)| &amp; \leq C|t-s|^{1/2}
\end{aligned}\]</span></p>
</section>
<section id="brownian-motion-as-the-limit-of-a-symmetric-random-walk." class="level3">
<h3 class="anchored" data-anchor-id="brownian-motion-as-the-limit-of-a-symmetric-random-walk.">Brownian motion as the limit of a symmetric random walk.</h3>
<p>Consider a random walk starting at <span class="math inline">\(0\)</span> with jumps <span class="math inline">\(h\)</span> and <span class="math inline">\(-h\)</span> equally at times <span class="math inline">\(\delta\)</span>, <span class="math inline">\(2\delta\)</span>, <span class="math inline">\(\ldots\)</span> where <span class="math inline">\(h\)</span> and <span class="math inline">\(\delta\)</span> are positive numbers. More precisely, let <span class="math inline">\(\{X_{n}\}_{n=1}^{\infty}\)</span> be a sequence of independent and identically distributed random variables with :</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\{X_{j}=h\} &amp; =\mathbb{P}\{X_{j}=-h\}=\frac{1}{2}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(Y_{\delta,h}(0)=0\)</span> and put:</p>
<p><span class="math display">\[\begin{aligned}
Y_{\delta,h}(n\delta) &amp; =X_{1}+X_{2}+\ldots+X_{n}
\end{aligned}\]</span></p>
<p>For <span class="math inline">\(t&gt;0\)</span>, define <span class="math inline">\(Y_{\delta,h}(t)\)</span> by linearization that is, for <span class="math inline">\(n\delta&lt;t&lt;(n+1)\delta\)</span>, define:</p>
<p><span class="math display">\[\begin{aligned}
Y_{\delta,h}(t) &amp; =\frac{(n+1)\delta-t}{\delta}Y_{\delta,h}(n\delta)+\frac{t-n\delta}{\delta}Y_{\delta,h}((n+1)\delta)
\end{aligned}\]</span></p>
<p>We can think of <span class="math inline">\(Y_{\delta,h}(t)\)</span> as the position of the random walk at time <span class="math inline">\(t\)</span>. In particular, <span class="math inline">\(X_{1}+X_{2}+\ldots+X_{n}\)</span> is the position of this random walk at time <span class="math inline">\(n\delta\)</span>.</p>
<p><em>Question</em>. What is the limit of the random walk <span class="math inline">\(Y_{\delta,h}\)</span> as <span class="math inline">\(\delta,h\to0\)</span>?</p>
<p>Recall that the characteristic function of a random variable <span class="math inline">\(X\)</span> is <span class="math inline">\(\phi_{X}(\lambda)=\mathbb{E}\exp[i\lambda X]\)</span>. In order to find out the answer, let us compute the following limit of the characteristic function of <span class="math inline">\(Y_{\delta,h}(t)\)</span>:</p>
<p><span class="math display">\[\lim_{\delta,h\to0}\mathbb{E}\exp\left[i\lambda Y_{\delta,h}(t)\right]\]</span></p>
<p>where <span class="math inline">\(\lambda\in\mathbf{R}\)</span>is fixed. For heuristic derivation, let <span class="math inline">\(t=n\delta\)</span> and so <span class="math inline">\(n=t/\delta\)</span>. Then we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\exp\left[i\lambda Y_{\delta,h}(t)\right] &amp; =\prod_{j=1}^{n}\mathbb{E}e^{i\lambda X_{j}}\\
&amp; =\prod_{j=1}^{n}\left(\frac{1}{2}e^{i\lambda h}+\frac{1}{2}e^{-i\lambda h}\right)\\
&amp; =\left(\frac{1}{2}e^{i\lambda h}+\frac{1}{2}e^{-i\lambda h}\right)^{n}\\
&amp; =\left(\cos\lambda h\right)^{n}\\
&amp; =\left(\cos\lambda h\right)^{t/\delta}
\end{aligned}\]</span></p>
<p>For fixed <span class="math inline">\(t\)</span> and <span class="math inline">\(\lambda\)</span>, when <span class="math inline">\(\delta\)</span> and <span class="math inline">\(h\)</span> independently approach <span class="math inline">\(0\)</span>, the limit of <span class="math inline">\(\mathbb{E}\exp\left[i\lambda Y_{\delta,h}(t)\right]\)</span> may not exist. For example, holding <span class="math inline">\(h\)</span> constant, letting <span class="math inline">\(\delta\to0\)</span>, since <span class="math inline">\(-1\leq\cos\theta\leq1\)</span>, the function <span class="math inline">\(\left(\cos\lambda h\right)^{t/\delta}\to0\)</span>. Holding <span class="math inline">\(\delta\)</span> constant, letting <span class="math inline">\(h\to0\)</span>, the function <span class="math inline">\(\left(\cos\lambda h\right)^{t/\delta}\to1\)</span>. In order for the limit to exist, we impose a certain relationship between <span class="math inline">\(\delta\)</span> and <span class="math inline">\(h\)</span>. However, depending on the relationship, we may obtain different limits.</p>
<p>Let <span class="math inline">\(u=\cos(\lambda h)^{1/\delta}\)</span>. Then <span class="math inline">\(\ln u=\frac{1}{\delta}\ln\cos(\lambda h)\)</span>. Note that:</p>
<p><span class="math display">\[\begin{aligned}
\cos(\lambda h) &amp; \approx1-\frac{1}{2}\lambda^{2}h^{2}
\end{aligned}\]</span></p>
<p>And <span class="math inline">\(\ln(1+x)\approx x\)</span>. Hence,</p>
<p><span class="math display">\[\ln\cos(\lambda h)\approx\ln\left(1-\frac{1}{2}\lambda^{2}h^{2}\right)\approx-\frac{1}{2}\lambda^{2}h^{2}\]</span></p>
<p>Therefore for small <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(h\)</span>, we have <span class="math inline">\(\ln u\approx-\frac{1}{2\delta}\lambda^{2}h^{2}\)</span> and so:</p>
<p><span class="math display">\[\begin{aligned}
u &amp; \approx\exp\left[-\frac{1}{2\delta}\lambda^{2}h^{2}\right]
\end{aligned}\]</span></p>
<p>In particular, if <span class="math inline">\(\delta\)</span> and <span class="math inline">\(h\)</span> are related by <span class="math inline">\(h^{2}=\delta\)</span>, then</p>
<p><span class="math display">\[\begin{aligned}
\lim_{\delta\to0}\mathbb{E}\exp\left[i\lambda Y_{\delta,h}(t)\right] &amp; =e^{-\frac{1}{2}\lambda^{2}t}
\end{aligned}\]</span></p>
<p>But, <span class="math inline">\(e^{-\frac{1}{2}\lambda^{2}t}\)</span> is the characteristic function of a Gaussian random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(t\)</span>. Thus, we have derived the following theorem about the limit of the random walk <span class="math inline">\(Y_{\delta,h}\)</span> as <span class="math inline">\(\delta,h\to0\)</span> in such a way that <span class="math inline">\(h^{2}=\delta\)</span>.</p>
<div class="thm">
<p>Let <span class="math inline">\(Y_{\delta,h}(t)\)</span> be the random walk starting at <span class="math inline">\(0\)</span> with jumps <span class="math inline">\(h\)</span> and <span class="math inline">\(-h\)</span> equally likely at times <span class="math inline">\(\delta\)</span>, <span class="math inline">\(2\delta\)</span>, <span class="math inline">\(3\delta\)</span>, <span class="math inline">\(\ldots\)</span>. Assume that <span class="math inline">\(h^{2}=\delta\)</span>. Then, for each <span class="math inline">\(t\geq0\)</span>, the limit:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{\delta\to0}Y_{\delta,h}(t) &amp; =B(t)
\end{aligned}\]</span> exists in distribution. Moreover, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}e^{i\lambda B(t)} &amp; =e^{-\frac{1}{2}\lambda^{2}t}
\end{aligned}\]</span></p>
</div>
<div class="thm">
<p><span id="th:quadratic-variation-of-bm-approaches-t-in-mean-square" data-label="th:quadratic-variation-of-bm-approaches-t-in-mean-square"></span>(Quadratic Variation of a Brownian motion). Let <span class="math inline">\((B_{t},t\ge0)\)</span> be a standard brownian motion. Then, for any sequence of partitions <span class="math inline">\((t_{j},j\leq n)\)</span> of <span class="math inline">\([0,t]\)</span> we have:</p>
<p><span class="math display">\[\begin{aligned}
\left\langle B\right\rangle _{t} &amp; =\sum_{j=1}^{n}(B_{t_{j+1}}-B_{t_{j}})^{2}\stackrel{L^{2}}{\to}t
\end{aligned}\]</span></p>
<p>where the convergence is in the <span class="math inline">\(L^{2}\)</span> sense.</p>
</div>
<div class="rem*">
<p>It is reasonable to have some sort of convergence as we are dealing with a sum of independent random variables. However, the conclusion would not hold if the increments were not squared. So there is something more at play here.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-\sum_{j=0}^{n-1}(t_{j+1}-t_{j})\right)^{2}\right]\\
&amp; =\mathbb{E}\left[\left(\sum_{j=0}^{n-1}\left\{ (B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\right\} \right)^{2}\right]
\end{aligned}\]</span></p>
<p>For simplicity, we define the variables <span class="math inline">\(X_{j}=(B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\)</span>. Then, we may write:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =\mathbb{E}\left[\left(\sum_{j=0}^{n-1}X_{j}\right)^{2}\right]\\
&amp; =\mathbb{E}\left[\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}X_{i}X_{j}\right]\\
&amp; =\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}\mathbb{E}[X_{i}X_{j}]
\end{aligned}\]</span></p>
<p>Now, the random variables <span class="math inline">\(X_{j}\)</span> are independent.</p>
<p>The expectation of <span class="math inline">\(X_{j}\)</span> is <span class="math inline">\(\mathbb{E}[X_{j}]=\mathbb{E}(B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})=0\)</span>.</p>
<p>Since, <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span> are independent, for <span class="math inline">\(i\neq j\)</span>, <span class="math inline">\(\mathbb{E}[X_{i}X_{j}]=\mathbb{E}X_{i}\cdot\mathbb{E}X_{j}=0\)</span>.</p>
<p>Hence, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =\sum_{i=0}^{n-1}\mathbb{E}[X_{i}^{2}]
\end{aligned}\]</span></p>
<p>We now develop the expectation of the square of <span class="math inline">\(X_{i}\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[X_{i}^{2}] &amp; =\mathbb{E}\left[\left((B(t_{i+1})-B(t_{i}))^{2}-(t_{i+1}-t_{i})\right)^{2}\right]\\
&amp; =\mathbb{E}\left[((B(t_{i+1})-B(t_{i}))^{4}-2(B(t_{i+1})-B(t_{i}))^{2}(t_{i+1}-t_{i})+(t_{i+1}-t_{i})^{2}\right]
\end{aligned}\]</span></p>
<p>The MGF of the random variable <span class="math inline">\(B(t_{i+1})-B(t_{i})\)</span> is :</p>
<p><span class="math display">\[\begin{aligned}
\phi(\lambda) &amp; =\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]\\
\phi'(\lambda) &amp; =\lambda(t_{i+1}-t_{i})\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]\\
\phi''(\lambda) &amp; =\left[(t_{i+1}-t_{i})+\lambda^{2}(t_{i+1}-t_{i})^{2}\right]\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]\\
\phi^{(3)}(\lambda) &amp; =\left[3\lambda(t_{i+1}-t_{i})^{2}+\lambda^{3}(t_{i+1}-t_{i})^{3}\right]\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]\\
\phi^{(4)}(\lambda) &amp; =\left[3(t_{i+1}-t_{i})^{2}+6\lambda^{2}(t_{i+1}-t_{i})^{3}+\lambda^{4}(t_{i+1}-t_{i})^{4}\right]\exp\left[\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right]
\end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(\mathbb{E}[(B(t_{i+1})-B(t_{i}))^{4}]=3(t_{i+1}-t_{i})^{2}\)</span>. Consequently,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}[X_{i}^{2}] &amp; =\mathbb{E}[(B(t_{i+1})-B(t_{i}))^{4}]-2(t_{i+1}-t_{i})\mathbb{E}[(B(t_{i+1})-B(t_{i}))^{2}]+(t_{i+1}-t_{i})^{2}\\
&amp; =3(t_{i+1}-t_{i})^{2}-2(t_{i+1}-t_{i})^{2}+(t_{i+1}-t_{i})^{2}\\
&amp; =2(t_{i+1}-t_{i})^{2}
\end{aligned}\]</span></p>
<p>Putting all this together, we finally have that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =2\sum_{i=0}^{n-1}(t_{i+1}-t_{i})^{2}\label{eq:second-moment-of-qv}\\
&amp; \leq2\left\Vert \Delta_{n}\right\Vert \sum_{i=0}^{n-1}(t_{i+1}-t_{i})\nonumber \\
&amp; =2\left\Vert \Delta_{n}\right\Vert \cdot t\nonumber
\end{aligned}\]</span></p>
<p>As <span class="math inline">\(n\to\infty\)</span>, <span class="math inline">\(\left\Vert \Delta_{n}\right\Vert \to0\)</span>. Hence,</p>
<p><span class="math display">\[\begin{aligned}
\lim_{n\to\infty}\mathbb{E}\left[\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =0
\end{aligned}\]</span></p>
<p>Hence, the sequence of random variables</p>
<p><span class="math display">\[\begin{aligned}
\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2} &amp; \stackrel{L^{2}}{\to}t
\end{aligned}\]</span> ◻</p>
</div>
<div class="cor">
<p>(Quadratic Variation of a Brownian Motion Path). Let <span class="math inline">\((B_{s},s\geq0)\)</span> be a Brownian motion. For every <span class="math inline">\(n\in\mathbf{N}\)</span>, consider the dyadic partition <span class="math inline">\((t_{j},j\leq2^{n})\)</span> of <span class="math inline">\([0,t]\)</span> where <span class="math inline">\(t_{j}=\frac{j}{2^{n}}t\)</span>. Then we have that:</p>
<p><span class="math display">\[\begin{aligned}
\left\langle B\right\rangle _{t} &amp; =\sum_{j=1}^{2^{n}-1}(B_{t_{j+1}}-B_{t_{j}})^{2}\stackrel{a.s.}{\to}t
\end{aligned}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> We have <span class="math inline">\((t_{i+1}-t_{i})=\frac{t}{2^{n}}.\)</span> Borrowing equation (<a href="#eq:second-moment-of-qv" data-reference-type="ref" data-reference="eq:second-moment-of-qv">[eq:second-moment-of-qv]</a>) from the proof of theorem (<a href="#th:quadratic-variation-of-bm-approaches-t-in-mean-square" data-reference-type="ref" data-reference="th:quadratic-variation-of-bm-approaches-t-in-mean-square">[th:quadratic-variation-of-bm-approaches-t-in-mean-square]</a>), we have that:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[\left(\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right] &amp; =2\sum_{i=0}^{2^{n}-1}\left(\frac{t}{2^{n}}\right)^{2}\\
&amp; =2\cdot(2^{n})\cdot\frac{t^{2}}{2^{2n}}\\
&amp; =\frac{2t^{2}}{2^{n}}
\end{aligned}\]</span></p>
<p>By Chebyshev’s inequality,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}\left(\left|\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right|&gt;\epsilon\right) &amp; \leq\frac{1}{\epsilon^{2}}\mathbb{E}\left[\left(\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right]\\
&amp; \leq\frac{1}{\epsilon^{2}}\cdot\frac{2t^{2}}{2^{n}}
\end{aligned}\]</span></p>
<p>Define <span class="math inline">\(A_{n}:=\left\{ \left|\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right|&gt;\epsilon\right\}\)</span>. Since, <span class="math inline">\(\sum\frac{1}{2^{n}}\)</span> is a convergent series, any multiple of it, <span class="math inline">\((2t^{2}/\epsilon^{2})\sum\frac{1}{2^{n}}\)</span> also converges. Now, <span class="math inline">\(0\leq\mathbb{P}(A_{n})\leq\frac{(2t^{2}/\epsilon^{2})}{2^{n}}\)</span>. By the comparison test, <span class="math inline">\(\sum\mathbb{P}(A_{n})\)</span> converges to a finite value. By Theorem (<a href="#th:sufficient-condition-for-almost-sure-convergence" data-reference-type="ref" data-reference="th:sufficient-condition-for-almost-sure-convergence">[th:sufficient-condition-for-almost-sure-convergence]</a>),</p>
<p><span class="math display">\[\begin{aligned}
\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2} &amp; \stackrel{a.s.}{\to}t
\end{aligned}\]</span> ◻</p>
</div>
<p>We are now ready to show that every Brownian motion path has infinite variation.</p>
<p>If <span class="math inline">\(g\)</span> is a <span class="math inline">\(C^{1}\)</span> function,</p>
<p><span class="math display">\[\begin{aligned}
\int_{0}^{t}|g'(t)|dt &amp; =\int_{0}^{t}\sqrt{g'(t)^{2}}dt\\
&amp; \leq\int_{0}^{t}\sqrt{1+g'(t)^{2}}dt\\
&amp; =l_{g}(t)
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(l_{g}(t)\)</span> is the arclength of the function <span class="math inline">\(g\)</span> between <span class="math inline">\([0,t]\)</span>. So, <span class="math inline">\(V_{g}(t)\leq l_{g}(t)\)</span> and further:</p>
<p><span class="math display">\[\begin{aligned}
l_{g}(t) &amp; =\int_{0}^{t}\sqrt{1+g'(t)^{2}}dt\\
&amp; \leq\int_{0}^{t}\left(1+\sqrt{g'(t)^{2}}\right)dt\\
&amp; \leq t+V_{g}(t)
\end{aligned}\]</span></p>
<p>Consequently,</p>
<p><span class="math display">\[\begin{aligned}
V_{g}(t) &amp; \leq l_{g}(t)\leq t+V_{g}(t)
\end{aligned}\]</span></p>
<p>The total variation of the function is finite if and only if it’s arclength is.</p>
<p>Hence, intuitively, our claim is that a Brownian motion path on <span class="math inline">\([0,T]\)</span> has infinite arc-length. Since <span class="math inline">\(g\in C^{1}([a,b])\Longrightarrow(V_{g}(t)&lt;\infty)\)</span>, it follows that <span class="math inline">\((V_{g}(t)\to\infty)\Longrightarrow g\notin C^{1}\)</span>.</p>
<div class="cor">
<p>(Brownian Motion paths have unbounded total variation.) <span id="th:bm-paths-have-unbounded-total-variation" data-label="th:bm-paths-have-unbounded-total-variation"></span> Let <span class="math inline">\((B_{s},s\geq0)\)</span> be a Brownian motion. Then, the random functions <span class="math inline">\(B(s,\omega)\)</span> on the interval <span class="math inline">\([0,t]\)</span> have unbounded variation almost surely.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Take the sequence of dyadic partitions of <span class="math inline">\([0,t]\)</span>: <span class="math inline">\(t_{j}=\frac{j}{2^{n}}t\)</span>, <span class="math inline">\(n\in\mathbf{N}\)</span>, <span class="math inline">\(j\leq2^{n}\)</span>. By pulling out the worst increment, we have the trivial bound for every <span class="math inline">\(\omega\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\sum_{j=0}^{2^{n}-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))^{2} &amp; \leq\max_{0\leq j\leq2^{n}}\left|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)\right|\cdot\sum_{j=0}^{2^{n}-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))\label{eq:trivial-upper-bound-on-quadratic-variation}
\end{aligned}\]</span></p>
<p>We proceed by contradiction. Let <span class="math inline">\(A'\)</span> be the set of all <span class="math inline">\(\omega\)</span>, for which the Brownian motion paths have bounded total variation. Let <span class="math inline">\(A\)</span> be event that the Brownian motion paths have unbounded variation.</p>
<p>By the definition of total variation, that would imply, <span class="math inline">\(\exists M\in\mathbf{N}\)</span> :</p>
<p><span class="math display">\[\begin{aligned}
(\forall\omega\in A')\quad\lim_{n\to\infty}\sum_{j=0}^{2^{n}-1}\left|(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))\right| &amp; &lt;M
\end{aligned}\]</span></p>
<p>Since Brownian Motion paths are continuous on the compact set <span class="math inline">\([\frac{j}{2^{n}}t,\frac{j+1}{2^{n}}t]\)</span>, they are uniformly continuous. So, as <span class="math inline">\(n\to\infty\)</span>, <span class="math inline">\(|t_{j+1}-t_{j}|\to0\)</span> and therefore <span class="math inline">\(|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)|\to0\)</span>. And consequently, <span class="math inline">\(\max_{0\leq j\leq2^{n}}\left|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)\right|\to0\)</span>.</p>
<p>Thus, for every <span class="math inline">\(\omega\in A'\)</span>, the right hand side of the inequality (<a href="#eq:trivial-upper-bound-on-quadratic-variation" data-reference-type="ref" data-reference="eq:trivial-upper-bound-on-quadratic-variation">[eq:trivial-upper-bound-on-quadratic-variation]</a>), converges to <span class="math inline">\(0\)</span> and therefore the left hand side converges to <span class="math inline">\(0\)</span>. But, this contradicts the fact that <span class="math inline">\(\left\langle B\right\rangle _{t}\stackrel{a.s.}{\to}t\)</span>. So, <span class="math inline">\(A'\)</span> is a null set, and <span class="math inline">\(\mathbb{P}(A')=0\)</span> and <span class="math inline">\(\mathbb{P}(A)=1\)</span>. This closes the proof. ◻</p>
</div>
</section>
</section>
<section id="what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance" class="level2">
<h2 class="anchored" data-anchor-id="what-exactly-is-omegamathcalfmathbbp-in-mathematical-finance">What exactly is <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> in mathematical finance?</h2>
<p>If we make the simplifying assumption that the process paths are continuous, we obtain the set of all continuous functions on <span class="math inline">\([0,T]\)</span>, denoted by <span class="math inline">\(C[0,T]\)</span>. This is a very rich space. In a more general model, it is assumed that the process paths are right continuous with left limits (regular right-continuous RRC, cadlag) functions.</p>
<p>Let the sample space <span class="math inline">\(\Omega=D[0,T]\)</span> be the set of all RRC functions on <span class="math inline">\([0,T]\)</span>. An element of this set is a RRC function from <span class="math inline">\([0,T]\)</span> into <span class="math inline">\(\mathbf{R}\)</span>. First we must decide what kind of sets of these functions are measurable? The simplest set for which we would like to calculate the probabilities are sets of the form <span class="math inline">\(\{a\leq S(t_{1})\leq b\}\)</span> for some <span class="math inline">\(t_{1}\)</span>. If <span class="math inline">\(S(t)\)</span> represents the price of a stock at time <span class="math inline">\(t\)</span>, then the probability of such a set gives the probability that the stock price at time <span class="math inline">\(t_{1}\)</span> is between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. We are also interested in how the price of the stock at time <span class="math inline">\(t_{1}\)</span> affects the price at another time <span class="math inline">\(t_{2}\)</span>. Thus, we need to talk about the joint distribution of stock prices <span class="math inline">\(S(t_{1})\)</span> and <span class="math inline">\(S(t_{2})\)</span>. This means that we need to define probability on the sets of the form <span class="math inline">\(\{S(t_{1})\in B_{1},S(t_{2})\in B_{2}\}\)</span> where <span class="math inline">\(B_{1}\)</span> and <span class="math inline">\(B_{2}\)</span> are intervals on the line. More generally, we would like to have all the finite-dimensional distributions of the process <span class="math inline">\(S(t)\)</span>, that is, the probabilities of the sets: <span class="math inline">\(\{S(t_{1})\in B_{1},S(t_{2})\in B_{2},\ldots,S(t_{n})\in B_{n}\}\)</span> for any choice of <span class="math inline">\(0\leq t_{1}\leq\ldots\leq t_{n}\leq T\)</span>.</p>
<p>The sets of the form <span class="math inline">\(A=\{\omega(\cdot)\in D[0,T]:\omega(t_{1})\in B_{1},\ldots,\omega(t_{n})\in B_{n}\}\)</span>, where <span class="math inline">\(B_{i}\)</span>’s are borel subsets of <span class="math inline">\(\mathbf{R}\)</span>, are called cylinder sets or finite-dimensional rectangles.</p>
<p>The stochastic process <span class="math inline">\(S(t)\)</span> is just a (function-valued) random variable on this sample space, which takes some value <span class="math inline">\(\omega(t)\)</span> - the value of the function <span class="math inline">\(\omega\)</span> at <span class="math inline">\(t\)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{R}\)</span> be the colllection of all cylindrical subsets of <span class="math inline">\(D[0,1]\)</span>. Obviously <span class="math inline">\(\mathcal{R}\)</span> is not a <span class="math inline">\(\sigma\)</span>-field.</p>
<p>Probability is first defined by on the elements of <span class="math inline">\(\mathcal{R}\)</span>. Let <span class="math inline">\(A\subseteq\mathcal{R}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(A) &amp; =\int_{B_{1}}\cdots\int_{B_{n}}\prod_{i=1}^{n}\frac{1}{\sqrt{(2\pi)(t_{i}-t_{i-1})}}\exp\left[-\frac{(u_{i}-u_{i-1})^{2}}{2(t_{i}-t_{i-1})}\right]du_{1}\cdots du_{n}
\end{aligned}\]</span></p>
<p>and then extended to the <span class="math inline">\(\sigma\)</span>-field generated by taking unions, complements and intersections of cylinders. We take the smallest <span class="math inline">\(\sigma\)</span>-algebra containing all the cylindrical subsets of <span class="math inline">\(D[0,1]\)</span>. Thus, <span class="math inline">\(\mathcal{F}=\mathcal{B}(D[0,1])\)</span>.</p>
<p>Hence, <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})=(D[0,1],\mathcal{B}(D[0,1]),\mathbb{P})\)</span> is a probability space. It is called the <em>Wiener space</em> and <span class="math inline">\(\mathbb{P}\)</span> here is called the <em>Wiener measure</em>.</p>
</section>
<section id="continuity-and-regularity-of-paths." class="level2">
<h2 class="anchored" data-anchor-id="continuity-and-regularity-of-paths.">Continuity and Regularity of paths.</h2>
<p>As discussed in the previous section, a stochastic process is determined by its finite-dimensional distribution. In studying stochastic processes, it is often natural to think of them as function-valued random variables in <span class="math inline">\(t\)</span>. Let <span class="math inline">\(S(t)\)</span> be defined for <span class="math inline">\(0\leq t\leq T\)</span>, then for a fixed <span class="math inline">\(\omega\)</span>, it is a function in <span class="math inline">\(t\)</span>, called the sample path or a realization of <span class="math inline">\(S\)</span>. Finite-dimensional distributions do not determine the continuity property of sample paths. The following example illustrates this.</p>
<div class="example">
<p><span id="ex:modifications-of-a-stochastic-process" data-label="ex:modifications-of-a-stochastic-process"></span>Let <span class="math inline">\(X(t)=0\)</span> for all <span class="math inline">\(t\)</span>, <span class="math inline">\(0\leq t\leq1\)</span> and <span class="math inline">\(\tau\)</span> be a uniformly distributed random variable on <span class="math inline">\([0,1]\)</span>. Let <span class="math inline">\(Y(t)=0\)</span> for <span class="math inline">\(t\neq\tau\)</span> and <span class="math inline">\(Y(t)=1\)</span> if <span class="math inline">\(t=\tau.\)</span> Then, for any fixed <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbb{P}(Y(t)\neq0)=\mathbb{P}(\tau=t)=0\)</span>, and hence <span class="math inline">\(\mathbb{P}(Y(t)=0)=1\)</span>. So, that all one-dimensional distributions of <span class="math inline">\(X(t)\)</span> and <span class="math inline">\(Y(t)\)</span> are the same. Similarly, all finite-dimensional distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the same. However, the sample paths of the process <span class="math inline">\(X\)</span>, that is, the functions <span class="math inline">\(X(t)_{0\leq t\leq1}\)</span> are continuous in <span class="math inline">\(t\)</span>, whereas every sample path <span class="math inline">\(Y(t)_{0\leq t\leq1}\)</span> has a jump at the (random) point <span class="math inline">\(\tau\)</span>. Notice that, <span class="math inline">\(\mathbb{P}(X(t)=Y(t))=1\)</span> for all <span class="math inline">\(t\)</span>, <span class="math inline">\(0\leq t\leq1\)</span>.</p>
</div>
<div class="defn">
<p>Two stochastic processes are called <em>versions</em> (modifications) of one another if</p>
<p><span class="math display">\[\mathbb{P}(X(t)=Y(t))=1\quad\text{for all }0\leq t\leq T\]</span></p>
</div>
<p>Thus, the two processes in the example (<a href="#ex:modifications-of-a-stochastic-process" data-reference-type="ref" data-reference="ex:modifications-of-a-stochastic-process">[ex:modifications-of-a-stochastic-process]</a>) are versions of one another, one has continuous sample paths, the other does not. If we agree to pick any version of the process we want, then we can pick the continous version when it exists. In general, we choose the smoothest possible version of the process.</p>
<p>For two processes, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, denote by <span class="math inline">\(N_{t}=\{X(t)\neq Y(t)\}\)</span>, <span class="math inline">\(0\leq t\leq T\)</span>. In the above example, <span class="math inline">\(\mathbb{P}(N_{t})=\mathbb{P}(\tau=t)=0\)</span> for any <span class="math inline">\(t\)</span>, <span class="math inline">\(0\leq t\leq1\)</span>. However, <span class="math inline">\(\mathbb{P}(\bigcup_{0\leq t\leq1}N_{t})=\mathbb{P}(\tau=t\:\text{for some }t\:\text{in }[0,1])=1\)</span>. Although, each of <span class="math inline">\(N_{t}\)</span> is a <span class="math inline">\(\mathbb{P}\)</span>-null set, the union <span class="math inline">\(N=\bigcup_{0\leq t\leq1}N_{t}\)</span> contains uncountably many null sets, and in this particular case it is a set of of probability one.</p>
<p>If it happens that <span class="math inline">\(\mathbb{P}(N)=0\)</span>, then <span class="math inline">\(N\)</span> is called an <em>evanescent set</em>, and the processes <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are called <em>indistinguishable</em>. Note that in this case, <span class="math inline">\(\mathbb{P}(\{\omega:\exists t:X(t)\neq Y(t)\})=\mathbb{P}(\bigcup_{0\leq t\leq1}\{X(t)\neq Y(t))=0\)</span> and <span class="math inline">\(\mathbb{P}(\bigcap_{0\leq t\leq1}\{X(t)=Y(t)\})=1\)</span>. It is clear, that if the time is discrete, then any two versions of the process are indistinguishable. It is also not hard to see, that if <span class="math inline">\(X(t)\)</span> and <span class="math inline">\(Y(t)\)</span> are versions of one another and they are both right-continuous, they are indistinguishable.</p>
<div class="thm">
<p>(Paul Levy’s construction of Brownian Motion). Standard Brownian motion exists.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> I reproduce the standard proof as present in <em>Brownian Motion</em> by Morters and Peres. I added some remarks for greater clarity.</p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{D}_{n} &amp; =\left\{ \frac{k}{2^{n}}:k=0,1,2,\ldots,2^{n}\right\}
\end{aligned}\]</span></p>
<p>be a finite set of dyadic points.</p>
<p>Let</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{D} &amp; =\bigcup_{n=0}^{\infty}\mathcal{D}_{n}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(\{Z_{t}:t\in\mathcal{D}\}\)</span> be a collection of independent, standard normally distributed random variables. This is a countable set of random variables.</p>
<p>Let <span class="math inline">\(B(0):=0\)</span> and <span class="math inline">\(B(1):=Z_{1}\)</span>.</p>
<p>For each <span class="math inline">\(n\in\mathbf{N}\)</span>, we define the random variables <span class="math inline">\(B(d)\)</span>, <span class="math inline">\(d\in\mathcal{D}_{n}\)</span> such that, the following invariant holds:</p>
<p>(1) for all <span class="math inline">\(r&lt;s&lt;t\)</span> in <span class="math inline">\(\mathcal{D}_{n}\)</span> the random variable <span class="math inline">\(B(t)-B(s)\)</span> is normally distributed with mean zero and variance <span class="math inline">\(t-s\)</span> and is independent of <span class="math inline">\(B(s)-B(r)\)</span>.</p>
<p>(2) the vectors <span class="math inline">\((B(d):d\in\mathcal{D}_{n})\)</span> and <span class="math inline">\((Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n})\)</span> are independent.</p>
<p>Note that we have already done this for <span class="math inline">\(\mathcal{D}_{0}=\{0,1\}\)</span>. Proceeding inductively, let’s assume that the above holds for some <span class="math inline">\(n-1\)</span>. We are interested to prove that the invariant also holds for <span class="math inline">\(n\)</span>.</p>
<p>We define <span class="math inline">\(B(d)\)</span> for <span class="math inline">\(d\in\mathcal{D}_{n}\backslash\mathcal{D}_{n-1}\)</span> by:</p>
<p><span class="math display">\[\begin{aligned}
B(d) &amp; =\frac{B(d-2^{-n})+B(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>Note that, the points <span class="math inline">\(0,\frac{1}{2^{n-1}},\ldots,\frac{k}{2^{n-1}},\frac{k+1}{2^{n-1}},\ldots,1\)</span> belong to <span class="math inline">\(\mathcal{D}_{n-1}\)</span>. The first summand is the linear interpolation of the values of <span class="math inline">\(B\)</span> at the neighbouring points of <span class="math inline">\(d\)</span> in <span class="math inline">\(\mathcal{D}_{n-1}\)</span>. That is,</p>
<p><span class="math display">\[\begin{aligned}
B\left(\frac{2k+1}{2^{n}}\right) &amp; =\frac{B\left(\frac{k}{2^{n-1}}\right)+B\left(\frac{k+1}{2^{n-1}}\right)}{2}+\frac{Z_{d}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(P(n-1)\)</span> holds, <span class="math inline">\(B(d-2^{-n})\)</span> and <span class="math inline">\(B(d+2^{-n})\)</span> are have no dependence on <span class="math inline">\((Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n-1})\)</span>. Consequently, <span class="math inline">\(B(d)\)</span> has no dependence on <span class="math inline">\((Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n})\)</span> and the second property is fulfilled.</p>
<p>Moreover, as <span class="math inline">\(\frac{1}{2}[B(d+2^{-n})-B(d-2^{-n})]\)</span> depends only on <span class="math inline">\((Z_{t}:t\in\mathcal{D}_{n-1})\)</span>, it is independent of <span class="math inline">\(\frac{Z_{d}}{2^{(n+1)/2}}\)</span>. By our induction assumptions, they are both nromally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\frac{1}{2^{(n+1)}}\)</span>.</p>
<p>So, their sum and difference random variables</p>
<p><span class="math display">\[\begin{aligned}
B(d)-B(d-2^{-n}) &amp; =\frac{B(d+2^{-n})-B(d-2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\\
B(d+2^{-n})-B(d) &amp; =\frac{B(d+2^{-n})-B(d-2^{-n})}{2}-\frac{Z_{d}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>are also independent, with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\frac{1}{2^{n}}\)</span> (the variance of independent random variables is the sum of the variances).</p>
<p>Indeed all increments <span class="math inline">\(B(d)-B(d-2^{-n})\)</span> for <span class="math inline">\(d\in\mathcal{D}_{n}\setminus\{0\}\)</span> are independent. To see this, it suffices to show that they are pairwise independent. We have seen in the previous paragraph that the pairs <span class="math inline">\(B(d)-B(d-2^{-n})\)</span> and <span class="math inline">\(B(d+2^{-n})-B(d)\)</span> with <span class="math inline">\(d\in\mathcal{D}_{n}\setminus\mathcal{D}_{n-1}\)</span> are independent. The other possibility is that the increments are over the intervals separated by some <span class="math inline">\(d\in\mathcal{D}_{n-1}\)</span>. For concreteness, if <span class="math inline">\(n\)</span> were <span class="math inline">\(3\)</span>, then the increments, <span class="math inline">\(B_{7/8}-B_{6/8}\)</span> and <span class="math inline">\(B_{5/8}-B_{4/8}\)</span> are seperated by <span class="math inline">\(d=\frac{3}{4}\in\mathcal{D}_{2}\)</span>. Choose <span class="math inline">\(d\in\mathcal{D}_{j}\)</span> with this property and minimal <span class="math inline">\(j\)</span>, so, the two intervals are contained in <span class="math inline">\([d-2^{-j},d]\)</span> and <span class="math inline">\([d,d+2^{-j}]\)</span> respectively. By induction, the increments over these two intervals of length <span class="math inline">\(2^{-j}\)</span> are independent and the increments over the intervals of length <span class="math inline">\(2^{-n}\)</span> are constructed from the independent increments <span class="math inline">\(B(d)-B(d-2^{-j})\)</span> and <span class="math inline">\(B(d+2^{-j})-B(d)\)</span> using a disjoint set of variables <span class="math inline">\((Z_{t}:t\in\mathcal{D}_{n})\)</span>. Hence, they are independent and this implies pairwise independence. This implies the first property. Consequently, the vector of increments <span class="math inline">\((B(d)-B(d-2^{-n})\)</span> for all <span class="math inline">\(d\in\mathcal{D}_{n}\)</span> is Gaussian.</p>
<p>Having thus chosen the value of the process on all the dyadic points, we interpolate between them. Formally, we define:</p>
<p><span class="math display">\[\begin{aligned}
F_{0}(t) &amp; =\begin{cases}
Z_{1} &amp; \text{for }t=1\\
0 &amp; \text{for }t=0\\
\text{\text{linear in between}}
\end{cases}
\end{aligned}\]</span></p>
<p>and for each <span class="math inline">\(n\geq1\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
F_{n}(t) &amp; =\begin{cases}
\frac{Z_{t}}{2^{(n+1)/2}} &amp; \text{for }t\in\mathcal{D}\setminus\mathcal{D}_{n-1}\\
0 &amp; \text{for }t\in\mathcal{D}_{n-1}\\
\text{\text{linear between consecutive points in }\ensuremath{\mathcal{D}_{n}}}
\end{cases}
\end{aligned}\]</span></p>
<p>These functions are continuous on <span class="math inline">\([0,1]\)</span> and for all <span class="math inline">\(n\)</span> and <span class="math inline">\(d\in\mathcal{D}_{n}\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
B(d) &amp; =\sum_{i=0}^{n}F_{i}(d)=\sum_{i=0}^{\infty}F_{i}(d)\label{eq:claim-of-induction-for-bd}
\end{aligned}\]</span></p>
<p>To see this, assume that above equation holds for all <span class="math inline">\(d\in\mathcal{D}_{n-1}\)</span>.</p>
<p>Let’s consider the point <span class="math inline">\(d\in\mathcal{D}_{n}\setminus\mathcal{D}_{n-1}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
B(d) &amp; =\frac{B(d-2^{-n})+B(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\nonumber \\
&amp; =\sum_{i=0}^{n-1}\frac{F_{i}(d-2^{-n})+F_{i}(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\label{eq:expression-for-bd}
\end{aligned}\]</span></p>
<p>Now, <span class="math inline">\(d-2^{-n}\)</span> and <span class="math inline">\(d+2^{-n}\)</span> belong to <span class="math inline">\(\mathcal{D}_{n-1}\)</span> and are not in <span class="math inline">\(\bigcup_{i&lt;n-1}\mathcal{D}_{i}\)</span>. Therefore, for <span class="math inline">\(i=0,1,\ldots,n-2\)</span>, the points <span class="math inline">\((d-2^{-n},F_{i}(d-2^{-n}))\)</span> and <span class="math inline">\((d+2^{-n},F_{i}(d+2^{-n})\)</span> lie on some straight line and have <span class="math inline">\((d,F_{i}(d))\)</span> as their midpoint. Moreover, <span class="math inline">\(d-2^{-n}\)</span> and <span class="math inline">\(d+2^{-n}\)</span> are vertices in <span class="math inline">\(\mathcal{D}_{n-1}\)</span>. So, by definition of <span class="math inline">\(F_{n-1}(d)\)</span>, we have <span class="math inline">\(F_{n-1}(d)=[F_{n-1}(d-2^{-n})+F_{n-1}(d+2^{-n})]/2\)</span>.</p>
<p>To summarize, the first term on the right hand side of expression (<a href="#eq:expression-for-bd" data-reference-type="ref" data-reference="eq:expression-for-bd">[eq:expression-for-bd]</a>) is equal to <span class="math inline">\(\sum_{i=0}^{n-1}F_{i}(d)\)</span>. By mathematical induction, it follows that the claim (<a href="#eq:claim-of-induction-for-bd" data-reference-type="ref" data-reference="eq:claim-of-induction-for-bd">[eq:claim-of-induction-for-bd]</a>) is true for all <span class="math inline">\(n\in\mathbf{N}\)</span>.</p>
<p>It’s extremely easy to find an upper bound on the probability contained in the Gaussian tails. Suppose <span class="math inline">\(X\sim N(0,1)\)</span> and let <span class="math inline">\(x&gt;0\)</span>. We are interested in the tail probability <span class="math inline">\(\mathbb{P}(X&gt;x)\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(X&gt;x) &amp; =\int_{x}^{\infty}e^{-x^{2}/2}dx=\int_{x}^{\infty}\frac{xe^{-x^{2}/2}dx}{x}
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(u=\frac{1}{x}\)</span> and <span class="math inline">\(dv=xe^{-x^{2}/2}dx\)</span>. We have:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(u=\frac{1}{x}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(dv=xe^{-x^{2}/2}dx\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(du=-\frac{1}{x^{2}}dx\)</span></td>
<td style="text-align: center;"><span class="math inline">\(v=-e^{-x^{2}/2}\)</span></td>
</tr>
</tbody>
</table>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(X&gt;x) &amp; =-\left.\frac{1}{x}e^{-x^{2}/2}\right|_{x}^{\infty}-\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}dx\\
&amp; =\frac{e^{-x^{2}/2}}{x}-\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}dx\\
&amp; \quad\left\{ I(x)=\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}\geq0\right\} \\
&amp; \leq\frac{e^{-x^{2}/2}}{x}
\end{aligned}\]</span></p>
<p>Thus, for <span class="math inline">\(c&gt;1\)</span> and large <span class="math inline">\(n\)</span>, we have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(|Z_{d}|\geq c\sqrt{n}) &amp; \leq\frac{1}{c\sqrt{n}}e^{-c^{2}n/2}\leq\exp\left(-\frac{c^{2}n}{2}\right)
\end{aligned}\]</span></p>
<p>So, the series:</p>
<p><span class="math display">\[\begin{aligned}
\sum_{n=0}^{\infty}\mathbb{P}\left\{ \text{There exists atleast one }d\in\mathcal{D}_{n}\text{ with }|Z_{d}|\geq c\sqrt{n}\right\}  &amp; \leq\sum_{n=0}^{\infty}\sum_{d\in\mathcal{D}_{n}}\mathbb{P}\left\{ |Z_{d}|\geq c\sqrt{n}\right\} \\
&amp; \leq\sum_{n=0}^{\infty}(2^{n}+1)\exp\left(-\frac{c^{2}n}{2}\right)
\end{aligned}\]</span></p>
<p>Now, the series <span class="math inline">\((a_{n})\)</span> given by, <span class="math inline">\(a_{n}:=(2^{n}+1)e^{-c^{2}n/2}\)</span> has the ratio between successive terms:</p>
<p><span class="math display">\[\begin{aligned}
\lim\left|\frac{a_{n+1}}{a_{n}}\right| &amp; =\lim_{n\to\infty}\frac{2^{n+1}+1}{2^{n}+1}\cdot\frac{e^{(c^{2}n)/2}}{e^{c^{2}(n+1)/2}}\\
&amp; =\lim_{n\to\infty}\frac{\frac{1}{2}+\frac{1}{2^{n}}}{1+\frac{1}{2^{n}}}\cdot\frac{1}{e^{c^{2}/2}}\\
&amp; =\frac{1}{2e^{c^{2}/2}}
\end{aligned}\]</span></p>
<p>If this ratio is less than unity, that is <span class="math inline">\(c&gt;\sqrt{2\log2}\)</span>, than by the ratio test, <span class="math inline">\(\sum(2^{n}+1)e^{-c^{2}n/2}\)</span> converges to a finite value. Fix such a <span class="math inline">\(c\)</span>.</p>
<p>By BCL1(Borel-Cantelli Lemma), if <span class="math inline">\(A_{n}:=\left\{ \text{There exists atleast one }d\in\mathcal{D}_{n}\text{ with }|Z_{d}|\geq c\sqrt{n}\right\}\)</span> and <span class="math inline">\(\sum_{n=0}^{\infty}\mathbb{P}(A_{n})\)</span> converges to a finite value, then the event <span class="math inline">\(A_{n}\)</span> occurs finitely many times with probability <span class="math inline">\(1\)</span>. There exists <span class="math inline">\(N\in\mathbf{N}\)</span>, such that for all <span class="math inline">\(n\geq N\)</span>, <span class="math inline">\(A_{n}\)</span> fails to occur with probability <span class="math inline">\(1\)</span>. Thus, for all <span class="math inline">\(n\geq N\)</span>, <span class="math inline">\(\{Z_{d}\leq c\sqrt{n}\}\)</span> occurs with probability <span class="math inline">\(1\)</span>. It follows that:</p>
<p><span class="math display">\[\begin{aligned}
\sup_{t\in[0,1]}F_{n}(t) &amp; \leq\frac{c\sqrt{n}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>Define</p>
<p><span class="math display">\[\begin{aligned}
M_{n} &amp; =\frac{c\sqrt{n}}{2^{(n+1)/2}}
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\(\sum M_{n}\)</span> converges, by the Weierstrass <span class="math inline">\(M\)</span>-test, the infinite series of functions <span class="math inline">\(\sum_{n=0}^{\infty}F_{n}(t)\)</span> converges uniformly on <span class="math inline">\([0,1].\)</span> Since, each <span class="math inline">\(F_{n}(t)\)</span> is piecewise linear and continuous, by the Term-by-Term continuity theorem, <span class="math inline">\(\sum_{n=0}^{\infty}F_{n}(t)\)</span> is continuous on <span class="math inline">\([0,1]\)</span>. ◻</p>
</div>
</section>
<section id="a-point-of-comparison-the-poisson-process." class="level2">
<h2 class="anchored" data-anchor-id="a-point-of-comparison-the-poisson-process.">A point of comparison: The Poisson Process.</h2>
<p>Like the Brownian motion, the Poisson process is defined as a process with stationary and independent increments.</p>
<div class="defn">
<p><span id="def:poisson-process" data-label="def:poisson-process"></span> A process <span class="math inline">\((N_{t},t\geq0)\)</span> defined on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> has the distribution of the Poisson process with rate <span class="math inline">\(\lambda&gt;0\)</span>, if and only if the following hold:</p>
<p>(1) <span class="math inline">\(N_{0}=0\)</span>.</p>
<p>(2) For any <span class="math inline">\(s&lt;t\)</span>, the increment <span class="math inline">\(N_{t}-N_{s}\)</span> is a Poisson random variable with parameter <span class="math inline">\(\lambda(t-s).\)</span></p>
<p>(3) For any <span class="math inline">\(n\in\mathbf{N}\)</span> and any choice <span class="math inline">\(0&lt;t_{1}&lt;t_{2}&lt;\ldots&lt;t_{n}&lt;\infty\)</span>, the increments <span class="math inline">\(N_{t_{2}}-N_{t_{1}},N_{t_{3}}-N_{t_{2}},\ldots,N_{t_{n}}-N_{t_{n-1}}\)</span> are independent.</p>
</div>
<p>Poisson paths can be sampled using this definition. By construction, it is not hard to see that the paths of Poisson processes are piecewise, constant, integer-valued and non-decreasing. In particular, the paths of Poisson processes have finite variation. Poisson paths are much simpler than the ones of Brownian motion in many ways!</p>
<div class="example">
<p>(Simulating the Poisson Process.) Use the definition (<a href="#def:poisson-process" data-reference-type="ref" data-reference="def:poisson-process">[def:poisson-process]</a>) to generate <span class="math inline">\(10\)</span> paths of the Poisson process with rate <span class="math inline">\(1\)</span> on the interval <span class="math inline">\([0,10]\)</span> with step-size <span class="math inline">\(0.01\)</span>.</p>
</div>
<pre data-caption="Generating 10 paths of a Poisson process"><code>def generatePoissonProcess(lam,T,stepSize):
    N = int(T/stepSize)
    x = np.random.poisson(lam=lam,size=N)
    y = np.cumsum(x)
    y = np.concatenate([[0.0],y])
    return y</code></pre>
<p>We can construct a Poisson process as follows. Consider <span class="math inline">\((\tau_{j},j\in\mathbf{N})\)</span> IID exponential random variables with parameter <span class="math inline">\(1/\lambda\)</span>. One should think of <span class="math inline">\(\tau_{j}\)</span> as the waiting time from the <span class="math inline">\((j-1)\)</span>st to the <span class="math inline">\(j\)</span>th jump. Then, one defines :</p>
<p><span class="math display">\[\begin{aligned}
N_{t} &amp; =\#\{k:\tau_{1}+\tau_{2}+\ldots+\tau_{k}\leq t\}\\
&amp; =\text{Number of jumps upto and including time }t
\end{aligned}\]</span></p>
<p>Now, here is an idea! What about defining a new process with stationary and independent increments using a given distribution other than Poisson and Gaussian? Is this even possible? The answer is yes, but only if the distribution satisfies the property of being <em>infinitely divisible</em>. To see this, consider the value of the process at time <span class="math inline">\(1\)</span>, <span class="math inline">\(N_{1}\)</span>. Then, no matter how many subintervals we chop the interval <span class="math inline">\([0,1]\)</span> into, we must have the increments add up to <span class="math inline">\(N_{1}\)</span>. In other words, we must be able to write <span class="math inline">\(N_{1}\)</span> as a sum of <span class="math inline">\(n\)</span> IID random variables for every possible <span class="math inline">\(n\)</span>. This is certainly true for Poisson random variables and Gaussian random variables. Another example is the Cauchy distribution. In general, processes that can be constructed using independent, stationary increments are called Levy processes.</p>
<div class="example">
<p><strong>Time Inversion.</strong> Let <span class="math inline">\((B_{t},t\geq0)\)</span> be a standard brownian motion. We consider the process:</p>
<p><span class="math display">\[\begin{aligned}
X_{t} &amp; =tB_{1/t}\quad\text{for }t&gt;0
\end{aligned}\]</span></p>
<p><em>This property relates the behavior of <span class="math inline">\(t\)</span> large to the behavior of <span class="math inline">\(t\)</span> small.</em></p>
</div>
<p>(a) Show that <span class="math inline">\((X_{t},t&gt;0)\)</span> has the distribution of Brownian motion on <span class="math inline">\(t&gt;0\)</span>.</p>
<p><em>Proof.</em></p>
<p>Like <span class="math inline">\(B(t)\)</span>, it is an easy exercise to prove that <span class="math inline">\(X(t)\)</span> is also a Gaussian process.</p>
<p>We have, <span class="math inline">\(\mathbb{E}[X_{s}]=0\)</span>.</p>
<p>Let <span class="math inline">\(s&lt;t\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
Cov(X_{s},X_{t}) &amp; =\mathbb{E}[sB(1/s)\cdot tB(1/t)]\\
&amp; =st\mathbb{E}[B(1/s)\cdot B(1/t)]\\
&amp; =st\cdot\frac{1}{t}\\
&amp; \quad\left\{ \because\frac{1}{t}&lt;\frac{1}{s}\right\} \\
&amp; =s
\end{aligned}\]</span></p>
<p>Consequently, <span class="math inline">\(X(t)\)</span> has the distribution of a Brownian motion.</p>
<p>(b) Argue that <span class="math inline">\(X(t)\)</span> converges to <span class="math inline">\(0\)</span> as <span class="math inline">\(t\to0\)</span> in the sense of <span class="math inline">\(L^{2}\)</span>-convergence. It is possible to show convergence almost surely so that <span class="math inline">\((X_{t},t\geq0)\)</span> is really a Brownian motion for <span class="math inline">\(t\geq0\)</span>.</p>
<p><em>Solution</em>.</p>
<p>Let <span class="math inline">\((t_{n})\)</span> be any arbitrary sequence of positive real numbers approaching <span class="math inline">\(0\)</span> and consider the sequence of random variables <span class="math inline">\((X(t_{n}))_{n=1}^{\infty}\)</span>. We have:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{E}\left[X(t_{n})^{2}\right] &amp; =\mathbb{E}\left[t_{n}^{2}B(1/t_{n})^{2}\right]\\
&amp; =t_{n}^{2}\mathbb{E}\left[B(1/t_{n})^{2}\right]\\
&amp; =t_{n}^{2}\cdot\frac{1}{t_{n}}\\
&amp; =t_{n}
\end{aligned}\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[\begin{aligned}
\lim\mathbb{E}\left[X(t_{n})^{2}\right] &amp; =\lim t_{n}=0
\end{aligned}\]</span></p>
<p>Since <span class="math inline">\((t_{n})\)</span> was an arbitrary sequence, it follows that <span class="math inline">\(\lim_{t\to0}\mathbb{E}[(X(t))^{2}]=0\)</span>.</p>
<p>(c) Use this property of Brownian motion to show the law of large numbers for Brownian motion:</p>
<p><span class="math display">\[\begin{aligned}
\lim_{t\to\infty}\frac{X(t)}{t} &amp; =0\quad\text{almost surely}
\end{aligned}\]</span></p>
<p><em>Solution.</em></p>
<p>What we need to do is to show that <span class="math inline">\(X(t)\to0\)</span> as <span class="math inline">\(t\to0\)</span> almost surely. That would show that <span class="math inline">\(\frac{B(1/t)}{1/t}\to0\)</span> as <span class="math inline">\(t\to0\)</span> almost surely, which is the same as showing <span class="math inline">\(\frac{B(t)}{t}\to0\)</span> as <span class="math inline">\(t\to\infty\)</span>, which is the law of large numbers for Brownian motion.</p>
<p>What we have done in part (b), is to prove the claim that <span class="math inline">\(\mathbb{E}[X(t)^{2}]\to0\)</span> as <span class="math inline">\(t\to0\)</span>, which shows convergence in the <span class="math inline">\(L^{2}\)</span> sense and hence convergence in probability. This is infact the weak law of large numbers. <span class="math inline">\(\frac{B(t)}{t}\stackrel{\mathbb{\mathbf{P}}}{\to}0\)</span> as <span class="math inline">\(t\to\infty\)</span>.</p>
<p>For <span class="math inline">\(t&gt;0\)</span>, continuity is clear. However, it is the proof that as <span class="math inline">\(t\to0\)</span>, <span class="math inline">\(X(t)\to0\)</span> almost surely which we have not done.</p>
<p>Note that, the limit <span class="math inline">\(X(t)\to0\)</span> as <span class="math inline">\(t\to0\)</span> if and only if <span class="math inline">\((\forall n\geq1)\)</span>, <span class="math inline">\((\exists m\geq1)\)</span>, such that <span class="math inline">\(\forall r\in\mathbb{Q}\cap(0,\frac{1}{m}]\)</span>, we have <span class="math inline">\(|X(r)|=\left|rB\left(\frac{1}{r}\right)\right|\leq\frac{1}{n}\)</span>.</p>
<p>To understand the above, we just recall the <span class="math inline">\(\epsilon-\delta\)</span> definition of continuity. Note that <span class="math inline">\(\frac{1}{n}\)</span> plays the role of <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\frac{1}{m}\)</span> works as <span class="math inline">\(\delta\)</span>.</p>
<p>That is,</p>
<p><span class="math display">\[\begin{aligned}
\Omega^{X}:=\left\{ \lim_{t\to0}X(t)=0\right\}  &amp; =\bigcap_{n\geq1}\bigcup_{m\geq1}\bigcap_{r\in\mathbb{Q}\cap(0,\frac{1}{m}]}\left\{ \left|X(r)\right|\leq\frac{1}{n}\right\}
\end{aligned}\]</span></p>
<p>Also, note that <span class="math inline">\(X(t)\)</span> is continuous on all <span class="math inline">\([a,1]\)</span> for all <span class="math inline">\(a&gt;0\)</span>, thus, uniformly continuous on <span class="math inline">\([a,1]\)</span>, and hence uniformly continuous on <span class="math inline">\(\mathbb{Q}\cap(0,1]\)</span>. So, there exists a continuous extension of <span class="math inline">\(X(t)\)</span> on <span class="math inline">\([0,1]\)</span>. We already know from part (a), that <span class="math inline">\((X(t))_{t&gt;0}\)</span> and <span class="math inline">\((B(t))_{t&gt;0}\)</span> have the same finite dimensional distributions. Therefore, the RHS event has the same probability as <span class="math inline">\(\Omega^{B}:=\bigcap_{n\geq1}\bigcup_{m\geq1}\bigcap_{r\in\mathbb{Q}\cap(0,\frac{1}{m}]}\left\{ \left|B(r)\right|\leq\frac{1}{n}\right\}\)</span>. Since <span class="math inline">\(B(t)\to0\)</span> as <span class="math inline">\(t\to0\)</span> almost surely, the event <span class="math inline">\(\Omega^{B}\)</span> has probability <span class="math inline">\(1\)</span>. Thus, <span class="math inline">\(\mathbb{P}\left\{ \lim_{t\to0}X(t)=0\right\} =1\)</span>.</p>
<p>This actually shows that <span class="math inline">\(X(t)\)</span> is a bonafide standard brownian motion, as we have established continuity as well.</p>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="references" class="level1 appendix"><h2 class="anchored quarto-appendix-heading">References</h2><div class="quarto-appendix-contents">

<ul>
<li><em><a href="https://www.amazon.co.uk/Introduction-Stochastic-Calculus-Applications-3Rd/dp/1848168322">Introduction to Stochastic Calculus with Applications</a>, Fima C Klebaner</em></li>
<li><em><a href="https://www.amazon.co.uk/Brownian-Motion-Calculus-Ubbo-Wiersema/dp/0470021705">Brownian Motion Calculus</a>, Ubbo Wiersema</em></li>
</ul>


<!-- -->

</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="quasar-chunawala/quantdev" data-repo-id="R_kgDOL2t5-A" data-category="General" data-category-id="DIC_kwDOL2t5-M4ClndQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Properties of Brownian Motion"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Quasar"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-04-27"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Stochastic Calculus]      </span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "image.jpg"</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu"># Properties of Brownian Motion.</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of Brownian Motion.</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>Let $B(t)$ be a fixed Brownian motion. We give below some simple</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>properties that follow directly from the definition of the Brownian Motion.</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>::: prop</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>For any $t\geq0$, $B(t)$ is normally distributed with mean $0$ and</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>variance $t$. For any $s,t\geq0$ we have</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}(B_{s}B_{t})=\min<span class="sc">\{</span>s,t<span class="sc">\}</span>$.</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>*Proof.* From condition (1), we have that $B_{0}=0$. From condition (2),</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>$B_{t}-B_{0}=B_{t}$ is normally distributed with mean $0$ and variance</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>$t$.</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>Assume that $s&lt;t$.</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>We have:</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(B_{s}B_{t}) &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">B_{s}(B_{t}-B_{s}+B_{s})\right</span><span class="co">]</span> &amp; <span class="sc">\{</span>\text{Write }B_{t}=B_{t}-B_{s}+B_{s}<span class="sc">\}\\</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">B_{s}(B_{t}-B_{s})</span><span class="co">]</span>+\mathbb{E}<span class="co">[</span><span class="ot">B_{s}^{2}</span><span class="co">]</span> &amp; <span class="sc">\{</span>\text{Linearity of expectations}<span class="sc">\}\\</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">B_{s}</span><span class="co">]</span>\mathbb{E}(B_{t}-B_{s})+s &amp; <span class="sc">\{</span>B_{s},(B_{t}-B_{s})\text{ are independent}<span class="sc">\}\\</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a> &amp; =0\cdot0+s<span class="sc">\\</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a> &amp; =s</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>This closes the proof. ◻</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>::: prop</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>(Translation Invariance) For fixed $t_{0}\geq0$, the stochastic process</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t)=B(t+t_{0})-B(t_{0})$ is also a Brownian motion.</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>*Proof.* Firstly, the stochastic process $\tilde{B}(t)$ is such that:</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>1<span class="sc">\)</span> $\tilde{B}(0)=B(t_{0})-B(t_{0})=0$. Hence, it satisfies condition</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>(1).</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>2<span class="sc">\)</span> Let $s&lt;t$. We have:</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t)-\tilde{B}(s)=B(t+t_{0})-B(s+t_{0})$ which a Gaussian</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>random variable with mean 0 and variance $t-s$. Hence, for $a\leq b$,</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>\mathbb{P}<span class="sc">\{</span>a\leq &amp; \tilde{B}(t)\leq b<span class="sc">\}</span>=\frac{1}{\sqrt{2\pi(t-s)}}\int_{a}^{b}e^{-\frac{x^{2}}{2(t-s)}}dx</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>Hence, it satisfies condition (2).</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>3<span class="sc">\)</span> To check condition (3) for $\tilde{B}(t)$, we may assume</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>$t_{0}&gt;0$. Then, for any $0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}$, we</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>have:</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>$$0&lt;t_{0}\leq t_{0}+t_{1}\leq t_{0}+t_{2}\leq\ldots\leq t_{0}+t_{n}$$</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>So, $B(t_{1}+t_{0})-B(t_{0})$, $B(t_{2}+t_{0})-B(t_{1}+t_{0})$,</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>$\ldots$, $B(t_{k}+t_{0})-B(t_{k-1}+t_{0})$, $\ldots$,</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>$B(t_{n}+t_{0})-B(t_{n-1}+t_{0})$ are independent random variables.</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>Consequently, $\tilde{B}(t)$ satisfies condition (3).</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>This closes the proof. ◻</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>The above translation invariance property says that a Brownian motion</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>starts afresh at any moment as a new Brownian motion.</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>::: prop</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>(Scaling Invariance) For any real number $\lambda&gt;0$, the stochastic</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>process $\tilde{B}(t)=B(\lambda t)/\sqrt{\lambda}$ is also a Brownian</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>motion.</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>*Proof.* The scaled stochastic process $\tilde{B}(t)$ is such that:</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>1<span class="sc">\)</span> $\tilde{B}(0)=0$. Hence it satisfies condition (1).</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>2<span class="sc">\)</span> Let $s&lt;t$. Then, $\lambda s&lt;\lambda t$. We have:</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>\tilde{B}(t)-\tilde{B}(s) &amp; =\frac{1}{\sqrt{\lambda}}(B(\lambda t)-B(\lambda s))</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>Now, $B(\lambda t)-B(\lambda s)$ is a Gaussian random variable with mean</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>$0$ and variance $\lambda(t-s)$. We know that, if $X$ is a random</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>variable with mean $\mu$ and variance $\sigma^{2}$,</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>$Z=\left(\frac{X-\mu}{\sigma}\right)$ has mean $0$ and variance $1$.</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>Consequently, $\frac{B(\lambda t)-B(\lambda s)}{\sqrt{\lambda}}$ is a</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>Gaussian random variable with mean $0$ and variance $(t-s)$.</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>Hence, $\tilde{B}(t)-\tilde{B}(s)$ is normal distributed with mean $0$</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>and variance $t-s$ and it satisfies condition (2).</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>3<span class="sc">\)</span> To check condition (3) for $\tilde{B}(t)$, we may assume</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>$t_{0}&gt;0$. Then, for any $0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}$, we</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>have:</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>$$0\leq\lambda t_{1}\leq\lambda t_{2}\leq\ldots\leq\lambda t_{n}$$</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>Consequently, the random variables</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>$B(\lambda t_{k})-B(\lambda t_{k-1})$, $k=1,2,3,\ldots,n$ are</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>independent. Hence it follows that</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>$\frac{1}{\sqrt{\lambda}}<span class="co">[</span><span class="ot">B(\lambda t_{k})-B(\lambda t_{k-1})</span><span class="co">]</span>$ for</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>$k=1,2,\ldots,n$ are also independent random variables.</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>This closes the proof. ◻</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>It follows from the scaling invariance property that for any $\lambda&gt;0$</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>and $0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}$, the random vectors:</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>$$(B(\lambda t_{1}),B(\lambda t_{2}),\ldots,B(\lambda t_{n}))\quad(\sqrt{\lambda}B(t_{1}),\sqrt{\lambda}B(t_{1}),\ldots,\sqrt{\lambda}B(t_{n}))$$</span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>have the same distribution.</span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>The scaling property shows that Brownian motion is *self-similar*, much like a fractal. To see this, suppose we zoom into a Brownian motion path very close to zero, say on the interval $<span class="co">[</span><span class="ot">0,10^{-6}</span><span class="co">]</span>$. If the Brownian motion path were smooth and differentiable, the closer we zoom in around the origin, the flatter the function will look. In the limit, we would</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>essentially see a straight line given by the derivative at $0$. However, what we see with the Brownian motion is very different. The scaling property means that for $a=10^{-6}$,</span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>(B_{10^{-6}t,}t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>) &amp; \stackrel{\text{distrib.}}{=}(10^{-3}B_{t},t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>)</span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>where $\stackrel{\text{distrib.}}{=}$ means equality of the distribution</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>of the two processes. In other words, Brownian motion on $<span class="co">[</span><span class="ot">0,10^{-6}</span><span class="co">]</span>$</span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>looks like a Browian motion on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$, but with its amplitude</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>multiplied by a factor of $10^{-3}$. In particular, it will remain</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a>rugged as we zoom in, unlike a smooth function.</span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>::: prop</span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>[]{#prop:brownian-motion-symmetry-of-reflection-at-time-s</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>label="prop:brownian-motion-symmetry-of-reflection-at-time-s"}(Reflection</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>at time $s$) The process $(-B_{t},t\geq0)$ is a Brownian motion. More</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>generally, for any $s\geq0$, the process $(\tilde{B}(t),t\geq0)$ defined</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>by:</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>\tilde{B}(t) &amp; =\begin{cases}</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>B_{t} &amp; \text{if }t\leq s<span class="sc">\\</span></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>B_{s}-(B_{t}-B_{s}) &amp; \text{if }t&gt;s</span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>\end{cases}\label{eq:reflection-property}</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>is a Brownian motion.</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>*Proof.* (a) Consider the process $\tilde{B}(t)=(-B_{t},t\geq0)$.</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>1<span class="sc">\)</span> $\tilde{B}(0)=0$.</span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>2<span class="sc">\)</span> If $X$ is a Gaussian random variable with mean $0$ and variance</span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a>$t-s$, $-X$ is also Gaussian with mean $0$ and variance $t-s$. Thus,</span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t)-\tilde{B}(s)=-(B(t)-B(s))$ is also Gaussian with mean $0$</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>and variance $(t-s)$. Hence condition (2) is satisfied.</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>3<span class="sc">\)</span> Assume that $0\leq t_{0}\leq t_{1}\leq\ldots\leq t_{n}$. Then, the</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>random variables $-(B(t_{k})-B(t_{k-1}))$ are independent for</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>$k=1,2,3,\ldots,n$. Hence, condition (3) is satisfied.</span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>b<span class="sc">\)</span> Consider the process $\tilde{B}(t)$ as defined in</span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[eq:reflection-property\]</span><span class="co">](#eq:reflection-property)</span>{reference-type="ref"</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>reference="eq:reflection-property"}).</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>Fix an $s\geq0$.</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>1<span class="sc">\)</span> Let $t=0$. Then, $t\leq s$. $\tilde{B}(t)=\tilde{B}(0)=B(0)=0$.</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>2<span class="sc">\)</span> Let $t_{1}&lt;t_{2}\leq s$. Then,</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t_{2})-\tilde{B}(t_{1})=B(t_{2})-B(t_{1})$. This is a</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>Gaussian random variable with mean $0$ and variance $t_{2}-t_{1}$.</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>Let $t_{1}&lt;s&lt;t_{2}$. Then,</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t_{2})-\tilde{B}(t_{1})=B(s)-(B(t_{2})-B(s))-B(t_{1})=(B(s)-B(t_{1}))-(B(t_{2})-B(s))$.</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>Since, $B(s)-B(t_{1})$ and $B(t_{2})-B(s)$ are independent Gaussian</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>random variables, any linear combination of these is Gaussian. Moreover,</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>its mean is zero. The variance is given by:</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>Var<span class="co">[</span><span class="ot">\tilde{B}(t_{2})-\tilde{B}(t_{1})</span><span class="co">]</span> &amp; =Var<span class="co">[</span><span class="ot">B(s)-B(t_{1})</span><span class="co">]</span>+Var<span class="co">[</span><span class="ot">B(t_{2})-B(s)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a> &amp; =(s-t_{1})+(t_{2}-s)<span class="sc">\\</span></span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a> &amp; =t_{2}-t_{1}</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>Let $s&lt;t_{1}&lt;t_{2}$. Then, $$\begin{aligned}</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>\tilde{B}(t_{2})-\tilde{B}(t_{1}) &amp; =B_{s}-(B_{t_{2}}-B_{s})-(B_{s}-(B_{t_{1}}-B_{s}))<span class="sc">\\</span></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a> &amp; =\cancel{B_{s}}-(B_{t_{2}}-\cancel{B_{s}})-(\cancel{B_{s}}-(B_{t_{1}}-\cancel{B_{s}}))<span class="sc">\\</span></span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a> &amp; =-(B_{t_{2}}-B_{t_{1}})</span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>Hence, $\tilde{B}(t_{2})-\tilde{B}(t_{1})$ is again a Gaussian random</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>variable with mean $0$ and variance $t_{2}-t_{1}$. Hence, condition (3)</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>is satisfied.</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>3<span class="sc">\)</span> Assume that</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>$0\leq t_{1}\leq\ldots\leq t_{k-1}\leq s\leq t_{k}\leq\ldots\leq t_{n}$.</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>From the above discussion, the increments</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t_{2})-\tilde{B}(t_{1})$, $\ldots$,</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(s)-\tilde{B}(t_{k-1})$, $\tilde{B}(t_{k})-\tilde{B}(s)$,</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>$\ldots$, $\tilde{B}(t_{n})-\tilde{B}(t_{n-1})$ are independent</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a>increments. The increment $\tilde{B}(t_{k})-\tilde{B}(t_{k-1})$ only</span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>depends on the random variables $\tilde{B}(s)-\tilde{B}(t_{k-1})$ and</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t_{k})-\tilde{B}(s)$. Thus,</span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t_{2})-\tilde{B}(t_{1})$, $\ldots$,</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t_{k})-\tilde{B}(t_{k-1})$, $\ldots$,</span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a>$\tilde{B}(t_{n})-\tilde{B}(t_{n-1})$ are independent. ◻</span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>::: prop</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>(Time Reversal). Let $(B_{t},t\geq0)$ be a Brownian motion. Show that</span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a>the process $(B_{1}-B_{1-t},t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>)$ has the distribution of a</span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a>standard brownian motion on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$.</span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>*Proof.* (1) At $t=0$, $B(1)-B(1-t)=B(1)-B(1)=0$.</span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>2<span class="sc">\)</span> Let $s&lt;t$. Then, $1-t&lt;1-s$. So, the increment :</span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a>(B(1)-B(1-t))-(B(1)-B(1-s)) &amp; =B(1-s)-B(1-t)</span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a>has a Gaussian distribution. It's mean is $0$ and variance is</span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>$(1-s)-(1-t)=t-s$.</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>3<span class="sc">\)</span> Let $0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{n}$. Then:</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>$$1-t_{n}\leq\ldots\leq1-t_{k}\leq1-t_{k-1}\leq\ldots\leq1-t_{2}\leq1-t_{1}$$</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a>Consider the increments of the process for $k=1,2,\ldots,n$:</span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a>(B(1)-B(1-t_{k}))-(B(1)-B(1-t_{k-1})) &amp; =B(1-t_{k-1})-B(1-t_{k})</span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a>They are independent random variables. Hence, condition (3) is</span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a>satisfied. ◻</span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a>(Evaluating Brownian Probabilities). Let's compute the probability that</span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a>$B_{1}&gt;0$ and $B_{2}&gt;0$. We know from the definition that</span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a>$(B_{1},B_{2})$ is a Gaussian vector with mean $0$ and covariance</span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a>matrix:</span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a>C &amp; =\left[\begin{array}{cc}</span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a>1 &amp; 1<span class="sc">\\</span></span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a>1 &amp; 2</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a>\end{array}\right]</span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a>The determinant of $C$ is $1$. By performing row operations on the</span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>augmented matrix $<span class="co">[</span><span class="ot">C|I</span><span class="co">]</span>$ we find that:</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a>C^{-1} &amp; =\left[\begin{array}{cc}</span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a>2 &amp; -1<span class="sc">\\</span></span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a>-1 &amp; 1</span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a>\end{array}\right]</span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a>Thus, the probability $\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0)$ can be expressed as:</span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0) &amp; =\frac{1}{\sqrt{(2\pi)^{2}}}\int_{0}^{\infty}\int_{0}^{\infty}\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}(2x_{1}^{2}-2x_{1}x_{2}+x_{2}^{2}\right</span><span class="co">]</span>dx_{2}dx_{1}</span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a>This integral can be evaluated using a calculator or software and is</span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a>equal to $3/8$. The probability can also be computed using the</span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a>independence of increments. The increments $(B_{1},B_{2}-B_{1})$ are IID</span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a>standard Gaussians. We know their joint PDF. It remains to integrate</span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a>over the correct region of $\mathbf{R}^{2}$ which in this case will be:</span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a>D^{*} &amp; =<span class="sc">\{</span>(z_{1},z_{2}):(z_{1}&gt;0,z_{1}+z_{2}&gt;0)<span class="sc">\}</span></span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a>We have:</span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(B_{1}&gt;0,B_{2}&gt;0) &amp; =\frac{1}{2\pi}\int_{0}^{\infty}\int_{z_{2}=-z_{1}}^{z_{2}=\infty}e^{-(z_{1}^{2}+z_{2}^{2})/2}dz_{2}dz_{1}</span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a>It turns out that this integral can be evaluated exactly. Indeed by</span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a>writing $B_{1}=Z_{1}$ and $Z_{2}=B_{2}-B_{1}$ and splitting the</span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a>probability on the event $<span class="sc">\{</span>Z_{2}\geq0<span class="sc">\}</span>$ and its complement, we have</span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a>that $\mathbb{P}(B_{1}\geq0,B_{2}\geq0)$ equals:</span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(B_{1}\geq0,B_{2}\geq0) &amp; =\mathbb{P}(Z_{1}\geq0,Z_{1}+Z_{2}&gt;0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}+Z_{2}&gt;0,Z_{2}&lt;0)<span class="sc">\\</span></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{P}(Z_{1}\geq0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;-Z_{2},-Z_{2}&gt;0)<span class="sc">\\</span></span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{P}(Z_{1}\geq0,Z_{2}\geq0)+\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;Z_{2},Z_{2}&gt;0)<span class="sc">\\</span></span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{4}+\frac{1}{8}<span class="sc">\\</span></span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{3}{8}</span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a>Note that, by symmetry,</span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(Z_{1}\geq0,Z_{1}&gt;Z_{2},Z_{2}&gt;0)=\mathbb{P}(Z_{1}\geq0,Z_{1}\leq Z_{2},Z_{2}&gt;0)=\frac{1}{8}$.</span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a>(Another look at Ornstein Uhlenbeck process.) Consider the process</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a>$(X_{t},t\in\mathbf{R})$ defined by :</span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a>X_{t} &amp; =\frac{e^{-2t}}{\sqrt{2}}B(e^{4t}),\quad t\in\mathbf{R}</span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>Here the process $(B_{e^{4t}},t\ge0)$ is called a time change of</span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a>Brownian motion, since the time is now quantitfied by an increasing</span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a>function of $t$ namely $e^{4t}$. The example $(B(\lambda t),t\geq0)$ in</span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a>the scaling property is another example of time change.</span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a>It turns out that $(X_{t},t\in\mathbf{R})$ is a stationary</span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a>Ornstein-Uhlenbeck process. (Here the index of time is $\mathbf{R}$</span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a>instead of $[0,\infty)$, but the definition also applies as the process</span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a>is stationary. Since the original brownian motion $B(t)$ is a Gaussian</span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a>process, any finite dimensional vector $(B(t_{1}),\ldots,B(t_{n}))$ is</span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a>Gaussian. It follows that:</span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a>$$(B(T_{1}),\ldots,B(T_{n}))=\frac{1}{\sqrt{2}}(e^{-2t_{1}}B(e^{4t_{1}}),\ldots,e^{-2t_{n}}B(e^{4t_{n}}))$$</span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a>is also a Gaussian vector. (Note, once we fix</span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a>$t_{1},t_{2},\ldots,t_{n}$, $e^{-4t_{1}},\ldots,e^{-4t_{n}}$ are</span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a>constants.) Hence, $(X_{t},t\in\mathbf{R})$ is a Gaussian process.</span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a>The mean of $(X_{t},t\in\mathbf{R})$ is:</span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">X_{t}</span><span class="co">]</span> &amp; =\frac{e^{-2t}}{\sqrt{2}}\mathbb{E}<span class="co">[</span><span class="ot">B(e^{4t})</span><span class="co">]</span>=0</span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a>And if $s&lt;t$,</span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">X_{s}X_{t}</span><span class="co">]</span> &amp; =\frac{e^{-2(s+t)}}{2}\mathbb{E}<span class="co">[</span><span class="ot">B(e^{4s})B(e^{4t})</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{e^{-2(s+t)}}{2}e^{4s}<span class="sc">\\</span></span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{e^{-2(t-s)}}{2}</span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a>Two Gaussian processes having the same mean and covariance have the same</span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a>distribution. Hence, it proves the claim that $(X_{t})$ is a stationary</span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a>OU process.</span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of the paths.</span></span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a>First we review the definitions of the Riemann integral and the</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a>Riemann-Stieljtes integral in Calculus.</span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a>A partition $P$ of $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$ is a *finite* set of points from $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$ that</span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a>includes both $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>.$The notational convention is to always list the</span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a>points of a partition $P=<span class="sc">\{</span>a=x_{0},x_{1},x_{2},\ldots,x_{n}=b<span class="sc">\}</span>$ in</span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a>increasing order. Thus:</span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>$$a=x_{0}&lt;x_{1}&lt;\ldots&lt;x_{k-1}&lt;x_{k}&lt;\ldots&lt;x_{n}=b$$</span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a>For each subinterval $<span class="co">[</span><span class="ot">x_{k-1},x_{k}</span><span class="co">]</span>$ of $P$, let</span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a>m_{k} &amp; =\inf<span class="sc">\{</span>f(x):x\in<span class="co">[</span><span class="ot">x_{k-1},x_{k}</span><span class="co">]</span><span class="sc">\}\\</span></span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a>M_{k} &amp; =\sup<span class="sc">\{</span>f(x):x\in<span class="co">[</span><span class="ot">x_{k-1},x_{k}</span><span class="co">]</span><span class="sc">\}</span></span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a>The lower sum of $f$ with respect to $P$ is given by :</span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a>L(f,P) &amp; =\sum_{k=1}^{n}m_{k}(x_{k}-x_{k-1})</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a>The upper sum of $f$ with respect to $P$ is given by:</span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a>U(f,P) &amp; =\sum_{k=1}^{n}M_{k}(x_{k}-x_{k-1})</span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a>For a particular partition $P$, it is clear that $U(f,P)\geq L(f,P)$</span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a>because $M_{k}\geq m_{k}$ for all $k=0,1,2,\ldots,n$.</span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a>A partition $Q$ is called a *refinement* of $P$ if $Q$ contains all of</span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a>the points of $P$; that is $Q\subseteq P$.</span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a>::: lem</span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a>If $P\subseteq Q$, then $L(f,P)\leq L(f,Q)$ and $U(f,Q)\leq U(f,P)$.</span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a>*Proof.* Consider what happens when we refine $P$ by adding a single</span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>point $z$ to some subinterval $<span class="co">[</span><span class="ot">x_{k-1},x_{k}</span><span class="co">]</span>$ of $P$. We have:</span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a>m_{k}(x_{k}-x_{k-1}) &amp; =m_{k}(x_{k}-z)+m_{k}(z-x_{k-1})<span class="sc">\\</span></span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a> &amp; \leq m_{k}'(x_{k}-z)+m_{k}''(z-x_{k-1})</span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a>m_{k}' &amp; =\inf<span class="sc">\{</span>f(x):x\in<span class="co">[</span><span class="ot">z,x_{k}</span><span class="co">]</span><span class="sc">\}\\</span></span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a>m_{k}'' &amp; =\inf<span class="sc">\{</span>f(x):x\in<span class="co">[</span><span class="ot">x_{k-1},z</span><span class="co">]</span><span class="sc">\}</span></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a>By induction we have:</span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a>L(f,P) &amp; \leq L(f,Q)<span class="sc">\\</span></span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a>U(f,Q) &amp; \leq U(f,P)</span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$ ◻</span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a>::: lem</span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a>If $P_{1}$ and $P_{2}$ are any two partitions of $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$, then</span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a>$L(f,P_{1})\leq U(f,P_{2})$.</span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a>*Proof.* Let $Q=P_{1}\cup P_{2}$. Then, $P_{1}\subseteq Q$ and</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a>$P_{2}\subseteq Q$. Thus,</span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a>$L(f,P_{1})\leq L(f,Q)\leq U(f,Q)\leq L(f,P_{2})$. ◻</span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a>Let $\mathcal{P}$ be the collection of all possible partitions of the</span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a>interval $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$. The upper integral of $f$ is defined to be:</span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a>U(f) &amp; =\inf<span class="sc">\{</span>U(f,P):P\in\mathcal{P}<span class="sc">\}</span></span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a>The lower integral of $f$ is defined by:</span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a>L(f) &amp; =\sup<span class="sc">\{</span>L(f,P):P\in\mathcal{P}<span class="sc">\}</span></span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a>Consider the set of all upper sums of $f$ -</span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a>$<span class="sc">\{</span>U(f,P):P\in\mathcal{P}<span class="sc">\}</span>$. Take an arbitrary partition</span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a>$P'\in\mathcal{P}$. Since $L(f,P')\leq U(f,P)$ for all</span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a>$P\in\mathcal{P}$, by the Axiom of Completeness(AoC),</span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a>$\inf<span class="sc">\{</span>U(f,P):P\in\mathcal{P}<span class="sc">\}</span>$ exists.We can similarly argue for the</span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a>supremum of all lower Riemann sums.</span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a>::: lem</span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a>For any bounded function $f$ on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$, it is always the case that</span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a>$U(f)\geq L(f)$.</span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a>*Proof.* By the properties of the infimum of a set,</span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a>$(\forall\epsilon&gt;0)$, $\exists P(\epsilon)$ such that</span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a>$U(f)&lt;U(f,P(\epsilon))&lt;U(f)+\epsilon$. Pick</span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a>$\epsilon=1,\frac{1}{2},\frac{1}{3}\ldots,\frac{1}{n},\ldots$. Thus, we</span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a>can produce a sequence of partitions $P_{n}$ such that :</span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a>$$U(f)&lt;\ldots&lt;U(f,P_{n})&lt;U(f)+\frac{1}{n}$$</span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a>Consequently, $\lim U(f,P_{n})=U(f)$. Similarly, we can produce a</span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a>sequence of partitions $(Q_{m})$ such that :</span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a>$$L(f)-\frac{1}{m}&lt;\ldots&lt;L(f,Q_{m})&lt;L(f)$$</span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a>We know that:</span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a>L(f,Q_{m}) &amp; \leq U(f,P_{n})</span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a>Keeping $m$ fixed and passing to the limit, as $n\to\infty$ on both</span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a>sides, we have:</span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}L(f,Q_{m}) &amp; \leq\lim_{n\to\infty}U(f,P_{n})\quad\left<span class="sc">\{</span> \text{Order Limit Theorem}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a>L(f,Q_{m}) &amp; \leq U(f)</span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a>Now, passing to the limit, as $m\to\infty$ on both sides, we have:</span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a>\lim_{m\to\infty}L(f,Q_{m}) &amp; \leq\lim_{m\to\infty}U(f)\quad\left<span class="sc">\{</span> \text{Order Limit Theorem}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a>L(f) &amp; \leq U(f)</span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$ ◻</span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a>(Riemann Integrability). A bounded function $f$ on the interval $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$</span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a>is said to be Riemann integrable if $U(f)=L(f)$. In this case, we define</span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a>$\int_{a}^{b}f$ or $\int_{a}^{b}f(x)dx$ to be the common value:</span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a>\int_{a}^{b}f(x)dx &amp; =U(f)=L(f)</span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a>(Integrability Criterion) A bounded function $f$ is integrable on</span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a>$<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$ if and only if, for every $\epsilon&gt;0$, there exists a partition</span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a>$P_{\epsilon}$ of $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$ such that:</span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a>U(f,P_{\epsilon})-L(f,P_{\epsilon}) &amp; &lt;\epsilon</span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a>*Proof.* ($\Longleftarrow$ direction.) Let $\epsilon&gt;0$. If such a</span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a>partition $P_{\epsilon}$ exists, then:</span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a>$$U(f)-L(f)\leq U(f,P_{\epsilon})-L(f,P_{\epsilon})&lt;\epsilon$$</span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a>Because $\epsilon$ is arbitrary, it follows that $U(f)=L(f)$ and hence</span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a>$f$ is Riemann integrable.</span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a>($\Longrightarrow$ direction.) Let $f$ be a bounded function on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$</span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a>such that $f$ is Riemann integrable.</span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a>Pick an arbitrary $\epsilon&gt;0$.</span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a>Then, since $U(f)=\inf<span class="sc">\{</span>U(f,P):P\in\mathcal{P}<span class="sc">\}</span>$, there exists</span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a>$P_{\epsilon}\in\mathcal{P}$, such that</span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a>$U(f)&lt;U(f,P_{\epsilon})&lt;U(f)+\frac{\epsilon}{2}$. Since</span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a>$L(f)=\sup<span class="sc">\{</span>L(f,P):P\in\mathcal{P}<span class="sc">\}</span>$, there exists</span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a>$P_{\epsilon}\in\mathcal{P}$, such that</span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a>$L(f)-\frac{\epsilon}{2}&lt;L(f,P_{\epsilon})&lt;L(f)$. Consequently,</span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a>U(f,P_{\epsilon})-L(f,P_{\epsilon}) &amp; &lt;U(f)+\frac{\epsilon}{2}-\left(L(f)-\frac{\epsilon}{2}\right)<span class="sc">\\</span></span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a> &amp; =U(f)-L(f)+\epsilon<span class="sc">\\</span></span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a> &amp; =\epsilon</span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$ ◻</span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a><span class="fu">### Functions considered in Stochastic Calculus.</span></span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a>A point $c$ is called a discontinuity of the first kind or jump point if</span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a>both limits $g(c+)=\lim_{t\uparrow c}g(t)$ and</span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a>$g(c-)=\lim_{t\downarrow c}g(t)$ exist and are not equal. The jump at</span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a>$c$ is defined as $\Delta g(c)=g(c+)-g(c-)$. Any other discontinuity is</span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a>said to be of the second kind.</span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a>Consider the function</span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a>f(x) &amp; =\sin\left(\frac{1}{x}\right)</span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a>Let $x_{n}=\frac{1}{2n\pi}$. Then, $f(x_{n})=(0,0,0,\ldots)$. Next,</span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a>consider $y_{n}=\frac{1}{\pi/2+2n\pi}$. Then, $f(y_{n})=(1,1,1,\ldots)$.</span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a>Consequently, $f$ is not continuous at $0$. Hence, limits from the left</span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a>or right don't exist. Consequently, this is a discontinuity of the</span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a>second kind.</span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a>Functions in stochastic calculus are functions without discontinuities</span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a>of the second kind, that is functions have both left and right hand</span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a>limits at any point of the domain and have one-sided limits at the</span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a>boundary. These functions are called *regular* functions. It is often</span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a>agreed to identify functions if they have the same right and left limits</span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a>at any point.</span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a>The class $D=D<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>$ of right-continuous functions on $<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>$ with left</span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a>limits has a special name, *cadlag* functions (which is the abbreviation</span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a>of right continuous with left limits in French). Sometimes these</span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a>processes are called R.R.C. for regular right continuous. Notice that</span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a>this class of processes includes $C$, the class of continuous functions.</span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a>Let $g\in D$ be a cadlag function, then, by definition, all the</span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a>discontinuities of $g$ are jumps. An important result in analysis is</span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a>that, a function can have no more than a countable number of</span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a>discontinuities.</span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variation of a function. </span></span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a>If $g$ is a function of a real variable, its variation over the interval</span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a>$<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$ is defined as:</span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a>V_{g}(<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>) &amp; =\sup\left<span class="sc">\{</span> \sum_{i=1}^{n}\left|g(t_{i})-g(t_{i-1})\right|\right<span class="sc">\}</span> \label{eq:total-variation-of-a-function}</span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a>where the supremum is taken over all partitions $P\in\mathcal{P}$.</span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a>Clearly, by the Triangle Inequality, the sums in</span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[eq:total-variation-of-a-function\]</span><span class="co">](#eq:total-variation-of-a-function)</span>{reference-type="ref"</span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a>reference="eq:total-variation-of-a-function"}) increase as new points</span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a>are added to the partitions. Therefore, the variation of $g$ is:</span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a>V_{g}(<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}\left|g(t_{i})-g(t_{i-1})\right|</span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a>where $||\Delta_{n}||=\max_{1\leq i\leq n}(t_{i}-t_{i-1})$. If</span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a>$V_{g}(<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>)$ is finite, then $g$ is said to be a function of finite</span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a>variation on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$. If $g$ is a function of $t\geq0$, then the</span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a>variation of $g$ as a function of $t$ is defined by:</span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a>V_{g}(t) &amp; =V_{g}(<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>)</span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a>Clearly, $V_{g}(t)$ is an increasing function of $t$.</span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a>$g$ is a function of finite variation if $V_{g}(t)&lt;\infty$ for all</span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a>$t\in[0,\infty)$. $g$ is of bounded variation if</span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a>$\sup_{t}V_{g}(t)&lt;\infty$, in other words there exists $C$, for all $t$,</span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a>such that $V_{g}(t)&lt;C$. Here $C$ is independent of $t$.</span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>1<span class="sc">\)</span> If $g(t)$ is increasing then for any $i$,</span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a>$g(t_{i})\geq g(t_{i-1})$, resulting in a telescopic sum, where all</span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a>terms excluding the first and the last cancel out, leaving</span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a>V_{g}(t) &amp; =g(t)-g(0)</span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>2<span class="sc">\)</span> If $g(t)$ is decreasing, then similarly,</span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a>V_{g}(t) &amp; =g(0)-g(t)</span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a>If $g(t)$ is differentiable with continuous derivative $g'(t)$,</span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a>$g(t)=\int_{0}^{t}g'(s)ds$ then</span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a>V_{g}(t) &amp; =\int_{0}^{t}|g'(s)|ds</span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a>*Proof.* By definition,</span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a>V_{g}(t) &amp; =\lim_{||\Delta_{n}\to0||}\sum_{i=1}^{n}|g(t_{i})-g(t_{i-1})|</span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a>Since $g$ is continuous and differentiable on $<span class="co">[</span><span class="ot">t_{i-1},t_{i}</span><span class="co">]</span>$, there</span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a>exists $z_{i}\in(t_{i-1},t_{i})$ such, that</span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a>$g(t_{i})-g(t_{i-1})=g'(z_{i})(t_{i}-t_{i-1})$. Therefore, we can write:</span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-669"><a href="#cb2-669" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-670"><a href="#cb2-670" aria-hidden="true" tabindex="-1"></a>{1}</span>
<span id="cb2-671"><a href="#cb2-671" aria-hidden="true" tabindex="-1"></a>V_{g}(t) &amp; =\lim_{||\Delta_{n}\to0||}\sum_{i=1}^{n}|g'(z_{i})|(t_{i}-t_{i-1})<span class="sc">\\</span></span>
<span id="cb2-672"><a href="#cb2-672" aria-hidden="true" tabindex="-1"></a> &amp; =\int_{0}^{t}|g'(s)|ds</span>
<span id="cb2-673"><a href="#cb2-673" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$ ◻</span>
<span id="cb2-674"><a href="#cb2-674" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-675"><a href="#cb2-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-676"><a href="#cb2-676" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb2-677"><a href="#cb2-677" aria-hidden="true" tabindex="-1"></a>If $g$ is continuous, $g'$ exists and $\int_{0}^{t}|g'(s)|ds$ is finite,</span>
<span id="cb2-678"><a href="#cb2-678" aria-hidden="true" tabindex="-1"></a>then $g$ is of finite variation.</span>
<span id="cb2-679"><a href="#cb2-679" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-680"><a href="#cb2-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-681"><a href="#cb2-681" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-682"><a href="#cb2-682" aria-hidden="true" tabindex="-1"></a>The function $g(t)=t\sin(1/t)$ for $t&gt;0$ and $g(0)=0$ is continuous on</span>
<span id="cb2-683"><a href="#cb2-683" aria-hidden="true" tabindex="-1"></a>$<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ and differentiable at all points except zero, but is not of</span>
<span id="cb2-684"><a href="#cb2-684" aria-hidden="true" tabindex="-1"></a>bounded variation on any interval that includes $0$. Consider the</span>
<span id="cb2-685"><a href="#cb2-685" aria-hidden="true" tabindex="-1"></a>partition $<span class="sc">\{</span>x_{n}<span class="sc">\}</span>=\left<span class="sc">\{</span> \frac{1}{\pi/2+n\pi}\right<span class="sc">\}</span>$. Thus,</span>
<span id="cb2-686"><a href="#cb2-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-687"><a href="#cb2-687" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-688"><a href="#cb2-688" aria-hidden="true" tabindex="-1"></a>\sin(\frac{1}{x_{n}}) &amp; =\begin{cases}</span>
<span id="cb2-689"><a href="#cb2-689" aria-hidden="true" tabindex="-1"></a>1 &amp; \text{if }n\text{ is even}<span class="sc">\\</span></span>
<span id="cb2-690"><a href="#cb2-690" aria-hidden="true" tabindex="-1"></a>-1 &amp; \text{if }n\text{ is odd}</span>
<span id="cb2-691"><a href="#cb2-691" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb2-692"><a href="#cb2-692" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-693"><a href="#cb2-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-694"><a href="#cb2-694" aria-hidden="true" tabindex="-1"></a>Thus,</span>
<span id="cb2-695"><a href="#cb2-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-696"><a href="#cb2-696" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-697"><a href="#cb2-697" aria-hidden="true" tabindex="-1"></a>f(x_{n}) &amp; =\begin{cases}</span>
<span id="cb2-698"><a href="#cb2-698" aria-hidden="true" tabindex="-1"></a>x_{n} &amp; n\text{ is even}<span class="sc">\\</span></span>
<span id="cb2-699"><a href="#cb2-699" aria-hidden="true" tabindex="-1"></a>-x_{n} &amp; n\text{ is odd}</span>
<span id="cb2-700"><a href="#cb2-700" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb2-701"><a href="#cb2-701" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-702"><a href="#cb2-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-703"><a href="#cb2-703" aria-hidden="true" tabindex="-1"></a>Therefore,</span>
<span id="cb2-704"><a href="#cb2-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-705"><a href="#cb2-705" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-706"><a href="#cb2-706" aria-hidden="true" tabindex="-1"></a>\sum_{n=1}^{m}|f(x_{n})-f(x_{n-1})| &amp; =\sum_{n=1}^{m}(x_{n}+x_{n-1})<span class="sc">\\</span></span>
<span id="cb2-707"><a href="#cb2-707" aria-hidden="true" tabindex="-1"></a> &amp; =x_{0}+x_{n}+2\sum_{n=1}^{m-1}x_{n}<span class="sc">\\</span></span>
<span id="cb2-708"><a href="#cb2-708" aria-hidden="true" tabindex="-1"></a> &amp; \geq\sum_{n=1}^{m-1}x_{n}</span>
<span id="cb2-709"><a href="#cb2-709" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-710"><a href="#cb2-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-711"><a href="#cb2-711" aria-hidden="true" tabindex="-1"></a>This is the lower bound on the variation of $g$ on the partition</span>
<span id="cb2-712"><a href="#cb2-712" aria-hidden="true" tabindex="-1"></a>$<span class="sc">\{</span>0,x_{m},\ldots,x_{1},x_{0},1<span class="sc">\}</span>$. Now, passing to the limit as $m$</span>
<span id="cb2-713"><a href="#cb2-713" aria-hidden="true" tabindex="-1"></a>approaches infinity, $\sum\frac{1}{\pi/2+n\pi}$ is a divergent series.</span>
<span id="cb2-714"><a href="#cb2-714" aria-hidden="true" tabindex="-1"></a>Consequently, $V_{g}(<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>)$ has unbounded variation.</span>
<span id="cb2-715"><a href="#cb2-715" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-716"><a href="#cb2-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-717"><a href="#cb2-717" aria-hidden="true" tabindex="-1"></a><span class="fu">### Jordan Decomposition.</span></span>
<span id="cb2-718"><a href="#cb2-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-719"><a href="#cb2-719" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb2-720"><a href="#cb2-720" aria-hidden="true" tabindex="-1"></a>Any function $g:[0,\infty)\to\mathbf{R}$ is of bounded variation if and</span>
<span id="cb2-721"><a href="#cb2-721" aria-hidden="true" tabindex="-1"></a>only if it can be expressed as the difference of two increasing</span>
<span id="cb2-722"><a href="#cb2-722" aria-hidden="true" tabindex="-1"></a>functions:</span>
<span id="cb2-723"><a href="#cb2-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-724"><a href="#cb2-724" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-725"><a href="#cb2-725" aria-hidden="true" tabindex="-1"></a>g(t) &amp; =a(t)-b(t)</span>
<span id="cb2-726"><a href="#cb2-726" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-727"><a href="#cb2-727" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-728"><a href="#cb2-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-729"><a href="#cb2-729" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-730"><a href="#cb2-730" aria-hidden="true" tabindex="-1"></a>*Proof.* ($\Longrightarrow$direction). If $g$ is of finite variation,</span>
<span id="cb2-731"><a href="#cb2-731" aria-hidden="true" tabindex="-1"></a>$V_{g}(t)&lt;\infty$ for all $t$, and we can write:</span>
<span id="cb2-732"><a href="#cb2-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-733"><a href="#cb2-733" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-734"><a href="#cb2-734" aria-hidden="true" tabindex="-1"></a>g(t) &amp; =V_{g}(t)-(V_{g}(t)-g(t))</span>
<span id="cb2-735"><a href="#cb2-735" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-736"><a href="#cb2-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-737"><a href="#cb2-737" aria-hidden="true" tabindex="-1"></a>Let $a(t)=V_{g}(t)$ and $b(t)=V_{g}(t)-g(t)$. Clearly, both $a(t)$ and</span>
<span id="cb2-738"><a href="#cb2-738" aria-hidden="true" tabindex="-1"></a>$b(t)$ are increasing functions.</span>
<span id="cb2-739"><a href="#cb2-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-740"><a href="#cb2-740" aria-hidden="true" tabindex="-1"></a>($\Longleftarrow$direction). Suppose a function $g$ can be expressed as</span>
<span id="cb2-741"><a href="#cb2-741" aria-hidden="true" tabindex="-1"></a>a difference of two bounded increasing functions. Then,</span>
<span id="cb2-742"><a href="#cb2-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-743"><a href="#cb2-743" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-744"><a href="#cb2-744" aria-hidden="true" tabindex="-1"></a>V_{g}(t) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}|(a(t_{i})-b(t_{i}))-(a(t_{i-1})-b(t_{i-1})|<span class="sc">\\</span></span>
<span id="cb2-745"><a href="#cb2-745" aria-hidden="true" tabindex="-1"></a> &amp; \quad<span class="sc">\{</span>\text{ Telescoping sum }<span class="sc">\}\\</span></span>
<span id="cb2-746"><a href="#cb2-746" aria-hidden="true" tabindex="-1"></a> &amp; =a(t)-b(t)-(a(0)-b(0))</span>
<span id="cb2-747"><a href="#cb2-747" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-748"><a href="#cb2-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-749"><a href="#cb2-749" aria-hidden="true" tabindex="-1"></a>Since both $a(t)$ and $b(t)$ are bounded, $g$ has bounded variation. ◻</span>
<span id="cb2-750"><a href="#cb2-750" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-751"><a href="#cb2-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-752"><a href="#cb2-752" aria-hidden="true" tabindex="-1"></a><span class="fu">### Riemann-Stieltjes Integral.</span></span>
<span id="cb2-753"><a href="#cb2-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-754"><a href="#cb2-754" aria-hidden="true" tabindex="-1"></a>Let $g$ be a montonically increasing function on a finite closed</span>
<span id="cb2-755"><a href="#cb2-755" aria-hidden="true" tabindex="-1"></a>interval $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$. A bounded function $f$ defined on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$ is said to</span>
<span id="cb2-756"><a href="#cb2-756" aria-hidden="true" tabindex="-1"></a>*Riemann-Stieltjes integrable* with respect to $g$ if the following</span>
<span id="cb2-757"><a href="#cb2-757" aria-hidden="true" tabindex="-1"></a>limit exists:</span>
<span id="cb2-758"><a href="#cb2-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-759"><a href="#cb2-759" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-760"><a href="#cb2-760" aria-hidden="true" tabindex="-1"></a>\int_{a}^{b}f(t)dg(t) &amp; =\lim_{||\Delta_{n}||\to0}\sum_{i=1}^{n}f(\tau_{i})(g(t_{i})-g(t_{i-1}))\label{eq:riemann-stieltjes-integral}</span>
<span id="cb2-761"><a href="#cb2-761" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-762"><a href="#cb2-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-763"><a href="#cb2-763" aria-hidden="true" tabindex="-1"></a>where $\tau_{i}$ is an evaluation point in the interval</span>
<span id="cb2-764"><a href="#cb2-764" aria-hidden="true" tabindex="-1"></a>$<span class="co">[</span><span class="ot">t_{i-1},t_{i}</span><span class="co">]</span>$. It is a well-known fact that continuous functions are</span>
<span id="cb2-765"><a href="#cb2-765" aria-hidden="true" tabindex="-1"></a>Riemann integrable and Riemann-Stieltjes integrable with respect to any</span>
<span id="cb2-766"><a href="#cb2-766" aria-hidden="true" tabindex="-1"></a>monotonically increasing function on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$.</span>
<span id="cb2-767"><a href="#cb2-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-768"><a href="#cb2-768" aria-hidden="true" tabindex="-1"></a>We ask the following question. For any continuous functions $f$ and $g$</span>
<span id="cb2-769"><a href="#cb2-769" aria-hidden="true" tabindex="-1"></a>on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$, can we define the integral $\int_{a}^{b}f(t)dg(t)$ by</span>
<span id="cb2-770"><a href="#cb2-770" aria-hidden="true" tabindex="-1"></a>Equation</span>
<span id="cb2-771"><a href="#cb2-771" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[eq:riemann-stieltjes-integral\]</span><span class="co">](#eq:riemann-stieltjes-integral)</span>{reference-type="ref"</span>
<span id="cb2-772"><a href="#cb2-772" aria-hidden="true" tabindex="-1"></a>reference="eq:riemann-stieltjes-integral"})?</span>
<span id="cb2-773"><a href="#cb2-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-774"><a href="#cb2-774" aria-hidden="true" tabindex="-1"></a>Consider the special case $f=g$, namely, the integral:</span>
<span id="cb2-775"><a href="#cb2-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-776"><a href="#cb2-776" aria-hidden="true" tabindex="-1"></a>$$\int_{a}^{b}f(t)df(t)$$</span>
<span id="cb2-777"><a href="#cb2-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-778"><a href="#cb2-778" aria-hidden="true" tabindex="-1"></a>Let $\Delta_{n}=<span class="sc">\{</span>a=t_{0},t_{1},\ldots,t_{n}=b<span class="sc">\}</span>$ be a partition of</span>
<span id="cb2-779"><a href="#cb2-779" aria-hidden="true" tabindex="-1"></a>$<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$. Let $L_{n}$ and $R_{n}$ denote the corresponding Riemann sums</span>
<span id="cb2-780"><a href="#cb2-780" aria-hidden="true" tabindex="-1"></a>with the evaluation points $\tau_{i}=t_{i-1}$ and $\tau_{i}=t_{i}$,</span>
<span id="cb2-781"><a href="#cb2-781" aria-hidden="true" tabindex="-1"></a>respectively, namely,</span>
<span id="cb2-782"><a href="#cb2-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-783"><a href="#cb2-783" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-784"><a href="#cb2-784" aria-hidden="true" tabindex="-1"></a>L_{n} &amp; =\sum_{i=1}^{n}f(t_{i-1})(f(t_{i})-f(t_{i-1}))\label{eq:left-riemann-sum}<span class="sc">\\</span></span>
<span id="cb2-785"><a href="#cb2-785" aria-hidden="true" tabindex="-1"></a>R_{n} &amp; =\sum_{i=1}^{n}f(t_{i})(f(t_{i})-f(t_{i-1}))\label{eq:right-riemann-sum}</span>
<span id="cb2-786"><a href="#cb2-786" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-787"><a href="#cb2-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-788"><a href="#cb2-788" aria-hidden="true" tabindex="-1"></a>Is it true that, $\lim L_{n}=\lim R_{n}$ as $||\Delta_{n}||\to0$?</span>
<span id="cb2-789"><a href="#cb2-789" aria-hidden="true" tabindex="-1"></a>Observe that:</span>
<span id="cb2-790"><a href="#cb2-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-791"><a href="#cb2-791" aria-hidden="true" tabindex="-1"></a>$$R_{n}-L_{n}=\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\label{eq:quadratic-variation}$$</span>
<span id="cb2-792"><a href="#cb2-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-793"><a href="#cb2-793" aria-hidden="true" tabindex="-1"></a>$$R_{n}+L_{n}=\sum_{i=1}^{n}(f(t_{i})^{2}-f(t_{i-1})^{2})=f(b)^{2}-f(a)^{2}\label{eq:sum-of-left-and-right-riemann-sums}$$</span>
<span id="cb2-794"><a href="#cb2-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-795"><a href="#cb2-795" aria-hidden="true" tabindex="-1"></a>Therefore, $R_{n}$ and $L_{n}$ are given by:</span>
<span id="cb2-796"><a href="#cb2-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-797"><a href="#cb2-797" aria-hidden="true" tabindex="-1"></a>$$R_{n}=\frac{1}{2}\left(f(b)^{2}-f(a)^{2}+\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\right)$$</span>
<span id="cb2-798"><a href="#cb2-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-799"><a href="#cb2-799" aria-hidden="true" tabindex="-1"></a>$$L_{n}=\frac{1}{2}\left(f(b)^{2}-f(a)^{2}-\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}\right)$$</span>
<span id="cb2-800"><a href="#cb2-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-801"><a href="#cb2-801" aria-hidden="true" tabindex="-1"></a>The limit of the right-hand side of equation</span>
<span id="cb2-802"><a href="#cb2-802" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[eq:quadratic-variation\]</span><span class="co">](#eq:quadratic-variation)</span>{reference-type="ref"</span>
<span id="cb2-803"><a href="#cb2-803" aria-hidden="true" tabindex="-1"></a>reference="eq:quadratic-variation"}) is called the *quadratic variation*</span>
<span id="cb2-804"><a href="#cb2-804" aria-hidden="true" tabindex="-1"></a>of the function $f$ on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$. Obviously,</span>
<span id="cb2-805"><a href="#cb2-805" aria-hidden="true" tabindex="-1"></a>$\lim_{||\Delta_{n}||\to0}R_{n}\neq\lim_{||\Delta_{n}||\to0}L_{n}$ if</span>
<span id="cb2-806"><a href="#cb2-806" aria-hidden="true" tabindex="-1"></a>and only the quadratic variation of the function $f$ is non-zero.</span>
<span id="cb2-807"><a href="#cb2-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-808"><a href="#cb2-808" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-809"><a href="#cb2-809" aria-hidden="true" tabindex="-1"></a>Let $f$ be a $C^{1}$-function that is $f'(t)$ is a continuous function.</span>
<span id="cb2-810"><a href="#cb2-810" aria-hidden="true" tabindex="-1"></a>Then, by the mean value theorem:</span>
<span id="cb2-811"><a href="#cb2-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-812"><a href="#cb2-812" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-813"><a href="#cb2-813" aria-hidden="true" tabindex="-1"></a>|R_{n}-L_{n}| &amp; =\sum_{i=1}^{n}(f(t_{i})-f(t_{i-1}))^{2}<span class="sc">\\</span></span>
<span id="cb2-814"><a href="#cb2-814" aria-hidden="true" tabindex="-1"></a> &amp; =\sum_{i=1}^{n}(f'(t_{i}^{*})(t_{i}-t_{i-1}))^{2}<span class="sc">\\</span></span>
<span id="cb2-815"><a href="#cb2-815" aria-hidden="true" tabindex="-1"></a> &amp; \quad<span class="sc">\{</span>\text{Mean Value Theorem}<span class="sc">\}\\</span></span>
<span id="cb2-816"><a href="#cb2-816" aria-hidden="true" tabindex="-1"></a> &amp; \leq\sum_{i=1}^{n}\left\Vert f'\right\Vert _{\infty}^{2}(t_{i}-t_{i-1})^{2}<span class="sc">\\</span></span>
<span id="cb2-817"><a href="#cb2-817" aria-hidden="true" tabindex="-1"></a> &amp; \quad<span class="sc">\{</span>\text{ Interior Extremum Theorem }<span class="sc">\}\\</span></span>
<span id="cb2-818"><a href="#cb2-818" aria-hidden="true" tabindex="-1"></a> &amp; \leq\left\Vert f'\right\Vert _{\infty}^{2}\left\Vert \Delta_{n}\right\Vert \sum_{i=1}^{n}(t_{i}-t_{i-1})<span class="sc">\\</span></span>
<span id="cb2-819"><a href="#cb2-819" aria-hidden="true" tabindex="-1"></a> &amp; =\left\Vert f'\right\Vert _{\infty}^{2}\left\Vert \Delta_{n}\right\Vert (b-a)</span>
<span id="cb2-820"><a href="#cb2-820" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-821"><a href="#cb2-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-822"><a href="#cb2-822" aria-hidden="true" tabindex="-1"></a>where $\left\Vert f'\right\Vert _{\infty}=\sup_{x\in<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>}f(x)$. Thus,</span>
<span id="cb2-823"><a href="#cb2-823" aria-hidden="true" tabindex="-1"></a>the limit as $\left\Vert \Delta_{n}\right\Vert \to0$ of the distance</span>
<span id="cb2-824"><a href="#cb2-824" aria-hidden="true" tabindex="-1"></a>$|R_{n}-L_{n}|$ also approaches zero. Thus, $\lim L_{n}=\lim R_{n}$ as</span>
<span id="cb2-825"><a href="#cb2-825" aria-hidden="true" tabindex="-1"></a>$\left\Vert \Delta_{n}\right\Vert \to0$ and the Riemann-Stieltjes</span>
<span id="cb2-826"><a href="#cb2-826" aria-hidden="true" tabindex="-1"></a>integral exists. By equation</span>
<span id="cb2-827"><a href="#cb2-827" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[eq:sum-of-left-and-right-riemann-sums\]</span><span class="co">](#eq:sum-of-left-and-right-riemann-sums)</span>{reference-type="ref"</span>
<span id="cb2-828"><a href="#cb2-828" aria-hidden="true" tabindex="-1"></a>reference="eq:sum-of-left-and-right-riemann-sums"}), we have:</span>
<span id="cb2-829"><a href="#cb2-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-830"><a href="#cb2-830" aria-hidden="true" tabindex="-1"></a>$$\lim_{\left\Vert \Delta_{n}\right\Vert \to0}L_{n}=\lim_{\left\Vert \Delta_{n}\right\Vert \to0}R_{n}=\frac{1}{2}(f(b)^{2}-f(a)^{2})$$</span>
<span id="cb2-831"><a href="#cb2-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-832"><a href="#cb2-832" aria-hidden="true" tabindex="-1"></a>On the other hand, for such a $C^{1}$-function $f$, we may simply define</span>
<span id="cb2-833"><a href="#cb2-833" aria-hidden="true" tabindex="-1"></a>the integral $\int_{a}^{b}f(t)df(t)$ by:</span>
<span id="cb2-834"><a href="#cb2-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-835"><a href="#cb2-835" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-836"><a href="#cb2-836" aria-hidden="true" tabindex="-1"></a>\int_{a}^{b}f(t)df(t) &amp; =\int_{a}^{b}f(t)f'(t)dt</span>
<span id="cb2-837"><a href="#cb2-837" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-838"><a href="#cb2-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-839"><a href="#cb2-839" aria-hidden="true" tabindex="-1"></a>Then, by the fundamental theorem of Calculus:</span>
<span id="cb2-840"><a href="#cb2-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-841"><a href="#cb2-841" aria-hidden="true" tabindex="-1"></a>$$\int_{a}^{b}f(t)df(t)=\int_{a}^{b}f(t)f'(t)dt=\frac{1}{2}f(t)^{2}|_{a}^{b}=\frac{1}{2}(f(b)^{2}-f(a)^{2})$$</span>
<span id="cb2-842"><a href="#cb2-842" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-843"><a href="#cb2-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-844"><a href="#cb2-844" aria-hidden="true" tabindex="-1"></a>::: rem*</span>
<span id="cb2-845"><a href="#cb2-845" aria-hidden="true" tabindex="-1"></a>There is a very close relationship between functions with bounded</span>
<span id="cb2-846"><a href="#cb2-846" aria-hidden="true" tabindex="-1"></a>variation and functions for which the classical integral makes sense.</span>
<span id="cb2-847"><a href="#cb2-847" aria-hidden="true" tabindex="-1"></a>For the Ito integral, the quadratic variation plays a similar role. The</span>
<span id="cb2-848"><a href="#cb2-848" aria-hidden="true" tabindex="-1"></a>quadratic variation of a smooth fuction $f\in C^{1}(<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>)$ is zero.</span>
<span id="cb2-849"><a href="#cb2-849" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-850"><a href="#cb2-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-851"><a href="#cb2-851" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-852"><a href="#cb2-852" aria-hidden="true" tabindex="-1"></a>[]{#ex:non-zero-quadratic-variation-example</span>
<span id="cb2-853"><a href="#cb2-853" aria-hidden="true" tabindex="-1"></a>label="ex:non-zero-quadratic-variation-example"}Suppose $f$ is a</span>
<span id="cb2-854"><a href="#cb2-854" aria-hidden="true" tabindex="-1"></a>continuous function satisfying the condition</span>
<span id="cb2-855"><a href="#cb2-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-856"><a href="#cb2-856" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-857"><a href="#cb2-857" aria-hidden="true" tabindex="-1"></a>|f(t)-f(s)| &amp; \leq C|t-s|^{1/2}</span>
<span id="cb2-858"><a href="#cb2-858" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-859"><a href="#cb2-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-860"><a href="#cb2-860" aria-hidden="true" tabindex="-1"></a>where $0&lt;C&lt;1$.</span>
<span id="cb2-861"><a href="#cb2-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-862"><a href="#cb2-862" aria-hidden="true" tabindex="-1"></a>In this case we have:</span>
<span id="cb2-863"><a href="#cb2-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-864"><a href="#cb2-864" aria-hidden="true" tabindex="-1"></a>$$0\leq|R_{n}-L_{n}|\leq C^{2}\sum_{i=1}^{n}(t_{i}-t_{i-1})=C^{2}(b-a)$$</span>
<span id="cb2-865"><a href="#cb2-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-866"><a href="#cb2-866" aria-hidden="true" tabindex="-1"></a>Hence, $\lim R_{n}\neq\lim L_{n}$ as</span>
<span id="cb2-867"><a href="#cb2-867" aria-hidden="true" tabindex="-1"></a>$\left\Vert \Delta_{n}\right\Vert \to0$ when $a\neq b$. Consequently,</span>
<span id="cb2-868"><a href="#cb2-868" aria-hidden="true" tabindex="-1"></a>the integral $\int_{a}^{b}f(t)df(t)$ cannot be defined for such a</span>
<span id="cb2-869"><a href="#cb2-869" aria-hidden="true" tabindex="-1"></a>function $f$. Observe that the quandratic variation of the function is</span>
<span id="cb2-870"><a href="#cb2-870" aria-hidden="true" tabindex="-1"></a>$b-a$ (non-zero).</span>
<span id="cb2-871"><a href="#cb2-871" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-872"><a href="#cb2-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-873"><a href="#cb2-873" aria-hidden="true" tabindex="-1"></a>We see from the above examples, that definining the integral</span>
<span id="cb2-874"><a href="#cb2-874" aria-hidden="true" tabindex="-1"></a>$\int_{a}^{b}f(t)dg(t)$ even when $f=g$ is a non-trivial problem.</span>
<span id="cb2-875"><a href="#cb2-875" aria-hidden="true" tabindex="-1"></a>Consider the question posed earlier - if $f$ and $g$ are continuous</span>
<span id="cb2-876"><a href="#cb2-876" aria-hidden="true" tabindex="-1"></a>functions on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$, can we define the integral</span>
<span id="cb2-877"><a href="#cb2-877" aria-hidden="true" tabindex="-1"></a>$\int_{a}^{b}f(t)dg(t)$? There is no simple answer to this question. But</span>
<span id="cb2-878"><a href="#cb2-878" aria-hidden="true" tabindex="-1"></a>then in view of example</span>
<span id="cb2-879"><a href="#cb2-879" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[ex:non-zero-quadratic-variation-example\]</span><span class="co">](#ex:non-zero-quadratic-variation-example)</span>{reference-type="ref"</span>
<span id="cb2-880"><a href="#cb2-880" aria-hidden="true" tabindex="-1"></a>reference="ex:non-zero-quadratic-variation-example"}), we can ask</span>
<span id="cb2-881"><a href="#cb2-881" aria-hidden="true" tabindex="-1"></a>another question:</span>
<span id="cb2-882"><a href="#cb2-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-883"><a href="#cb2-883" aria-hidden="true" tabindex="-1"></a>*Question*. Are there continuous functions $f$ satisfying the condition</span>
<span id="cb2-884"><a href="#cb2-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-885"><a href="#cb2-885" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-886"><a href="#cb2-886" aria-hidden="true" tabindex="-1"></a>|f(t)-f(s)| &amp; \leq C|t-s|^{1/2}</span>
<span id="cb2-887"><a href="#cb2-887" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-888"><a href="#cb2-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-889"><a href="#cb2-889" aria-hidden="true" tabindex="-1"></a><span class="fu">### Brownian motion as the limit of a symmetric random walk.</span></span>
<span id="cb2-890"><a href="#cb2-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-891"><a href="#cb2-891" aria-hidden="true" tabindex="-1"></a>Consider a random walk starting at $0$ with jumps $h$ and $-h$ equally</span>
<span id="cb2-892"><a href="#cb2-892" aria-hidden="true" tabindex="-1"></a>at times $\delta$, $2\delta$, $\ldots$ where $h$ and $\delta$ are</span>
<span id="cb2-893"><a href="#cb2-893" aria-hidden="true" tabindex="-1"></a>positive numbers. More precisely, let $<span class="sc">\{</span>X_{n}<span class="sc">\}</span>_{n=1}^{\infty}$ be a</span>
<span id="cb2-894"><a href="#cb2-894" aria-hidden="true" tabindex="-1"></a>sequence of independent and identically distributed random variables</span>
<span id="cb2-895"><a href="#cb2-895" aria-hidden="true" tabindex="-1"></a>with :</span>
<span id="cb2-896"><a href="#cb2-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-897"><a href="#cb2-897" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-898"><a href="#cb2-898" aria-hidden="true" tabindex="-1"></a>\mathbb{P}<span class="sc">\{</span>X_{j}=h<span class="sc">\}</span> &amp; =\mathbb{P}<span class="sc">\{</span>X_{j}=-h<span class="sc">\}</span>=\frac{1}{2}</span>
<span id="cb2-899"><a href="#cb2-899" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-900"><a href="#cb2-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-901"><a href="#cb2-901" aria-hidden="true" tabindex="-1"></a>Let $Y_{\delta,h}(0)=0$ and put:</span>
<span id="cb2-902"><a href="#cb2-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-903"><a href="#cb2-903" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-904"><a href="#cb2-904" aria-hidden="true" tabindex="-1"></a>Y_{\delta,h}(n\delta) &amp; =X_{1}+X_{2}+\ldots+X_{n}</span>
<span id="cb2-905"><a href="#cb2-905" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-906"><a href="#cb2-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-907"><a href="#cb2-907" aria-hidden="true" tabindex="-1"></a>For $t&gt;0$, define $Y_{\delta,h}(t)$ by linearization that is, for</span>
<span id="cb2-908"><a href="#cb2-908" aria-hidden="true" tabindex="-1"></a>$n\delta&lt;t&lt;(n+1)\delta$, define:</span>
<span id="cb2-909"><a href="#cb2-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-910"><a href="#cb2-910" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-911"><a href="#cb2-911" aria-hidden="true" tabindex="-1"></a>Y_{\delta,h}(t) &amp; =\frac{(n+1)\delta-t}{\delta}Y_{\delta,h}(n\delta)+\frac{t-n\delta}{\delta}Y_{\delta,h}((n+1)\delta)</span>
<span id="cb2-912"><a href="#cb2-912" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-913"><a href="#cb2-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-914"><a href="#cb2-914" aria-hidden="true" tabindex="-1"></a>We can think of $Y_{\delta,h}(t)$ as the position of the random walk at</span>
<span id="cb2-915"><a href="#cb2-915" aria-hidden="true" tabindex="-1"></a>time $t$. In particular, $X_{1}+X_{2}+\ldots+X_{n}$ is the position of</span>
<span id="cb2-916"><a href="#cb2-916" aria-hidden="true" tabindex="-1"></a>this random walk at time $n\delta$.</span>
<span id="cb2-917"><a href="#cb2-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-918"><a href="#cb2-918" aria-hidden="true" tabindex="-1"></a>*Question*. What is the limit of the random walk $Y_{\delta,h}$ as</span>
<span id="cb2-919"><a href="#cb2-919" aria-hidden="true" tabindex="-1"></a>$\delta,h\to0$?</span>
<span id="cb2-920"><a href="#cb2-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-921"><a href="#cb2-921" aria-hidden="true" tabindex="-1"></a>Recall that the characteristic function of a random variable $X$ is</span>
<span id="cb2-922"><a href="#cb2-922" aria-hidden="true" tabindex="-1"></a>$\phi_{X}(\lambda)=\mathbb{E}\exp<span class="co">[</span><span class="ot">i\lambda X</span><span class="co">]</span>$. In order to find out the</span>
<span id="cb2-923"><a href="#cb2-923" aria-hidden="true" tabindex="-1"></a>answer, let us compute the following limit of the characteristic</span>
<span id="cb2-924"><a href="#cb2-924" aria-hidden="true" tabindex="-1"></a>function of $Y_{\delta,h}(t)$:</span>
<span id="cb2-925"><a href="#cb2-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-926"><a href="#cb2-926" aria-hidden="true" tabindex="-1"></a>$$\lim_{\delta,h\to0}\mathbb{E}\exp\left<span class="co">[</span><span class="ot">i\lambda Y_{\delta,h}(t)\right</span><span class="co">]</span>$$</span>
<span id="cb2-927"><a href="#cb2-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-928"><a href="#cb2-928" aria-hidden="true" tabindex="-1"></a>where $\lambda\in\mathbf{R}$is fixed. For heuristic derivation, let</span>
<span id="cb2-929"><a href="#cb2-929" aria-hidden="true" tabindex="-1"></a>$t=n\delta$ and so $n=t/\delta$. Then we have:</span>
<span id="cb2-930"><a href="#cb2-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-931"><a href="#cb2-931" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-932"><a href="#cb2-932" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\exp\left<span class="co">[</span><span class="ot">i\lambda Y_{\delta,h}(t)\right</span><span class="co">]</span> &amp; =\prod_{j=1}^{n}\mathbb{E}e^{i\lambda X_{j}}<span class="sc">\\</span></span>
<span id="cb2-933"><a href="#cb2-933" aria-hidden="true" tabindex="-1"></a> &amp; =\prod_{j=1}^{n}\left(\frac{1}{2}e^{i\lambda h}+\frac{1}{2}e^{-i\lambda h}\right)<span class="sc">\\</span></span>
<span id="cb2-934"><a href="#cb2-934" aria-hidden="true" tabindex="-1"></a> &amp; =\left(\frac{1}{2}e^{i\lambda h}+\frac{1}{2}e^{-i\lambda h}\right)^{n}<span class="sc">\\</span></span>
<span id="cb2-935"><a href="#cb2-935" aria-hidden="true" tabindex="-1"></a> &amp; =\left(\cos\lambda h\right)^{n}<span class="sc">\\</span></span>
<span id="cb2-936"><a href="#cb2-936" aria-hidden="true" tabindex="-1"></a> &amp; =\left(\cos\lambda h\right)^{t/\delta}</span>
<span id="cb2-937"><a href="#cb2-937" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-938"><a href="#cb2-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-939"><a href="#cb2-939" aria-hidden="true" tabindex="-1"></a>For fixed $t$ and $\lambda$, when $\delta$ and $h$ independently</span>
<span id="cb2-940"><a href="#cb2-940" aria-hidden="true" tabindex="-1"></a>approach $0$, the limit of</span>
<span id="cb2-941"><a href="#cb2-941" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}\exp\left<span class="co">[</span><span class="ot">i\lambda Y_{\delta,h}(t)\right</span><span class="co">]</span>$ may not exist. For</span>
<span id="cb2-942"><a href="#cb2-942" aria-hidden="true" tabindex="-1"></a>example, holding $h$ constant, letting $\delta\to0$, since</span>
<span id="cb2-943"><a href="#cb2-943" aria-hidden="true" tabindex="-1"></a>$-1\leq\cos\theta\leq1$, the function</span>
<span id="cb2-944"><a href="#cb2-944" aria-hidden="true" tabindex="-1"></a>$\left(\cos\lambda h\right)^{t/\delta}\to0$. Holding $\delta$ constant,</span>
<span id="cb2-945"><a href="#cb2-945" aria-hidden="true" tabindex="-1"></a>letting $h\to0$, the function</span>
<span id="cb2-946"><a href="#cb2-946" aria-hidden="true" tabindex="-1"></a>$\left(\cos\lambda h\right)^{t/\delta}\to1$. In order for the limit to</span>
<span id="cb2-947"><a href="#cb2-947" aria-hidden="true" tabindex="-1"></a>exist, we impose a certain relationship between $\delta$ and $h$.</span>
<span id="cb2-948"><a href="#cb2-948" aria-hidden="true" tabindex="-1"></a>However, depending on the relationship, we may obtain different limits.</span>
<span id="cb2-949"><a href="#cb2-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-950"><a href="#cb2-950" aria-hidden="true" tabindex="-1"></a>Let $u=\cos(\lambda h)^{1/\delta}$. Then</span>
<span id="cb2-951"><a href="#cb2-951" aria-hidden="true" tabindex="-1"></a>$\ln u=\frac{1}{\delta}\ln\cos(\lambda h)$. Note that:</span>
<span id="cb2-952"><a href="#cb2-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-953"><a href="#cb2-953" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-954"><a href="#cb2-954" aria-hidden="true" tabindex="-1"></a>\cos(\lambda h) &amp; \approx1-\frac{1}{2}\lambda^{2}h^{2}</span>
<span id="cb2-955"><a href="#cb2-955" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-956"><a href="#cb2-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-957"><a href="#cb2-957" aria-hidden="true" tabindex="-1"></a>And $\ln(1+x)\approx x$. Hence,</span>
<span id="cb2-958"><a href="#cb2-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-959"><a href="#cb2-959" aria-hidden="true" tabindex="-1"></a>$$\ln\cos(\lambda h)\approx\ln\left(1-\frac{1}{2}\lambda^{2}h^{2}\right)\approx-\frac{1}{2}\lambda^{2}h^{2}$$</span>
<span id="cb2-960"><a href="#cb2-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-961"><a href="#cb2-961" aria-hidden="true" tabindex="-1"></a>Therefore for small $\lambda$ and $h$, we have</span>
<span id="cb2-962"><a href="#cb2-962" aria-hidden="true" tabindex="-1"></a>$\ln u\approx-\frac{1}{2\delta}\lambda^{2}h^{2}$ and so:</span>
<span id="cb2-963"><a href="#cb2-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-964"><a href="#cb2-964" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-965"><a href="#cb2-965" aria-hidden="true" tabindex="-1"></a>u &amp; \approx\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2\delta}\lambda^{2}h^{2}\right</span><span class="co">]</span></span>
<span id="cb2-966"><a href="#cb2-966" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-967"><a href="#cb2-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-968"><a href="#cb2-968" aria-hidden="true" tabindex="-1"></a>In particular, if $\delta$ and $h$ are related by $h^{2}=\delta$, then</span>
<span id="cb2-969"><a href="#cb2-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-970"><a href="#cb2-970" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-971"><a href="#cb2-971" aria-hidden="true" tabindex="-1"></a>\lim_{\delta\to0}\mathbb{E}\exp\left<span class="co">[</span><span class="ot">i\lambda Y_{\delta,h}(t)\right</span><span class="co">]</span> &amp; =e^{-\frac{1}{2}\lambda^{2}t}</span>
<span id="cb2-972"><a href="#cb2-972" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-973"><a href="#cb2-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-974"><a href="#cb2-974" aria-hidden="true" tabindex="-1"></a>But, $e^{-\frac{1}{2}\lambda^{2}t}$ is the characteristic function of a</span>
<span id="cb2-975"><a href="#cb2-975" aria-hidden="true" tabindex="-1"></a>Gaussian random variable with mean $0$ and variance $t$. Thus, we have</span>
<span id="cb2-976"><a href="#cb2-976" aria-hidden="true" tabindex="-1"></a>derived the following theorem about the limit of the random walk</span>
<span id="cb2-977"><a href="#cb2-977" aria-hidden="true" tabindex="-1"></a>$Y_{\delta,h}$ as $\delta,h\to0$ in such a way that $h^{2}=\delta$.</span>
<span id="cb2-978"><a href="#cb2-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-979"><a href="#cb2-979" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb2-980"><a href="#cb2-980" aria-hidden="true" tabindex="-1"></a>Let $Y_{\delta,h}(t)$ be the random walk starting at $0$ with jumps $h$</span>
<span id="cb2-981"><a href="#cb2-981" aria-hidden="true" tabindex="-1"></a>and $-h$ equally likely at times $\delta$, $2\delta$, $3\delta$,</span>
<span id="cb2-982"><a href="#cb2-982" aria-hidden="true" tabindex="-1"></a>$\ldots$. Assume that $h^{2}=\delta$. Then, for each $t\geq0$, the</span>
<span id="cb2-983"><a href="#cb2-983" aria-hidden="true" tabindex="-1"></a>limit:</span>
<span id="cb2-984"><a href="#cb2-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-985"><a href="#cb2-985" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-986"><a href="#cb2-986" aria-hidden="true" tabindex="-1"></a>\lim_{\delta\to0}Y_{\delta,h}(t) &amp; =B(t)</span>
<span id="cb2-987"><a href="#cb2-987" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$ exists in distribution. Moreover, we have:</span>
<span id="cb2-988"><a href="#cb2-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-989"><a href="#cb2-989" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-990"><a href="#cb2-990" aria-hidden="true" tabindex="-1"></a>\mathbb{E}e^{i\lambda B(t)} &amp; =e^{-\frac{1}{2}\lambda^{2}t}</span>
<span id="cb2-991"><a href="#cb2-991" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-992"><a href="#cb2-992" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-993"><a href="#cb2-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-994"><a href="#cb2-994" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb2-995"><a href="#cb2-995" aria-hidden="true" tabindex="-1"></a>[]{#th:quadratic-variation-of-bm-approaches-t-in-mean-square</span>
<span id="cb2-996"><a href="#cb2-996" aria-hidden="true" tabindex="-1"></a>label="th:quadratic-variation-of-bm-approaches-t-in-mean-square"}(Quadratic</span>
<span id="cb2-997"><a href="#cb2-997" aria-hidden="true" tabindex="-1"></a>Variation of a Brownian motion). Let $(B_{t},t\ge0)$ be a standard</span>
<span id="cb2-998"><a href="#cb2-998" aria-hidden="true" tabindex="-1"></a>brownian motion. Then, for any sequence of partitions $(t_{j},j\leq n)$</span>
<span id="cb2-999"><a href="#cb2-999" aria-hidden="true" tabindex="-1"></a>of $<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>$ we have:</span>
<span id="cb2-1000"><a href="#cb2-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1001"><a href="#cb2-1001" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1002"><a href="#cb2-1002" aria-hidden="true" tabindex="-1"></a>\left\langle B\right\rangle _{t} &amp; =\sum_{j=1}^{n}(B_{t_{j+1}}-B_{t_{j}})^{2}\stackrel{L^{2}}{\to}t</span>
<span id="cb2-1003"><a href="#cb2-1003" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1004"><a href="#cb2-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1005"><a href="#cb2-1005" aria-hidden="true" tabindex="-1"></a>where the convergence is in the $L^{2}$ sense.</span>
<span id="cb2-1006"><a href="#cb2-1006" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1007"><a href="#cb2-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1008"><a href="#cb2-1008" aria-hidden="true" tabindex="-1"></a>::: rem*</span>
<span id="cb2-1009"><a href="#cb2-1009" aria-hidden="true" tabindex="-1"></a>It is reasonable to have some sort of convergence as we are dealing with</span>
<span id="cb2-1010"><a href="#cb2-1010" aria-hidden="true" tabindex="-1"></a>a sum of independent random variables. However, the conclusion would not</span>
<span id="cb2-1011"><a href="#cb2-1011" aria-hidden="true" tabindex="-1"></a>hold if the increments were not squared. So there is something more at</span>
<span id="cb2-1012"><a href="#cb2-1012" aria-hidden="true" tabindex="-1"></a>play here.</span>
<span id="cb2-1013"><a href="#cb2-1013" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1014"><a href="#cb2-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1015"><a href="#cb2-1015" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-1016"><a href="#cb2-1016" aria-hidden="true" tabindex="-1"></a>*Proof.* We have:</span>
<span id="cb2-1017"><a href="#cb2-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1018"><a href="#cb2-1018" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1019"><a href="#cb2-1019" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right</span><span class="co">]</span> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-\sum_{j=0}^{n-1}(t_{j+1}-t_{j})\right)^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1020"><a href="#cb2-1020" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}\left\{ (B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\right\} \right)^{2}\right</span><span class="co">]</span></span>
<span id="cb2-1021"><a href="#cb2-1021" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1022"><a href="#cb2-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1023"><a href="#cb2-1023" aria-hidden="true" tabindex="-1"></a>For simplicity, we define the variables</span>
<span id="cb2-1024"><a href="#cb2-1024" aria-hidden="true" tabindex="-1"></a>$X_{j}=(B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})$. Then, we may write:</span>
<span id="cb2-1025"><a href="#cb2-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1026"><a href="#cb2-1026" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1027"><a href="#cb2-1027" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right</span><span class="co">]</span> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}X_{j}\right)^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1028"><a href="#cb2-1028" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}X_{i}X_{j}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1029"><a href="#cb2-1029" aria-hidden="true" tabindex="-1"></a> &amp; =\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}\mathbb{E}<span class="co">[</span><span class="ot">X_{i}X_{j}</span><span class="co">]</span></span>
<span id="cb2-1030"><a href="#cb2-1030" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1031"><a href="#cb2-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1032"><a href="#cb2-1032" aria-hidden="true" tabindex="-1"></a>Now, the random variables $X_{j}$ are independent.</span>
<span id="cb2-1033"><a href="#cb2-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1034"><a href="#cb2-1034" aria-hidden="true" tabindex="-1"></a>The expectation of $X_{j}$ is</span>
<span id="cb2-1035"><a href="#cb2-1035" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}<span class="co">[</span><span class="ot">X_{j}</span><span class="co">]</span>=\mathbb{E}(B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})=0$.</span>
<span id="cb2-1036"><a href="#cb2-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1037"><a href="#cb2-1037" aria-hidden="true" tabindex="-1"></a>Since, $X_{i}$ and $X_{j}$ are independent, for $i\neq j$,</span>
<span id="cb2-1038"><a href="#cb2-1038" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}<span class="co">[</span><span class="ot">X_{i}X_{j}</span><span class="co">]</span>=\mathbb{E}X_{i}\cdot\mathbb{E}X_{j}=0$.</span>
<span id="cb2-1039"><a href="#cb2-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1040"><a href="#cb2-1040" aria-hidden="true" tabindex="-1"></a>Hence, we have:</span>
<span id="cb2-1041"><a href="#cb2-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1042"><a href="#cb2-1042" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1043"><a href="#cb2-1043" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right</span><span class="co">]</span> &amp; =\sum_{i=0}^{n-1}\mathbb{E}<span class="co">[</span><span class="ot">X_{i}^{2}</span><span class="co">]</span></span>
<span id="cb2-1044"><a href="#cb2-1044" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1045"><a href="#cb2-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1046"><a href="#cb2-1046" aria-hidden="true" tabindex="-1"></a>We now develop the expectation of the square of $X_{i}$. We have:</span>
<span id="cb2-1047"><a href="#cb2-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1048"><a href="#cb2-1048" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1049"><a href="#cb2-1049" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">X_{i}^{2}</span><span class="co">]</span> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">\left((B(t_{i+1})-B(t_{i}))^{2}-(t_{i+1}-t_{i})\right)^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1050"><a href="#cb2-1050" aria-hidden="true" tabindex="-1"></a> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">((B(t_{i+1})-B(t_{i}))^{4}-2(B(t_{i+1})-B(t_{i}))^{2}(t_{i+1}-t_{i})+(t_{i+1}-t_{i})^{2}\right</span><span class="co">]</span></span>
<span id="cb2-1051"><a href="#cb2-1051" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1052"><a href="#cb2-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1053"><a href="#cb2-1053" aria-hidden="true" tabindex="-1"></a>The MGF of the random variable $B(t_{i+1})-B(t_{i})$ is :</span>
<span id="cb2-1054"><a href="#cb2-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1055"><a href="#cb2-1055" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1056"><a href="#cb2-1056" aria-hidden="true" tabindex="-1"></a>\phi(\lambda) &amp; =\exp\left<span class="co">[</span><span class="ot">\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1057"><a href="#cb2-1057" aria-hidden="true" tabindex="-1"></a>\phi'(\lambda) &amp; =\lambda(t_{i+1}-t_{i})\exp\left<span class="co">[</span><span class="ot">\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1058"><a href="#cb2-1058" aria-hidden="true" tabindex="-1"></a>\phi''(\lambda) &amp; =\left<span class="co">[</span><span class="ot">(t_{i+1}-t_{i})+\lambda^{2}(t_{i+1}-t_{i})^{2}\right</span><span class="co">]</span>\exp\left<span class="co">[</span><span class="ot">\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1059"><a href="#cb2-1059" aria-hidden="true" tabindex="-1"></a>\phi^{(3)}(\lambda) &amp; =\left<span class="co">[</span><span class="ot">3\lambda(t_{i+1}-t_{i})^{2}+\lambda^{3}(t_{i+1}-t_{i})^{3}\right</span><span class="co">]</span>\exp\left<span class="co">[</span><span class="ot">\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1060"><a href="#cb2-1060" aria-hidden="true" tabindex="-1"></a>\phi^{(4)}(\lambda) &amp; =\left<span class="co">[</span><span class="ot">3(t_{i+1}-t_{i})^{2}+6\lambda^{2}(t_{i+1}-t_{i})^{3}+\lambda^{4}(t_{i+1}-t_{i})^{4}\right</span><span class="co">]</span>\exp\left<span class="co">[</span><span class="ot">\frac{\lambda^{2}(t_{i+1}-t_{i})}{2}\right</span><span class="co">]</span></span>
<span id="cb2-1061"><a href="#cb2-1061" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1062"><a href="#cb2-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1063"><a href="#cb2-1063" aria-hidden="true" tabindex="-1"></a>Thus, $\mathbb{E}<span class="co">[</span><span class="ot">(B(t_{i+1})-B(t_{i}))^{4}</span><span class="co">]</span>=3(t_{i+1}-t_{i})^{2}$.</span>
<span id="cb2-1064"><a href="#cb2-1064" aria-hidden="true" tabindex="-1"></a>Consequently,</span>
<span id="cb2-1065"><a href="#cb2-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1066"><a href="#cb2-1066" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1067"><a href="#cb2-1067" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">X_{i}^{2}</span><span class="co">]</span> &amp; =\mathbb{E}<span class="co">[</span><span class="ot">(B(t_{i+1})-B(t_{i}))^{4}</span><span class="co">]</span>-2(t_{i+1}-t_{i})\mathbb{E}<span class="co">[</span><span class="ot">(B(t_{i+1})-B(t_{i}))^{2}</span><span class="co">]</span>+(t_{i+1}-t_{i})^{2}<span class="sc">\\</span></span>
<span id="cb2-1068"><a href="#cb2-1068" aria-hidden="true" tabindex="-1"></a> &amp; =3(t_{i+1}-t_{i})^{2}-2(t_{i+1}-t_{i})^{2}+(t_{i+1}-t_{i})^{2}<span class="sc">\\</span></span>
<span id="cb2-1069"><a href="#cb2-1069" aria-hidden="true" tabindex="-1"></a> &amp; =2(t_{i+1}-t_{i})^{2}</span>
<span id="cb2-1070"><a href="#cb2-1070" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1071"><a href="#cb2-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1072"><a href="#cb2-1072" aria-hidden="true" tabindex="-1"></a>Putting all this together, we finally have that:</span>
<span id="cb2-1073"><a href="#cb2-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1074"><a href="#cb2-1074" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1075"><a href="#cb2-1075" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right</span><span class="co">]</span> &amp; =2\sum_{i=0}^{n-1}(t_{i+1}-t_{i})^{2}\label{eq:second-moment-of-qv}<span class="sc">\\</span></span>
<span id="cb2-1076"><a href="#cb2-1076" aria-hidden="true" tabindex="-1"></a> &amp; \leq2\left\Vert \Delta_{n}\right\Vert \sum_{i=0}^{n-1}(t_{i+1}-t_{i})\nonumber <span class="sc">\\</span></span>
<span id="cb2-1077"><a href="#cb2-1077" aria-hidden="true" tabindex="-1"></a> &amp; =2\left\Vert \Delta_{n}\right\Vert \cdot t\nonumber </span>
<span id="cb2-1078"><a href="#cb2-1078" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1079"><a href="#cb2-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1080"><a href="#cb2-1080" aria-hidden="true" tabindex="-1"></a>As $n\to\infty$, $\left\Vert \Delta_{n}\right\Vert \to0$. Hence,</span>
<span id="cb2-1081"><a href="#cb2-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1082"><a href="#cb2-1082" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1083"><a href="#cb2-1083" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right</span><span class="co">]</span> &amp; =0</span>
<span id="cb2-1084"><a href="#cb2-1084" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1085"><a href="#cb2-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1086"><a href="#cb2-1086" aria-hidden="true" tabindex="-1"></a>Hence, the sequence of random variables</span>
<span id="cb2-1087"><a href="#cb2-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1088"><a href="#cb2-1088" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1089"><a href="#cb2-1089" aria-hidden="true" tabindex="-1"></a>\sum_{j=0}^{n-1}(B(t_{j+1})-B(t_{j}))^{2} &amp; \stackrel{L^{2}}{\to}t</span>
<span id="cb2-1090"><a href="#cb2-1090" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$ ◻</span>
<span id="cb2-1091"><a href="#cb2-1091" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1092"><a href="#cb2-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1093"><a href="#cb2-1093" aria-hidden="true" tabindex="-1"></a>::: cor</span>
<span id="cb2-1094"><a href="#cb2-1094" aria-hidden="true" tabindex="-1"></a>(Quadratic Variation of a Brownian Motion Path). Let $(B_{s},s\geq0)$ be</span>
<span id="cb2-1095"><a href="#cb2-1095" aria-hidden="true" tabindex="-1"></a>a Brownian motion. For every $n\in\mathbf{N}$, consider the dyadic</span>
<span id="cb2-1096"><a href="#cb2-1096" aria-hidden="true" tabindex="-1"></a>partition $(t_{j},j\leq2^{n})$ of $<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>$ where</span>
<span id="cb2-1097"><a href="#cb2-1097" aria-hidden="true" tabindex="-1"></a>$t_{j}=\frac{j}{2^{n}}t$. Then we have that:</span>
<span id="cb2-1098"><a href="#cb2-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1099"><a href="#cb2-1099" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1100"><a href="#cb2-1100" aria-hidden="true" tabindex="-1"></a>\left\langle B\right\rangle _{t} &amp; =\sum_{j=1}^{2^{n}-1}(B_{t_{j+1}}-B_{t_{j}})^{2}\stackrel{a.s.}{\to}t</span>
<span id="cb2-1101"><a href="#cb2-1101" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1102"><a href="#cb2-1102" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1103"><a href="#cb2-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1104"><a href="#cb2-1104" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-1105"><a href="#cb2-1105" aria-hidden="true" tabindex="-1"></a>*Proof.* We have $(t_{i+1}-t_{i})=\frac{t}{2^{n}}.$ Borrowing equation</span>
<span id="cb2-1106"><a href="#cb2-1106" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[eq:second-moment-of-qv\]</span><span class="co">](#eq:second-moment-of-qv)</span>{reference-type="ref"</span>
<span id="cb2-1107"><a href="#cb2-1107" aria-hidden="true" tabindex="-1"></a>reference="eq:second-moment-of-qv"}) from the proof of theorem</span>
<span id="cb2-1108"><a href="#cb2-1108" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[th:quadratic-variation-of-bm-approaches-t-in-mean-square\]</span><span class="co">](#th:quadratic-variation-of-bm-approaches-t-in-mean-square)</span>{reference-type="ref"</span>
<span id="cb2-1109"><a href="#cb2-1109" aria-hidden="true" tabindex="-1"></a>reference="th:quadratic-variation-of-bm-approaches-t-in-mean-square"}),</span>
<span id="cb2-1110"><a href="#cb2-1110" aria-hidden="true" tabindex="-1"></a>we have that:</span>
<span id="cb2-1111"><a href="#cb2-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1112"><a href="#cb2-1112" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1113"><a href="#cb2-1113" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right</span><span class="co">]</span> &amp; =2\sum_{i=0}^{2^{n}-1}\left(\frac{t}{2^{n}}\right)^{2}<span class="sc">\\</span></span>
<span id="cb2-1114"><a href="#cb2-1114" aria-hidden="true" tabindex="-1"></a> &amp; =2\cdot(2^{n})\cdot\frac{t^{2}}{2^{2n}}<span class="sc">\\</span></span>
<span id="cb2-1115"><a href="#cb2-1115" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{2t^{2}}{2^{n}}</span>
<span id="cb2-1116"><a href="#cb2-1116" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1117"><a href="#cb2-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1118"><a href="#cb2-1118" aria-hidden="true" tabindex="-1"></a>By Chebyshev's inequality,</span>
<span id="cb2-1119"><a href="#cb2-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1120"><a href="#cb2-1120" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1121"><a href="#cb2-1121" aria-hidden="true" tabindex="-1"></a>\mathbb{P}\left(\left|\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right|&gt;\epsilon\right) &amp; \leq\frac{1}{\epsilon^{2}}\mathbb{E}\left<span class="co">[</span><span class="ot">\left(\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right)^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1122"><a href="#cb2-1122" aria-hidden="true" tabindex="-1"></a> &amp; \leq\frac{1}{\epsilon^{2}}\cdot\frac{2t^{2}}{2^{n}}</span>
<span id="cb2-1123"><a href="#cb2-1123" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1124"><a href="#cb2-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1125"><a href="#cb2-1125" aria-hidden="true" tabindex="-1"></a>Define</span>
<span id="cb2-1126"><a href="#cb2-1126" aria-hidden="true" tabindex="-1"></a>$A_{n}:=\left<span class="sc">\{</span> \left|\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2}-t\right|&gt;\epsilon\right<span class="sc">\}</span>$.</span>
<span id="cb2-1127"><a href="#cb2-1127" aria-hidden="true" tabindex="-1"></a>Since, $\sum\frac{1}{2^{n}}$ is a convergent series, any multiple of it,</span>
<span id="cb2-1128"><a href="#cb2-1128" aria-hidden="true" tabindex="-1"></a>$(2t^{2}/\epsilon^{2})\sum\frac{1}{2^{n}}$ also converges. Now,</span>
<span id="cb2-1129"><a href="#cb2-1129" aria-hidden="true" tabindex="-1"></a>$0\leq\mathbb{P}(A_{n})\leq\frac{(2t^{2}/\epsilon^{2})}{2^{n}}$. By the</span>
<span id="cb2-1130"><a href="#cb2-1130" aria-hidden="true" tabindex="-1"></a>comparison test, $\sum\mathbb{P}(A_{n})$ converges to a finite value. By</span>
<span id="cb2-1131"><a href="#cb2-1131" aria-hidden="true" tabindex="-1"></a>Theorem</span>
<span id="cb2-1132"><a href="#cb2-1132" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[th:sufficient-condition-for-almost-sure-convergence\]</span><span class="co">](#th:sufficient-condition-for-almost-sure-convergence)</span>{reference-type="ref"</span>
<span id="cb2-1133"><a href="#cb2-1133" aria-hidden="true" tabindex="-1"></a>reference="th:sufficient-condition-for-almost-sure-convergence"}),</span>
<span id="cb2-1134"><a href="#cb2-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1135"><a href="#cb2-1135" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1136"><a href="#cb2-1136" aria-hidden="true" tabindex="-1"></a>\sum_{j=0}^{2^{n}-1}(B(t_{j+1})-B(t_{j}))^{2} &amp; \stackrel{a.s.}{\to}t</span>
<span id="cb2-1137"><a href="#cb2-1137" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$ ◻</span>
<span id="cb2-1138"><a href="#cb2-1138" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1139"><a href="#cb2-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1140"><a href="#cb2-1140" aria-hidden="true" tabindex="-1"></a>We are now ready to show that every Brownian motion path has infinite</span>
<span id="cb2-1141"><a href="#cb2-1141" aria-hidden="true" tabindex="-1"></a>variation.</span>
<span id="cb2-1142"><a href="#cb2-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1143"><a href="#cb2-1143" aria-hidden="true" tabindex="-1"></a>If $g$ is a $C^{1}$ function,</span>
<span id="cb2-1144"><a href="#cb2-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1145"><a href="#cb2-1145" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1146"><a href="#cb2-1146" aria-hidden="true" tabindex="-1"></a>\int_{0}^{t}|g'(t)|dt &amp; =\int_{0}^{t}\sqrt{g'(t)^{2}}dt<span class="sc">\\</span></span>
<span id="cb2-1147"><a href="#cb2-1147" aria-hidden="true" tabindex="-1"></a> &amp; \leq\int_{0}^{t}\sqrt{1+g'(t)^{2}}dt<span class="sc">\\</span></span>
<span id="cb2-1148"><a href="#cb2-1148" aria-hidden="true" tabindex="-1"></a> &amp; =l_{g}(t)</span>
<span id="cb2-1149"><a href="#cb2-1149" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1150"><a href="#cb2-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1151"><a href="#cb2-1151" aria-hidden="true" tabindex="-1"></a>where $l_{g}(t)$ is the arclength of the function $g$ between $<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>$.</span>
<span id="cb2-1152"><a href="#cb2-1152" aria-hidden="true" tabindex="-1"></a>So, $V_{g}(t)\leq l_{g}(t)$ and further:</span>
<span id="cb2-1153"><a href="#cb2-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1154"><a href="#cb2-1154" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1155"><a href="#cb2-1155" aria-hidden="true" tabindex="-1"></a>l_{g}(t) &amp; =\int_{0}^{t}\sqrt{1+g'(t)^{2}}dt<span class="sc">\\</span></span>
<span id="cb2-1156"><a href="#cb2-1156" aria-hidden="true" tabindex="-1"></a> &amp; \leq\int_{0}^{t}\left(1+\sqrt{g'(t)^{2}}\right)dt<span class="sc">\\</span></span>
<span id="cb2-1157"><a href="#cb2-1157" aria-hidden="true" tabindex="-1"></a> &amp; \leq t+V_{g}(t)</span>
<span id="cb2-1158"><a href="#cb2-1158" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1159"><a href="#cb2-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1160"><a href="#cb2-1160" aria-hidden="true" tabindex="-1"></a>Consequently,</span>
<span id="cb2-1161"><a href="#cb2-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1162"><a href="#cb2-1162" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1163"><a href="#cb2-1163" aria-hidden="true" tabindex="-1"></a>V_{g}(t) &amp; \leq l_{g}(t)\leq t+V_{g}(t)</span>
<span id="cb2-1164"><a href="#cb2-1164" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1165"><a href="#cb2-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1166"><a href="#cb2-1166" aria-hidden="true" tabindex="-1"></a>The total variation of the function is finite if and only if it's</span>
<span id="cb2-1167"><a href="#cb2-1167" aria-hidden="true" tabindex="-1"></a>arclength is.</span>
<span id="cb2-1168"><a href="#cb2-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1169"><a href="#cb2-1169" aria-hidden="true" tabindex="-1"></a>Hence, intuitively, our claim is that a Brownian motion path on $<span class="co">[</span><span class="ot">0,T</span><span class="co">]</span>$</span>
<span id="cb2-1170"><a href="#cb2-1170" aria-hidden="true" tabindex="-1"></a>has infinite arc-length. Since</span>
<span id="cb2-1171"><a href="#cb2-1171" aria-hidden="true" tabindex="-1"></a>$g\in C^{1}(<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>)\Longrightarrow(V_{g}(t)&lt;\infty)$, it follows that</span>
<span id="cb2-1172"><a href="#cb2-1172" aria-hidden="true" tabindex="-1"></a>$(V_{g}(t)\to\infty)\Longrightarrow g\notin C^{1}$.</span>
<span id="cb2-1173"><a href="#cb2-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1174"><a href="#cb2-1174" aria-hidden="true" tabindex="-1"></a>::: cor</span>
<span id="cb2-1175"><a href="#cb2-1175" aria-hidden="true" tabindex="-1"></a>(Brownian Motion paths have unbounded total variation.)</span>
<span id="cb2-1176"><a href="#cb2-1176" aria-hidden="true" tabindex="-1"></a>[]{#th:bm-paths-have-unbounded-total-variation</span>
<span id="cb2-1177"><a href="#cb2-1177" aria-hidden="true" tabindex="-1"></a>label="th:bm-paths-have-unbounded-total-variation"} Let $(B_{s},s\geq0)$</span>
<span id="cb2-1178"><a href="#cb2-1178" aria-hidden="true" tabindex="-1"></a>be a Brownian motion. Then, the random functions $B(s,\omega)$ on the</span>
<span id="cb2-1179"><a href="#cb2-1179" aria-hidden="true" tabindex="-1"></a>interval $<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>$ have unbounded variation almost surely.</span>
<span id="cb2-1180"><a href="#cb2-1180" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1181"><a href="#cb2-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1182"><a href="#cb2-1182" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-1183"><a href="#cb2-1183" aria-hidden="true" tabindex="-1"></a>*Proof.* Take the sequence of dyadic partitions of $<span class="co">[</span><span class="ot">0,t</span><span class="co">]</span>$:</span>
<span id="cb2-1184"><a href="#cb2-1184" aria-hidden="true" tabindex="-1"></a>$t_{j}=\frac{j}{2^{n}}t$, $n\in\mathbf{N}$, $j\leq2^{n}$. By pulling out</span>
<span id="cb2-1185"><a href="#cb2-1185" aria-hidden="true" tabindex="-1"></a>the worst increment, we have the trivial bound for every $\omega$:</span>
<span id="cb2-1186"><a href="#cb2-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1187"><a href="#cb2-1187" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1188"><a href="#cb2-1188" aria-hidden="true" tabindex="-1"></a>\sum_{j=0}^{2^{n}-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))^{2} &amp; \leq\max_{0\leq j\leq2^{n}}\left|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)\right|\cdot\sum_{j=0}^{2^{n}-1}(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))\label{eq:trivial-upper-bound-on-quadratic-variation}</span>
<span id="cb2-1189"><a href="#cb2-1189" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1190"><a href="#cb2-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1191"><a href="#cb2-1191" aria-hidden="true" tabindex="-1"></a>We proceed by contradiction. Let $A'$ be the set of all $\omega$, for</span>
<span id="cb2-1192"><a href="#cb2-1192" aria-hidden="true" tabindex="-1"></a>which the Brownian motion paths have bounded total variation. Let $A$ be</span>
<span id="cb2-1193"><a href="#cb2-1193" aria-hidden="true" tabindex="-1"></a>event that the Brownian motion paths have unbounded variation.</span>
<span id="cb2-1194"><a href="#cb2-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1195"><a href="#cb2-1195" aria-hidden="true" tabindex="-1"></a>By the definition of total variation, that would imply,</span>
<span id="cb2-1196"><a href="#cb2-1196" aria-hidden="true" tabindex="-1"></a>$\exists M\in\mathbf{N}$ :</span>
<span id="cb2-1197"><a href="#cb2-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1198"><a href="#cb2-1198" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1199"><a href="#cb2-1199" aria-hidden="true" tabindex="-1"></a>(\forall\omega\in A')\quad\lim_{n\to\infty}\sum_{j=0}^{2^{n}-1}\left|(B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega))\right| &amp; &lt;M</span>
<span id="cb2-1200"><a href="#cb2-1200" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1201"><a href="#cb2-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1202"><a href="#cb2-1202" aria-hidden="true" tabindex="-1"></a>Since Brownian Motion paths are continuous on the compact set</span>
<span id="cb2-1203"><a href="#cb2-1203" aria-hidden="true" tabindex="-1"></a>$[\frac{j}{2^{n}}t,\frac{j+1}{2^{n}}t]$, they are uniformly continuous.</span>
<span id="cb2-1204"><a href="#cb2-1204" aria-hidden="true" tabindex="-1"></a>So, as $n\to\infty$, $|t_{j+1}-t_{j}|\to0$ and therefore</span>
<span id="cb2-1205"><a href="#cb2-1205" aria-hidden="true" tabindex="-1"></a>$|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)|\to0$. And consequently,</span>
<span id="cb2-1206"><a href="#cb2-1206" aria-hidden="true" tabindex="-1"></a>$\max_{0\leq j\leq2^{n}}\left|B_{t_{j+1}}(\omega)-B_{t_{j}}(\omega)\right|\to0$.</span>
<span id="cb2-1207"><a href="#cb2-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1208"><a href="#cb2-1208" aria-hidden="true" tabindex="-1"></a>Thus, for every $\omega\in A'$, the right hand side of the inequality</span>
<span id="cb2-1209"><a href="#cb2-1209" aria-hidden="true" tabindex="-1"></a>([\[eq:trivial-upper-bound-on-quadratic-variation\]](#eq:trivial-upper-bound-on-quadratic-variation){reference-type="ref"</span>
<span id="cb2-1210"><a href="#cb2-1210" aria-hidden="true" tabindex="-1"></a>reference="eq:trivial-upper-bound-on-quadratic-variation"}), converges</span>
<span id="cb2-1211"><a href="#cb2-1211" aria-hidden="true" tabindex="-1"></a>to $0$ and therefore the left hand side converges to $0$. But, this</span>
<span id="cb2-1212"><a href="#cb2-1212" aria-hidden="true" tabindex="-1"></a>contradicts the fact that</span>
<span id="cb2-1213"><a href="#cb2-1213" aria-hidden="true" tabindex="-1"></a>$\left\langle B\right\rangle _{t}\stackrel{a.s.}{\to}t$. So, $A'$ is a</span>
<span id="cb2-1214"><a href="#cb2-1214" aria-hidden="true" tabindex="-1"></a>null set, and $\mathbb{P}(A')=0$ and $\mathbb{P}(A)=1$. This closes the</span>
<span id="cb2-1215"><a href="#cb2-1215" aria-hidden="true" tabindex="-1"></a>proof. ◻</span>
<span id="cb2-1216"><a href="#cb2-1216" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1217"><a href="#cb2-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1218"><a href="#cb2-1218" aria-hidden="true" tabindex="-1"></a>## What exactly is $(\Omega,\mathcal{F},\mathbb{P})$ in mathematical finance?</span>
<span id="cb2-1219"><a href="#cb2-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1220"><a href="#cb2-1220" aria-hidden="true" tabindex="-1"></a>If we make the simplifying assumption that the process paths are</span>
<span id="cb2-1221"><a href="#cb2-1221" aria-hidden="true" tabindex="-1"></a>continuous, we obtain the set of all continuous functions on $[0,T]$,</span>
<span id="cb2-1222"><a href="#cb2-1222" aria-hidden="true" tabindex="-1"></a>denoted by $C[0,T]$. This is a very rich space. In a more general model,</span>
<span id="cb2-1223"><a href="#cb2-1223" aria-hidden="true" tabindex="-1"></a>it is assumed that the process paths are right continuous with left</span>
<span id="cb2-1224"><a href="#cb2-1224" aria-hidden="true" tabindex="-1"></a>limits (regular right-continuous RRC, cadlag) functions.</span>
<span id="cb2-1225"><a href="#cb2-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1226"><a href="#cb2-1226" aria-hidden="true" tabindex="-1"></a>Let the sample space $\Omega=D[0,T]$ be the set of all RRC functions on</span>
<span id="cb2-1227"><a href="#cb2-1227" aria-hidden="true" tabindex="-1"></a>$[0,T]$. An element of this set is a RRC function from $[0,T]$ into</span>
<span id="cb2-1228"><a href="#cb2-1228" aria-hidden="true" tabindex="-1"></a>$\mathbf{R}$. First we must decide what kind of sets of these functions</span>
<span id="cb2-1229"><a href="#cb2-1229" aria-hidden="true" tabindex="-1"></a>are measurable? The simplest set for which we would like to calculate</span>
<span id="cb2-1230"><a href="#cb2-1230" aria-hidden="true" tabindex="-1"></a>the probabilities are sets of the form $\{a\leq S(t_{1})\leq b\}$ for</span>
<span id="cb2-1231"><a href="#cb2-1231" aria-hidden="true" tabindex="-1"></a>some $t_{1}$. If $S(t)$ represents the price of a stock at time $t$,</span>
<span id="cb2-1232"><a href="#cb2-1232" aria-hidden="true" tabindex="-1"></a>then the probability of such a set gives the probability that the stock</span>
<span id="cb2-1233"><a href="#cb2-1233" aria-hidden="true" tabindex="-1"></a>price at time $t_{1}$ is between $a$ and $b$. We are also interested in</span>
<span id="cb2-1234"><a href="#cb2-1234" aria-hidden="true" tabindex="-1"></a>how the price of the stock at time $t_{1}$ affects the price at another</span>
<span id="cb2-1235"><a href="#cb2-1235" aria-hidden="true" tabindex="-1"></a>time $t_{2}$. Thus, we need to talk about the joint distribution of</span>
<span id="cb2-1236"><a href="#cb2-1236" aria-hidden="true" tabindex="-1"></a>stock prices $S(t_{1})$ and $S(t_{2})$. This means that we need to</span>
<span id="cb2-1237"><a href="#cb2-1237" aria-hidden="true" tabindex="-1"></a>define probability on the sets of the form</span>
<span id="cb2-1238"><a href="#cb2-1238" aria-hidden="true" tabindex="-1"></a>$\{S(t_{1})\in B_{1},S(t_{2})\in B_{2}\}$ where $B_{1}$ and $B_{2}$ are</span>
<span id="cb2-1239"><a href="#cb2-1239" aria-hidden="true" tabindex="-1"></a>intervals on the line. More generally, we would like to have all the</span>
<span id="cb2-1240"><a href="#cb2-1240" aria-hidden="true" tabindex="-1"></a>finite-dimensional distributions of the process $S(t)$, that is, the</span>
<span id="cb2-1241"><a href="#cb2-1241" aria-hidden="true" tabindex="-1"></a>probabilities of the sets:</span>
<span id="cb2-1242"><a href="#cb2-1242" aria-hidden="true" tabindex="-1"></a>$\{S(t_{1})\in B_{1},S(t_{2})\in B_{2},\ldots,S(t_{n})\in B_{n}\}$ for</span>
<span id="cb2-1243"><a href="#cb2-1243" aria-hidden="true" tabindex="-1"></a>any choice of $0\leq t_{1}\leq\ldots\leq t_{n}\leq T$.</span>
<span id="cb2-1244"><a href="#cb2-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1245"><a href="#cb2-1245" aria-hidden="true" tabindex="-1"></a>The sets of the form</span>
<span id="cb2-1246"><a href="#cb2-1246" aria-hidden="true" tabindex="-1"></a>$A=\{\omega(\cdot)\in D[0,T]:\omega(t_{1})\in B_{1},\ldots,\omega(t_{n})\in B_{n}\}$,</span>
<span id="cb2-1247"><a href="#cb2-1247" aria-hidden="true" tabindex="-1"></a>where $B_{i}$'s are borel subsets of $\mathbf{R}$, are called cylinder</span>
<span id="cb2-1248"><a href="#cb2-1248" aria-hidden="true" tabindex="-1"></a>sets or finite-dimensional rectangles.</span>
<span id="cb2-1249"><a href="#cb2-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1250"><a href="#cb2-1250" aria-hidden="true" tabindex="-1"></a>The stochastic process $S(t)$ is just a (function-valued) random</span>
<span id="cb2-1251"><a href="#cb2-1251" aria-hidden="true" tabindex="-1"></a>variable on this sample space, which takes some value $\omega(t)$ - the</span>
<span id="cb2-1252"><a href="#cb2-1252" aria-hidden="true" tabindex="-1"></a>value of the function $\omega$ at $t$.</span>
<span id="cb2-1253"><a href="#cb2-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1254"><a href="#cb2-1254" aria-hidden="true" tabindex="-1"></a>Let $\mathcal{R}$ be the colllection of all cylindrical subsets of</span>
<span id="cb2-1255"><a href="#cb2-1255" aria-hidden="true" tabindex="-1"></a>$D[0,1]$. Obviously $\mathcal{R}$ is not a $\sigma$-field.</span>
<span id="cb2-1256"><a href="#cb2-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1257"><a href="#cb2-1257" aria-hidden="true" tabindex="-1"></a>Probability is first defined by on the elements of $\mathcal{R}$. Let</span>
<span id="cb2-1258"><a href="#cb2-1258" aria-hidden="true" tabindex="-1"></a>$A\subseteq\mathcal{R}$.</span>
<span id="cb2-1259"><a href="#cb2-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1260"><a href="#cb2-1260" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1261"><a href="#cb2-1261" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(A) &amp; =\int_{B_{1}}\cdots\int_{B_{n}}\prod_{i=1}^{n}\frac{1}{\sqrt{(2\pi)(t_{i}-t_{i-1})}}\exp\left[-\frac{(u_{i}-u_{i-1})^{2}}{2(t_{i}-t_{i-1})}\right]du_{1}\cdots du_{n}</span>
<span id="cb2-1262"><a href="#cb2-1262" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1263"><a href="#cb2-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1264"><a href="#cb2-1264" aria-hidden="true" tabindex="-1"></a>and then extended to the $\sigma$-field generated by taking unions,</span>
<span id="cb2-1265"><a href="#cb2-1265" aria-hidden="true" tabindex="-1"></a>complements and intersections of cylinders. We take the smallest</span>
<span id="cb2-1266"><a href="#cb2-1266" aria-hidden="true" tabindex="-1"></a>$\sigma$-algebra containing all the cylindrical subsets of $D[0,1]$.</span>
<span id="cb2-1267"><a href="#cb2-1267" aria-hidden="true" tabindex="-1"></a>Thus, $\mathcal{F}=\mathcal{B}(D[0,1])$.</span>
<span id="cb2-1268"><a href="#cb2-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1269"><a href="#cb2-1269" aria-hidden="true" tabindex="-1"></a>Hence,</span>
<span id="cb2-1270"><a href="#cb2-1270" aria-hidden="true" tabindex="-1"></a>$(\Omega,\mathcal{F},\mathbb{P})=(D[0,1],\mathcal{B}(D[0,1]),\mathbb{P})$</span>
<span id="cb2-1271"><a href="#cb2-1271" aria-hidden="true" tabindex="-1"></a>is a probability space. It is called the *Wiener space* and $\mathbb{P}$</span>
<span id="cb2-1272"><a href="#cb2-1272" aria-hidden="true" tabindex="-1"></a>here is called the *Wiener measure*.</span>
<span id="cb2-1273"><a href="#cb2-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1274"><a href="#cb2-1274" aria-hidden="true" tabindex="-1"></a>## Continuity and Regularity of paths.</span>
<span id="cb2-1275"><a href="#cb2-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1276"><a href="#cb2-1276" aria-hidden="true" tabindex="-1"></a>As discussed in the previous section, a stochastic process is determined</span>
<span id="cb2-1277"><a href="#cb2-1277" aria-hidden="true" tabindex="-1"></a>by its finite-dimensional distribution. In studying stochastic</span>
<span id="cb2-1278"><a href="#cb2-1278" aria-hidden="true" tabindex="-1"></a>processes, it is often natural to think of them as function-valued</span>
<span id="cb2-1279"><a href="#cb2-1279" aria-hidden="true" tabindex="-1"></a>random variables in $t$. Let $S(t)$ be defined for $0\leq t\leq T$, then</span>
<span id="cb2-1280"><a href="#cb2-1280" aria-hidden="true" tabindex="-1"></a>for a fixed $\omega$, it is a function in $t$, called the sample path or</span>
<span id="cb2-1281"><a href="#cb2-1281" aria-hidden="true" tabindex="-1"></a>a realization of $S$. Finite-dimensional distributions do not determine</span>
<span id="cb2-1282"><a href="#cb2-1282" aria-hidden="true" tabindex="-1"></a>the continuity property of sample paths. The following example</span>
<span id="cb2-1283"><a href="#cb2-1283" aria-hidden="true" tabindex="-1"></a>illustrates this.</span>
<span id="cb2-1284"><a href="#cb2-1284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1285"><a href="#cb2-1285" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-1286"><a href="#cb2-1286" aria-hidden="true" tabindex="-1"></a>[]{#ex:modifications-of-a-stochastic-process</span>
<span id="cb2-1287"><a href="#cb2-1287" aria-hidden="true" tabindex="-1"></a>label="ex:modifications-of-a-stochastic-process"}Let $X(t)=0$ for all</span>
<span id="cb2-1288"><a href="#cb2-1288" aria-hidden="true" tabindex="-1"></a>$t$, $0\leq t\leq1$ and $\tau$ be a uniformly distributed random</span>
<span id="cb2-1289"><a href="#cb2-1289" aria-hidden="true" tabindex="-1"></a>variable on $[0,1]$. Let $Y(t)=0$ for $t\neq\tau$ and $Y(t)=1$ if</span>
<span id="cb2-1290"><a href="#cb2-1290" aria-hidden="true" tabindex="-1"></a>$t=\tau.$ Then, for any fixed $t$,</span>
<span id="cb2-1291"><a href="#cb2-1291" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(Y(t)\neq0)=\mathbb{P}(\tau=t)=0$, and hence</span>
<span id="cb2-1292"><a href="#cb2-1292" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(Y(t)=0)=1$. So, that all one-dimensional distributions of</span>
<span id="cb2-1293"><a href="#cb2-1293" aria-hidden="true" tabindex="-1"></a>$X(t)$ and $Y(t)$ are the same. Similarly, all finite-dimensional</span>
<span id="cb2-1294"><a href="#cb2-1294" aria-hidden="true" tabindex="-1"></a>distributions of $X$ and $Y$ are the same. However, the sample paths of</span>
<span id="cb2-1295"><a href="#cb2-1295" aria-hidden="true" tabindex="-1"></a>the process $X$, that is, the functions $X(t)_{0\leq t\leq1}$ are</span>
<span id="cb2-1296"><a href="#cb2-1296" aria-hidden="true" tabindex="-1"></a>continuous in $t$, whereas every sample path $Y(t)_{0\leq t\leq1}$ has a</span>
<span id="cb2-1297"><a href="#cb2-1297" aria-hidden="true" tabindex="-1"></a>jump at the (random) point $\tau$. Notice that,</span>
<span id="cb2-1298"><a href="#cb2-1298" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(X(t)=Y(t))=1$ for all $t$, $0\leq t\leq1$.</span>
<span id="cb2-1299"><a href="#cb2-1299" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1300"><a href="#cb2-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1301"><a href="#cb2-1301" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb2-1302"><a href="#cb2-1302" aria-hidden="true" tabindex="-1"></a>Two stochastic processes are called *versions* (modifications) of one</span>
<span id="cb2-1303"><a href="#cb2-1303" aria-hidden="true" tabindex="-1"></a>another if</span>
<span id="cb2-1304"><a href="#cb2-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1305"><a href="#cb2-1305" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(X(t)=Y(t))=1\quad\text{for all }0\leq t\leq T$$</span>
<span id="cb2-1306"><a href="#cb2-1306" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1307"><a href="#cb2-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1308"><a href="#cb2-1308" aria-hidden="true" tabindex="-1"></a>Thus, the two processes in the example</span>
<span id="cb2-1309"><a href="#cb2-1309" aria-hidden="true" tabindex="-1"></a>([\[ex:modifications-of-a-stochastic-process\]](#ex:modifications-of-a-stochastic-process){reference-type="ref"</span>
<span id="cb2-1310"><a href="#cb2-1310" aria-hidden="true" tabindex="-1"></a>reference="ex:modifications-of-a-stochastic-process"}) are versions of</span>
<span id="cb2-1311"><a href="#cb2-1311" aria-hidden="true" tabindex="-1"></a>one another, one has continuous sample paths, the other does not. If we</span>
<span id="cb2-1312"><a href="#cb2-1312" aria-hidden="true" tabindex="-1"></a>agree to pick any version of the process we want, then we can pick the</span>
<span id="cb2-1313"><a href="#cb2-1313" aria-hidden="true" tabindex="-1"></a>continous version when it exists. In general, we choose the smoothest</span>
<span id="cb2-1314"><a href="#cb2-1314" aria-hidden="true" tabindex="-1"></a>possible version of the process.</span>
<span id="cb2-1315"><a href="#cb2-1315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1316"><a href="#cb2-1316" aria-hidden="true" tabindex="-1"></a>For two processes, $X$ and $Y$, denote by $N_{t}=\{X(t)\neq Y(t)\}$,</span>
<span id="cb2-1317"><a href="#cb2-1317" aria-hidden="true" tabindex="-1"></a>$0\leq t\leq T$. In the above example,</span>
<span id="cb2-1318"><a href="#cb2-1318" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(N_{t})=\mathbb{P}(\tau=t)=0$ for any $t$, $0\leq t\leq1$.</span>
<span id="cb2-1319"><a href="#cb2-1319" aria-hidden="true" tabindex="-1"></a>However,</span>
<span id="cb2-1320"><a href="#cb2-1320" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(\bigcup_{0\leq t\leq1}N_{t})=\mathbb{P}(\tau=t\:\text{for some }t\:\text{in }[0,1])=1$.</span>
<span id="cb2-1321"><a href="#cb2-1321" aria-hidden="true" tabindex="-1"></a>Although, each of $N_{t}$ is a $\mathbb{P}$-null set, the union</span>
<span id="cb2-1322"><a href="#cb2-1322" aria-hidden="true" tabindex="-1"></a>$N=\bigcup_{0\leq t\leq1}N_{t}$ contains uncountably many null sets, and</span>
<span id="cb2-1323"><a href="#cb2-1323" aria-hidden="true" tabindex="-1"></a>in this particular case it is a set of of probability one.</span>
<span id="cb2-1324"><a href="#cb2-1324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1325"><a href="#cb2-1325" aria-hidden="true" tabindex="-1"></a>If it happens that $\mathbb{P}(N)=0$, then $N$ is called an *evanescent</span>
<span id="cb2-1326"><a href="#cb2-1326" aria-hidden="true" tabindex="-1"></a>set*, and the processes $X$ and $Y$ are called *indistinguishable*. Note</span>
<span id="cb2-1327"><a href="#cb2-1327" aria-hidden="true" tabindex="-1"></a>that in this case,</span>
<span id="cb2-1328"><a href="#cb2-1328" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}(\{\omega:\exists t:X(t)\neq Y(t)\})=\mathbb{P}(\bigcup_{0\leq t\leq1}\{X(t)\neq Y(t))=0$</span>
<span id="cb2-1329"><a href="#cb2-1329" aria-hidden="true" tabindex="-1"></a>and $\mathbb{P}(\bigcap_{0\leq t\leq1}\{X(t)=Y(t)\})=1$. It is clear,</span>
<span id="cb2-1330"><a href="#cb2-1330" aria-hidden="true" tabindex="-1"></a>that if the time is discrete, then any two versions of the process are</span>
<span id="cb2-1331"><a href="#cb2-1331" aria-hidden="true" tabindex="-1"></a>indistinguishable. It is also not hard to see, that if $X(t)$ and $Y(t)$</span>
<span id="cb2-1332"><a href="#cb2-1332" aria-hidden="true" tabindex="-1"></a>are versions of one another and they are both right-continuous, they are</span>
<span id="cb2-1333"><a href="#cb2-1333" aria-hidden="true" tabindex="-1"></a>indistinguishable.</span>
<span id="cb2-1334"><a href="#cb2-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1335"><a href="#cb2-1335" aria-hidden="true" tabindex="-1"></a>::: thm</span>
<span id="cb2-1336"><a href="#cb2-1336" aria-hidden="true" tabindex="-1"></a>(Paul Levy's construction of Brownian Motion). Standard Brownian motion</span>
<span id="cb2-1337"><a href="#cb2-1337" aria-hidden="true" tabindex="-1"></a>exists.</span>
<span id="cb2-1338"><a href="#cb2-1338" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1339"><a href="#cb2-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1340"><a href="#cb2-1340" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb2-1341"><a href="#cb2-1341" aria-hidden="true" tabindex="-1"></a>*Proof.* I reproduce the standard proof as present in *Brownian Motion*</span>
<span id="cb2-1342"><a href="#cb2-1342" aria-hidden="true" tabindex="-1"></a>by Morters and Peres. I added some remarks for greater clarity.</span>
<span id="cb2-1343"><a href="#cb2-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1344"><a href="#cb2-1344" aria-hidden="true" tabindex="-1"></a>Let</span>
<span id="cb2-1345"><a href="#cb2-1345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1346"><a href="#cb2-1346" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1347"><a href="#cb2-1347" aria-hidden="true" tabindex="-1"></a>\mathcal{D}_{n} &amp; =\left\{ \frac{k}{2^{n}}:k=0,1,2,\ldots,2^{n}\right\} </span>
<span id="cb2-1348"><a href="#cb2-1348" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1349"><a href="#cb2-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1350"><a href="#cb2-1350" aria-hidden="true" tabindex="-1"></a>be a finite set of dyadic points.</span>
<span id="cb2-1351"><a href="#cb2-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1352"><a href="#cb2-1352" aria-hidden="true" tabindex="-1"></a>Let</span>
<span id="cb2-1353"><a href="#cb2-1353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1354"><a href="#cb2-1354" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1355"><a href="#cb2-1355" aria-hidden="true" tabindex="-1"></a>\mathcal{D} &amp; =\bigcup_{n=0}^{\infty}\mathcal{D}_{n}</span>
<span id="cb2-1356"><a href="#cb2-1356" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1357"><a href="#cb2-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1358"><a href="#cb2-1358" aria-hidden="true" tabindex="-1"></a>Let $\{Z_{t}:t\in\mathcal{D}\}$ be a collection of independent, standard</span>
<span id="cb2-1359"><a href="#cb2-1359" aria-hidden="true" tabindex="-1"></a>normally distributed random variables. This is a countable set of random</span>
<span id="cb2-1360"><a href="#cb2-1360" aria-hidden="true" tabindex="-1"></a>variables.</span>
<span id="cb2-1361"><a href="#cb2-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1362"><a href="#cb2-1362" aria-hidden="true" tabindex="-1"></a>Let $B(0):=0$ and $B(1):=Z_{1}$.</span>
<span id="cb2-1363"><a href="#cb2-1363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1364"><a href="#cb2-1364" aria-hidden="true" tabindex="-1"></a>For each $n\in\mathbf{N}$, we define the random variables $B(d)$,</span>
<span id="cb2-1365"><a href="#cb2-1365" aria-hidden="true" tabindex="-1"></a>$d\in\mathcal{D}_{n}$ such that, the following invariant holds:</span>
<span id="cb2-1366"><a href="#cb2-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1367"><a href="#cb2-1367" aria-hidden="true" tabindex="-1"></a>\(1\) for all $r&lt;s&lt;t$ in $\mathcal{D}_{n}$ the random variable</span>
<span id="cb2-1368"><a href="#cb2-1368" aria-hidden="true" tabindex="-1"></a>$B(t)-B(s)$ is normally distributed with mean zero and variance $t-s$</span>
<span id="cb2-1369"><a href="#cb2-1369" aria-hidden="true" tabindex="-1"></a>and is independent of $B(s)-B(r)$.</span>
<span id="cb2-1370"><a href="#cb2-1370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1371"><a href="#cb2-1371" aria-hidden="true" tabindex="-1"></a>\(2\) the vectors $(B(d):d\in\mathcal{D}_{n})$ and</span>
<span id="cb2-1372"><a href="#cb2-1372" aria-hidden="true" tabindex="-1"></a>$(Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n})$ are independent.</span>
<span id="cb2-1373"><a href="#cb2-1373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1374"><a href="#cb2-1374" aria-hidden="true" tabindex="-1"></a>Note that we have already done this for $\mathcal{D}_{0}=\{0,1\}$.</span>
<span id="cb2-1375"><a href="#cb2-1375" aria-hidden="true" tabindex="-1"></a>Proceeding inductively, let's assume that the above holds for some</span>
<span id="cb2-1376"><a href="#cb2-1376" aria-hidden="true" tabindex="-1"></a>$n-1$. We are interested to prove that the invariant also holds for $n$.</span>
<span id="cb2-1377"><a href="#cb2-1377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1378"><a href="#cb2-1378" aria-hidden="true" tabindex="-1"></a>We define $B(d)$ for $d\in\mathcal{D}_{n}\backslash\mathcal{D}_{n-1}$</span>
<span id="cb2-1379"><a href="#cb2-1379" aria-hidden="true" tabindex="-1"></a>by:</span>
<span id="cb2-1380"><a href="#cb2-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1381"><a href="#cb2-1381" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1382"><a href="#cb2-1382" aria-hidden="true" tabindex="-1"></a>B(d) &amp; =\frac{B(d-2^{-n})+B(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}</span>
<span id="cb2-1383"><a href="#cb2-1383" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1384"><a href="#cb2-1384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1385"><a href="#cb2-1385" aria-hidden="true" tabindex="-1"></a>Note that, the points</span>
<span id="cb2-1386"><a href="#cb2-1386" aria-hidden="true" tabindex="-1"></a>$0,\frac{1}{2^{n-1}},\ldots,\frac{k}{2^{n-1}},\frac{k+1}{2^{n-1}},\ldots,1$</span>
<span id="cb2-1387"><a href="#cb2-1387" aria-hidden="true" tabindex="-1"></a>belong to $\mathcal{D}_{n-1}$. The first summand is the linear</span>
<span id="cb2-1388"><a href="#cb2-1388" aria-hidden="true" tabindex="-1"></a>interpolation of the values of $B$ at the neighbouring points of $d$ in</span>
<span id="cb2-1389"><a href="#cb2-1389" aria-hidden="true" tabindex="-1"></a>$\mathcal{D}_{n-1}$. That is,</span>
<span id="cb2-1390"><a href="#cb2-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1391"><a href="#cb2-1391" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1392"><a href="#cb2-1392" aria-hidden="true" tabindex="-1"></a>B\left(\frac{2k+1}{2^{n}}\right) &amp; =\frac{B\left(\frac{k}{2^{n-1}}\right)+B\left(\frac{k+1}{2^{n-1}}\right)}{2}+\frac{Z_{d}}{2^{(n+1)/2}}</span>
<span id="cb2-1393"><a href="#cb2-1393" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1394"><a href="#cb2-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1395"><a href="#cb2-1395" aria-hidden="true" tabindex="-1"></a>Since $P(n-1)$ holds, $B(d-2^{-n})$ and $B(d+2^{-n})$ are have no</span>
<span id="cb2-1396"><a href="#cb2-1396" aria-hidden="true" tabindex="-1"></a>dependence on $(Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n-1})$.</span>
<span id="cb2-1397"><a href="#cb2-1397" aria-hidden="true" tabindex="-1"></a>Consequently, $B(d)$ has no dependence on</span>
<span id="cb2-1398"><a href="#cb2-1398" aria-hidden="true" tabindex="-1"></a>$(Z_{t}:t\in\mathcal{D}\setminus\mathcal{D}_{n})$ and the second</span>
<span id="cb2-1399"><a href="#cb2-1399" aria-hidden="true" tabindex="-1"></a>property is fulfilled.</span>
<span id="cb2-1400"><a href="#cb2-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1401"><a href="#cb2-1401" aria-hidden="true" tabindex="-1"></a>Moreover, as $\frac{1}{2}[B(d+2^{-n})-B(d-2^{-n})]$ depends only on</span>
<span id="cb2-1402"><a href="#cb2-1402" aria-hidden="true" tabindex="-1"></a>$(Z_{t}:t\in\mathcal{D}_{n-1})$, it is independent of</span>
<span id="cb2-1403"><a href="#cb2-1403" aria-hidden="true" tabindex="-1"></a>$\frac{Z_{d}}{2^{(n+1)/2}}$. By our induction assumptions, they are both</span>
<span id="cb2-1404"><a href="#cb2-1404" aria-hidden="true" tabindex="-1"></a>nromally distributed with mean $0$ and variance $\frac{1}{2^{(n+1)}}$.</span>
<span id="cb2-1405"><a href="#cb2-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1406"><a href="#cb2-1406" aria-hidden="true" tabindex="-1"></a>So, their sum and difference random variables</span>
<span id="cb2-1407"><a href="#cb2-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1408"><a href="#cb2-1408" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1409"><a href="#cb2-1409" aria-hidden="true" tabindex="-1"></a>B(d)-B(d-2^{-n}) &amp; =\frac{B(d+2^{-n})-B(d-2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\\</span>
<span id="cb2-1410"><a href="#cb2-1410" aria-hidden="true" tabindex="-1"></a>B(d+2^{-n})-B(d) &amp; =\frac{B(d+2^{-n})-B(d-2^{-n})}{2}-\frac{Z_{d}}{2^{(n+1)/2}}</span>
<span id="cb2-1411"><a href="#cb2-1411" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1412"><a href="#cb2-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1413"><a href="#cb2-1413" aria-hidden="true" tabindex="-1"></a>are also independent, with mean $0$ and variance $\frac{1}{2^{n}}$ (the</span>
<span id="cb2-1414"><a href="#cb2-1414" aria-hidden="true" tabindex="-1"></a>variance of independent random variables is the sum of the variances).</span>
<span id="cb2-1415"><a href="#cb2-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1416"><a href="#cb2-1416" aria-hidden="true" tabindex="-1"></a>Indeed all increments $B(d)-B(d-2^{-n})$ for</span>
<span id="cb2-1417"><a href="#cb2-1417" aria-hidden="true" tabindex="-1"></a>$d\in\mathcal{D}_{n}\setminus\{0\}$ are independent. To see this, it</span>
<span id="cb2-1418"><a href="#cb2-1418" aria-hidden="true" tabindex="-1"></a>suffices to show that they are pairwise independent. We have seen in the</span>
<span id="cb2-1419"><a href="#cb2-1419" aria-hidden="true" tabindex="-1"></a>previous paragraph that the pairs $B(d)-B(d-2^{-n})$ and</span>
<span id="cb2-1420"><a href="#cb2-1420" aria-hidden="true" tabindex="-1"></a>$B(d+2^{-n})-B(d)$ with $d\in\mathcal{D}_{n}\setminus\mathcal{D}_{n-1}$</span>
<span id="cb2-1421"><a href="#cb2-1421" aria-hidden="true" tabindex="-1"></a>are independent. The other possibility is that the increments are over</span>
<span id="cb2-1422"><a href="#cb2-1422" aria-hidden="true" tabindex="-1"></a>the intervals separated by some $d\in\mathcal{D}_{n-1}$. For</span>
<span id="cb2-1423"><a href="#cb2-1423" aria-hidden="true" tabindex="-1"></a>concreteness, if $n$ were $3$, then the increments, $B_{7/8}-B_{6/8}$</span>
<span id="cb2-1424"><a href="#cb2-1424" aria-hidden="true" tabindex="-1"></a>and $B_{5/8}-B_{4/8}$ are seperated by</span>
<span id="cb2-1425"><a href="#cb2-1425" aria-hidden="true" tabindex="-1"></a>$d=\frac{3}{4}\in\mathcal{D}_{2}$. Choose $d\in\mathcal{D}_{j}$ with</span>
<span id="cb2-1426"><a href="#cb2-1426" aria-hidden="true" tabindex="-1"></a>this property and minimal $j$, so, the two intervals are contained in</span>
<span id="cb2-1427"><a href="#cb2-1427" aria-hidden="true" tabindex="-1"></a>$[d-2^{-j},d]$ and $[d,d+2^{-j}]$ respectively. By induction, the</span>
<span id="cb2-1428"><a href="#cb2-1428" aria-hidden="true" tabindex="-1"></a>increments over these two intervals of length $2^{-j}$ are independent</span>
<span id="cb2-1429"><a href="#cb2-1429" aria-hidden="true" tabindex="-1"></a>and the increments over the intervals of length $2^{-n}$ are constructed</span>
<span id="cb2-1430"><a href="#cb2-1430" aria-hidden="true" tabindex="-1"></a>from the independent increments $B(d)-B(d-2^{-j})$ and</span>
<span id="cb2-1431"><a href="#cb2-1431" aria-hidden="true" tabindex="-1"></a>$B(d+2^{-j})-B(d)$ using a disjoint set of variables</span>
<span id="cb2-1432"><a href="#cb2-1432" aria-hidden="true" tabindex="-1"></a>$(Z_{t}:t\in\mathcal{D}_{n})$. Hence, they are independent and this</span>
<span id="cb2-1433"><a href="#cb2-1433" aria-hidden="true" tabindex="-1"></a>implies pairwise independence. This implies the first property.</span>
<span id="cb2-1434"><a href="#cb2-1434" aria-hidden="true" tabindex="-1"></a>Consequently, the vector of increments $(B(d)-B(d-2^{-n})$ for all</span>
<span id="cb2-1435"><a href="#cb2-1435" aria-hidden="true" tabindex="-1"></a>$d\in\mathcal{D}_{n}$ is Gaussian.</span>
<span id="cb2-1436"><a href="#cb2-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1437"><a href="#cb2-1437" aria-hidden="true" tabindex="-1"></a>Having thus chosen the value of the process on all the dyadic points, we</span>
<span id="cb2-1438"><a href="#cb2-1438" aria-hidden="true" tabindex="-1"></a>interpolate between them. Formally, we define:</span>
<span id="cb2-1439"><a href="#cb2-1439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1440"><a href="#cb2-1440" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1441"><a href="#cb2-1441" aria-hidden="true" tabindex="-1"></a>F_{0}(t) &amp; =\begin{cases}</span>
<span id="cb2-1442"><a href="#cb2-1442" aria-hidden="true" tabindex="-1"></a>Z_{1} &amp; \text{for }t=1\\</span>
<span id="cb2-1443"><a href="#cb2-1443" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{for }t=0\\</span>
<span id="cb2-1444"><a href="#cb2-1444" aria-hidden="true" tabindex="-1"></a>\text{\text{linear in between}}</span>
<span id="cb2-1445"><a href="#cb2-1445" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb2-1446"><a href="#cb2-1446" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1447"><a href="#cb2-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1448"><a href="#cb2-1448" aria-hidden="true" tabindex="-1"></a>and for each $n\geq1$,</span>
<span id="cb2-1449"><a href="#cb2-1449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1450"><a href="#cb2-1450" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1451"><a href="#cb2-1451" aria-hidden="true" tabindex="-1"></a>F_{n}(t) &amp; =\begin{cases}</span>
<span id="cb2-1452"><a href="#cb2-1452" aria-hidden="true" tabindex="-1"></a>\frac{Z_{t}}{2^{(n+1)/2}} &amp; \text{for }t\in\mathcal{D}\setminus\mathcal{D}_{n-1}\\</span>
<span id="cb2-1453"><a href="#cb2-1453" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{for }t\in\mathcal{D}_{n-1}\\</span>
<span id="cb2-1454"><a href="#cb2-1454" aria-hidden="true" tabindex="-1"></a>\text{\text{linear between consecutive points in }\ensuremath{\mathcal{D}_{n}}}</span>
<span id="cb2-1455"><a href="#cb2-1455" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb2-1456"><a href="#cb2-1456" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1457"><a href="#cb2-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1458"><a href="#cb2-1458" aria-hidden="true" tabindex="-1"></a>These functions are continuous on $[0,1]$ and for all $n$ and</span>
<span id="cb2-1459"><a href="#cb2-1459" aria-hidden="true" tabindex="-1"></a>$d\in\mathcal{D}_{n}$, we have:</span>
<span id="cb2-1460"><a href="#cb2-1460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1461"><a href="#cb2-1461" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1462"><a href="#cb2-1462" aria-hidden="true" tabindex="-1"></a>B(d) &amp; =\sum_{i=0}^{n}F_{i}(d)=\sum_{i=0}^{\infty}F_{i}(d)\label{eq:claim-of-induction-for-bd}</span>
<span id="cb2-1463"><a href="#cb2-1463" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1464"><a href="#cb2-1464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1465"><a href="#cb2-1465" aria-hidden="true" tabindex="-1"></a>To see this, assume that above equation holds for all</span>
<span id="cb2-1466"><a href="#cb2-1466" aria-hidden="true" tabindex="-1"></a>$d\in\mathcal{D}_{n-1}$.</span>
<span id="cb2-1467"><a href="#cb2-1467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1468"><a href="#cb2-1468" aria-hidden="true" tabindex="-1"></a>Let's consider the point</span>
<span id="cb2-1469"><a href="#cb2-1469" aria-hidden="true" tabindex="-1"></a>$d\in\mathcal{D}_{n}\setminus\mathcal{D}_{n-1}$.</span>
<span id="cb2-1470"><a href="#cb2-1470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1471"><a href="#cb2-1471" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1472"><a href="#cb2-1472" aria-hidden="true" tabindex="-1"></a>B(d) &amp; =\frac{B(d-2^{-n})+B(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\nonumber \\</span>
<span id="cb2-1473"><a href="#cb2-1473" aria-hidden="true" tabindex="-1"></a> &amp; =\sum_{i=0}^{n-1}\frac{F_{i}(d-2^{-n})+F_{i}(d+2^{-n})}{2}+\frac{Z_{d}}{2^{(n+1)/2}}\label{eq:expression-for-bd}</span>
<span id="cb2-1474"><a href="#cb2-1474" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1475"><a href="#cb2-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1476"><a href="#cb2-1476" aria-hidden="true" tabindex="-1"></a>Now, $d-2^{-n}$ and $d+2^{-n}$ belong to $\mathcal{D}_{n-1}$ and are not</span>
<span id="cb2-1477"><a href="#cb2-1477" aria-hidden="true" tabindex="-1"></a>in $\bigcup_{i&lt;n-1}\mathcal{D}_{i}$. Therefore, for $i=0,1,\ldots,n-2$,</span>
<span id="cb2-1478"><a href="#cb2-1478" aria-hidden="true" tabindex="-1"></a>the points $(d-2^{-n},F_{i}(d-2^{-n}))$ and $(d+2^{-n},F_{i}(d+2^{-n})$</span>
<span id="cb2-1479"><a href="#cb2-1479" aria-hidden="true" tabindex="-1"></a>lie on some straight line and have $(d,F_{i}(d))$ as their midpoint.</span>
<span id="cb2-1480"><a href="#cb2-1480" aria-hidden="true" tabindex="-1"></a>Moreover, $d-2^{-n}$ and $d+2^{-n}$ are vertices in $\mathcal{D}_{n-1}$.</span>
<span id="cb2-1481"><a href="#cb2-1481" aria-hidden="true" tabindex="-1"></a>So, by definition of $F_{n-1}(d)$, we have</span>
<span id="cb2-1482"><a href="#cb2-1482" aria-hidden="true" tabindex="-1"></a>$F_{n-1}(d)=[F_{n-1}(d-2^{-n})+F_{n-1}(d+2^{-n})]/2$.</span>
<span id="cb2-1483"><a href="#cb2-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1484"><a href="#cb2-1484" aria-hidden="true" tabindex="-1"></a>To summarize, the first term on the right hand side of expression</span>
<span id="cb2-1485"><a href="#cb2-1485" aria-hidden="true" tabindex="-1"></a>([\[eq:expression-for-bd\]](#eq:expression-for-bd){reference-type="ref"</span>
<span id="cb2-1486"><a href="#cb2-1486" aria-hidden="true" tabindex="-1"></a>reference="eq:expression-for-bd"}) is equal to</span>
<span id="cb2-1487"><a href="#cb2-1487" aria-hidden="true" tabindex="-1"></a>$\sum_{i=0}^{n-1}F_{i}(d)$. By mathematical induction, it follows that</span>
<span id="cb2-1488"><a href="#cb2-1488" aria-hidden="true" tabindex="-1"></a>the claim</span>
<span id="cb2-1489"><a href="#cb2-1489" aria-hidden="true" tabindex="-1"></a>([\[eq:claim-of-induction-for-bd\]](#eq:claim-of-induction-for-bd){reference-type="ref"</span>
<span id="cb2-1490"><a href="#cb2-1490" aria-hidden="true" tabindex="-1"></a>reference="eq:claim-of-induction-for-bd"}) is true for all</span>
<span id="cb2-1491"><a href="#cb2-1491" aria-hidden="true" tabindex="-1"></a>$n\in\mathbf{N}$.</span>
<span id="cb2-1492"><a href="#cb2-1492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1493"><a href="#cb2-1493" aria-hidden="true" tabindex="-1"></a>It's extremely easy to find an upper bound on the probability contained</span>
<span id="cb2-1494"><a href="#cb2-1494" aria-hidden="true" tabindex="-1"></a>in the Gaussian tails. Suppose $X\sim N(0,1)$ and let $x&gt;0$. We are</span>
<span id="cb2-1495"><a href="#cb2-1495" aria-hidden="true" tabindex="-1"></a>interested in the tail probability $\mathbb{P}(X&gt;x)$. We have:</span>
<span id="cb2-1496"><a href="#cb2-1496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1497"><a href="#cb2-1497" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1498"><a href="#cb2-1498" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(X&gt;x) &amp; =\int_{x}^{\infty}e^{-x^{2}/2}dx=\int_{x}^{\infty}\frac{xe^{-x^{2}/2}dx}{x}</span>
<span id="cb2-1499"><a href="#cb2-1499" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1500"><a href="#cb2-1500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1501"><a href="#cb2-1501" aria-hidden="true" tabindex="-1"></a>Let $u=\frac{1}{x}$ and $dv=xe^{-x^{2}/2}dx$. We have:</span>
<span id="cb2-1502"><a href="#cb2-1502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1503"><a href="#cb2-1503" aria-hidden="true" tabindex="-1"></a><span class="in">       $u=\frac{1}{x}$       $dv=xe^{-x^{2}/2}dx$</span></span>
<span id="cb2-1504"><a href="#cb2-1504" aria-hidden="true" tabindex="-1"></a>  ------------------------- ----------------------</span>
<span id="cb2-1505"><a href="#cb2-1505" aria-hidden="true" tabindex="-1"></a>   $du=-\frac{1}{x^{2}}dx$    $v=-e^{-x^{2}/2}$</span>
<span id="cb2-1506"><a href="#cb2-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1507"><a href="#cb2-1507" aria-hidden="true" tabindex="-1"></a>Thus,</span>
<span id="cb2-1508"><a href="#cb2-1508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1509"><a href="#cb2-1509" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1510"><a href="#cb2-1510" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(X&gt;x) &amp; =-\left.\frac{1}{x}e^{-x^{2}/2}\right|_{x}^{\infty}-\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}dx<span class="sc">\\</span></span>
<span id="cb2-1511"><a href="#cb2-1511" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{e^{-x^{2}/2}}{x}-\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}dx<span class="sc">\\</span></span>
<span id="cb2-1512"><a href="#cb2-1512" aria-hidden="true" tabindex="-1"></a> &amp; \quad\left<span class="sc">\{</span> I(x)=\int_{x}^{\infty}\frac{e^{-x^{2}/2}}{x^{2}}\geq0\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb2-1513"><a href="#cb2-1513" aria-hidden="true" tabindex="-1"></a> &amp; \leq\frac{e^{-x^{2}/2}}{x}</span>
<span id="cb2-1514"><a href="#cb2-1514" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1515"><a href="#cb2-1515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1516"><a href="#cb2-1516" aria-hidden="true" tabindex="-1"></a>Thus, for $c&gt;1$ and large $n$, we have:</span>
<span id="cb2-1517"><a href="#cb2-1517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1518"><a href="#cb2-1518" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1519"><a href="#cb2-1519" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(|Z_{d}|\geq c\sqrt{n}) &amp; \leq\frac{1}{c\sqrt{n}}e^{-c^{2}n/2}\leq\exp\left(-\frac{c^{2}n}{2}\right)</span>
<span id="cb2-1520"><a href="#cb2-1520" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1521"><a href="#cb2-1521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1522"><a href="#cb2-1522" aria-hidden="true" tabindex="-1"></a>So, the series:</span>
<span id="cb2-1523"><a href="#cb2-1523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1524"><a href="#cb2-1524" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1525"><a href="#cb2-1525" aria-hidden="true" tabindex="-1"></a>\sum_{n=0}^{\infty}\mathbb{P}\left<span class="sc">\{</span> \text{There exists atleast one }d\in\mathcal{D}_{n}\text{ with }|Z_{d}|\geq c\sqrt{n}\right\}  &amp; \leq\sum_{n=0}^{\infty}\sum_{d\in\mathcal{D}_{n}}\mathbb{P}\left\{ |Z_{d}|\geq c\sqrt{n}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb2-1526"><a href="#cb2-1526" aria-hidden="true" tabindex="-1"></a> &amp; \leq\sum_{n=0}^{\infty}(2^{n}+1)\exp\left(-\frac{c^{2}n}{2}\right)</span>
<span id="cb2-1527"><a href="#cb2-1527" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1528"><a href="#cb2-1528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1529"><a href="#cb2-1529" aria-hidden="true" tabindex="-1"></a>Now, the series $(a_{n})$ given by, $a_{n}:=(2^{n}+1)e^{-c^{2}n/2}$ has</span>
<span id="cb2-1530"><a href="#cb2-1530" aria-hidden="true" tabindex="-1"></a>the ratio between successive terms:</span>
<span id="cb2-1531"><a href="#cb2-1531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1532"><a href="#cb2-1532" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1533"><a href="#cb2-1533" aria-hidden="true" tabindex="-1"></a>\lim\left|\frac{a_{n+1}}{a_{n}}\right| &amp; =\lim_{n\to\infty}\frac{2^{n+1}+1}{2^{n}+1}\cdot\frac{e^{(c^{2}n)/2}}{e^{c^{2}(n+1)/2}}<span class="sc">\\</span></span>
<span id="cb2-1534"><a href="#cb2-1534" aria-hidden="true" tabindex="-1"></a> &amp; =\lim_{n\to\infty}\frac{\frac{1}{2}+\frac{1}{2^{n}}}{1+\frac{1}{2^{n}}}\cdot\frac{1}{e^{c^{2}/2}}<span class="sc">\\</span></span>
<span id="cb2-1535"><a href="#cb2-1535" aria-hidden="true" tabindex="-1"></a> &amp; =\frac{1}{2e^{c^{2}/2}}</span>
<span id="cb2-1536"><a href="#cb2-1536" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1537"><a href="#cb2-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1538"><a href="#cb2-1538" aria-hidden="true" tabindex="-1"></a>If this ratio is less than unity, that is $c&gt;\sqrt{2\log2}$, than by the</span>
<span id="cb2-1539"><a href="#cb2-1539" aria-hidden="true" tabindex="-1"></a>ratio test, $\sum(2^{n}+1)e^{-c^{2}n/2}$ converges to a finite value.</span>
<span id="cb2-1540"><a href="#cb2-1540" aria-hidden="true" tabindex="-1"></a>Fix such a $c$.</span>
<span id="cb2-1541"><a href="#cb2-1541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1542"><a href="#cb2-1542" aria-hidden="true" tabindex="-1"></a>By BCL1(Borel-Cantelli Lemma), if</span>
<span id="cb2-1543"><a href="#cb2-1543" aria-hidden="true" tabindex="-1"></a>$A_{n}:=\left<span class="sc">\{</span> \text{There exists atleast one }d\in\mathcal{D}_{n}\text{ with }|Z_{d}|\geq c\sqrt{n}\right<span class="sc">\}</span>$</span>
<span id="cb2-1544"><a href="#cb2-1544" aria-hidden="true" tabindex="-1"></a>and $\sum_{n=0}^{\infty}\mathbb{P}(A_{n})$ converges to a finite value,</span>
<span id="cb2-1545"><a href="#cb2-1545" aria-hidden="true" tabindex="-1"></a>then the event $A_{n}$ occurs finitely many times with probability $1$.</span>
<span id="cb2-1546"><a href="#cb2-1546" aria-hidden="true" tabindex="-1"></a>There exists $N\in\mathbf{N}$, such that for all $n\geq N$, $A_{n}$</span>
<span id="cb2-1547"><a href="#cb2-1547" aria-hidden="true" tabindex="-1"></a>fails to occur with probability $1$. Thus, for all $n\geq N$,</span>
<span id="cb2-1548"><a href="#cb2-1548" aria-hidden="true" tabindex="-1"></a>$<span class="sc">\{</span>Z_{d}\leq c\sqrt{n}<span class="sc">\}</span>$ occurs with probability $1$. It follows that:</span>
<span id="cb2-1549"><a href="#cb2-1549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1550"><a href="#cb2-1550" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1551"><a href="#cb2-1551" aria-hidden="true" tabindex="-1"></a>\sup_{t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>}F_{n}(t) &amp; \leq\frac{c\sqrt{n}}{2^{(n+1)/2}}</span>
<span id="cb2-1552"><a href="#cb2-1552" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1553"><a href="#cb2-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1554"><a href="#cb2-1554" aria-hidden="true" tabindex="-1"></a>Define</span>
<span id="cb2-1555"><a href="#cb2-1555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1556"><a href="#cb2-1556" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1557"><a href="#cb2-1557" aria-hidden="true" tabindex="-1"></a>M_{n} &amp; =\frac{c\sqrt{n}}{2^{(n+1)/2}}</span>
<span id="cb2-1558"><a href="#cb2-1558" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1559"><a href="#cb2-1559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1560"><a href="#cb2-1560" aria-hidden="true" tabindex="-1"></a>Since $\sum M_{n}$ converges, by the Weierstrass $M$-test, the infinite</span>
<span id="cb2-1561"><a href="#cb2-1561" aria-hidden="true" tabindex="-1"></a>series of functions $\sum_{n=0}^{\infty}F_{n}(t)$ converges uniformly on</span>
<span id="cb2-1562"><a href="#cb2-1562" aria-hidden="true" tabindex="-1"></a>$<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>.$ Since, each $F_{n}(t)$ is piecewise linear and continuous, by</span>
<span id="cb2-1563"><a href="#cb2-1563" aria-hidden="true" tabindex="-1"></a>the Term-by-Term continuity theorem, $\sum_{n=0}^{\infty}F_{n}(t)$ is</span>
<span id="cb2-1564"><a href="#cb2-1564" aria-hidden="true" tabindex="-1"></a>continuous on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. ◻</span>
<span id="cb2-1565"><a href="#cb2-1565" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1566"><a href="#cb2-1566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1567"><a href="#cb2-1567" aria-hidden="true" tabindex="-1"></a><span class="fu">## A point of comparison: The Poisson Process.</span></span>
<span id="cb2-1568"><a href="#cb2-1568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1569"><a href="#cb2-1569" aria-hidden="true" tabindex="-1"></a>Like the Brownian motion, the Poisson process is defined as a process</span>
<span id="cb2-1570"><a href="#cb2-1570" aria-hidden="true" tabindex="-1"></a>with stationary and independent increments.</span>
<span id="cb2-1571"><a href="#cb2-1571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1572"><a href="#cb2-1572" aria-hidden="true" tabindex="-1"></a>::: defn</span>
<span id="cb2-1573"><a href="#cb2-1573" aria-hidden="true" tabindex="-1"></a>[]{#def:poisson-process label="def:poisson-process"} A process</span>
<span id="cb2-1574"><a href="#cb2-1574" aria-hidden="true" tabindex="-1"></a>$(N_{t},t\geq0)$ defined on $(\Omega,\mathcal{F},\mathbb{P})$ has the</span>
<span id="cb2-1575"><a href="#cb2-1575" aria-hidden="true" tabindex="-1"></a>distribution of the Poisson process with rate $\lambda&gt;0$, if and only</span>
<span id="cb2-1576"><a href="#cb2-1576" aria-hidden="true" tabindex="-1"></a>if the following hold:</span>
<span id="cb2-1577"><a href="#cb2-1577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1578"><a href="#cb2-1578" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>1<span class="sc">\)</span> $N_{0}=0$.</span>
<span id="cb2-1579"><a href="#cb2-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1580"><a href="#cb2-1580" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>2<span class="sc">\)</span> For any $s&lt;t$, the increment $N_{t}-N_{s}$ is a Poisson random</span>
<span id="cb2-1581"><a href="#cb2-1581" aria-hidden="true" tabindex="-1"></a>variable with parameter $\lambda(t-s).$</span>
<span id="cb2-1582"><a href="#cb2-1582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1583"><a href="#cb2-1583" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>3<span class="sc">\)</span> For any $n\in\mathbf{N}$ and any choice</span>
<span id="cb2-1584"><a href="#cb2-1584" aria-hidden="true" tabindex="-1"></a>$0&lt;t_{1}&lt;t_{2}&lt;\ldots&lt;t_{n}&lt;\infty$, the increments</span>
<span id="cb2-1585"><a href="#cb2-1585" aria-hidden="true" tabindex="-1"></a>$N_{t_{2}}-N_{t_{1}},N_{t_{3}}-N_{t_{2}},\ldots,N_{t_{n}}-N_{t_{n-1}}$</span>
<span id="cb2-1586"><a href="#cb2-1586" aria-hidden="true" tabindex="-1"></a>are independent.</span>
<span id="cb2-1587"><a href="#cb2-1587" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1588"><a href="#cb2-1588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1589"><a href="#cb2-1589" aria-hidden="true" tabindex="-1"></a>Poisson paths can be sampled using this definition. By construction, it</span>
<span id="cb2-1590"><a href="#cb2-1590" aria-hidden="true" tabindex="-1"></a>is not hard to see that the paths of Poisson processes are piecewise,</span>
<span id="cb2-1591"><a href="#cb2-1591" aria-hidden="true" tabindex="-1"></a>constant, integer-valued and non-decreasing. In particular, the paths of</span>
<span id="cb2-1592"><a href="#cb2-1592" aria-hidden="true" tabindex="-1"></a>Poisson processes have finite variation. Poisson paths are much simpler</span>
<span id="cb2-1593"><a href="#cb2-1593" aria-hidden="true" tabindex="-1"></a>than the ones of Brownian motion in many ways!</span>
<span id="cb2-1594"><a href="#cb2-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1595"><a href="#cb2-1595" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-1596"><a href="#cb2-1596" aria-hidden="true" tabindex="-1"></a>(Simulating the Poisson Process.) Use the definition</span>
<span id="cb2-1597"><a href="#cb2-1597" aria-hidden="true" tabindex="-1"></a>(<span class="co">[</span><span class="ot">\[def:poisson-process\]</span><span class="co">](#def:poisson-process)</span>{reference-type="ref"</span>
<span id="cb2-1598"><a href="#cb2-1598" aria-hidden="true" tabindex="-1"></a>reference="def:poisson-process"}) to generate $10$ paths of the Poisson</span>
<span id="cb2-1599"><a href="#cb2-1599" aria-hidden="true" tabindex="-1"></a>process with rate $1$ on the interval $<span class="co">[</span><span class="ot">0,10</span><span class="co">]</span>$ with step-size $0.01$.</span>
<span id="cb2-1600"><a href="#cb2-1600" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1601"><a href="#cb2-1601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1602"><a href="#cb2-1602" aria-hidden="true" tabindex="-1"></a><span class="in">``` {caption="Generating 10 paths of a Poisson process"}</span></span>
<span id="cb2-1603"><a href="#cb2-1603" aria-hidden="true" tabindex="-1"></a><span class="in">def generatePoissonProcess(lam,T,stepSize):</span></span>
<span id="cb2-1604"><a href="#cb2-1604" aria-hidden="true" tabindex="-1"></a><span class="in">    N = int(T/stepSize)</span></span>
<span id="cb2-1605"><a href="#cb2-1605" aria-hidden="true" tabindex="-1"></a><span class="in">    x = np.random.poisson(lam=lam,size=N)</span></span>
<span id="cb2-1606"><a href="#cb2-1606" aria-hidden="true" tabindex="-1"></a><span class="in">    y = np.cumsum(x)</span></span>
<span id="cb2-1607"><a href="#cb2-1607" aria-hidden="true" tabindex="-1"></a><span class="in">    y = np.concatenate([[0.0],y])</span></span>
<span id="cb2-1608"><a href="#cb2-1608" aria-hidden="true" tabindex="-1"></a><span class="in">    return y</span></span>
<span id="cb2-1609"><a href="#cb2-1609" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-1610"><a href="#cb2-1610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1611"><a href="#cb2-1611" aria-hidden="true" tabindex="-1"></a>We can construct a Poisson process as follows. Consider</span>
<span id="cb2-1612"><a href="#cb2-1612" aria-hidden="true" tabindex="-1"></a>$(\tau_{j},j\in\mathbf{N})$ IID exponential random variables with</span>
<span id="cb2-1613"><a href="#cb2-1613" aria-hidden="true" tabindex="-1"></a>parameter $1/\lambda$. One should think of $\tau_{j}$ as the waiting</span>
<span id="cb2-1614"><a href="#cb2-1614" aria-hidden="true" tabindex="-1"></a>time from the $(j-1)$st to the $j$th jump. Then, one defines :</span>
<span id="cb2-1615"><a href="#cb2-1615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1616"><a href="#cb2-1616" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1617"><a href="#cb2-1617" aria-hidden="true" tabindex="-1"></a>N_{t} &amp; =<span class="sc">\#\{</span>k:\tau_{1}+\tau_{2}+\ldots+\tau_{k}\leq t<span class="sc">\}\\</span></span>
<span id="cb2-1618"><a href="#cb2-1618" aria-hidden="true" tabindex="-1"></a> &amp; =\text{Number of jumps upto and including time }t</span>
<span id="cb2-1619"><a href="#cb2-1619" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1620"><a href="#cb2-1620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1621"><a href="#cb2-1621" aria-hidden="true" tabindex="-1"></a>Now, here is an idea! What about defining a new process with stationary</span>
<span id="cb2-1622"><a href="#cb2-1622" aria-hidden="true" tabindex="-1"></a>and independent increments using a given distribution other than Poisson</span>
<span id="cb2-1623"><a href="#cb2-1623" aria-hidden="true" tabindex="-1"></a>and Gaussian? Is this even possible? The answer is yes, but only if the</span>
<span id="cb2-1624"><a href="#cb2-1624" aria-hidden="true" tabindex="-1"></a>distribution satisfies the property of being *infinitely divisible*. To</span>
<span id="cb2-1625"><a href="#cb2-1625" aria-hidden="true" tabindex="-1"></a>see this, consider the value of the process at time $1$, $N_{1}$. Then,</span>
<span id="cb2-1626"><a href="#cb2-1626" aria-hidden="true" tabindex="-1"></a>no matter how many subintervals we chop the interval $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ into, we</span>
<span id="cb2-1627"><a href="#cb2-1627" aria-hidden="true" tabindex="-1"></a>must have the increments add up to $N_{1}$. In other words, we must be</span>
<span id="cb2-1628"><a href="#cb2-1628" aria-hidden="true" tabindex="-1"></a>able to write $N_{1}$ as a sum of $n$ IID random variables for every</span>
<span id="cb2-1629"><a href="#cb2-1629" aria-hidden="true" tabindex="-1"></a>possible $n$. This is certainly true for Poisson random variables and</span>
<span id="cb2-1630"><a href="#cb2-1630" aria-hidden="true" tabindex="-1"></a>Gaussian random variables. Another example is the Cauchy distribution.</span>
<span id="cb2-1631"><a href="#cb2-1631" aria-hidden="true" tabindex="-1"></a>In general, processes that can be constructed using independent,</span>
<span id="cb2-1632"><a href="#cb2-1632" aria-hidden="true" tabindex="-1"></a>stationary increments are called Levy processes.</span>
<span id="cb2-1633"><a href="#cb2-1633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1634"><a href="#cb2-1634" aria-hidden="true" tabindex="-1"></a>::: example</span>
<span id="cb2-1635"><a href="#cb2-1635" aria-hidden="true" tabindex="-1"></a>**Time Inversion.** Let $(B_{t},t\geq0)$ be a standard brownian motion.</span>
<span id="cb2-1636"><a href="#cb2-1636" aria-hidden="true" tabindex="-1"></a>We consider the process:</span>
<span id="cb2-1637"><a href="#cb2-1637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1638"><a href="#cb2-1638" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1639"><a href="#cb2-1639" aria-hidden="true" tabindex="-1"></a>X_{t} &amp; =tB_{1/t}\quad\text{for }t&gt;0</span>
<span id="cb2-1640"><a href="#cb2-1640" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1641"><a href="#cb2-1641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1642"><a href="#cb2-1642" aria-hidden="true" tabindex="-1"></a>*This property relates the behavior of $t$ large to the behavior of $t$</span>
<span id="cb2-1643"><a href="#cb2-1643" aria-hidden="true" tabindex="-1"></a>small.*</span>
<span id="cb2-1644"><a href="#cb2-1644" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-1645"><a href="#cb2-1645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1646"><a href="#cb2-1646" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>a<span class="sc">\)</span> Show that $(X_{t},t&gt;0)$ has the distribution of Brownian motion on</span>
<span id="cb2-1647"><a href="#cb2-1647" aria-hidden="true" tabindex="-1"></a>$t&gt;0$.</span>
<span id="cb2-1648"><a href="#cb2-1648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1649"><a href="#cb2-1649" aria-hidden="true" tabindex="-1"></a>*Proof.*</span>
<span id="cb2-1650"><a href="#cb2-1650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1651"><a href="#cb2-1651" aria-hidden="true" tabindex="-1"></a>Like $B(t)$, it is an easy exercise to prove that $X(t)$ is also a</span>
<span id="cb2-1652"><a href="#cb2-1652" aria-hidden="true" tabindex="-1"></a>Gaussian process.</span>
<span id="cb2-1653"><a href="#cb2-1653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1654"><a href="#cb2-1654" aria-hidden="true" tabindex="-1"></a>We have, $\mathbb{E}<span class="co">[</span><span class="ot">X_{s}</span><span class="co">]</span>=0$.</span>
<span id="cb2-1655"><a href="#cb2-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1656"><a href="#cb2-1656" aria-hidden="true" tabindex="-1"></a>Let $s&lt;t$. We have:</span>
<span id="cb2-1657"><a href="#cb2-1657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1658"><a href="#cb2-1658" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1659"><a href="#cb2-1659" aria-hidden="true" tabindex="-1"></a>Cov(X_{s},X_{t}) &amp; =\mathbb{E}<span class="co">[</span><span class="ot">sB(1/s)\cdot tB(1/t)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1660"><a href="#cb2-1660" aria-hidden="true" tabindex="-1"></a> &amp; =st\mathbb{E}<span class="co">[</span><span class="ot">B(1/s)\cdot B(1/t)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1661"><a href="#cb2-1661" aria-hidden="true" tabindex="-1"></a> &amp; =st\cdot\frac{1}{t}<span class="sc">\\</span></span>
<span id="cb2-1662"><a href="#cb2-1662" aria-hidden="true" tabindex="-1"></a> &amp; \quad\left<span class="sc">\{</span> \because\frac{1}{t}&lt;\frac{1}{s}\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb2-1663"><a href="#cb2-1663" aria-hidden="true" tabindex="-1"></a> &amp; =s</span>
<span id="cb2-1664"><a href="#cb2-1664" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1665"><a href="#cb2-1665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1666"><a href="#cb2-1666" aria-hidden="true" tabindex="-1"></a>Consequently, $X(t)$ has the distribution of a Brownian motion.</span>
<span id="cb2-1667"><a href="#cb2-1667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1668"><a href="#cb2-1668" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>b<span class="sc">\)</span> Argue that $X(t)$ converges to $0$ as $t\to0$ in the sense of</span>
<span id="cb2-1669"><a href="#cb2-1669" aria-hidden="true" tabindex="-1"></a>$L^{2}$-convergence. It is possible to show convergence almost surely so</span>
<span id="cb2-1670"><a href="#cb2-1670" aria-hidden="true" tabindex="-1"></a>that $(X_{t},t\geq0)$ is really a Brownian motion for $t\geq0$.</span>
<span id="cb2-1671"><a href="#cb2-1671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1672"><a href="#cb2-1672" aria-hidden="true" tabindex="-1"></a>*Solution*.</span>
<span id="cb2-1673"><a href="#cb2-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1674"><a href="#cb2-1674" aria-hidden="true" tabindex="-1"></a>Let $(t_{n})$ be any arbitrary sequence of positive real numbers</span>
<span id="cb2-1675"><a href="#cb2-1675" aria-hidden="true" tabindex="-1"></a>approaching $0$ and consider the sequence of random variables</span>
<span id="cb2-1676"><a href="#cb2-1676" aria-hidden="true" tabindex="-1"></a>$(X(t_{n}))_{n=1}^{\infty}$. We have:</span>
<span id="cb2-1677"><a href="#cb2-1677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1678"><a href="#cb2-1678" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1679"><a href="#cb2-1679" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">X(t_{n})^{2}\right</span><span class="co">]</span> &amp; =\mathbb{E}\left<span class="co">[</span><span class="ot">t_{n}^{2}B(1/t_{n})^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1680"><a href="#cb2-1680" aria-hidden="true" tabindex="-1"></a> &amp; =t_{n}^{2}\mathbb{E}\left<span class="co">[</span><span class="ot">B(1/t_{n})^{2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb2-1681"><a href="#cb2-1681" aria-hidden="true" tabindex="-1"></a> &amp; =t_{n}^{2}\cdot\frac{1}{t_{n}}<span class="sc">\\</span></span>
<span id="cb2-1682"><a href="#cb2-1682" aria-hidden="true" tabindex="-1"></a> &amp; =t_{n}</span>
<span id="cb2-1683"><a href="#cb2-1683" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1684"><a href="#cb2-1684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1685"><a href="#cb2-1685" aria-hidden="true" tabindex="-1"></a>Hence,</span>
<span id="cb2-1686"><a href="#cb2-1686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1687"><a href="#cb2-1687" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1688"><a href="#cb2-1688" aria-hidden="true" tabindex="-1"></a>\lim\mathbb{E}\left<span class="co">[</span><span class="ot">X(t_{n})^{2}\right</span><span class="co">]</span> &amp; =\lim t_{n}=0</span>
<span id="cb2-1689"><a href="#cb2-1689" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1690"><a href="#cb2-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1691"><a href="#cb2-1691" aria-hidden="true" tabindex="-1"></a>Since $(t_{n})$ was an arbitrary sequence, it follows that</span>
<span id="cb2-1692"><a href="#cb2-1692" aria-hidden="true" tabindex="-1"></a>$\lim_{t\to0}\mathbb{E}<span class="co">[</span><span class="ot">(X(t))^{2}</span><span class="co">]</span>=0$.</span>
<span id="cb2-1693"><a href="#cb2-1693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1694"><a href="#cb2-1694" aria-hidden="true" tabindex="-1"></a><span class="sc">\(</span>c<span class="sc">\)</span> Use this property of Brownian motion to show the law of large</span>
<span id="cb2-1695"><a href="#cb2-1695" aria-hidden="true" tabindex="-1"></a>numbers for Brownian motion:</span>
<span id="cb2-1696"><a href="#cb2-1696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1697"><a href="#cb2-1697" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1698"><a href="#cb2-1698" aria-hidden="true" tabindex="-1"></a>\lim_{t\to\infty}\frac{X(t)}{t} &amp; =0\quad\text{almost surely}</span>
<span id="cb2-1699"><a href="#cb2-1699" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1700"><a href="#cb2-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1701"><a href="#cb2-1701" aria-hidden="true" tabindex="-1"></a>*Solution.*</span>
<span id="cb2-1702"><a href="#cb2-1702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1703"><a href="#cb2-1703" aria-hidden="true" tabindex="-1"></a>What we need to do is to show that $X(t)\to0$ as $t\to0$ almost surely.</span>
<span id="cb2-1704"><a href="#cb2-1704" aria-hidden="true" tabindex="-1"></a>That would show that $\frac{B(1/t)}{1/t}\to0$ as $t\to0$ almost surely,</span>
<span id="cb2-1705"><a href="#cb2-1705" aria-hidden="true" tabindex="-1"></a>which is the same as showing $\frac{B(t)}{t}\to0$ as $t\to\infty$, which</span>
<span id="cb2-1706"><a href="#cb2-1706" aria-hidden="true" tabindex="-1"></a>is the law of large numbers for Brownian motion.</span>
<span id="cb2-1707"><a href="#cb2-1707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1708"><a href="#cb2-1708" aria-hidden="true" tabindex="-1"></a>What we have done in part (b), is to prove the claim that</span>
<span id="cb2-1709"><a href="#cb2-1709" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}<span class="co">[</span><span class="ot">X(t)^{2}</span><span class="co">]</span>\to0$ as $t\to0$, which shows convergence in the</span>
<span id="cb2-1710"><a href="#cb2-1710" aria-hidden="true" tabindex="-1"></a>$L^{2}$ sense and hence convergence in probability. This is infact the</span>
<span id="cb2-1711"><a href="#cb2-1711" aria-hidden="true" tabindex="-1"></a>weak law of large numbers.</span>
<span id="cb2-1712"><a href="#cb2-1712" aria-hidden="true" tabindex="-1"></a>$\frac{B(t)}{t}\stackrel{\mathbb{\mathbf{P}}}{\to}0$ as $t\to\infty$.</span>
<span id="cb2-1713"><a href="#cb2-1713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1714"><a href="#cb2-1714" aria-hidden="true" tabindex="-1"></a>For $t&gt;0$, continuity is clear. However, it is the proof that as</span>
<span id="cb2-1715"><a href="#cb2-1715" aria-hidden="true" tabindex="-1"></a>$t\to0$, $X(t)\to0$ almost surely which we have not done.</span>
<span id="cb2-1716"><a href="#cb2-1716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1717"><a href="#cb2-1717" aria-hidden="true" tabindex="-1"></a>Note that, the limit $X(t)\to0$ as $t\to0$ if and only if</span>
<span id="cb2-1718"><a href="#cb2-1718" aria-hidden="true" tabindex="-1"></a>$(\forall n\geq1)$, $(\exists m\geq1)$, such that</span>
<span id="cb2-1719"><a href="#cb2-1719" aria-hidden="true" tabindex="-1"></a>$\forall r\in\mathbb{Q}\cap(0,\frac{1}{m}]$, we have</span>
<span id="cb2-1720"><a href="#cb2-1720" aria-hidden="true" tabindex="-1"></a>$|X(r)|=\left|rB\left(\frac{1}{r}\right)\right|\leq\frac{1}{n}$.</span>
<span id="cb2-1721"><a href="#cb2-1721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1722"><a href="#cb2-1722" aria-hidden="true" tabindex="-1"></a>To understand the above, we just recall the $\epsilon-\delta$ definition</span>
<span id="cb2-1723"><a href="#cb2-1723" aria-hidden="true" tabindex="-1"></a>of continuity. Note that $\frac{1}{n}$ plays the role of $\epsilon$ and</span>
<span id="cb2-1724"><a href="#cb2-1724" aria-hidden="true" tabindex="-1"></a>$\frac{1}{m}$ works as $\delta$.</span>
<span id="cb2-1725"><a href="#cb2-1725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1726"><a href="#cb2-1726" aria-hidden="true" tabindex="-1"></a>That is,</span>
<span id="cb2-1727"><a href="#cb2-1727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1728"><a href="#cb2-1728" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb2-1729"><a href="#cb2-1729" aria-hidden="true" tabindex="-1"></a>\Omega^{X}:=\left<span class="sc">\{</span> \lim_{t\to0}X(t)=0\right<span class="sc">\}</span>  &amp; =\bigcap_{n\geq1}\bigcup_{m\geq1}\bigcap_{r\in\mathbb{Q}\cap(0,\frac{1}{m}]}\left<span class="sc">\{</span> \left|X(r)\right|\leq\frac{1}{n}\right<span class="sc">\}</span> </span>
<span id="cb2-1730"><a href="#cb2-1730" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb2-1731"><a href="#cb2-1731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1732"><a href="#cb2-1732" aria-hidden="true" tabindex="-1"></a>Also, note that $X(t)$ is continuous on all $<span class="co">[</span><span class="ot">a,1</span><span class="co">]</span>$ for all $a&gt;0$, thus,</span>
<span id="cb2-1733"><a href="#cb2-1733" aria-hidden="true" tabindex="-1"></a>uniformly continuous on $<span class="co">[</span><span class="ot">a,1</span><span class="co">]</span>$, and hence uniformly continuous on</span>
<span id="cb2-1734"><a href="#cb2-1734" aria-hidden="true" tabindex="-1"></a>$\mathbb{Q}\cap(0,1]$. So, there exists a continuous extension of $X(t)$</span>
<span id="cb2-1735"><a href="#cb2-1735" aria-hidden="true" tabindex="-1"></a>on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. We already know from part (a), that $(X(t))_{t&gt;0}$ and</span>
<span id="cb2-1736"><a href="#cb2-1736" aria-hidden="true" tabindex="-1"></a>$(B(t))_{t&gt;0}$ have the same finite dimensional distributions.</span>
<span id="cb2-1737"><a href="#cb2-1737" aria-hidden="true" tabindex="-1"></a>Therefore, the RHS event has the same probability as</span>
<span id="cb2-1738"><a href="#cb2-1738" aria-hidden="true" tabindex="-1"></a>$\Omega^{B}:=\bigcap_{n\geq1}\bigcup_{m\geq1}\bigcap_{r\in\mathbb{Q}\cap(0,\frac{1}{m}]}\left<span class="sc">\{</span> \left|B(r)\right|\leq\frac{1}{n}\right<span class="sc">\}</span>$.</span>
<span id="cb2-1739"><a href="#cb2-1739" aria-hidden="true" tabindex="-1"></a>Since $B(t)\to0$ as $t\to0$ almost surely, the event $\Omega^{B}$ has</span>
<span id="cb2-1740"><a href="#cb2-1740" aria-hidden="true" tabindex="-1"></a>probability $1$. Thus,</span>
<span id="cb2-1741"><a href="#cb2-1741" aria-hidden="true" tabindex="-1"></a>$\mathbb{P}\left<span class="sc">\{</span> \lim_{t\to0}X(t)=0\right<span class="sc">\}</span> =1$.</span>
<span id="cb2-1742"><a href="#cb2-1742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1743"><a href="#cb2-1743" aria-hidden="true" tabindex="-1"></a>This actually shows that $X(t)$ is a bonafide standard brownian motion,</span>
<span id="cb2-1744"><a href="#cb2-1744" aria-hidden="true" tabindex="-1"></a>as we have established continuity as well.</span>
<span id="cb2-1745"><a href="#cb2-1745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1746"><a href="#cb2-1746" aria-hidden="true" tabindex="-1"></a><span class="fu"># References {.appendix}</span></span>
<span id="cb2-1747"><a href="#cb2-1747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-1748"><a href="#cb2-1748" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*[Introduction to Stochastic Calculus with Applications](https://www.amazon.co.uk/Introduction-Stochastic-Calculus-Applications-3Rd/dp/1848168322), Fima C Klebaner*</span>
<span id="cb2-1749"><a href="#cb2-1749" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*[Brownian Motion Calculus](https://www.amazon.co.uk/Brownian-Motion-Calculus-Ubbo-Wiersema/dp/0470021705), Ubbo Wiersema*</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>
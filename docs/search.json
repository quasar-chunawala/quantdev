[
  {
    "objectID": "posts/tridiagonal-systems/index.html",
    "href": "posts/tridiagonal-systems/index.html",
    "title": "Tridiagonal Systems",
    "section": "",
    "text": "The special case of a system of linear equations that is tridiagonal, that is, has non-zero elements only on the diagonal plus or minus one column, is one that occurs frequently. Also common are systems that are band-diagonal, with non-zero elements only along a few diagonal lines adjacent to the main diagonal (above and below).\nFor triadiagonal sets, the procedures \\(LU\\)-decomposition, forward- and back- substitution each take only \\(O(n)\\) operations and the whole solution can be coded very concisely. In this blog post, I am going to explore solving triadiagonal matrix systems. I closely follow Daniel Duffy’s exposition in Chapter 13 of his excellent book Financial Instrument Pricing using C++.\nLet \\(A\\) be a \\(m \\times n\\) general banded matrix with \\(kl\\) subdiagonals and \\(ku\\) superdiagonals. Then, \\(a_{ij}=0\\), when \\(|i - j| &gt; kl + ku  + 1\\). All non-zero elements are positioned on the main diagonal, \\(kl\\) subdiagonals below it and \\(ku\\) superdiagonals above it.\n\nA diagonal matrix is a \\(n \\times n\\) band matrix with \\(kl = ku = 0\\).\nA Toeplitz matrix is a \\(n \\times n\\) band matrix \\(T_n=[t_{k,j};k,j=0,1,\\ldots,n-1]\\) where \\(t_{k,j}=t_{k-j}\\). That is, a matrix of the form: \\[\nT_n = \\begin{bmatrix}\nt_0 & t_{-1} & t_{-2} & \\ldots & t_{-(n-1)}\\\\\nt_1 & t_0 & t_{-1} & \\ldots & t_{-(n-2)}\\\\\nt_2 & t_1 & t_{0} & \\ldots & t_{-(n-3)}\\\\\n\\vdots & & & \\ddots & \\\\\nt_{n-1} & t_{n-2} & t_{n-3} & \\ldots & t_{0}\n\\end{bmatrix}\n\\]\nA tridiagonal (Jacobi) matrix is a \\(n \\times n\\) band matrix of width three \\(kl = ku = 1\\). \\[\n\\begin{bmatrix}\nb_0 & c_0 & 0 & \\ldots \\\\\na_1 & b_1 & c_1 & \\ldots \\\\\n0 & a_2 & b_2 & \\ldots \\\\\n& & & \\ldots \\\\\n& & & \\ldots & a_{n-2} & b_{n-2} & c_{n-2}\\\\\n& & & \\ldots & 0 & a_{n-1} & b_{n-1}\n\\end{bmatrix}\n\\]\n\nConsider a two-point boundary value problem on the interval \\((0,1)\\) with Dirichlet boundary conditions:\n\\[\n\\begin{align*}\n\\frac{d^2 u}{d x^2} &= f(x), \\quad 0 &lt; x &lt; 1\\\\\nu(0) &= \\phi, \\\\\nu(1) &= \\psi\n\\end{align*}\n\\tag{1}\\]\nWe approximate the solution \\(u\\) by creating a discrete mesh of points defined by \\(\\{x_j\\}\\), \\(j=0,\\ldots,N\\) where \\(N\\) is a positive integer. At each interior mesh point the second derivative in the Equation 1 can be approximated by a second-order divided difference. The corresponding discrete scheme is:\n\\[\n\\begin{matrix}\nU_0 &- 2U_1 &+ U_2 & & & & & & & &= h^2 f_1 \\\\\n& U_1 &- 2U_2 &+ U_3  & & & & & & &= h^2 f_2 \\\\\n& & U_2 &- 2U_3 &+ U_4 & & & & & &= h^2 f_3 \\\\\n& &     &       &      & \\ldots & & & & & \\vdots \\\\\n& &     &       &      & \\ldots & U_{N-3} &- 2U_{N-2} &+ U_{N-1} &  &= h^2 f_{N-2}\\\\\n& &     &       &      & \\ldots &  & U_{N-2} &-2 U_{N-1} &+ U_N &= h^2 f_{N-1}\\\\\n\\end{matrix}\n\\]\nSince \\(U_0 = \\phi\\) and \\(U_N = \\psi\\), we have \\(N-1\\) equations in \\({N-1}\\) unknowns. These can be arranged in the matrix form as:\n\\[\n\\begin{bmatrix}\n-2 & 1\\\\\n1  &-2 & 1  &   & \\ldots &   &    &  \\\\\n   & 1 &-2  & 1 & \\ldots &   &    &  \\\\\n   &   &    &   & \\ldots &   &    &  \\\\\n   &   &    &   & \\ldots & 1 & -2 & 1 \\\\\n   &   &    &   & \\ldots &   &  1 & -2\n\\end{bmatrix}\\begin{bmatrix}\nU_1 \\\\\nU_2 \\\\\n\\vdots\\\\\nU_{N-2} \\\\\nU_{N-1}\n\\end{bmatrix} = \\begin{bmatrix}\nh^2 f_1 - \\phi\\\\\nh^2 f_2 \\\\\n\\vdots\\\\\nh^2 f_{N-2} \\\\\nh^2 f_{N-1} - \\psi\n\\end{bmatrix}\n\\]\nor in matrix form \\(AU=F\\)."
  },
  {
    "objectID": "posts/tridiagonal-systems/index.html#introduction",
    "href": "posts/tridiagonal-systems/index.html#introduction",
    "title": "Tridiagonal Systems",
    "section": "",
    "text": "The special case of a system of linear equations that is tridiagonal, that is, has non-zero elements only on the diagonal plus or minus one column, is one that occurs frequently. Also common are systems that are band-diagonal, with non-zero elements only along a few diagonal lines adjacent to the main diagonal (above and below).\nFor triadiagonal sets, the procedures \\(LU\\)-decomposition, forward- and back- substitution each take only \\(O(n)\\) operations and the whole solution can be coded very concisely. In this blog post, I am going to explore solving triadiagonal matrix systems. I closely follow Daniel Duffy’s exposition in Chapter 13 of his excellent book Financial Instrument Pricing using C++.\nLet \\(A\\) be a \\(m \\times n\\) general banded matrix with \\(kl\\) subdiagonals and \\(ku\\) superdiagonals. Then, \\(a_{ij}=0\\), when \\(|i - j| &gt; kl + ku  + 1\\). All non-zero elements are positioned on the main diagonal, \\(kl\\) subdiagonals below it and \\(ku\\) superdiagonals above it.\n\nA diagonal matrix is a \\(n \\times n\\) band matrix with \\(kl = ku = 0\\).\nA Toeplitz matrix is a \\(n \\times n\\) band matrix \\(T_n=[t_{k,j};k,j=0,1,\\ldots,n-1]\\) where \\(t_{k,j}=t_{k-j}\\). That is, a matrix of the form: \\[\nT_n = \\begin{bmatrix}\nt_0 & t_{-1} & t_{-2} & \\ldots & t_{-(n-1)}\\\\\nt_1 & t_0 & t_{-1} & \\ldots & t_{-(n-2)}\\\\\nt_2 & t_1 & t_{0} & \\ldots & t_{-(n-3)}\\\\\n\\vdots & & & \\ddots & \\\\\nt_{n-1} & t_{n-2} & t_{n-3} & \\ldots & t_{0}\n\\end{bmatrix}\n\\]\nA tridiagonal (Jacobi) matrix is a \\(n \\times n\\) band matrix of width three \\(kl = ku = 1\\). \\[\n\\begin{bmatrix}\nb_0 & c_0 & 0 & \\ldots \\\\\na_1 & b_1 & c_1 & \\ldots \\\\\n0 & a_2 & b_2 & \\ldots \\\\\n& & & \\ldots \\\\\n& & & \\ldots & a_{n-2} & b_{n-2} & c_{n-2}\\\\\n& & & \\ldots & 0 & a_{n-1} & b_{n-1}\n\\end{bmatrix}\n\\]\n\nConsider a two-point boundary value problem on the interval \\((0,1)\\) with Dirichlet boundary conditions:\n\\[\n\\begin{align*}\n\\frac{d^2 u}{d x^2} &= f(x), \\quad 0 &lt; x &lt; 1\\\\\nu(0) &= \\phi, \\\\\nu(1) &= \\psi\n\\end{align*}\n\\tag{1}\\]\nWe approximate the solution \\(u\\) by creating a discrete mesh of points defined by \\(\\{x_j\\}\\), \\(j=0,\\ldots,N\\) where \\(N\\) is a positive integer. At each interior mesh point the second derivative in the Equation 1 can be approximated by a second-order divided difference. The corresponding discrete scheme is:\n\\[\n\\begin{matrix}\nU_0 &- 2U_1 &+ U_2 & & & & & & & &= h^2 f_1 \\\\\n& U_1 &- 2U_2 &+ U_3  & & & & & & &= h^2 f_2 \\\\\n& & U_2 &- 2U_3 &+ U_4 & & & & & &= h^2 f_3 \\\\\n& &     &       &      & \\ldots & & & & & \\vdots \\\\\n& &     &       &      & \\ldots & U_{N-3} &- 2U_{N-2} &+ U_{N-1} &  &= h^2 f_{N-2}\\\\\n& &     &       &      & \\ldots &  & U_{N-2} &-2 U_{N-1} &+ U_N &= h^2 f_{N-1}\\\\\n\\end{matrix}\n\\]\nSince \\(U_0 = \\phi\\) and \\(U_N = \\psi\\), we have \\(N-1\\) equations in \\({N-1}\\) unknowns. These can be arranged in the matrix form as:\n\\[\n\\begin{bmatrix}\n-2 & 1\\\\\n1  &-2 & 1  &   & \\ldots &   &    &  \\\\\n   & 1 &-2  & 1 & \\ldots &   &    &  \\\\\n   &   &    &   & \\ldots &   &    &  \\\\\n   &   &    &   & \\ldots & 1 & -2 & 1 \\\\\n   &   &    &   & \\ldots &   &  1 & -2\n\\end{bmatrix}\\begin{bmatrix}\nU_1 \\\\\nU_2 \\\\\n\\vdots\\\\\nU_{N-2} \\\\\nU_{N-1}\n\\end{bmatrix} = \\begin{bmatrix}\nh^2 f_1 - \\phi\\\\\nh^2 f_2 \\\\\n\\vdots\\\\\nh^2 f_{N-2} \\\\\nh^2 f_{N-1} - \\psi\n\\end{bmatrix}\n\\]\nor in matrix form \\(AU=F\\)."
  },
  {
    "objectID": "posts/tridiagonal-systems/index.html#thomas-algorithm",
    "href": "posts/tridiagonal-systems/index.html#thomas-algorithm",
    "title": "Tridiagonal Systems",
    "section": "Thomas Algorithm",
    "text": "Thomas Algorithm\nThe Thomas algorithm is an efficient way of solving tridiagonal matrix systems. It is based on \\(LU\\)-decomposition in which the matrix system \\(Ax=r\\) is written as \\(LUx=r\\), where \\(L\\) is a lower-triangular matrix and \\(U\\) is an upper triangular matrix. The system can be efficiently solved by setting \\(Ux=\\rho\\) and then solving first \\(L\\rho=r\\) and then \\(Ux=\\rho\\) for \\(x\\). The Thomas algorithm consists of two steps. In step 1, decomposing the matrix \\(M = LU\\) and solving \\(L\\rho=r\\) are accomplished in a single downwards sweep, taking us straight from \\(Ax=r\\) to \\(Ux=\\rho\\). In step 2, the equation \\(Ux = \\rho\\) is solved for \\(x\\) in an upward sweep.\n\nStage 1\nIn the first stage, the matrix equation \\(Ax=r\\) is converted to the form \\(Ux=\\rho\\). Initially, the matrix equation looks like this:\n\\[\n\\begin{bmatrix}\n{\\color{blue}b_1} & {\\color{blue}c_1} & 0 & 0 & 0 & 0\\\\\n{\\color{blue}a_2} & {\\color{blue}b_2} & {\\color{blue}c_2} & 0 & 0 & 0\\\\\n0 & {\\color{blue}a_3} & {\\color{blue}b_3} & {\\color{blue}c_3} & 0 & 0\\\\\n0 & 0 & {\\color{blue}a_4} & {\\color{blue}b_4} & {\\color{blue}c_4} & 0\\\\\n0 & 0 & 0 & {\\color{blue}a_5} & {\\color{blue}b_5} & {\\color{blue}c_5}\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n{\\color{blue}r_1} \\\\\n{\\color{blue}r_2} \\\\\n{\\color{blue}r_3} \\\\\n{\\color{blue}r_4} \\\\\n{\\color{blue}r_5} \\\\\n{\\color{blue}r_6}\n\\end{bmatrix}\n\\]\nRow \\(1\\):\n\\[\n{\\color{blue}b_1} x_1 + {\\color{blue}c_1} x_2 = {\\color{blue}r_1}\n\\]\nDividing throughout by \\(\\color{blue}b_1\\),\n\\[\nx_1 + {\\color{blue}\\frac{c_1}{b_1}} x_2 = {\\color{blue}\\frac{r_1}{b_1}}\n\\]\nRewrite:\n\\[\nx_1 + {\\color{red}\\gamma_1} x_2 = {\\color{red}\\rho_1}, \\quad {\\color{red}\\gamma_1} = {\\color{blue}\\frac{c_1}{b_1}}, \\quad {\\color{red}\\rho_1} = {\\color{blue}\\frac{r_1}{b_1}}\n\\]\n\\[\n\\begin{bmatrix}\n{\\color{red}1} & {\\color{red}\\gamma_1} & 0 & 0 & 0 & 0\\\\\n{\\color{blue}a_2} & {\\color{blue}b_2} & {\\color{blue}c_2} & 0 & 0 & 0\\\\\n0 & {\\color{blue}a_3} & {\\color{blue}b_3} & {\\color{blue}c_3} & 0 & 0\\\\\n0 & 0 & {\\color{blue}a_4} & {\\color{blue}b_4} & {\\color{blue}c_4} & 0\\\\\n0 & 0 & 0 & {\\color{blue}a_5} & {\\color{blue}b_5} & {\\color{blue}c_5}\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n{\\color{red}\\rho_1} \\\\\n{\\color{blue}r_2} \\\\\n{\\color{blue}r_3} \\\\\n{\\color{blue}r_4} \\\\\n{\\color{blue}r_5} \\\\\n{\\color{blue}r_6}\n\\end{bmatrix}\n\\]\nRow \\(2\\):\n\\[\n{\\color{blue}a_2} x_1 + {\\color{blue}b_2} x_2 + {\\color{blue}c_2} x_3 = {\\color{blue}r_2}\n\\]\nUse \\(a_2\\) times row \\(1\\) of the matrix to eliminate the first term\n\\[\na_2(x_1 + {\\color{red}\\gamma_1}x_2 = {\\color{red}\\rho_1})\n\\]\n\\[\n\\begin{array}{c|cccc}\n\\text{Row 2} & a_2 x_1 &+ b_2 x_2 &+ c_2 x_3 &= r_2\\\\\na_2 \\times \\text{Row 1} & a_2 x_1 &+ a_2 \\gamma_1 x_2 & &= a_2\\rho_1\\\\\n\\hline\n\\text{New Row 2} & & (b_2 - a_2 \\gamma_1) x_2 &+ c_2 x_3  &= r_2 - a_2 \\rho_1\n\\end{array}\n\\]\nDividing throughout by \\((b_2 - a_2 \\gamma_1)\\), we get:\n\\[\nx_2 + \\frac{c_2}{b_2 - a_2 \\gamma_1}x_3 = \\frac{(r_2 - a_2 \\rho_1)}{(b_2 - a_2 \\gamma_1)}\n\\]\nWe can rewrite this as:\n\\[\nx_2 + \\gamma_2 x_3 = \\rho_2, \\quad \\gamma_2 = \\frac{c_2}{b_2 - a_2 \\gamma_1}, \\quad \\rho_2 = \\frac{(r_2 - a_2 \\rho_1)}{(b_2 - a_2 \\gamma_1)}\n\\]\n\\[\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & a_3 & b_3 & c_3 & 0 & 0\\\\\n0 & 0 & a_4 & b_4 & c_4 & 0\\\\\n0 & 0 & 0 & a_5 & b_5 & c_5\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\nr_3 \\\\\nr_4 \\\\\nr_5 \\\\\nr_6\n\\end{bmatrix}\n\\]\nRow \\(3\\):\n\\[\na_3 x_2 + b_3 x_3 + c_3 x_4 = r_3\n\\]\nUse \\(a_3\\) times row \\(2\\) of the matrix to eliminate the first term:\n\\[\n\\begin{array}{c|cccc}\n\\text{Row 3} & a_3 x_2 &+ b_3 x_3 &+ c_3 x_4 &= r_3\\\\\na_3 \\times \\text{Row 2} & a_3 x_2 &+ a_3 \\gamma_2 x_3 & &= a_3\\rho_2\\\\\n\\hline\n\\text{New Row 3} & & (b_3 - a_3 \\gamma_2) x_3 &+ c_3 x_4  &= r_3 - a_3 \\rho_2\n\\end{array}\n\\]\nDividing throughout by \\((b_3 - a_3 \\gamma_2)\\), we have:\n\\[\nx_3 + \\frac{c_3}{b_3 - a_3 \\gamma_2} x_4 = \\frac{r_3 - a_3\\rho_2}{b_3 - a_3 \\gamma_2}\n\\]\nWe can rewrite this as:\n\\[\nx_3 + \\gamma_3 x_4 = \\rho_3, \\quad  \\gamma_3 = \\frac{c_3}{b_3 - a_3 \\gamma_2}, \\quad \\rho_3=\\frac{r_3 - a_3 \\rho_2}{b_3 - a_3 \\gamma_2}\n\\]\nContinuing in this fashion, we get:\n\\[\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & a_6 & b_6\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\nr_6\n\\end{bmatrix}\n\\]\nRow \\(6\\):\n\\[\na_6 x_5 + a_6 x_6 = r_6\n\\]\nUse \\(a_6\\) times row 5 of the matrix:\n\\[a_6(x_5 + \\gamma_5 x_6 = \\rho_5)\\]\n\\[\n\\begin{array}{c|cccc}\n\\text{Row 6} & a_6 x_5 &+ b_6 x_6 &= r_6\\\\\na_6 \\times \\text{Row 5} & a_6 x_5 &+ a_6 \\gamma_5 x_6  &= a_6\\rho_5\\\\\n\\hline\n\\text{New Row 3} & & (b_6 - a_6 \\gamma_5) x_6  &= r_6 - a_6 \\rho_5\n\\end{array}\n\\]\nDividing throughout by \\((b_6 - a_6 \\gamma_5)\\), we can rewrite:\n\\[\nx_6 = \\rho_6, \\quad \\rho_6 = \\frac{r_6 - a_6 \\rho_5}{b_6 - a_6 \\gamma_5}\n\\]\n\\[\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\n\\rho_6\n\\end{bmatrix}\n\\]\nThese steps may be summarized as compute the following sequences:\n\\[\n\\gamma_1 = \\frac{c_1}{b_1}, \\quad \\rho_1 = \\frac{r_1}{b_1}\n\\]\nAnd \\[\\gamma_j = \\frac{c_j}{b_j - a_j \\gamma_{j-1}}, \\quad \\rho_j = \\frac{r_j - a_j \\rho_{j-1}}{b_j - a_j \\gamma_{j-1}}\\]\nfor \\(j=2:6\\).\nAt this point, the matrix has been reduced to the upper diagonal form, so our equations are of the form \\(Ux = \\rho\\).\n\n\nStage 2\nThe matrix is now in a form which is trivial to solve for \\(x\\). We start with the last row and work our way up. The final equation is already solved.\n\\[\nx_6 = \\rho_6\n\\]\n\\[\n\\begin{bmatrix}\n1 & \\gamma_1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & \\gamma_2 & 0 & 0 & 0\\\\\n0 & 1 & 1 & \\gamma_3 & 0 & 0\\\\\n0 & 0 & 0 & 1 & \\gamma_4 & 0\\\\\n0 & 0 & 0 & 0 & 1 & \\gamma_5\\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\nx_6\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\rho_1 \\\\\n\\rho_2 \\\\\n\\rho_3 \\\\\n\\rho_4 \\\\\n\\rho_5 \\\\\n\\rho_6\n\\end{bmatrix}\n\\]\nRow \\(5\\): \\[\nx_5 + \\gamma_5 x_6 = \\rho_5\n\\]\nRearrange to get:\n\\[\nx_5 = \\rho_5 - \\gamma_5 x_6\n\\]\nRow \\(4\\):\n\\[\nx_4 + \\gamma_4 x_5 = \\rho_4\n\\]\nRearrange to get:\n\\[\nx_4 = \\rho_4 - \\gamma_4 x_5\n\\]\nContinuing in this fashion, we find that, \\(x_6 = \\rho_6\\) and\n\\[\nx_j = \\rho_j - \\gamma_j x_{j+1}\n\\]\nfor all \\(j=1:5\\)."
  },
  {
    "objectID": "posts/tridiagonal-systems/index.html#computational-solution",
    "href": "posts/tridiagonal-systems/index.html#computational-solution",
    "title": "Tridiagonal Systems",
    "section": "Computational Solution",
    "text": "Computational Solution\nLet’s quickly code up the algorithm in Julia.\n\nfunction thomasAlgorithm(a, b, c, r)\n    N = size(a)[1]\n\n    # Stage 1\n    γ = Array{Float64,1}(undef,N)\n    ρ = Array{Float64,1}(undef,N)\n    u = Array{Float64,1}(undef,N)\n\n    γ[1] = c[1]/b[1]\n    ρ[1] = r[1]/b[1]\n\n    for j=2:N\n        γ[j] = c[j]/(b[j] - a[j] * γ[j-1])\n        ρ[j] = (r[j] - a[j] * ρ[j-1])/(b[j] - a[j] * γ[j-1])\n    end\n\n    # Stage 2\n    u[N] =  ρ[N]\n\n    for j=reverse(1:N-1)\n        u[j] = ρ[j] - γ[j] * u[j+1]\n    end\n\n    return u\nend\n\n# Test Case\n\na = Array{Float64,1}([0, 2, 2, 2])\nb = Array{Float64,1}([3, 3, 3, 3])\nc = Array{Float64,1}([2, 2, 2, 0])\nr = Array{Float64,1}([12, 17, 14, 7])\nu = thomasAlgorithm(a, b, c, r)\n\nprint(u)\n\n[2.0, 3.0, 1.9999999999999996, 1.0000000000000004]\n\n\nHere is an implementation in modern C++:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;functional&gt;\n#include &lt;concepts&gt;\n\ntemplate &lt;typename T&gt;\nconcept Real = std::integral&lt;T&gt; || std::floating_point&lt;T&gt;;\n\ntemplate &lt;typename T&gt;\nrequires Real&lt;T&gt;\nusing Function = std::function&lt;void(std::vector&lt;T&gt;, std::vector&lt;T&gt;, std::vector&lt;T&gt;, std::vector&lt;T&gt;, std::vector&lt;T&gt;&)&gt;;\n\ntemplate &lt;typename T&gt;\nrequires Real&lt;T&gt;\nvoid thomasAlgorithm(\n      std::vector&lt;T&gt; a\n    , std::vector&lt;T&gt; b\n    , std::vector&lt;T&gt; c\n    , std::vector&lt;T&gt; r\n    , std::vector&lt;T&gt;&x \n    ){\n    //Stage-1\n    int N = a.size();\n    std::vector&lt;T&gt; gamma(N);\n    std::vector&lt;T&gt; rho(N);\n    x = std::vector&lt;T&gt;(N);\n\n    gamma[0] = c[0]/b[0]; \n    rho[0] = r[0]/b[0];\n\n    for(int j{1}; j &lt; N; ++j)\n    {\n        gamma[j] = c[j]/(b[j] - a[j] * gamma[j-1]);\n        rho[j] = (r[j] - a[j] * rho[j-1])/(b[j] - a[j] * gamma[j-1]);\n    }\n\n    //Stage-2\n    x[N-1] = rho[N-1];\n    for(int j{N-2}; j &gt;= 0; j--)\n    {\n        x[j] = rho[j] - gamma[j] * x[j+1];\n    }\n}\n\ntemplate &lt;typename T&gt;\nrequires Real&lt;T&gt;\nclass LUTridiagonalSolver{\n    private:\n        std::vector&lt;T&gt; m_a;\n        std::vector&lt;T&gt; m_b;\n        std::vector&lt;T&gt; m_c;\n        std::vector&lt;T&gt; m_r;\n        std::vector&lt;T&gt; m_x;\n        Function&lt;T&gt; m_LUTridiagonalSolverStrategy;\n    \n    public:\n        LUTridiagonalSolver() = default;\n        LUTridiagonalSolver(\n              std::vector&lt;T&gt; a\n            , std::vector&lt;T&gt; b\n            , std::vector&lt;T&gt; c\n            , std::vector&lt;T&gt; r\n            , Function&lt;T&gt; solver) \n            : m_a {std::move(a)}\n            , m_b {std::move(b)}\n            , m_c {std::move(c)}\n            , m_r {std::move(r)}\n            , m_LUTridiagonalSolverStrategy {solver} \n            {}\n\n        std::vector&lt;T&gt; solve(){\n            m_LUTridiagonalSolverStrategy(m_a, m_b, m_c, m_r, m_x);\n            return m_x;\n        }\n\n        LUTridiagonalSolver(const LUTridiagonalSolver& ) = delete;\n        LUTridiagonalSolver operator=(LUTridiagonalSolver& ) = delete;\n        ~LUTridiagonalSolver(){}\n};\n\nint main()\n{\n    std::vector&lt;double&gt; a{0, 2, 2, 2};\n    std::vector&lt;double&gt; b{3, 3, 3, 3};\n    std::vector&lt;double&gt; c{2, 2, 2, 0};\n    std::vector&lt;double&gt; r{12, 17, 14, 7};\n\n    LUTridiagonalSolver&lt;double&gt; solver(a, b, c, r, thomasAlgorithm&lt;double&gt;);\n    std::vector&lt;double&gt; u = solver.solve();\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/tridiagonal-systems/index.html#deriving-the-one-dimensional-heat-equation",
    "href": "posts/tridiagonal-systems/index.html#deriving-the-one-dimensional-heat-equation",
    "title": "Tridiagonal Systems",
    "section": "Deriving the one-dimensional Heat equation",
    "text": "Deriving the one-dimensional Heat equation\nConsider a slender homogenous rod, lying along the \\(x\\)-axis and insulated, so that no heat can escape across its longitudinal surface. In addition, we make the simplifying assumption that the temperature in the rod is constant on each cross-section perpendicular to the \\(x\\)-axis, and thus that the flow of heat in the rod takes place only in the \\(x\\)-direction.\nConsider a small segment of the rod at position \\(x\\) of length \\(\\Delta x\\).\nThe thermal energy in this segment at time \\(t\\) is:\n\\[\nE(x,x+\\Delta x, t) \\approx u(x,t) s \\rho \\Delta x\n\\]\nwhere \\(s\\) is the constant of specific heat i.e. amount of heat required to raise one unit of mass by one unit of temperature, \\(\\rho\\) is the mass density.\nFourier’s law of heat conduction quantifies the idea that heat flows from warmer to colder regions and states that the (rightward) heat flux density \\(\\phi(x,t)\\) (the flow of heat energy per unit area per unit time, SI units \\(J/s/m^2\\)) at any point is:\n\\[\n\\phi(x,t) = -K_0 u_x (x, t)\n\\]\nwhere \\(K_0\\) is the thermal conductivity of the rod. The negative sign shows that the heat flows from higher temperature regions to colder temperature regions.\nAppealing to the law of conservation of energy:\n\\[\n\\begin{align*}\n\\underbrace{\\frac{\\partial}{\\partial t}(u(x,t) s \\rho \\Delta x)}_{\\text{Heat flux through segment}} = \\underbrace{(-K_0 u_x(x, t))}_{\\text{Flux in}} - \\underbrace{(- K_0 u_x(x + \\Delta x,t))}_{\\text{Flux out}}\n\\end{align*}\n\\tag{2}\\]\nDividing throughout by \\(\\Delta x\\) we have:\n\\[\n\\begin{align*}\nu_t(x,t) \\approx \\frac{K_0}{s \\rho } \\frac{u_x(x+\\Delta x, t) - u_x(x,t)}{\\Delta x}\n\\end{align*}\n\\]\nLetting \\(\\Delta x \\to 0\\) improves the approximation and leads to the heat equation:\n\\[\nu_t=c^2 u_{xx}\n\\]\nwhere \\(c^2 = \\frac{K_0}{\\rho s}\\) is called the thermal diffusivity."
  },
  {
    "objectID": "posts/tridiagonal-systems/index.html#the-crank-nicolson-and-theta-methods",
    "href": "posts/tridiagonal-systems/index.html#the-crank-nicolson-and-theta-methods",
    "title": "Tridiagonal Systems",
    "section": "The Crank-Nicolson and Theta methods",
    "text": "The Crank-Nicolson and Theta methods\nConsider the initial boundary value problem for the \\(1\\)d-heat equation:\n\\[\n\\begin{align*}\nu_t &= a^2 u_{xx}, \\quad & 0 &lt; x &lt; L, t&gt;0\\\\\nu(x,0) &= f(x), \\quad 0 \\leq x \\leq L \\\\\nu(0,t) &= A, \\\\\nu(L,t) &= B\n\\end{align*}\n\\]\nIn this case, we can assume without the loss of generality that \\(L = 1\\). Here, \\(a\\), \\(A\\) and \\(B\\) are constants.\nWe find a solution to this system in the case when \\(A = B = 0\\) and \\(a = 1\\) by the method of the separation of variables. In this case, the analytical solution is:\n\\[\nu(x,t) = \\frac{8}{\\pi^2}\\sum_{n=1}^{\\infty}\\frac{1}{n^2}\\sin\\left(\\frac{n\\pi}{2}\\right)\\sin(n\\pi x)\\exp(-n^2 \\pi^2 t)\n\\]\nand we are going to use this solution as a benchmark against which the numerical solutions can be compared.\nWe can discretize a parabolic PDE in the space dimension (using centered difference schemes) while keeping the time variable continuous. We examine the following initial boundary value problem for the \\(1\\)d-heat equation on the unit interval with zero Dirichlet boundary conditions.\nThe problem is:\n\\[\n\\begin{align*}\nu_t &= u_{xx}, \\quad & 0 &lt; x &lt; 1, t&gt;0\\\\\nu(x,0) &= f(x), \\quad 0 \\leq x \\leq 1 \\\\\nu(0,t) &= u(1,t) = 0\n\\end{align*}\n\\tag{3}\\]\nWe partition the space interval \\((0,1)\\) into \\(J\\) subintervals and we approximate Equation 3 by the semi-discrete scheme:\n\\[\n\\begin{align*}\n\\frac{dU_j}{dt} &= \\frac{1}{h^2}(U_{j+1} - 2U_j + U_{j-1}), \\quad 1 \\leq j \\leq J-1 \\\\\nU_0(t) &= U_J(t) = 0, \\quad t &gt; 0 \\\\\nU_j(0) &= f(x_j)\n\\end{align*}\n\\]\nwhere \\(h = 1/J\\) is the constant mesh size. The \\(U_j\\)’s are functions of time \\(t\\). So, we define the following vectors:\n\\[\n\\begin{align*}\nU(t) &= (U_1(t),U_2(t),\\ldots,U_J(t))^T \\\\\nU^0 &= (f(x_1),f(x_2),\\ldots,f(x_{J-1}))^T\n\\end{align*}\n\\]\nThen, we can rewrite the system Equation 3 as a system of ordinary differential equations:\n\\[\n\\begin{align*}\n\\frac{dU}{dt} &= AU\\\\\nU(0) &= U^0\n\\end{align*}\n\\tag{4}\\]\nwhere the matrix \\(A\\) is given by:\n\\[\nA = \\frac{1}{h^2}\\begin{bmatrix}\n-2 & 1 & 0 & \\ldots \\\\\n1  &-2 & 1 & \\ldots \\\\\n0  & 1 & -2 & \\ldots \\\\\n   &   &    & \\ldots \\\\\n      &   &    & \\ldots & 1 & -2 & 1\\\\\n      &   &    & \\ldots & 0 & 1  & -2\\\\\n\\end{bmatrix}\n\\]\nThere are many discretization schemes. I plan to explore various finite difference schemes and their application to derivatives pricing in future posts. For now, I will concentrate on the one-step explicit and implicit methods to discretise the system of ODEs(Equation 4) as:\n\\[\n\\begin{align*}\n\\frac{U^{n+1 - U^n}}{\\Delta t} &= \\theta AU^{n+1} + (1-\\theta)AU^{n}, \\quad 0 \\leq n \\leq N-1, 0 \\leq \\theta \\leq 1 \\\\\nU^{0} &= U(0)\n\\end{align*}\n\\tag{5}\\]\nIn this case, \\(\\Delta t\\) is the constant mesh size in time.\nWe can rewrite Equation 5 in the equivalent form:\n\\[\n\\begin{align*}\nU^{n+1} - U^{n} &= \\theta \\Delta t A U^{n+1} + \\Delta t (I- \\theta)AU^{n} \\\\\n[I - \\Delta t A]U^{n+1} &= (\\Delta t (1 - \\theta) + 1)AU^n\n\\end{align*}\n\\tag{6}\\]\nor formally as:\n\\[\n\\begin{align*}\nU^{n+1} = [1-\\Delta t \\theta A]^{-1} (I + \\Delta t(I - \\theta)) A U^n\n\\end{align*}\n\\tag{7}\\]\nwhere \\(I\\) is the identity matrix.\nSome special cases of \\(\\theta\\) are:\n\\[\n\\begin{align*}\n\\theta &= 1, \\quad \\text{Implicit Euler Scheme}\\\\\n\\theta &= 0, \\quad \\text{Explicit Euler Scheme}\\\\\n\\theta &= 1/2,\\quad \\text{Crank-Nicolson Scheme}\n\\end{align*}\n\\tag{8}\\]\nWhen the schemes are implicit, we can solve the system of equations (Equation 5) at each time level \\(n+1\\) using the Thomas algorithm. No matrix inversion is needed in the case of explicit schemes. The formulation (Equation 4) is called the method of lines and it corresponds to semi-discretization of the PDE in the space direction while keeping the time variable continuous (I will explore MOL in future posts).\nWe can write the scheme (Equation 6 - Equation 8) in the component form:\n\\[\n\\begin{align*}\n\\frac{U^{n+1}j - U^{n}j}{\\Delta t} = \\theta(U^{n+1}_{j+1}-2U^{n+1}_j + U^{n+1}_{j-1})/h^2 + (1-\\theta)(U^{n}_{j+1}-2U^{n}_j + U^{n}_{j-1})/h^2\n\\end{align*}\n\\tag{9}\\]\nor equivalently:\n\\[\n\\begin{align*}\n{U^{n+1}j - U^{n}_j} &= \\lambda\\theta(U^{n+1}_{j+1}-2U^{n+1}_j + U^{n+1}_{j-1})/h^2 \\\\&+ (1-\\theta)(U^{n}_{j+1}-2U^{n}_j + U^{n}_{j-1})\n\\end{align*}\n\\]\nwhere \\(\\lambda = \\Delta t/h^2\\).\nFinally:\n\\[\n\\begin{align*}\n-\\lambda \\theta U^{n+1}_{j+1} + (1+2\\lambda \\theta)U_j^{n+1} - \\lambda \\theta U^{n+1}_{j-1} \\\\= \\lambda (1-\\theta)U^{n}_{j+1}+(1-2\\lambda(1-\\theta))U^{n}_j + \\lambda(1-\\theta)U^{n}_{j-1}\n\\end{align*}\n\\tag{10}\\]\nThe system (Equation 10) is tridiagonal and we can apply the Thomas algorithm to solve it. In the case of the explicit Euler scheme \\((\\theta = 0)\\), these algorithms are not needed, because the solution at time level \\(n+1\\) can be explicitly computed:\n\\[\n\\begin{align*}\nU^{n+1}_j = \\lambda U^{n}_{j+1} + (1-2\\lambda)U^{n}_j +\\lambda U^{n}_{j-1}\n\\end{align*}\n\\tag{11}\\]\n\nComputational Solution\nI implemented the algorithm in Equation 10. This is a one-step marching scheme called BTCS(Backward in Time, Centered in Space) that computes the solution at time level \\(n+1\\) in terms of the solution at time \\(n\\). Since there are three unknowns to be computed at each time level \\(n+1\\), we need to use the Thomas algorithm. The main steps in the algorithm are:\n\nChoose input parameters and generate meshes\nDefine the initial solution and the boundary conditions\nCompute the solution at each time upto and including expiration."
  },
  {
    "objectID": "posts/template-programming/index.html",
    "href": "posts/template-programming/index.html",
    "title": "Template programming",
    "section": "",
    "text": "C++11 introduced variadic templates which permit functions to accept a variable number of arguments. They also permit template types such as std::tuple that can hold a variable number of elements. The main language mechanism enabling variadic templates is parameter packs, which hold an arbitrary number of values or types. Some things are easier to do with parameter packs - for instance passing the values they comprise to a function. Other tasks are a bit trickier to accomplish, such as iterating over a parameter pack or extracting specific elements. However, these things can generally be accomplished through various idioms, some more unwieldy then others.\nBetween C++11 and C++20, the language gained several improvements to variadic templates. Improvements to other features, such as concepts and lambdas, have also created new options for manipulating parameter packs in C++20. Ideally, cataloging these tricks make it easier for people to do what they need with variadic templates."
  },
  {
    "objectID": "posts/template-programming/index.html#an-overview-of-variadic-templates",
    "href": "posts/template-programming/index.html#an-overview-of-variadic-templates",
    "title": "Template programming",
    "section": "An overview of variadic templates",
    "text": "An overview of variadic templates\nA template parameter pack is a template parameter that accepts zero or more template arguments. A function parameter pack is a function parameter that accepts zero or more function arguments. A variadic template is template that captures a parameter pack in its template arguments or function arguments. A parameter pack is captured by introducing an identifier prefixed by an ellipsis, as in ...X. Once captured, a parameter pack can later be used in a pattern expanded by an ellipsis (...), generally to the right of the pattern, as in X.... Pack expansion is conceptually equivalent to having one copy of the pattern for each element of the parameter pack.\n#include &lt;iostream&gt;\n\ntemplate &lt;typename T&gt;\nT sum(T x){\n    return x;\n}\n\ntemplate &lt;typename T, typename... Args&gt;\nT sum(T x, Args... args){\n    return x + sum&lt;Args...&gt;(args...);\n}\n\nint main()\n{   \n    double result = sum(1.0, 2.0, 3.0, 4.0, 5.0);\n    std::cout &lt;&lt; \"result = \" &lt;&lt;  result;\n    return 0;\n}\nCompiler Explorer\nThe sum() function takes one or more arguments. The first argument is always captured by the parameter x and the rest of the arguments are captured by the pack ...args on line 9."
  },
  {
    "objectID": "posts/template-programming/index.html#expanding-parameter-packs",
    "href": "posts/template-programming/index.html#expanding-parameter-packs",
    "title": "Template programming",
    "section": "Expanding parameter packs",
    "text": "Expanding parameter packs\nWhen using a variadic template, we often use a recursive logic with two overloads : one for the general case and one for ending the recursion. For instance:\n#include &lt;iostream&gt;\n\ntemplate &lt;typename T&gt;\nT min(T a, T b)\n{\n    return a &lt; b ? a : b;\n}\n\ntemplate &lt;typename T, typename... Args&gt;\nT min(T first, Args... rest){\n    return min(first, min(rest...));\n}\n\nint main()\n{\n    int a = 2, b = 3, c = 4, d = 5;\n    int minValue {0};\n    minValue = min(a, b);\n    minValue = min(a, b, c);\n    minValue = min(a, b, c, d);\n    return 0;\n}\nCompiler Explorer\nThe below code snip is a minimalistic example of tuple. The first class is the primary template. The primary template tuple has two member variables : first of type Type and rest of type Types... . This means that a template of N elements will contain the first element, and another tuple; this second tuple in turn contains the second element and yet another tuple; so on and so forth.\nA captured parameter pack must be used in a pattern that is expanded with an ellipsis (...). A pattern is a set of tokens containing the identifiers of one or more parameter packs. On line 11, we capture a parameter pack rest consisting of a sequence of values rest[i] each of type Types[i] for the i-th position in parameter pack Types. On line 13, we expand the pattern rest.\n// Variadic class templates and parameter pack expansion\n#include &lt;functional&gt;\n#include &lt;utility&gt;\n#include &lt;iostream&gt;\n\ntemplate &lt;typename Type, typename... Types&gt;\nstruct tuple{\n    Type first_;\n    tuple&lt;Types...&gt; rest_;\n\n    tuple(Type first, Types... rest) \n        : first_(first)\n        , rest_(rest...)\n        {}\n};\n\ntemplate &lt;typename T&gt;\nstruct tuple&lt;T&gt;{\n    T first_;\n\n    tuple(T first) : first_(first) {}\n};\n\nint main()\n{   \n    tuple&lt;double, double, double&gt; x1(3.0, 4.0, 5.0);\n    return 0;\n}\nCompiler Explorer\nWhen a pattern contains more than one parameter pack, all packs must have the same length. This length determines the number of times the pattern is conceptually replicated in the expansion, once for each position in the expanded pack(s). Consider the following code snippet:\n// An example with two parameter packs\n#include &lt;iostream&gt;\n#include &lt;type_traits&gt;\n#include &lt;tuple&gt;\n\ntemplate &lt;std::same_as&lt;char&gt;... C&gt;\nvoid expand(C... c)\n{\n    std::tuple&lt;C...&gt; tpl(c...);\n\n    const char msg[] = { C(std::toupper(c))..., '\\0' };\n    //Do something\n}\nint main()\n{   \n    expand('t','e','m','p','l','a','t','e','s');\n    return 0;\n}\nOn line 7, tuple&lt;C...&gt; expands the pack C in the template-argument list, while tpl(c...) expands c in an initializer list (which, not to be confused with std::initializer_list is the C++ grammar for comma-separated lists of expressions passed as arguments to function calls and constructors).\nOn line 9, we expand the pattern C(std::toupper(c)) in another initializer list. This is an example of a pattern with two packs, C and c, both of which have the same length and are expanded in lockstep. (std::toupper() returns an int rather than a char so requires a cast).\n\nsizeof...(pack)\nThe number of arguments in a parameter pack can be retrieved at compile-time with the sizeof... operator. This operator returns a constexpr value of the std::size_t type. Let’s see this in action:\n#include &lt;iostream&gt;\n#include &lt;array&gt;\ntemplate &lt;typename... Args&gt;\nconstexpr auto get_type_sizes(Args... args){\n    return std::array&lt;std::size_t, sizeof...(Args)&gt;{sizeof(args)...};\n}\n\nint main()\n{\n    auto sizes = get_type_sizes&lt;char, int, long, double&gt;('a', 2, 3L, 3.14);\n    return 0;\n}\nCompiler Explorer\nIn this snippet, sizeof...(Args) evaluates to \\(4\\) at compile-time, while sizeof(args)... is expanded to the following comma-separated pack of arguments: sizeof(char), sizeof(int), sizeof(long) and sizeof(double).\nIn most cases, an expanded pattern is conceptually equivalent to the number of copies of the pattern equal to the size of the parameter pack. Unless otherwise noted, a pattern is expanded by appending an ellipsis (...). Here is a list of contexts in which a pattern can be expanded:\n\nInside template parameters and function parameters, a pack expansion behaves like a comma separated list of patterns. An example in template parameters is the expansion of T in inner here:\n\ntemplate &lt;typename... T&gt;\nstruct outer{\n    template &lt;T... args&gt;\n    struct inner{};\n};\n\nouter&lt;int, double, char[5]&gt; a{};\nAn example in function parameters is the expansion of Args..., when you call foo:\ntemplate &lt;typename... Args&gt;\nvoid foo(Args... args){}\n\nfoo(42);\nfoo(42, 'a');\n\nIn template argument lists as in std::tuple&lt;C...&gt;, the pack expands to the equivalent of a comma separated list of template arguments.\nIn function argument lists when a captured parameter pack appears inside the parenthesis of a function call. The largest expression to the left of the ellipsis (...) is the pattern that is expanded.\n\ntemplate&lt;typename T&gt;\nT step_it(T value){\n    return value + 1;\n}\nT sum(T x){\n    return x;\n}\n\nT sum(T first, T... args){\n    return (first + sum(args...));\n}\n\ntemplate &lt;typename... T&gt;\nvoid do_sums(T... args)\n{\n    auto s1 = sum(args...); \n    // sum(1, 2, 3, 4)\n\n    auto s2 = sum(42, args...);\n    // sum(42, 1, 2, 3, 4)\n\n    auto s3 = sum(step_it(args)...);\n    // sum(2, 3, 4, 5)\n}\n\ndo_sums(1, 2, 3, 4);\n\nIn base specifier lists, to specify one base class for each member of a type parameter pack e.g.:\n\ntemplate &lt;typename Base...&gt;\nstruct MyStruct : Base...{\n    MyStruct();\n};\n\nWhen initializing base classes in a mem-initializer list in a class constructor, the pack expansion initializes a list of base classes based on a type parameter pack:\n\ntemplate&lt;typename... Base&gt;\nstruct MyStruct: Base...{\n    /* Default c'ctor */\n    MyStruct() : Base...() {}\n\n    MyStruct(const Base&... args) : Base{args}... {}\n};\n\nIn initializer lists, the pack exmpansion is conceptually equivlent to a comma-separated list of instances of the pattern.\n\n#include &lt;iostream&gt;\n#include &lt;array&gt;\n\ntemplate&lt;typename... Args&gt;\nstruct sum_wrapper{\n    sum_wrapper(Args... args){\n        result = (... + args);\n    }\n    std::common_type_t&lt;Args...&gt; result;\n};\n\ntemplate&lt;typename... T&gt;\nvoid parenthesized(T... args){\n    std::array&lt;std::common_type_t&lt;T...&gt;,sizeof...(T)&gt; arr {args...};\n    //std::array&lt;int, 4&gt; {1, 2, 3, 4}\n\n    sum_wrapper sw1(args...);\n    //value = 1 + 2 + 3 + 4\n\n    sum_wrapper sw2(++args...);\n    //value = 2 + 3 + 4 + 5\n}\n\nint main()\n{\n    parenthesized(1, 2, 3, 4);\n    return 0;\n}\nCompiler Explorer\n\nIn the context of deriving from a pack of base classes, it is useful to introduce names from the base classes into the definition of the derived class. Therefore, a pack expansion may also appear in a using declaration.\n\n#include &lt;iostream&gt;\n#include &lt;array&gt;\n\nstruct A{\n    void execute() { std::cout &lt;&lt; \"A::execute()\\n\"; }\n};\n\nstruct B{\n    void execute() { std::cout &lt;&lt; \"B::execute()\\n\"; }\n};\n\nstruct C{\n    void execute() { std::cout &lt;&lt; \"C::execute()\\n\"; }\n};\n\ntemplate&lt;typename... Bases&gt;\nstruct X : public Bases...\n{\n    X(Bases const& ... args) : Bases(args)... {}\n    using Bases::execute...;\n    // Conceptually equivalent to\n    // using A::f;\n    // using B::f;\n    // using C::f;\n};\n\nint main()\n{\n    A a; B b; C c; X x(a, b, c);\n    x.A::execute();\n    x.B::execute();\n    x.C::execute();\n\n    \n    return 0;\n}\nCompiler Explorer\n\nLambda Captures - The capture clause of a lambda expression may contain a pack expansion.\n\n#include &lt;iostream&gt;\n\ntemplate&lt;typename... Args&gt;\nstd::common_type_t&lt;Args...&gt; add(Args... args){\n    return (... + args);\n}\n\ntemplate&lt;typename... T&gt;\nvoid captures(T... args){\n    auto l = [args...]{\n        return add(args...);\n    };\n\n    l();\n}\n\nint main()\n{\n    captures(1, 2, 3, 4);\n    return 0;\n}\n\nFold expressions - These are similar to left fold and right fold in functional programming.\n\ntemplate&lt;typename... T&gt;\nint sum(T... args){\n    return (args + ...);\n}\nA pattern may itself contain an expanded parameter pack, in which case there is no need for the inner and outer packs to contain the same number of elements. The expanded inner pack simply becomes a part of the pattern around the outer pack. For example:\n#include &lt;iostream&gt;\n\ntemplate&lt;typename... Args&gt;\nstd::common_type_t&lt;Args...&gt; sum(Args... il){\n    return (... + il);\n}\n\ntemplate&lt;int... N&gt;\nstruct Nested_sum{\n\n    template&lt;typename... Args&gt;\n    int nested_sum(Args... args){\n        return sum(sum(N...,args)...);\n    }\n};\nint main()\n{\n    Nested_sum&lt;1,2&gt; ns{};\n    int result = ns.nested_sum(100, 200);\n    // Equivalent to : sum(sum(1, 2, 100), sum(1, 2, 200))\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/singular-value-decomposition/index.html",
    "href": "posts/singular-value-decomposition/index.html",
    "title": "Singular Value Decomposition(SVD)",
    "section": "",
    "text": "Rectangular matrices do not have eigenvalues. However, we might look at the eigenvalues of the symmetric, positive semidefinite square Gram matrix \\(K=AA^T\\). Perhaps the eigenvalues of \\(K\\) might form an important role for general matrices. They were first studied by the German mathematician Erhard Schmidt in early days of the 20th century.\nSince \\(K=AA^T\\) is necessarily positive semi-definite, its eigenvalues are necessarily non-negative, \\(\\lambda_i \\geq 0\\), which justifies the positivity of the singular values of \\(A\\) - independently of whether \\(A\\) itself has positive, negative or even complex eigenvalues, or is rectangular and has no eigenvalues at all. I will follow the standard convention, and always label the singular values in decreasing order, so that \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r\\).\nIn the special case of symmetric matrices, there is a direct connection between their singular values and their (necessarily real) eigenvalues.\nProof.\nWhen \\(A\\) is symmetric, \\(K=A^T A = A^2\\). So, if\n\\[\nA \\mathbf{v} = \\lambda \\mathbf{v}\n\\]\nthen\n\\[\nK \\mathbf{v} = A^2 \\mathbf{v} = A(A \\mathbf{v}) = A(\\lambda \\mathbf{v}) = \\lambda A \\mathbf{v} = \\lambda^2 \\mathbf{v}\n\\]\nThus, every eigenvector \\(\\mathbf{v}\\) of \\(A\\) is also an eigenvector of \\(K\\) with eigenvalue \\(\\lambda^2\\). So, the eigenvector basis of \\(A\\) is also an eigenvector basis for \\(K\\), and forms a complete system of singular vectors for \\(A\\). \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/singular-value-decomposition/index.html#svd-factorization",
    "href": "posts/singular-value-decomposition/index.html#svd-factorization",
    "title": "Singular Value Decomposition(SVD)",
    "section": "SVD Factorization",
    "text": "SVD Factorization\nThe generalization of the spectral theorem to non-symmetric matrices is known as the singular value decomposition, commonly abbreviated SVD. Unlike the former, which applies to only symmetric matrices, every nonzero matrix possesses a SVD factorization.\n\nTheorem 1 (SVD Factorization) Every non-zero real \\(m \\times n\\) matrix \\(A\\) of rank \\(r &gt; 0\\) can be factored:\n\\[ A = U \\Sigma V^T \\]\ninto the product of an \\(m \\times r\\) matrix \\(U\\), the \\(r \\times r\\) diagonal matrix \\(\\Sigma = diag(\\sigma_1,\\ldots,\\sigma_r)\\) and an \\(r \\times n\\) matrix \\(V^T\\), such that \\(U\\) and \\(V\\) are orthonormal matrices.\n\nProof.\nLet’s begin by writing the desired factorization as \\(AQ = P \\Sigma\\). The individual columns"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html",
    "href": "posts/optimization_algorithms/index.html",
    "title": "Optimization Algorithms",
    "section": "",
    "text": "Definition. Let \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) be a scalar-valued function. The gradient vector of \\(f\\) is defined as:\n\\[\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n\\end{align*}\\]\nThe graph of the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is the hypersurface in \\(\\mathbf{R}^{n+1}\\) given by the equation \\(x_{n+1}=f(x_1,\\ldots,x_n)\\).\nDefinition. \\(f\\) is said to be differentiable at \\(\\mathbf{a}\\) if all the partial derivatives \\(f_{x_i}(\\mathbf{a})\\) exist and if the function \\(h(\\mathbf{x})\\) defined by:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\nis a good linear approximation to \\(f\\) near \\(a\\), meaning that:\n\\[\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n\\end{align*}\\]\nIf \\(f\\) is differentiable at \\(\\mathbf{a},f(\\mathbf{a})\\), then the hypersurface determined by the graph has a tangent hyperplane at \\((\\mathbf{a},f(\\mathbf{a}))\\) given by the equation:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\n\n\nLet \\(f(x,y)\\) be a scalar-valued function of two variables. We understand the partial derivative \\(\\frac{\\partial f}{\\partial x}(a,b)\\) as the slope at the point \\((a,b,f(a,b))\\) of the curve obtained as the intersection of the surface \\(z=f(x,y)\\) and the plane \\(y=b\\). The other partial derivative has a geometric interpretation. However, the surface \\(z=f(x,y)\\) contains infinitely many curves passing through \\((a,b,f(a,b))\\) whose slope we might choose to measure. The directional derivative enables us to do this.\nIntuitively, \\(\\frac{\\partial f}{\\partial x}(a,b)\\) is as the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a}=(a,b)\\) in the \\(\\mathbf{i}\\) direction.\nMathematically, by the definition of the derivative of \\(f\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nSimilarly, we have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n\\end{align*}\\]\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose \\(\\mathbf{v}\\) is a unit vector in \\(\\mathbf{R}^2\\). The quantity:\n\\[\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nis nothing more than the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a} = (a,b)\\) in the direction specified by \\(\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}\\).\nDefinition. Let \\(\\mathbf{v}\\in \\mathbf{R}^n\\) be any unit vector, then the directional derivative of \\(f\\) at \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{v}\\), denoted \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nLet’s define a new function \\(F\\) of a single variable \\(t\\), by holding everything else constant:\n\\[\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v})\n\\end{align*}\\]\nThen, by the definition of directional derivatives, we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0)\n\\end{align*}\\]\nThat is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n\\end{align*}\\]\nLet \\(\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}\\). Then, by the chain rule:\n\\[\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector \\(\\mathbf{v}\\).\nTheorem. Let \\(f:X\\to\\mathbf{R}\\) be differentiable at \\(\\mathbf{a}\\in X\\). Then, the directional derivative \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) exists for all directions \\(\\mathbf{v}\\in\\mathbf{R}^n\\) and moreover we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\\]\n\n\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship’s instruments measures the external atmospheric pressure on your ship as a function \\(f(x,y,z)\\) of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point \\(\\mathbf{a}=(a,b,c)\\) in the direction of the unit vector \\(\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}\\), the rate of change of pressure is given by:\n\\[\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n\\end{align*}\\]\nwhere \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and the gradient vector \\(\\nabla f(\\mathbf{a})\\). Because, \\(-1 \\leq \\cos \\theta \\leq 1\\), and \\(||\\mathbf{u}||=1\\), we have:\n\\[\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\\]\nMoreover, \\(\\cos \\theta = 1\\) when \\(\\theta = 0\\) and \\(\\cos \\theta = -1\\) when \\(\\theta = \\pi\\).\nTheorem. The directional derivative \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) is maximized, with respect to the direction, when \\(\\mathbf{u}\\) points in the direction of the gradient vector \\(f(\\mathbf{a})\\) and is minimized when \\(\\mathbf{u}\\) points in the opposite direction. Furthermore, the maximum and minimum values of \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) are \\(||\\nabla f(\\mathbf{a})||\\) and \\(-||\\nabla f(\\mathbf{a})||\\).\nTheorem Let \\(f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}\\) be a function of class \\(C^1\\). If \\(\\mathbf{x}_0\\) is a point on the level set \\(S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}\\), then the gradient vector \\(\\nabla f(\\mathbf{x}_0) \\in \\mathbf{R}^n\\) is perpendicular to \\(S\\).\nProof. We need to establish the following: if \\(\\mathbf{v}\\) is any vector tangent to \\(S\\) at \\(\\mathbf{x}_0\\), then \\(\\nabla f(\\mathbf{x}_0)\\) is perpendicular to \\(\\mathbf{v}\\) (i.e. \\(\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0\\)). By a tangent vector to \\(S\\) at \\(\\mathbf{x}_0\\), we mean that \\(\\mathbf{v}\\) is the velocity vector of a curve \\(C\\) that lies in \\(S\\) and passes through \\(\\mathbf{x}_0\\).\nLet \\(C\\) be given parametrically by \\(\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))\\) where \\(a &lt; t &lt; b\\) and \\(\\mathbf{x}(t_0) = \\mathbf{x}_0\\) for some number \\(t_0\\) in \\((a,b)\\).\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nEvaluation at \\(t = t_0\\), yields:\n\\[\\begin{align*}\n\\nabla f (\\mathbf{x}(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n\\end{align*}\\]\nOn the other hand, since \\(C\\) is contained in \\(S\\), \\(f(\\mathbf{x})=c\\). So,\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n\\end{align*}\\]\nPutting the above two facts together, we have the desired result."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#gradient-vector",
    "href": "posts/optimization_algorithms/index.html#gradient-vector",
    "title": "Optimization Algorithms",
    "section": "",
    "text": "Definition. Let \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) be a scalar-valued function. The gradient vector of \\(f\\) is defined as:\n\\[\\begin{align*}\n\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\ldots,\\frac{\\partial f}{\\partial x_n}\\right]\n\\end{align*}\\]\nThe graph of the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is the hypersurface in \\(\\mathbf{R}^{n+1}\\) given by the equation \\(x_{n+1}=f(x_1,\\ldots,x_n)\\).\nDefinition. \\(f\\) is said to be differentiable at \\(\\mathbf{a}\\) if all the partial derivatives \\(f_{x_i}(\\mathbf{a})\\) exist and if the function \\(h(\\mathbf{x})\\) defined by:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\nis a good linear approximation to \\(f\\) near \\(a\\), meaning that:\n\\[\\begin{align*}\nL = \\lim_{\\mathbf{x} \\to \\mathbf{a}} \\frac{f(\\mathbf{x}) - h(\\mathbf{x})}{||\\mathbf{x} - \\mathbf{a}||} = 0\n\\end{align*}\\]\nIf \\(f\\) is differentiable at \\(\\mathbf{a},f(\\mathbf{a})\\), then the hypersurface determined by the graph has a tangent hyperplane at \\((\\mathbf{a},f(\\mathbf{a}))\\) given by the equation:\n\\[\\begin{align*}\nh(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})\\cdot (\\mathbf{x}-\\mathbf{a})\n\\end{align*}\\]\n\n\nLet \\(f(x,y)\\) be a scalar-valued function of two variables. We understand the partial derivative \\(\\frac{\\partial f}{\\partial x}(a,b)\\) as the slope at the point \\((a,b,f(a,b))\\) of the curve obtained as the intersection of the surface \\(z=f(x,y)\\) and the plane \\(y=b\\). The other partial derivative has a geometric interpretation. However, the surface \\(z=f(x,y)\\) contains infinitely many curves passing through \\((a,b,f(a,b))\\) whose slope we might choose to measure. The directional derivative enables us to do this.\nIntuitively, \\(\\frac{\\partial f}{\\partial x}(a,b)\\) is as the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a}=(a,b)\\) in the \\(\\mathbf{i}\\) direction.\nMathematically, by the definition of the derivative of \\(f\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}(a,b) &= \\lim_{h \\to 0} \\frac{f(a+h,b) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + (h,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f((a,b) + h(1,0)) - f(a,b)}{h}\\\\\n&=\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{i}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nSimilarly, we have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial y}(a,b) = \\lim_{h\\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{j})-f(\\mathbf{a})}{h}\n\\end{align*}\\]\nWriting partial derivatives as we have enables us to see that they are special cases of a more general type of derivative. Suppose \\(\\mathbf{v}\\) is a unit vector in \\(\\mathbf{R}^2\\). The quantity:\n\\[\\begin{align*}\n\\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nis nothing more than the rate of change of \\(f\\) as we move infinitesimally from \\(\\mathbf{a} = (a,b)\\) in the direction specified by \\(\\mathbf{v}=(A,B) = A\\mathbf{i} + B\\mathbf{j}\\).\nDefinition. Let \\(\\mathbf{v}\\in \\mathbf{R}^n\\) be any unit vector, then the directional derivative of \\(f\\) at \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{v}\\), denoted \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{v}) - f(\\mathbf{a})}{h}\n\\end{align*}\\]\nLet’s define a new function \\(F\\) of a single variable \\(t\\), by holding everything else constant:\n\\[\\begin{align*}\nF(t) = f(\\mathbf{a} + t\\mathbf{v})\n\\end{align*}\\]\nThen, by the definition of directional derivatives, we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) &= \\lim_{t\\to 0} \\frac{f(\\mathbf{a} + t\\mathbf{v}) - f(\\mathbf{a})}{t}\\\\\n&= \\lim_{t\\to 0} \\frac{F(t) - F(0)}{t - 0} \\\\\n&= F'(0)\n\\end{align*}\\]\nThat is:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v})\\vert_{t=0}\n\\end{align*}\\]\nLet \\(\\mathbf{x}(t) = \\mathbf{a}+t\\mathbf{v}\\). Then, by the chain rule:\n\\[\\begin{align*}\n\\frac{d}{dt} f(\\mathbf{a} + t\\mathbf{v}) &= Df(\\mathbf{x}) D\\mathbf{x}(t) \\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nThis equation emphasizes the geometry of the situation. The directional derivative is just the dot product of the gradient vector and the direction vector \\(\\mathbf{v}\\).\nTheorem. Let \\(f:X\\to\\mathbf{R}\\) be differentiable at \\(\\mathbf{a}\\in X\\). Then, the directional derivative \\(D_{\\mathbf{v}}f(\\mathbf{a})\\) exists for all directions \\(\\mathbf{v}\\in\\mathbf{R}^n\\) and moreover we have:\n\\[\\begin{align*}\nD_{\\mathbf{v}}f(\\mathbf{a}) = \\nabla f(\\mathbf{x})\\cdot \\mathbf{v}\n\\end{align*}\\]\n\n\n\nSuppose you are traveling in space near the planet Nilrebo and that one of your spaceship’s instruments measures the external atmospheric pressure on your ship as a function \\(f(x,y,z)\\) of position. Assume quite reasonably that this function is differentiable. Then, the directional derivative exists and if you travel from point \\(\\mathbf{a}=(a,b,c)\\) in the direction of the unit vector \\(\\mathbf{u}=u\\mathbf{i}+v\\mathbf{j}+w\\mathbf{k}\\), the rate of change of pressure is given by:\n\\[\\begin{align*}\nD_{\\mathbf{u}}f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} = ||\\nabla f(\\mathbf{a})|| \\cdot ||\\mathbf{u}|| \\cos \\theta\n\\end{align*}\\]\nwhere \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and the gradient vector \\(\\nabla f(\\mathbf{a})\\). Because, \\(-1 \\leq \\cos \\theta \\leq 1\\), and \\(||\\mathbf{u}||=1\\), we have:\n\\[\\begin{align*}\n- ||\\nabla f(\\mathbf{a})|| \\leq D_{\\mathbf{u}}f(\\mathbf{a}) \\leq ||\\nabla f(\\mathbf{a})||\n\\end{align*}\\]\nMoreover, \\(\\cos \\theta = 1\\) when \\(\\theta = 0\\) and \\(\\cos \\theta = -1\\) when \\(\\theta = \\pi\\).\nTheorem. The directional derivative \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) is maximized, with respect to the direction, when \\(\\mathbf{u}\\) points in the direction of the gradient vector \\(f(\\mathbf{a})\\) and is minimized when \\(\\mathbf{u}\\) points in the opposite direction. Furthermore, the maximum and minimum values of \\(D_{\\mathbf{u}}f(\\mathbf{a})\\) are \\(||\\nabla f(\\mathbf{a})||\\) and \\(-||\\nabla f(\\mathbf{a})||\\).\nTheorem Let \\(f:X \\subseteq \\mathbf{R}^n \\to \\mathbf{R}\\) be a function of class \\(C^1\\). If \\(\\mathbf{x}_0\\) is a point on the level set \\(S=\\{\\mathbf{x} \\in X | f(\\mathbf{x}) = c\\}\\), then the gradient vector \\(\\nabla f(\\mathbf{x}_0) \\in \\mathbf{R}^n\\) is perpendicular to \\(S\\).\nProof. We need to establish the following: if \\(\\mathbf{v}\\) is any vector tangent to \\(S\\) at \\(\\mathbf{x}_0\\), then \\(\\nabla f(\\mathbf{x}_0)\\) is perpendicular to \\(\\mathbf{v}\\) (i.e. \\(\\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v} = 0\\)). By a tangent vector to \\(S\\) at \\(\\mathbf{x}_0\\), we mean that \\(\\mathbf{v}\\) is the velocity vector of a curve \\(C\\) that lies in \\(S\\) and passes through \\(\\mathbf{x}_0\\).\nLet \\(C\\) be given parametrically by \\(\\mathbf{x}(t)=(x_1(t),\\ldots,x_n(t))\\) where \\(a &lt; t &lt; b\\) and \\(\\mathbf{x}(t_0) = \\mathbf{x}_0\\) for some number \\(t_0\\) in \\((a,b)\\).\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= Df(\\mathbf{x}) \\cdot \\mathbf{x}'(t)\\\\\n&= \\nabla f(\\mathbf{x}) \\cdot \\mathbf{v}\n\\end{align*}\\]\nEvaluation at \\(t = t_0\\), yields:\n\\[\\begin{align*}\n\\nabla f (\\mathbf{x}(t_0)) \\cdot \\mathbf{x}'(t_0) = \\nabla f(\\mathbf{x}_0) \\cdot \\mathbf{v}\n\\end{align*}\\]\nOn the other hand, since \\(C\\) is contained in \\(S\\), \\(f(\\mathbf{x})=c\\). So,\n\\[\\begin{align*}\n\\frac{d}{dt}[f(\\mathbf{x}(t))] &= \\frac{d}{dt}[c] = 0\n\\end{align*}\\]\nPutting the above two facts together, we have the desired result."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#gradient-descent---naive-implementation",
    "href": "posts/optimization_algorithms/index.html#gradient-descent---naive-implementation",
    "title": "Optimization Algorithms",
    "section": "Gradient Descent - Naive Implementation",
    "text": "Gradient Descent - Naive Implementation\nBeginning at \\(\\mathbf{x}_0\\), optimization algorithms generate a sequence of iterates \\(\\{\\mathbf{x}_k\\}_{k=0}^{\\infty}\\) that terminate when no more progress can be made or it seems a solution point has been approximated with sufficient accuracy. The gradient descent method is an optimization algorithm that moves along \\(\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)\\) at every step. Thus,\n\\[\\begin{align*}\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{d}_k\n\\end{align*}\\]\nIt can choose the step length \\(\\alpha_k\\) in a variety of ways. One advantage of steepest descent is that it requires the calculation of the gradient \\(\\nabla f(\\mathbf{x}_k)\\), but not of the second derivatives. However, it can be excruciatingly slow on difficult problems.\n\n%load_ext itikz\n\n\nfrom typing import Callable\nimport numpy as np\n\n\ndef gradient_descent(\n    func: Callable[[float], float],\n    alpha: float,\n    xval_0: np.array,\n    epsilon: float = 1e-5,\n    n_iter: int = 10000,\n    debug_step: int = 100,\n):\n    \"\"\"\n    The gradient descent algorithm.\n    \"\"\"\n\n    xval_hist = []\n    funcval_hist = []\n\n    xval_curr = xval_0\n    error = 1.0\n    i = 0\n\n    while np.linalg.norm(error) &gt; epsilon and i &lt; n_iter:\n        # Save down x_curr and func(x_curr)\n        xval_hist.append(xval_curr)\n        funcval_hist.append(func(xval_curr))\n\n        # Calculate the forward difference\n        bump = 0.001\n        num_dims = len(xval_curr)\n        xval_bump = xval_curr + np.eye(num_dims) * bump\n        xval_nobump = np.full((num_dims, num_dims), xval_curr)\n\n        grad = np.array(\n            [\n                (func(xval_h) - func(xval)) / bump\n                for xval_h, xval in zip(xval_bump, xval_nobump)\n            ]\n        )\n\n        # Compute the next iterate\n        xval_next = xval_curr - alpha * grad\n\n        # Compute the error vector\n        error = xval_next - xval_curr\n\n        if i % debug_step == 0:\n            print(\n                f\"x[{i}] = {xval_curr}, f({xval_curr}) = {func(xval_curr)}, f'({xval_curr}) = {grad}, error={error}\"\n            )\n\n        xval_curr = xval_next\n        i += 1\n\n    return xval_hist, funcval_hist\n\nOne infamous test function is the Rosenbrock function defined as:\n\\[\\begin{align*}\nf(x,y) = (a-x)^2 + b(y-x^2)^2\n\\end{align*}\\]\n\ndef rosenbrock(x):\n    return 1*(1-x[0])**2 + 100*(x[1]-x[0]**2)**2\n\ndef f(x):\n    return x[0]**2 + x[1]**2\n\nHere is the plot of the Rosenbrock function with parameters \\(a=1,b=100\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Plot of $f(x,y)=(1-x)^2 + 100(y-x^2)^2$},\n]\n    \\addplot3 [surf] {(1-x)^2 + 100*(y-x^2)^2};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\n\nx_history, f_x_history = gradient_descent(\n    func=rosenbrock,\n    alpha=0.001,\n    xval_0=np.array([-2.0, 2.0]),\n    epsilon=1e-7,\n    debug_step=1000,\n)\n\nprint(f\"x* = {x_history[-1]}, f(x*)={f_x_history[-1]}\")\n\nx[0] = [-2.  2.], f([-2.  2.]) = 409.0, f'([-2.  2.]) = [-1603.9997999  -399.9      ], error=[1.6039998 0.3999   ]\nx[1000] = [-0.34194164  0.12278388], f([-0.34194164  0.12278388]) = 1.804241076974863, f'([-0.34194164  0.12278388]) = [-1.8359394   1.27195859], error=[ 0.00183594 -0.00127196]\nx[2000] = [0.59082668 0.34719456], f([0.59082668 0.34719456]) = 0.16777685109400048, f'([0.59082668 0.34719456]) = [-0.23242066 -0.27632251], error=[0.00023242 0.00027632]\nx[3000] = [0.71914598 0.51617916], f([0.71914598 0.51617916]) = 0.0789773438798074, f'([0.71914598 0.51617916]) = [-0.06806067 -0.09835534], error=[6.80606659e-05 9.83553399e-05]\nx[4000] = [0.7626568  0.58094326], f([0.7626568  0.58094326]) = 0.05638109494458334, f'([0.7626568  0.58094326]) = [-0.02638936 -0.04042575], error=[2.63893643e-05 4.04257465e-05]\nx[5000] = [0.78028032 0.60825002], f([0.78028032 0.60825002]) = 0.04831123625687607, f'([0.78028032 0.60825002]) = [-0.01115051 -0.01747329], error=[1.11505139e-05 1.74732947e-05]\nx[6000] = [0.78785296 0.62017375], f([0.78785296 0.62017375]) = 0.045035368749296534, f'([0.78785296 0.62017375]) = [-0.00487137 -0.00770719], error=[4.87136843e-06 7.70718502e-06]\nx[7000] = [0.79118466 0.62545602], f([0.79118466 0.62545602]) = 0.04363059164103049, f'([0.79118466 0.62545602]) = [-0.00215834 -0.00342913], error=[2.1583377e-06 3.4291304e-06]\nx[8000] = [0.79266536 0.62781071], f([0.79266536 0.62781071]) = 0.04301342477692797, f'([0.79266536 0.62781071]) = [-0.00096218 -0.00153153], error=[9.62177510e-07 1.53153219e-06]\nx[9000] = [0.79332635 0.62886327], f([0.79332635 0.62886327]) = 0.042739342077472306, f'([0.79332635 0.62886327]) = [-0.0004301  -0.00068518], error=[4.30102710e-07 6.85176669e-07]\nx* = [0.7936218  0.62933403], f(x*)=0.04261711392593988"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#convergence.",
    "href": "posts/optimization_algorithms/index.html#convergence.",
    "title": "Optimization Algorithms",
    "section": "Convergence.",
    "text": "Convergence.\nWhen applying gradient descent in practice, we need to choose a value for the learning rate parameter \\(\\alpha\\). An error surface \\(E\\) is usually a convex function on the weight space \\(\\mathbf{w}\\). Intuitively, we might expect that increasing the value of \\(\\alpha\\) should lead to bigger steps through the weight space and hence faster convergence. However, the successive steps oscillate back and forth across the valley, and if we increase \\(\\alpha\\) too much, these oscillations will become divergent. Because \\(\\alpha\\) must be kept sufficiently small to avoid divergent oscillations across the valley, progress along the valley is very slow. Gradient descent then takes many small steps to reach the minimum and is a very inefficient procedure.\nWe can gain deeper insight into this problem, by considering a quadratic approximation to the error function in the neighbourhood of the minimum. Let the error function be given by:\n\\[\\begin{align*}\nf(w) = \\frac{1}{2}w^T A w - b^T w, \\quad w\\in\\mathbf{R}^n\n\\end{align*}\\]\nwhere \\(A\\) is symmetric and \\(A \\succ 0\\).\nDifferentiating on both sides, the gradient of the error function is:\n\\[\\begin{align*}\n\\nabla f(w) = Aw - b\n\\end{align*}\\]\nand the hessian is:\n\\[\\begin{align*}\n\\nabla^2 f(w) = A\n\\end{align*}\\]\nThe critical points of \\(f\\) are given by:\n\\[\\begin{align*}\n\\nabla f(w^*) &= 0\\\\\nAw^{*} - b &= 0\\\\\nw^{*} &= A^{-1}b\n\\end{align*}\\]\nand\n\\[\\begin{align*}\nf(w^{*}) &= \\frac{1}{2}(A^{-1}b)^T A (A^{-1}b) - b^T (A^{-1} b)\\\\\n&= \\frac{1}{2}b^T A^{-1} A A^{-1} b -b^T A^{-1} b \\\\\n&= \\frac{1}{2}b^T A^{-1} b - b^T A^{-1} b \\\\\n&= -\\frac{1}{2}b^T A^{-1} b\n\\end{align*}\\]\nTherefore, the iterates of \\(w\\) are:\n\\[\\begin{align*}\nw^{(k+1)} = w^{(k)} - \\alpha(Aw^{(k)} - b)\n\\end{align*}\\]\nBy the spectral theorem, every symmetric matrix \\(A\\) is orthogonally diagonalizable. So, \\(A\\) admits a factorization:\n\\[\\begin{align*}\nA = Q \\Lambda Q^T\n\\end{align*}\\]\nwhere \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\) and as per convention, we will assume that \\(\\lambda_i\\)are sorted from smallest \\(\\lambda_1\\) to biggest \\(\\lambda_n\\).\nRecall that \\(Q=[q_1,\\ldots,q_n]\\), where \\(q_i\\) are the eigenvectors of \\(A\\) and \\(Q\\) is the change of basis matrix from the standard basis to the eigenvector basis. So, if \\(a \\in \\mathbf{R}^n\\) are the coordinates of a vector in the standard basis and \\(b \\in \\mathbf{R}^n\\) are its coordinates in the eigenvector basis, then \\(a = Qb\\) or \\(b=Q^T a\\).\nLet \\(x^{(k)}=Q^T(w^{(k)}-w^{*})\\). Equivalently, \\(w^{(k)} = Qx^{(k)} + w^{*}\\). Thus, we are shifting the origin to \\(w^{*}\\) and changing the axes to be aligned with the eigenvectors. In this new coordinate system,\n\\[\\begin{align*}\nQx^{(k+1)} + w^{*} &= Qx^{(k)} + w^{*} - \\alpha(AQx^{(k)} + Aw^{*} - b)\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)} + Aw^{*} - b)\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)} + A(A^{-1}b) - b)\\\\\n& \\quad \\{\\text{Substituting } w^{*}=A^{-1}b \\}\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(AQx^{(k)})\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(Q\\Lambda Q^T Qx^{(k)})\\\\\n& \\quad \\{\\text{Substituting } A = Q\\Lambda Q^T \\}\\\\\nQx^{(k+1)} &= Qx^{(k)} - \\alpha(Q\\Lambda x^{(k)})\\\\\n& \\quad \\{\\text{Using } Q^T Q = I \\}\\\\\nx^{(k+1)} &= x^{(k)} - \\alpha\\Lambda x^{(k)}\n\\end{align*}\\]\nThe \\(i\\)-th coordinate of this recursive system is given by:\n\\[\\begin{align*}\nx_i^{(k+1)} &= x_i^{(k)} - \\alpha\\lambda_i x_i^{(k)}\\\\\n&= (1-\\alpha \\lambda_i)x_i^{(k)}\\\\\n&= (1-\\alpha \\lambda_i)^{k+1}x_i^{(0)}\n\\end{align*}\\]\nMoving back to our original space \\(w\\), we can see that:\n\\[\\begin{align*}\nw^{(k)} - w^{*} = Qx^{(k)} &= \\sum_i q_i x_i^{(k)}\\\\\n&= \\sum_i q_i (1-\\alpha \\lambda_i)^{k+1} x_i^{(0)}\n\\end{align*}\\]\nand there we have it - gradient descent in the closed form.\n\nDecomposing the error\nThe above equation admits a simple interpretation. Each element of \\(x^{(0)}\\) is the component of the error in the initial guess in \\(Q\\)-basis. There are \\(n\\) such errors and each of these errors follow their own, solitary path to the minimum, decreasing exponentially with a compounding rate of \\(1-\\alpha \\lambda_i\\). The closer that number is to \\(1\\), the slower it converges.\nFor most step-sizes, the eigenvectors with the largest eigenvalues converge the fastest. This triggers an explosion of progress in the first few iterations, before things slow down, as the eigenvectors with smaller eigenvalues’ struggles are revealed. It’s easy to visualize this - look at the sequences of \\(\\frac{1}{2^k}\\) and \\(\\frac{1}{3^k}\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Comparison of the rates of convergence},\n     xlabel={$n$},\n     ylabel={$f(n)$}\n]\n    \\addplot [domain=0:5,samples=400,blue] {1/(2^x)} node [midway,above] {$2^{-n}$};\n    \\addplot [domain=0:5,samples=400,red] {1/(3^x)} node [midway,below] {$3^{-n}$};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\n\n\nChoosing a step size\nThe above analysis gives us immediate guidance as to how to set a step-size \\(\\alpha\\). In order to converge, each \\(|1-\\alpha \\lambda_i| &lt; 1\\). All workable step-sizes, therefore, fall in the interval:\n\\[\\begin{align*}\n-1 &\\leq 1 - \\alpha \\lambda_i &\\leq 1 \\\\\n-2 &\\leq - \\alpha \\lambda_i &\\leq 0 \\\\\n0 &\\leq \\alpha \\lambda_i &\\leq 2\n\\end{align*}\\]\nBecause \\((1-\\alpha \\lambda_i)\\) could be either positive or negative, the overall convergence rate is determined by the slowest error component, which must be either \\(\\lambda_1\\) or \\(\\lambda_n\\):\n\\[\\begin{align*}\n\\text{rate}(\\alpha) = \\max \\{|1-\\alpha \\lambda_1|,|1-\\alpha \\lambda_n|\\}\n\\end{align*}\\]\nThe optimal learning rate is that which balances the convergence rate. Setting the convergence rate to be equal for the smallest and largest eigenvalues, we can solve for the optimal step size.\n\\[\\begin{align*}\n|1- \\alpha \\lambda_1| = |1- \\alpha \\lambda_n|\n\\end{align*}\\]\nAssuming \\(\\lambda_1 \\neq \\lambda_n\\):\n\\[\\begin{align*}\n1 - \\alpha \\lambda_1 &= -1 + \\alpha \\lambda_n\\\\\n\\alpha (\\lambda_1 + \\lambda_n) &= 2\\\\\n\\alpha^* &= \\frac{2}{\\lambda_1 + \\lambda_n}\n\\end{align*}\\]\nSo, the optimal convergence rate equals:\n\\[\\begin{align*}\n\\max \\{|1-\\alpha \\lambda_1|,|1-\\alpha \\lambda_n|\\} &= 1 - \\frac{2\\lambda_1}{\\lambda_1 + \\lambda_n} \\\\\n&= \\frac{\\lambda_n - \\lambda_1}{\\lambda_n + \\lambda_1}\\\\\n&= \\frac{\\kappa - 1}{\\kappa + 1}\n\\end{align*}\\]\nThe ratio \\(\\kappa = \\lambda_n / \\lambda_1\\) determines the convergence rate of the problem. Recall that the level curves of the error surface are ellipsoids. Hence, a poorly conditioned Hessian results in stretching one of the axes of the ellipses, and taken to its extreme, the contours are almost parallel. Since gradient vectors are orthogonal to the level curves, the optimizer keeps pin-balling between parallel lines and takes forever to reach the center."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#stochastic-gradient-descentsgd",
    "href": "posts/optimization_algorithms/index.html#stochastic-gradient-descentsgd",
    "title": "Optimization Algorithms",
    "section": "Stochastic Gradient Descent(SGD)",
    "text": "Stochastic Gradient Descent(SGD)\nIn machine learning applications, we typically want to minimize the loss function \\(\\mathcal{L}(w)\\) that has the form of a sum:\n\\[\\begin{align*}\n\\mathcal{L}(w) = \\frac{1}{n}\\sum_i L_i(w)\n\\end{align*}\\]\nwhere the weights \\(w\\) (and the biases) are to be estimated. Each summand function \\(L_i\\) is typically associated with the \\(i\\)-th sample in the data-set used for training.\nWhen we minimize the above function with respect to the weights and biases, a standard gradient descent method would perform the following operations:\n\\[\\begin{align*}\nw_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\frac{\\alpha_k}{n}\\sum_{i} \\nabla L_i(w_{k})\n\\end{align*}\\]\nIn the stochastic (or online) gradient descent algorithm, the true gradient of \\(\\mathcal{L}(w)\\) is approximated by the gradient at a single sample:\n\\[\\begin{align*}\nw_{k+1} := w_k - \\alpha_k \\nabla \\mathcal{L}(w_{k}) = w_k - \\alpha_k \\nabla L_i(w_{k})\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#sgdoptimizer-class",
    "href": "posts/optimization_algorithms/index.html#sgdoptimizer-class",
    "title": "Optimization Algorithms",
    "section": "SGDOptimizer class",
    "text": "SGDOptimizer class\nWe are now in a position to code the SGDOptimizer class.\n\n# Global imports\nimport numpy as np\nimport nnfs\nimport matplotlib.pyplot as plt\nfrom nnfs.datasets import spiral_data\n\nfrom dense_layer import DenseLayer\nfrom relu_activation import ReLUActivation\nfrom softmax_activation import SoftmaxActivation\n\nfrom loss import Loss\nfrom categorical_cross_entropy_loss import CategoricalCrossEntropyLoss\nfrom categorical_cross_entropy_softmax import CategoricalCrossEntropySoftmax\n\n\nclass SGDOptimizer:\n\n    # Initialize the optimizer\n    def __init__(self, learning_rate=1.0):\n        self.learning_rate = learning_rate\n\n    # Update the parameters\n    def update_params(self, layer):\n        layer.weights -= self.learning_rate * layer.dloss_dweights\n        layer.biases -= self.learning_rate * layer.dloss_dbiases\n\nLet’s play around with our optimizer.\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a DenseLayer with 2 input features and 64 neurons\ndense1 = DenseLayer(2, 64)\n\n# Create ReLU Activation (to be used with DenseLayer 1)\nactivation1 = ReLUActivation()\n\n# Create the second DenseLayer with 64 inputs and 3 output values\ndense2 = DenseLayer(64,3)\n\n# Create SoftmaxClassifer's combined loss and activation\nloss_activation = CategoricalCrossEntropySoftmax()\n\n# The next step is to create the optimizer object\noptimizer = SGDOptimizer()\n\nNow, we perform a forward pass of our sample data.\n\n# Perform a forward pass for our sample data\ndense1.forward(X)\n\n# Performs a forward pass through the activation function\n# takes the output of the first dense layer here\nactivation1.forward(dense1.output)\n\n# Performs a forward pass through the second DenseLayer\ndense2.forward(activation1.output)\n\n# Performs a forward pass through the activation/loss function\n# takes the output of the second DenseLayer and returns the loss\nloss = loss_activation.forward(dense2.output, y)\n\n# Let's print the loss value\nprint(f\"Loss = {loss}\")\n\n# Now we do our backward pass \nloss_activation.backward(loss_activation.output, y)\ndense2.backward(loss_activation.dloss_dz)\nactivation1.backward(dense2.dloss_dinputs)\ndense1.backward(activation1.dloss_dz)\n\n# Then finally we use our optimizer to update the weights and biases\noptimizer.update_params(dense1)\noptimizer.update_params(dense2)\n\nLoss = 1.0986526582562541\n\n\nThis is everything we need to train our model!\nBut why would we only perform this optimization only once, when we can perform it many times by leveraging Python’s looping capabilities? We will repeatedly perform a forward pass, backward pass and optimization until we reach some stopping point. Each full pass through all of the training data is called an epoch.\nIn most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of our training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass and optimization calculations.\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a dense layer with 2 input features and 64 output values\ndense1 = DenseLayer(2, 64)\n\n# Create ReLU Activation (to be used with the DenseLayer)\nactivation1 = ReLUActivation()\n\n# Create a second DenseLayer with 64 input features (as we take\n# output of the previous layer here) and 3 output values (output values)\ndense2 = DenseLayer(64, 3)\n\n# Create Softmax classifier's combined loss and activation\nloss_activation = CategoricalCrossEntropySoftmax()\n\n# Create optimizer\noptimizer = SGDOptimizer()\n\n# Train in loop\nfor epoch in range(10001):\n\n    # Perform a forward pass of our training data through this layer\n    dense1.forward(X)\n\n    # Perform a forward pass through the activation function\n    # takes the output of the first dense layer here\n    activation1.forward(dense1.output)\n\n    # Perform a forward pass through second DenseLayer\n    # takes the outputs of the activation function of first layer as inputs\n    dense2.forward(activation1.output)\n\n    # Perform a forward pass through the activation/loss function\n    # takes the output of the second DenseLayer here and returns the loss\n    loss = loss_activation.forward(dense2.output, y)\n\n    if not epoch % 1000:\n        print(f\"Epoch: {epoch}, Loss: {loss: .3f}\")\n\n    # Backward pass\n    loss_activation.backward(loss_activation.output, y)\n    dense2.backward(loss_activation.dloss_dz)\n    activation1.backward(dense2.dloss_dinputs)\n    dense1.backward(activation1.dloss_dz)\n\n    # Update the weights and the biases\n    optimizer.update_params(dense1)\n    optimizer.update_params(dense2)\n\nEpoch: 0, Loss:  1.099\nEpoch: 1000, Loss:  1.029\nEpoch: 2000, Loss:  0.962\nEpoch: 3000, Loss:  0.848\nEpoch: 4000, Loss:  0.699\nEpoch: 5000, Loss:  0.544\nEpoch: 6000, Loss:  0.508\nEpoch: 7000, Loss:  0.478\nEpoch: 8000, Loss:  0.460\nEpoch: 9000, Loss:  0.443\nEpoch: 10000, Loss:  0.419\n\n\nOur neural network mostly stays stuck at around a loss of \\(1.0\\) and later around \\(0.85\\)-\\(0.90\\) Given that this loss didn’t decrease much, we can assume that this learning rate being too high, also caused the model to get stuck in a local minimum, which we’ll learn more about soon. Iterating over more epochs, doesn’t seem helpful at this point, which tells us that we’re likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?\nRecall that we’re adjusting our weights and biases by applying some fraction, in this case \\(1.0\\) to the gradient and subtracting this from the weights and biases. This fraction is called the learning rate (LR) and is the primary adjustable parameter for the optimizer as it decreases loss."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#learning-rate-decay",
    "href": "posts/optimization_algorithms/index.html#learning-rate-decay",
    "title": "Optimization Algorithms",
    "section": "Learning Rate Decay",
    "text": "Learning Rate Decay\nThe idea of a learning rate decay is to start with a large learning rate, say \\(1.0\\) in our case and then decrease it during training. There are a few methods for doing this. One option is program a decay rate, which steadily decays the learning rate per batch or per epoch.\nLet’s plan to decay per step. This can also be referred to as \\(1/t\\) decaying or exponential decaying. Basically, we’re going to update the learning rate each step by the reciprocal of the step count fraction. This fraction is a new hyper parameter that we’ll add to the optimizer, called the learning rate decay.\n\ninitial_learning_rate = 1.0\nlearning_rate_decay = 0.1\n\nfor step in range(10):\n    learning_rate = initial_learning_rate * 1.0 / (1 + learning_rate_decay * step)\n    print(learning_rate)\n\n1.0\n0.9090909090909091\n0.8333333333333334\n0.7692307692307692\n0.7142857142857143\n0.6666666666666666\n0.625\n0.588235294117647\n0.5555555555555556\n0.5263157894736842\n\n\nThe derivative of the function \\(\\frac{1}{1+x}\\) is \\(-\\frac{1}{(1+x)^2}\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[\n     title={Plot of $f(x)=-\\frac{1}{(1+x)^2}$},\n     xlabel={$x$},\n     ylabel={$f(x)$}\n]\n    \\addplot [domain=0:1,samples=400] {-1/(( 1 + x)^2)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe learning rate drops fast initially, but the change in the learning rate lowers in each step. We can update our SGDOptimizer class to allow for the learning rate decay.\n\nclass SGDOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n        layer.weights += -self.current_learning_rate * layer.dloss_dweights\n        layer.biases += -self.current_learning_rate * layer.dloss_dbiases\n\n    def post_update_params(self):\n        self.iterations += 1\n\nLet’s use a decay rate of \\(0.01\\) and train our neural network again.\n\ndef train(decay):\n    # Create a dataset\n    X, y = spiral_data(samples=100, classes=3)\n\n    # Create a dense layer with 2 input features and 64 output values\n    dense1 = DenseLayer(2, 64)\n\n    # Create ReLU activation (to be used with the dense layer)\n    activation1 = ReLUActivation()\n\n    # Create second DenseLayer with 64 input features (as we take output of the\n    # previous layer here) and 3 output values\n    dense2 = DenseLayer(64, 3)\n\n    # Create Softmax classifier's combined loss and activation\n    loss_activation = CategoricalCrossEntropySoftmax()\n\n    # Create optimizer\n    optimizer = SGDOptimizer(learning_rate=1.0,decay=decay)\n\n    acc_vals = []\n    loss_vals = []\n    lr_vals = []\n\n    # Train in a loop\n    for epoch in range(10001):\n        # Perform a forward pass of our training data through this layer\n        dense1.forward(X)\n\n        # Perform a forward pass through the activation function\n        # takes the output of the first dense layer here\n        activation1.forward(dense1.output)\n\n        # Perform a forward pass through second DenseLayer\n        # takes the outputs of the activation function of first layer as inputs\n        dense2.forward(activation1.output)\n\n        # Perform a forward pass through the activation/loss function\n        # takes the output of the second DenseLayer here and returns the loss\n        loss = loss_activation.forward(dense2.output, y)\n\n        # Calculate accuracy from output of activation2 and targets\n        # Calculate values along the first axis\n        predictions = np.argmax(loss_activation.output, axis=1)\n        if len(y.shape) == 2:\n            y = np.argmax(y, axis=1)\n\n        accuracy = np.mean(predictions == y)\n\n        if epoch % 1000 == 0:\n            print(\n                f\"epoch: {epoch}, \\\n                acc : {accuracy:.3f}, \\\n                loss: {loss: .3f}, \\\n                lr : {optimizer.current_learning_rate}\"\n            )\n\n        acc_vals.append(accuracy)\n        loss_vals.append(loss)\n        lr_vals.append(optimizer.current_learning_rate)\n\n        # Backward pass\n        loss_activation.backward(loss_activation.output, y)\n        dense2.backward(loss_activation.dloss_dz)\n        activation1.backward(dense2.dloss_dinputs)\n        dense1.backward(activation1.dloss_dz)\n\n        # Update the weights and the biases\n        optimizer.pre_update_params()\n        optimizer.update_params(dense1)\n        optimizer.update_params(dense2)\n        optimizer.post_update_params()\n\n    return acc_vals, loss_vals, lr_vals\n\n\nacc_vals, loss_vals, lr_vals = train(decay=0.01)\n\nepoch: 0,                 acc : 0.333,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.477,                 loss:  1.066,                 lr : 0.09099181073703366\nepoch: 2000,                 acc : 0.457,                 loss:  1.065,                 lr : 0.047641734159123386\nepoch: 3000,                 acc : 0.453,                 loss:  1.065,                 lr : 0.03226847370119393\nepoch: 4000,                 acc : 0.450,                 loss:  1.064,                 lr : 0.02439619419370578\nepoch: 5000,                 acc : 0.440,                 loss:  1.064,                 lr : 0.019611688566385566\nepoch: 6000,                 acc : 0.443,                 loss:  1.063,                 lr : 0.016396130513198885\nepoch: 7000,                 acc : 0.447,                 loss:  1.063,                 lr : 0.014086491055078181\nepoch: 8000,                 acc : 0.447,                 loss:  1.063,                 lr : 0.012347203358439314\nepoch: 9000,                 acc : 0.447,                 loss:  1.062,                 lr : 0.010990218705352238\nepoch: 10000,                 acc : 0.447,                 loss:  1.062,                 lr : 0.009901970492127933\n\n\n\n\nShow the code\nplt.grid(True)\nepochs = np.linspace(0,10000,10001)\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(epochs,acc_vals)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(epochs,loss_vals)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.plot(epochs, lr_vals)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe optimization algorithm appears to be stuck and the reason is because the learning rate decayed far too quickly and became too small, trapping the optimizer in some local minimum. We can, instead, try to decay a bit slower by making our decay a smaller number. For example, let’s go with \\(10^{-3}\\).\n\nacc_vals, loss_vals, lr_vals = train(decay=1e-3)\n\nepoch: 0,                 acc : 0.327,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.410,                 loss:  1.066,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.413,                 loss:  1.055,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.457,                 loss:  1.014,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.527,                 loss:  0.968,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.547,                 loss:  0.935,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.563,                 loss:  0.918,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.573,                 loss:  0.900,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.577,                 loss:  0.882,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.590,                 loss:  0.860,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.603,                 loss:  0.845,                 lr : 0.09091735612328393\n\n\n\n\nShow the code\nplt.grid(True)\nepochs = np.linspace(0,10000,10001)\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.plot(epochs,acc_vals)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.plot(epochs,loss_vals)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplt.grid(True)\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.plot(epochs, lr_vals)\nplt.show()"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#stochastic-gradient-descent-with-momentum",
    "href": "posts/optimization_algorithms/index.html#stochastic-gradient-descent-with-momentum",
    "title": "Optimization Algorithms",
    "section": "Stochastic Gradient Descent with Momentum",
    "text": "Stochastic Gradient Descent with Momentum\nMomentum proposes a small tweak to gradient descent. We give gradient descent a short-term memory. Let’s define the updated velocity \\(z^{k+1}\\) to be weighted and controlled by the mass \\(\\beta\\). When \\(\\beta\\) is high, we simply use the velocity from the last time, that is, we are entirely driven by momentum. When \\(\\beta=0\\), the momentum is zero.\n\\[\\begin{align*}\nz^{(k+1)} &= \\beta z^{(k)} + \\nabla f(w^{(k)})\\\\\nw^{k+1} &= w^k - \\alpha z^{k+1}\n\\end{align*}\\]\n\\(z^{(k+1)}\\) is called the velocity. It accumulates the past gradients similar to how a heavy ball rolling down the error function landscape integrates over past forces. To see what’s happening in more detail, we can recursively write out:\n\\[\\begin{align*}\nz^{(k)} &= \\beta z^{k-1} + \\nabla f(w^{(k-1)}) \\\\\n&= \\beta(\\beta z^{k-2} + \\nabla f(w^{(k-2)})) + \\nabla f(w^{(k-1)})\\\\\n&= \\beta^2 z^{k-2} + \\beta \\nabla f(w^{(k-2)}) + \\nabla f(w^{(k-1)})\\\\\n&= \\beta^2 (\\beta z^{k-3} + \\nabla f(w^{(k-3)}) ) + \\beta \\nabla f(w^{(k-2)}) + \\nabla f(w^{(k-1)})\\\\\n&= \\sum_{t=0}^{k} \\beta^t \\nabla f(w^{(k-1-t)})\n\\end{align*}\\]\nThe new gradient replacement no longer points into the direction of steepest descent on a particular instance any longer but rather in the direction of an exponentially weighted average of past gradients.\n\nThe dynamics of Momentum\nSince \\(\\nabla f(w^k) = Aw^k - b\\), the update on the quadratic is:\n\\[\\begin{align*}\nz^{k+1} &= \\beta z^k + (Aw^k - b)\\\\\nw^{k+1} &= w^k - \\alpha z^{k+1}\n\\end{align*}\\]\nWe go through the same motions as before with the change of basis \\((w^k - w^{*})=Qx^k\\) and \\(z^k = Q y^k\\) to yield the update rule:\n\\[\\begin{align*}\nQ y^{k+1} &= \\beta Q y^k + (AQx^k + Aw^* - b)\\\\\nQ y^{k+1} &= \\beta Q y^k + (AQx^k + AA^{-1}b - b)\\\\\nQ y^{k+1} &= \\beta Q y^k + Q\\Lambda Q^T Q x^k\\\\\nQ y^{k+1} &= \\beta Q y^k + Q\\Lambda x^k\\\\\ny^{k+1} &= \\beta y^k + \\Lambda x^k\n\\end{align*}\\]\nor equivalently:\n\\[\\begin{align*}\ny_i^{k+1} &= \\beta y_i^k + \\lambda_i x_i^k\n\\end{align*}\\]\nMoreover,\n\\[\\begin{align*}\nQx^{k+1} + w^* &= Qx^k + w^* - \\alpha Qy^{k+1}\\\\\nx^{k+1} &= x^k - \\alpha y^{k+1}\n\\end{align*}\\]\nor equivalently:\n\\[\\begin{align*}\nx_i^{k+1} &= x_i^k - \\alpha y_i^{k+1}\n\\end{align*}\\]\nThis lets us rewrite our iterates as:\n\\[\\begin{align*}\n\\begin{bmatrix}\ny_i^{k+1}\\\\\nx_i^{k+1}\n\\end{bmatrix} &=\n\\begin{bmatrix}\n\\beta y_i^k + \\lambda_i x_i^k\\\\\n(1-\\alpha\\lambda_i)x_i^k - \\alpha \\beta y_i^k\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\beta & \\lambda_i\\\\\n- \\alpha \\beta & (1-\\alpha\\lambda_i)\n\\end{bmatrix}\n\\begin{bmatrix}\ny_i^k\\\\\nx_i^k\n\\end{bmatrix}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\begin{bmatrix}\ny_i^k\\\\\nx_i^k\n\\end{bmatrix} = R^k \\begin{bmatrix}\ny_i^0\\\\\nx_i^0\n\\end{bmatrix},\\quad\nR = \\begin{bmatrix}\n\\beta & \\lambda_i\\\\\n- \\alpha \\beta & (1-\\alpha\\lambda_i)\n\\end{bmatrix}\n\\end{align*}\\]\nIn the case of \\(2 \\times 2\\) matrix, there is an elegant little known formula in terms of the eigenvalues of the matrix \\(R\\), \\(\\sigma_1\\) and \\(\\sigma_2\\):\n\\[\\begin{align*}\nR^k = \\begin{cases}\n\\sigma_1^k R_1 - \\sigma_2^k R_2 & \\sigma_1 \\neq \\sigma_2,\\\\\n\\sigma_1^k(kR\\sigma_1-(k-1)I) & \\sigma_1 = \\sigma_2\n\\end{cases}\n\\quad\nR_j = \\frac{R-\\sigma_j I}{\\sigma_1 - \\sigma_2}\n\\end{align*}\\]\nThe formula is rather complicated, but the takeway here is that it plays the exact same role the individual convergence rates \\((1-\\alpha \\lambda_i)\\) do in gradient descent. The convergence rate is therefore the slowest of the two rates, \\(\\max \\{|\\sigma_1|,|\\sigma_2|\\}\\).\nFor what values of \\(\\alpha\\) and \\(\\beta\\) does momentum converge? Since we need both \\(\\sigma_1\\) and \\(\\sigma_2\\) to converge, our convergence criterion is now \\(\\max \\{|\\sigma_1|,|\\sigma_2|\\} &lt; 1\\).\nIt can be shown that when we choose an optimal value of the parameters \\(\\alpha\\) and \\(\\beta\\), the convergence rate is proportional to:\n\\[\\begin{align*}\n\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\n\\end{align*}\\]\nWith barely a modicum of extra effort, we have square-rooted the condition number."
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#adding-momentum-to-the-sgdoptimizer-class",
    "href": "posts/optimization_algorithms/index.html#adding-momentum-to-the-sgdoptimizer-class",
    "title": "Optimization Algorithms",
    "section": "Adding momentum to the SGDOptimizer class",
    "text": "Adding momentum to the SGDOptimizer class\nWe are now in a position to add momentum to the SGDOptimizer class.\n\nclass SGDOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n        self.beta = momentum\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n\n        # If we use momentum\n        if self.beta:\n\n            # If the layer does not contain momentum arrays, create them\n            # filled with zeros\n            if not hasattr(layer, \"weight_momentums\"):\n                layer.weight_momentums = np.zeros_like(layer.dloss_dweights)\n                # If there is no momentumm array for weights\n                # the array doesnt exist for biases yet either\n                layer.bias_momentums = np.zeros_like(layer.dloss_dbiases)\n\n            # Build weight updates with momentum - take previous\n            # updates multiplied by retain factor and update with\n            # with current gradients\n            # v[t+1] = \\beta * v[t] + \\alpha * dL/dw\n            weight_updates = (\n                self.beta * layer.weight_momentums\n                + self.current_learning_rate * layer.dloss_dweights\n            )\n            layer.weight_momentums = weight_updates\n\n            # Build bias updates\n            bias_updates = (\n                self.beta * layer.bias_momentums\n                + self.current_learning_rate * layer.dloss_dbiases\n            )\n            layer.bias_momentums = bias_updates\n        else:\n            # Vanilla SGD updates (as before momentum update)\n            weight_updates = self.current_learning_rate * layer.dloss_dweights\n            bias_updates = self.current_learning_rate * layer.dloss_dbiases\n\n        layer.weights -= weight_updates\n        layer.biases -= bias_updates\n\n    def post_update_params(self):\n        self.iterations += 1\n\nLet’s see an example illustrating how adding momentum changes the learning process. Keeping the same learning_rate=1.0 and decay=1e-3 from the previous training attempt and using a momentum of 0.50:\n\ndef train(decay, momentum):\n    # Create a dataset\n    X, y = spiral_data(samples=100, classes=3)\n\n    # Create a dense layer with 2 input features and 64 output values\n    dense1 = DenseLayer(2, 64)\n\n    # Create ReLU activation (to be used with the dense layer)\n    activation1 = ReLUActivation()\n\n    # Create second DenseLayer with 64 input features (as we take output of the\n    # previous layer here) and 3 output values\n    dense2 = DenseLayer(64, 3)\n\n    # Create Softmax classifier's combined loss and activation\n    loss_activation = CategoricalCrossEntropySoftmax()\n\n    # Create optimizer\n    optimizer = SGDOptimizer(learning_rate=1.0,decay=decay,momentum=momentum)\n\n    acc_vals = []\n    loss_vals = []\n    lr_vals = []\n\n    # Train in a loop\n    for epoch in range(10001):\n        # Perform a forward pass of our training data through this layer\n        dense1.forward(X)\n\n        # Perform a forward pass through the activation function\n        # takes the output of the first dense layer here\n        activation1.forward(dense1.output)\n\n        # Perform a forward pass through second DenseLayer\n        # takes the outputs of the activation function of first layer as inputs\n        dense2.forward(activation1.output)\n\n        # Perform a forward pass through the activation/loss function\n        # takes the output of the second DenseLayer here and returns the loss\n        loss = loss_activation.forward(dense2.output, y)\n\n        # Calculate accuracy from output of activation2 and targets\n        # Calculate values along the first axis\n        predictions = np.argmax(loss_activation.output, axis=1)\n        if len(y.shape) == 2:\n            y = np.argmax(y, axis=1)\n\n        accuracy = np.mean(predictions == y)\n\n        if epoch % 1000 == 0:\n            print(\n                f\"epoch: {epoch}, \\\n                acc : {accuracy:.3f}, \\\n                loss: {loss: .3f}, \\\n                lr : {optimizer.current_learning_rate}\"\n            )\n\n        acc_vals.append(accuracy)\n        loss_vals.append(loss)\n        lr_vals.append(optimizer.current_learning_rate)\n\n        # Backward pass\n        loss_activation.backward(loss_activation.output, y)\n        dense2.backward(loss_activation.dloss_dz)\n        activation1.backward(dense2.dloss_dinputs)\n        dense1.backward(activation1.dloss_dz)\n\n        # Update the weights and the biases\n        optimizer.pre_update_params()\n        optimizer.update_params(dense1)\n        optimizer.update_params(dense2)\n        optimizer.post_update_params()\n\n    return acc_vals, loss_vals, lr_vals\n\n\nacc_vals, loss_vals, lr_vals = train(decay=1e-3, momentum=0.5)\n\nepoch: 0,                 acc : 0.337,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.510,                 loss:  0.978,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.557,                 loss:  0.879,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.580,                 loss:  0.771,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.630,                 loss:  0.735,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.657,                 loss:  0.670,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.753,                 loss:  0.573,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.783,                 loss:  0.522,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.790,                 loss:  0.481,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.807,                 loss:  0.441,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.843,                 loss:  0.401,                 lr : 0.09091735612328393\n\n\nThe model achieved the lowest loss and the highest accuracy that we’ve seen so far. Can we do better? Sure, we can! Let’s try to set the momentum to \\(0.9\\):\n\nacc_vals, loss_vals, lr_vals = train(decay=1e-3, momentum=0.9)\n\nepoch: 0,                 acc : 0.340,                 loss:  1.099,                 lr : 1.0\nepoch: 1000,                 acc : 0.763,                 loss:  0.463,                 lr : 0.5002501250625312\nepoch: 2000,                 acc : 0.790,                 loss:  0.407,                 lr : 0.33344448149383127\nepoch: 3000,                 acc : 0.803,                 loss:  0.396,                 lr : 0.25006251562890724\nepoch: 4000,                 acc : 0.813,                 loss:  0.391,                 lr : 0.2000400080016003\nepoch: 5000,                 acc : 0.813,                 loss:  0.386,                 lr : 0.16669444907484582\nepoch: 6000,                 acc : 0.813,                 loss:  0.384,                 lr : 0.1428775539362766\nepoch: 7000,                 acc : 0.813,                 loss:  0.375,                 lr : 0.12501562695336915\nepoch: 8000,                 acc : 0.833,                 loss:  0.332,                 lr : 0.11112345816201799\nepoch: 9000,                 acc : 0.880,                 loss:  0.285,                 lr : 0.1000100010001\nepoch: 10000,                 acc : 0.880,                 loss:  0.277,                 lr : 0.09091735612328393"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#adagrad",
    "href": "posts/optimization_algorithms/index.html#adagrad",
    "title": "Optimization Algorithms",
    "section": "AdaGrad",
    "text": "AdaGrad\nIn real-world datasets, some input features are sparse and some features are dense. If we use the same learning rate \\(\\alpha\\) for all the weights, parameters associated with sparse features receive meaningful updates only when these features occur. Given a decreasing learning rate, we might end up with a situation where parameters for dense features converge rather quickly to their optimal values, whereas for sparse features, we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate decreases too slowly for dense features and too quickly for sparse features.\nThe update rule for adaptive step-size gradient descent is:\n\\[\\begin{align*}\n\\mathbf{g}_t &= \\frac{\\partial \\mathcal L}{\\partial \\mathbf{w}}\\\\\n\\mathbf{s}_t &= \\mathbf{s}_{t-1} + \\mathbf{g}_{t}^2 \\\\\n\\mathbf{w}_t &= \\mathbf{w}_{t-1} + \\frac{\\alpha}{\\sqrt{\\mathbf{s}_t+\\epsilon}}\\cdot \\mathbf{g}_t\n\\end{align*}\\]\nHere the operations are applied coordinate-wise. So, the jacobian \\(\\mathbf{g}_t^2\\) has entries \\(g_t^2\\). As before, \\(\\alpha\\) is the learning rate and \\(\\epsilon\\) is an additive constant that ensures that we do not divide by \\(0\\). Thus, the learning rate for features whose weights receive frequent updates is decreased faster, whilst for those features, whose weights receive infrequent updates, it is decreased slower.\nThus, Adagrad decreases the learning-rate dynamically on a per-coordinate basis.\n\nclass AdagradOptimizer:\n\n    # Initial optimizer - set settings\n    # learning rate of 1. is default for this optimizer\n    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7):\n        self.learning_rate = learning_rate\n        self.current_learning_rate = learning_rate\n        self.decay = decay\n        self.iterations = 0\n        self.epsilon = epsilon\n\n    # Call once before any parameter updates\n    def pre_update_params(self):\n        if self.decay:\n            self.current_learning_rate = self.learning_rate * (\n                1.0 / (1.0 + self.decay * self.iterations)\n            )\n\n    # Update parameters\n    def update_params(self, layer):\n        if not hasattr(layer, \"weight_cache\"):\n            layer.weight_cache = np.zeros_like(layer.weights)\n            layer.bias_cache = np.zeros_like(layer.biases)\n\n        # Update cache with squared current gradients\n        layer.weight_cache += layer.dloss_dweights**2\n        layer.bias_cache += layer.dloss_dbiases**2\n\n        # Vanilla SGD parameter update + normalization\n        # with square rooted cache\n        layer.weights += (\n            self.current_learning_rate\n            * layer.dloss_dweights\n            / (np.sqrt(layer.weight_cache) + self.epsilon)\n        )\n        layer.biases += (\n            self.current_learning_rate\n            * layer.dloss_dbiases\n            / (np.sqrt(layer.bias_cache) + self.epsilon)\n        )\n\n    def post_update_params(self):\n        self.iterations += 1"
  },
  {
    "objectID": "posts/optimization_algorithms/index.html#rmsprop",
    "href": "posts/optimization_algorithms/index.html#rmsprop",
    "title": "Optimization Algorithms",
    "section": "RMSProp",
    "text": "RMSProp\nOne of the key issues of Adagrad is that the learning rate decreases at a predefined schedule essentially at a rate proportional \\(\\frac{1}{\\sqrt{t}}\\). While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.\nTieleman and Hinton(2012) have proposed the RMSProp algorithm as a simple fix to decouple the rate scheduling from coordinate adaptive learning rates. The issue is that the squares of the gradient \\(\\mathbf{g}_t\\) keeps accumulating into the state vector \\(\\mathbf{s}_t = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2\\). As a result, \\(\\mathbf{s}_t\\) keeps on growing without bounds, essentially linearly as the algorithm converges.\n\nThe Algorithm\nThe update rule for the RMSProp algorithm is as follows:\n\\[\\begin{align*}\n\\mathbf{s}_t &= \\gamma \\mathbf{s}_{t-1} + (1- \\gamma)\\mathbf{g}_t^2\\\\\n\\mathbf{x}_t &= \\mathbf{x}_{t-1} - \\frac{\\alpha}{\\sqrt{\\mathbf{s}_t + \\epsilon}}\\odot \\mathbf{g}_t\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/norms/index.html",
    "href": "posts/norms/index.html",
    "title": "Norms",
    "section": "",
    "text": "Consider geometric vectors \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbf{R}^2\\). The scalar product(dot-product) of these two vectors is defined by:\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = x_1 y_1 + x_2 y_2\n\\]\nAn inner-product is a mathematical generalization of the dot-product.\n\n\\[\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\normp}[2]{\\left\\lVert\\mathbf{#1}\\right\\rVert_{#2}}\n\\newcommand\\inner[2]{\\left\\langle #1, #2 \\right\\rangle}\n\\newcommand{\\bf}[1]{\\mathbf{#1}}\n\\newcommand{\\R}{\\mathbf{R}}\n\\newcommand{\\RR}[1]{\\mathbf{R}^2}\n\\newcommand{\\RRR}[1]{\\mathbf{R}^3}\n\\newcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\CC}[1]{\\mathbf{C}^2}\n\\newcommand{\\CCC}[1]{\\mathbf{C}^3}\n\\]\n\n\nDefinition 1 (Inner product) Let \\(V\\) be a vector space and \\(F\\) be a scalar field, which is either \\(\\bf{R}\\) or \\(\\bf{C}\\). Let \\(\\inner{\\cdot}{\\cdot}\\) be a map from \\(V\\times V \\to F\\). Then, \\(\\inner{\\cdot}{\\cdot}\\) is an inner product if for all \\(\\bf{u},\\bf{v}, \\bf{w} \\in V\\), it satisfies:\n\n\n\\[\n\\inner{\\bf{v}}{\\bf{v}} \\geq 0 \\quad \\text { and } \\quad  \\inner{\\bf{v}}{\\bf{v}} = 0 \\Longleftrightarrow \\bf{v} = \\bf{0}\n\\]\n\n\n\n\\[\n\\inner{\\bf{u} + \\bf{v}}{\\bf{w}} = \\inner{\\bf{u}}{\\bf{w}} + \\inner{\\bf{v}}{\\bf{w}}\n\\]\n\n\n\n\\[\n\\begin{align*}\n\\inner{\\alpha \\bf{v}}{\\bf{w}} &= \\overline{\\alpha} \\inner{\\bf{v}}{\\bf{w}}\\\\\n\\inner{\\bf{v}}{\\alpha \\bf{w}} &= \\alpha \\inner{\\bf{v}}{\\bf{w}}\n\\end{align*}\n\\]\n\n\n\n\\[\n\\inner{\\bf{v}}{\\bf{w}} = \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\]\n\n\nThe most important example of inner-product is the Euclidean inner product on \\(\\C^n\\). Let \\(\\bf{w},\\bf{z}\\) be (column) vectors in \\(\\C^n\\).\n\\[\n\\inner{\\bf{w}}{\\bf{z}} = (\\bf{w}^H \\bf{z}) =  \\overline{w_1}z_1 + \\overline{w_2}z_2 + \\ldots + \\overline{w_n} z_n\n\\]\nFirstly,\n\\[\n\\begin{align*}\n\\inner{\\bf{v} + \\bf{w}}{\\bf{z}} &= (\\bf{v} + \\bf{w})^H \\bf{z} & \\{ \\text{ Definition }\\}\\\\\n&= (\\bf{v}^H + \\bf{w}^H)\\bf{z} & \\{ \\overline{z_1 + z_2} = \\overline{z_1} + \\overline{z_2}; z_1,z_2\\in \\C \\}\\\\\n&= \\bf{v}^H \\bf{z} + \\bf{w}^H \\bf{z}\\\\\n&= \\inner{\\bf{v}}{\\bf{z}} + \\inner{\\bf{w}}{\\bf{z}}\n\\end{align*}\n\\]\nSo, it is additive in the first slot.\nNext, let \\(\\alpha \\in \\C\\).\n\\[\n\\begin{align*}\n\\inner{\\alpha\\bf{u}}{\\bf{v}} &= (\\alpha \\bf{u})^H \\bf{v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\overline{\\alpha} \\bf{u}^H \\bf{v} = \\overline{\\alpha} \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\n\\inner{\\bf{u}}{\\alpha\\bf{v}} &= (\\bf{u})^H \\bf{ \\alpha v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\alpha \\bf{u}^H \\bf{v} = \\alpha \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n\\]\nIt is homogenous.\nFinally,\n\\[\n\\begin{align*}\n\\inner{\\bf{v}}{\\bf{w}} &= \\sum_{i=1}^n \\overline{v_i}w_i\\\\\n&= \\sum_{i=1}^n \\overline{v_i \\overline{w_i}}\\\\\n&= \\overline{\\left(\\sum_{i=1}^n \\overline{w_i} v_i\\right)}\\\\\n&= \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#inner-product",
    "href": "posts/norms/index.html#inner-product",
    "title": "Norms",
    "section": "",
    "text": "Consider geometric vectors \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbf{R}^2\\). The scalar product(dot-product) of these two vectors is defined by:\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = x_1 y_1 + x_2 y_2\n\\]\nAn inner-product is a mathematical generalization of the dot-product.\n\n\\[\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\normp}[2]{\\left\\lVert\\mathbf{#1}\\right\\rVert_{#2}}\n\\newcommand\\inner[2]{\\left\\langle #1, #2 \\right\\rangle}\n\\newcommand{\\bf}[1]{\\mathbf{#1}}\n\\newcommand{\\R}{\\mathbf{R}}\n\\newcommand{\\RR}[1]{\\mathbf{R}^2}\n\\newcommand{\\RRR}[1]{\\mathbf{R}^3}\n\\newcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\CC}[1]{\\mathbf{C}^2}\n\\newcommand{\\CCC}[1]{\\mathbf{C}^3}\n\\]\n\n\nDefinition 1 (Inner product) Let \\(V\\) be a vector space and \\(F\\) be a scalar field, which is either \\(\\bf{R}\\) or \\(\\bf{C}\\). Let \\(\\inner{\\cdot}{\\cdot}\\) be a map from \\(V\\times V \\to F\\). Then, \\(\\inner{\\cdot}{\\cdot}\\) is an inner product if for all \\(\\bf{u},\\bf{v}, \\bf{w} \\in V\\), it satisfies:\n\n\n\\[\n\\inner{\\bf{v}}{\\bf{v}} \\geq 0 \\quad \\text { and } \\quad  \\inner{\\bf{v}}{\\bf{v}} = 0 \\Longleftrightarrow \\bf{v} = \\bf{0}\n\\]\n\n\n\n\\[\n\\inner{\\bf{u} + \\bf{v}}{\\bf{w}} = \\inner{\\bf{u}}{\\bf{w}} + \\inner{\\bf{v}}{\\bf{w}}\n\\]\n\n\n\n\\[\n\\begin{align*}\n\\inner{\\alpha \\bf{v}}{\\bf{w}} &= \\overline{\\alpha} \\inner{\\bf{v}}{\\bf{w}}\\\\\n\\inner{\\bf{v}}{\\alpha \\bf{w}} &= \\alpha \\inner{\\bf{v}}{\\bf{w}}\n\\end{align*}\n\\]\n\n\n\n\\[\n\\inner{\\bf{v}}{\\bf{w}} = \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\]\n\n\nThe most important example of inner-product is the Euclidean inner product on \\(\\C^n\\). Let \\(\\bf{w},\\bf{z}\\) be (column) vectors in \\(\\C^n\\).\n\\[\n\\inner{\\bf{w}}{\\bf{z}} = (\\bf{w}^H \\bf{z}) =  \\overline{w_1}z_1 + \\overline{w_2}z_2 + \\ldots + \\overline{w_n} z_n\n\\]\nFirstly,\n\\[\n\\begin{align*}\n\\inner{\\bf{v} + \\bf{w}}{\\bf{z}} &= (\\bf{v} + \\bf{w})^H \\bf{z} & \\{ \\text{ Definition }\\}\\\\\n&= (\\bf{v}^H + \\bf{w}^H)\\bf{z} & \\{ \\overline{z_1 + z_2} = \\overline{z_1} + \\overline{z_2}; z_1,z_2\\in \\C \\}\\\\\n&= \\bf{v}^H \\bf{z} + \\bf{w}^H \\bf{z}\\\\\n&= \\inner{\\bf{v}}{\\bf{z}} + \\inner{\\bf{w}}{\\bf{z}}\n\\end{align*}\n\\]\nSo, it is additive in the first slot.\nNext, let \\(\\alpha \\in \\C\\).\n\\[\n\\begin{align*}\n\\inner{\\alpha\\bf{u}}{\\bf{v}} &= (\\alpha \\bf{u})^H \\bf{v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\overline{\\alpha} \\bf{u}^H \\bf{v} = \\overline{\\alpha} \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\n\\inner{\\bf{u}}{\\alpha\\bf{v}} &= (\\bf{u})^H \\bf{ \\alpha v}  & \\{ \\text{ Definition }\\}\\\\\n&= \\alpha \\bf{u}^H \\bf{v} = \\alpha \\inner{\\bf{u}}{\\bf{v}}\n\\end{align*}\n\\]\nIt is homogenous.\nFinally,\n\\[\n\\begin{align*}\n\\inner{\\bf{v}}{\\bf{w}} &= \\sum_{i=1}^n \\overline{v_i}w_i\\\\\n&= \\sum_{i=1}^n \\overline{v_i \\overline{w_i}}\\\\\n&= \\overline{\\left(\\sum_{i=1}^n \\overline{w_i} v_i\\right)}\\\\\n&= \\overline{\\inner{\\bf{w}}{\\bf{v}}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#norms",
    "href": "posts/norms/index.html#norms",
    "title": "Norms",
    "section": "Norms",
    "text": "Norms\nVery often, to quantify errors or measure distances one needs to compute the magnitude(length) of a vector or a matrix. Norms are a mathematical generalization(abstraction) for length.\n\nDefinition 2 (Vector norm) Let \\(\\nu:V \\to \\mathbf{R}\\). Then, \\(\\nu\\) is a (vector) norm if for all \\(\\mathbf{x},\\mathbf{y}\\in V\\) and for all \\(\\alpha \\in \\mathbf{C}\\), \\(\\nu(\\cdot)\\) satisfies:\n\nPositive Semi-Definiteness\n\\[\\nu(\\mathbf{x}) \\geq 0, \\quad \\forall \\bf{x}\\in V\\]\nand\n\\[\\nu(\\mathbf{x})=0 \\Longleftrightarrow \\mathbf{x}=\\mathbf{0}\\]\n\n\nHomogeneity\n\\[\\nu(\\alpha \\mathbf{x}) = |\\alpha|\\nu(\\mathbf{x})\\]\n\n\nTriangle inequality\n\\[\\nu(\\mathbf{x} + \\mathbf{y}) \\leq \\nu(\\mathbf{x}) + \\nu(\\mathbf{y})\\]\n\n\n\nThe vector \\(2-\\)norm\nThe length of a vector is most commonly measured by the square root of the sum of the squares of the components of the vector, also known as the euclidean norm.\n\nDefinition 3 (Vector \\(2-\\)norm) The vector \\(2-\\) norm, \\(||\\cdot||:\\mathbf{C}^n \\to \\mathbf{R}\\) is defined for \\(\\mathbf{x}\\in\\mathbf{C}^n\\) by:\n\\[\n\\norm{\\bf{x}}_2 = \\sqrt{|\\chi_1|^2 + |\\chi_2|^2 + |\\chi_n|^2} = \\sqrt{\\sum_{i=1}^n |\\chi_i^2|}\n\\]\nEquivalently, it can be defined as:\n\\[\n\\norm{\\bf{x}}_2 = \\sqrt{\\inner{\\bf{x}}{\\bf{x}}} =  (\\bf{x}^H \\bf{x})^{1/2} = \\sqrt{\\overline{\\chi_1}\\chi_1 +\\overline{\\chi_2}\\chi_2+\\ldots+\\overline{\\chi_n}\\chi_n}\n\\]\n\nTo prove that the vector \\(2-\\)norm is indeed a valid norm(just calling it a norm, doesn’t mean it is, after all), we need a result known as the Cauchy-Schwarz inequality. This inequality relates the magnitude of the dot-product(inner-product) of two vectors to the product of their two norms : if \\(\\bf{x},\\bf{y} \\in \\R^n\\), then \\(|\\bf{x}^T \\bf{y}|\\leq \\norm{\\bf{x}}_2\\cdot\\norm{\\bf{y}}_2\\).\nBefore we rigorously prove this result, let’s review the idea of orthogonality.\n\nDefinition 4 (Orthogonal vectors) Two vectors \\(\\bf{u},\\bf{v} \\in V\\) are said to be orthogonal to each other if and only if their inner product equals zero:\n\\[\n\\inner{\\bf{u}}{\\bf{v}} = 0\n\\]\n\n\nTheorem 1 (Pythagorean Theorem) If \\(\\bf{u}\\) and \\(\\bf{v}\\) are orthogonal vectors, then\n\\[\n\\inner{\\bf{u} + \\bf{v}}{\\bf{u} + \\bf{v}} = \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}}\n\\]\n\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\inner{\\bf{u} + \\bf{v}}{\\bf{u}+\\bf{v}} &= \\inner{\\bf{u}}{\\bf{u} + \\bf{v}} + \\inner{\\bf{v}}{\\bf{u} + \\bf{v}} & \\{ \\text{ Additivity in the first slot }\\}\\\\\n&= \\overline{\\inner{\\bf{u} + \\bf{v}}{\\bf{u}}} + \\overline{\\inner{\\bf{u} + \\bf{v}}{\\bf{v}}} & \\{ \\text{ Conjugate symmetry }\\}\\\\\n&= \\overline{\\inner{\\bf{u}}{\\bf{u}}} + \\overline{\\inner{\\bf{v}}{\\bf{u}}} + \\overline{\\inner{\\bf{u}}{\\bf{v}}} + \\overline{\\inner{\\bf{v}}{\\bf{v}}} \\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{u}}{\\bf{v}} + \\inner{\\bf{v}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}} \\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + 0 + 0 + \\inner{\\bf{v}}{\\bf{v}} & \\{ \\bf{u} \\perp \\bf{v}\\}\\\\\n&= \\inner{\\bf{u}}{\\bf{u}} + \\inner{\\bf{v}}{\\bf{v}}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nIn the special case that \\(V=\\C^n\\) or \\(V=\\R^n\\), the pythagorean theorem reduces to:\n\\[\n\\norm{\\bf{u} + \\bf{v}}_2^2 = \\norm{\\bf{u}}_2^2 + \\norm{\\bf{v}}_2^2\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#cauchy-schwarz-inequality",
    "href": "posts/norms/index.html#cauchy-schwarz-inequality",
    "title": "Norms",
    "section": "Cauchy-Schwarz Inequality",
    "text": "Cauchy-Schwarz Inequality\nSuppose \\(\\bf{u},\\bf{v}\\in V\\). We would like to write \\(\\bf{u}\\) as a scalar multiple of \\(\\bf{v}\\) plus a vector \\(\\bf{w}\\) orthogonal to \\(\\bf{v}\\), as suggested in the picture below. Intuitively, we would like to write an orthogonal decomposition of \\(\\bf{u}\\).\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows,arrows.meta --implicit-standalone\n\\begin{tikzpicture}[scale=2.0]\n    \\draw [-{Stealth[length=5mm]}](0.0,0.0) -- (7,0);\n    \\draw [-{Stealth[length=5mm]}] (0.0,0.0) -- (7,4);\n    \\node []  at (3.5,2.25) {\\large $\\mathbf{u}$};\n    \\draw [dashed] (7,0) -- (7,4);\n    \\node [circle,fill,minimum size = 0.5mm] at (5,0) {};\n    \\node []  at (5,-0.40) {\\large $\\mathbf{v}$};\n    \\node []  at (7,-0.40) {\\large $\\alpha\\mathbf{v}$};\n    \\node []  at (7.4,2.0) {\\large $\\mathbf{w}$};\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nTo discover how to write \\(\\bf{u}\\) as a scalar multiple of \\(\\bf{v}\\) plus a vector orthogonal to \\(\\bf{v}\\), let \\(\\alpha\\) denote a scalar. Then,\n\\[\n\\bf{u} = \\alpha \\bf{v} + (\\bf{u} - \\alpha \\bf{v})\n\\]\nThus, we need to choose \\(\\alpha\\) so that \\(\\bf{v}\\) and \\(\\bf{w} = \\bf{u} - \\alpha{v}\\) are mutually orthogonal. Thus, we must set:\n\\[\n\\inner{\\bf{u} - \\alpha\\bf{v}}{\\bf{v}} = \\inner{\\bf{u}}{\\bf{v}} - \\alpha \\inner{\\bf{v}}{\\bf{v}} = 0\n\\]\nThe equation above shows that we choose \\(\\alpha\\) to be \\(\\inner{\\bf{u}}{\\bf{v}}/\\inner{\\bf{v}}{\\bf{v}}\\) (assume that \\(\\bf{v} \\neq \\bf{0}\\) to avoid division by 0). Making this choice of \\(\\alpha\\), we can write:\n\\[\n\\bf{u} = \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\bf{v} + \\left(\\bf{u} - \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\bf{v}\\right)\n\\tag{1}\\]\nThe equation above will be used in the proof the Cauchy-Schwarz inequality, one of the most important inequalities in mathematics\n\nTheorem 2 (Cauchy-Schwarz Inequality) Let \\(\\bf{x},\\bf{y}\\in V\\). Then\n\\[\n|\\inner{\\bf{u}}{\\bf{v}}|^2 \\leq \\inner{\\bf{u}}{\\bf{u}}\\inner{\\bf{v}}{\\bf{v}}\n\\tag{2}\\]\n\nProof.\nLet \\(\\bf{u},\\bf{v} \\in V\\). If \\(\\bf{v} = \\bf{0}\\), then both sides of Equation 2 equal \\(0\\) and the inequality holds. Thus, we assume that \\(\\bf{v}\\neq \\bf{0}\\). Consider the orthogonal decomposition:\n\\[\n\\bf{u} = \\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v} + \\bf{w}\n\\]\nwhere \\(\\bf{w}\\) is orthogonal to \\(\\bf{v}\\) (\\(\\bf{w}\\) is taken to be the second term on the right hand side of Equation 1). By the Pythagorean theorem:\n\\[\n\\begin{align*}\n\\inner{\\bf{u}}{\\bf{u}} &= \\inner{\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v}}{\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}} \\bf{v}}+\\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\overline{\\left(\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\right)}\\left(\\frac{\\inner{\\bf{u}}{\\bf{v}}}{\\inner{\\bf{v}}{\\bf{v}}}\\right)\\inner{\\bf{v}}{\\bf{v}} + \\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\frac{\\overline{\\inner{\\bf{u}}{\\bf{v}}}\\inner{\\bf{u}}{\\bf{v}}}{\\overline{\\inner{\\bf{v}}{\\bf{v}}}} + \\inner{\\bf{w}}{\\bf{w}}\\\\\n&= \\frac{|\\inner{\\bf{u}}{\\bf{v}}|^2}{\\inner{\\bf{v}}{\\bf{v}}} + \\inner{\\bf{w}}{\\bf{w}}\n\\end{align*}\n\\]\nSince \\(\\inner{\\bf{w}}{\\bf{w}} \\geq 0\\), it follows that:\n\\[\n\\inner{\\bf{u}}{\\bf{u}} \\geq \\frac{|\\inner{\\bf{u}}{\\bf{v}}|^2}{\\inner{\\bf{v}}{\\bf{v}}}\n\\]\nConsequently, we have:\n\\[\n|\\inner{\\bf{u}}{\\bf{v}}|^2 \\leq \\inner{\\bf{u}}{\\bf{u}}\\inner{\\bf{v}}{\\bf{v}}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nIn the special case, that \\(V=\\R^n\\) or \\(V=\\C^n\\), we have:\n\\[\n|\\inner{\\bf{u}}{\\bf{v}}| \\leq \\norm{\\bf{u}}_2 \\norm{\\bf{v}}_2\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#euclidean-norm",
    "href": "posts/norms/index.html#euclidean-norm",
    "title": "Norms",
    "section": "Euclidean Norm",
    "text": "Euclidean Norm\n\nProposition 1 (Well-definedness of the Euclidean norm) Let \\(\\norm{\\cdot}:\\mathbf{C}^n \\to \\mathbf{C}\\) be the euclidean norm. Our claim is, it is well-defined.\n\nProof.\nLet \\(\\bf{z} = (z_1,z_2,\\ldots,z_n) \\in \\C^n\\). Clearly, it is positive semi-definite.\n\\[\n\\begin{align*}\n\\norm{\\bf{z}}_2 = \\bf{z}^H \\bf{z} &= \\overline{z_1} z_1 +\\overline{z_2}z_2 + \\ldots + \\overline{z_n} z_n\\\\\n&= \\sum_{i=1}^n |z_i|^2 \\geq 0\n\\end{align*}\n\\]\nIt is also homogenous. Let \\(\\alpha \\in \\C\\).\n\\[\n\\begin{align*}\n\\norm{\\alpha \\bf{z}}_2 &= \\norm{(\\alpha z_1, \\alpha z_2,\\ldots,\\alpha z_n)}_2\\\\\n&=\\sqrt{\\sum_{i=1}^n |\\alpha z_i|^2}\\\\\n&=|\\alpha|\\sqrt{\\sum_{i=1}^n |z_i|^2} \\\\\n&= |\\alpha|\\norm{\\bf{z}}_2\n\\end{align*}\n\\]\nLet’s verify, if the triangle inequality is satisfied. Let \\(\\bf{x}, \\bf{y}\\in\\C^n\\) be arbitrary vectors.\n\\[\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}}_2^2 &= |(\\bf{x} + \\bf{y})^H(\\bf{x} + \\bf{y})|\\\\\n&= |(\\bf{x}^H + \\bf{y}^H)(\\bf{x} + \\bf{y})|\\\\\n&= |\\bf{x}^H \\bf{x} + \\bf{y}^H \\bf{y} + \\bf{y}^H \\bf{x} + \\bf{x}^H \\bf{y}|\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 + |\\inner{\\bf{y}}{\\bf{x}}| + |\\inner{\\bf{x}}{\\bf{y}}|\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 + \\norm{\\bf{y}}_2 \\norm{\\bf{x}}_2  + \\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2 & \\{ \\text{ Cauchy-Schwarz } \\}\\\\\n&\\leq \\norm{\\bf{x}}_2^2 + \\norm{\\bf{y}}_2^2 +  2\\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2\\\\\n&= (\\norm{\\bf{x}}_2 + \\norm{\\bf{y}}_2)^2\n\\end{align*}\n\\]\nConsequently, \\(\\norm{\\bf{x} + \\bf{y}}_2 \\leq \\norm{\\bf{x}}_2 + \\norm{\\bf{y}}_2\\)."
  },
  {
    "objectID": "posts/norms/index.html#the-vector-1-norm",
    "href": "posts/norms/index.html#the-vector-1-norm",
    "title": "Norms",
    "section": "The vector \\(1-\\)norm",
    "text": "The vector \\(1-\\)norm\n\nDefinition 5 (The vector \\(1-\\)norm) The vector \\(1\\)-norm, \\(\\norm{\\cdot}_1 : \\C^n \\to \\R\\) is defined for all \\(\\bf{x}\\in\\C^n\\) by:\n\\[\n\\norm{\\bf{x}}_1 = |\\chi_1| + |\\chi_2| + \\ldots + |\\chi_n| =\\sum_{i=1}^n |\\chi_i|\n\\]\n\n\nTheorem 3 The vector \\(1\\)-norm is well-defined.\n\nProof.\nPositive semi-definitess.\nThe absolute value of complex numbers is non-negative.\n\\[\n\\norm{\\bf{x}}_1 = |\\chi_1| + |\\chi_2| + \\ldots + |\\chi_n| \\geq |\\chi_i| \\geq 0\n\\]\nHomogeneity.\n\\[\n\\norm{\\alpha\\bf{x}}_1 = \\sum_{i=1}^{n}|\\alpha \\chi_i| = |\\alpha| \\sum_{i=1}^{n}|\\chi_i| = |\\alpha| \\norm{\\bf{x}}_1\n\\]\nTriangle Inequality.\n\\[\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}} &= \\norm{(\\chi_1 + \\psi_1, \\ldots,\\chi_n + \\psi_n)}_1\\\\\n&= \\sum_{i=1}^n |\\chi_i + \\psi_i|\\\\\n&\\leq \\sum_{i=1}^n |\\chi_i| + |\\psi_i| & \\{ \\text{ Triangle inequality for complex numbers }\\}\\\\\n&= \\sum_{i=1}^n |\\chi_i| + \\sum_{i=1}^{n} |\\psi_i| & \\{ \\text{ Commutativity }\\}\\\\\n&= \\norm{\\bf{x}}_1 + \\norm{\\bf{y}}_1\n\\end{align*}\n\\]\nHence, the three axioms are satisfied. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#jensens-inequality",
    "href": "posts/norms/index.html#jensens-inequality",
    "title": "Norms",
    "section": "Jensen’s inequality",
    "text": "Jensen’s inequality\n\nConvex functions and combinations\nA function \\(f\\) is said to be convex on over an interval \\(I\\), if for all \\(x_1,x_2 \\in I\\), and every \\(p \\in [0,1]\\), we have:\n\\[\nf(px_1 + (1-p)x_2) \\leq pf(x_1) + (1-p)f(x_2)\n\\]\nIn other words, all chords(secants) joining any two points on \\(f\\), lie above the graph of \\(f\\). Note that, if \\(0 \\leq p \\leq 1\\), then \\(\\min(x_1,x_2) \\leq px_1 + (1-p)x_2 \\leq \\max(x_1,x_2)\\). More generally, for non-negative real numbers \\(p_1, p_2, \\ldots, p_n\\) summing to one, that is, satisfying \\(\\sum_{i=1}^n p_i = 1\\), and for any points \\(x_1,\\ldots,x_n \\in I\\), the point \\(\\sum_{i=1}^n \\lambda_i x_i\\) is called a convex combination of \\(x_1,\\ldots,x_n\\). Since:\n\\[ \\min(x_1,\\ldots,x_n) \\leq \\sum_{i=1}^n p_i x_i \\leq \\max(x_1,\\ldots,x_n)\\]\nevery convex combination of any finite number of points in \\(I\\) is again a point of \\(I\\).\nIntuitively, \\(\\sum_{i=1}^{n}p_i x_i\\) simply represents the center of mass of the points \\(x_1,\\ldots,x_n\\) with weights \\(p_1,\\ldots,p_n\\).\n\n\nProving Jensen’s inequality\nJensen’s inequality named after the Danish engineer Johan Jensen (1859-1925) can be stated as follows:\n\nTheorem 4 Let \\(n \\in \\bf{Z}_+\\) be a positive integer and let \\(f:I \\to \\R\\) be a convex function over the interval \\(I \\subseteq \\R\\). For any (not necessarily distinct) points \\(x_1,\\ldots,x_n \\in I\\), and non-negative real numbers \\(p_1,\\ldots,p_n \\in \\R\\) summing to one,\n\\[\nf(\\sum_{i=1}^n p_i x_i) \\leq \\sum_{i=1}^n p_i f(x_i)\n\\]\n\nProof.\nWe proceed by induction. Since \\(f\\) is convex, by definition, \\(\\forall x_1,x_2 \\in I\\), and any \\(p_1,p_2\\in \\R\\), such that \\(p_1 + p_2 = 1\\), we have \\(f(p_1 x_1 + p_2 x_2) \\leq p_1 f(x_1) + p_2 f(x_2)\\). So, the claim is true for \\(n=2\\).\nInductive hypothesis. Assume that \\(\\forall x_1,\\ldots,x_{k} \\in I\\) and any \\(p_1,\\ldots,p_k \\in \\R\\), such that \\(\\sum_{i=1}^k p_i = 1\\), we have \\(f(\\sum_{i=1}^k p_i x_i) \\leq \\sum_{i=1}^k p_i f(x_i)\\).\nClaim. The Jensen’s inequality holds for \\(k+1\\) points in \\(I\\).\nProof.\nLet \\(x_1,\\ldots,x_k, x_{k+1}\\) be arbitrary points in \\(I\\) and consider any convex combination of these points \\(\\sum_{i=1}^{k+1}p_i x_i\\), \\(p_i \\in [0,1], i \\in \\{1,2,3,\\ldots,k+1\\}, \\sum_{i=1}^{k+1}p_i = 1\\).\nDefine:\n\\[\nz := \\frac{p_1 x_1 + p_2 x_2 + \\ldots + p_k x_k}{\\sum_{i=1}^k p_i}\n\\]\nSince, \\(z\\) is a convex combination of \\(\\{x_1,\\ldots,x_k\\}\\), \\(z \\in I\\). Moreover, by the inductive hypothesis, since \\(f\\) is convex,\n\\[\n\\begin{align*}\nf(z) &= f\\left(\\frac{p_1 x_1 + p_2 x_2 + \\ldots + p_k x_k}{\\sum_{i=1}^k p_i}\\right)\\\\\n&\\leq \\frac{p_1}{\\sum_{i=1}^k p_i}f(x_1) + \\frac{p_2}{\\sum_{i=1}^k p_i}f(x_2) + \\ldots + \\frac{p_k}{\\sum_{i=1}^k p_i}f(x_k) \\\\\n&= \\frac{p_1}{1-p_{k+1}}f(x_1) + \\frac{p_2}{1-p_{k+1}}f(x_2) + \\ldots + \\frac{p_k}{1-p_{k+1}}f(x_k) \\\\\n\\end{align*}\n\\]\nSince \\(0 \\leq 1 - p_{k+1} \\leq 1\\), we deduce that:\n\\[\n(1 - p_{k+1})f(z) \\leq p_1 f(x_1) + \\ldots + p_k f(x_k)\n\\]\nWe have: \\[\n\\begin{align*}\nf(p_1 x_1 + \\ldots + p_k x_k + p_{k+1} x_{k+1}) &= f((1-p_{k+1})z + p_{k+1}x_{k+1})\\\\\n&\\leq (1-p_{k+1})f(z) + p_{k+1}f(x_{k+1}) & \\{ \\text{ Jensen's inequality for }n=2\\}\\\\\n&\\leq p_1 f(x_1) + \\ldots + p_k f(x_k) + p_{k+1}f(x_{k+1}) & \\{ \\text{ Deduction from the inductive hypothesis }\\}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#youngs-inequality",
    "href": "posts/norms/index.html#youngs-inequality",
    "title": "Norms",
    "section": "Young’s Inequality",
    "text": "Young’s Inequality\nYoung’s inequality is named after the English mathematician William Henry Young and can be stated as follows:\n\nTheorem 5 (Young’s inequality) For any non-negative real numbers \\(a\\) and \\(b\\) and any positive real numbers \\(p,q\\) satisfying \\(\\frac{1}{p} + \\frac{1}{q}=1\\), we have:\n\\[\nab \\leq \\frac{a^p}{p} + \\frac{b^q}{q}\n\\]\n\nProof.\nLet \\(f(x) = \\log x\\). Since \\(f\\) is concave, we can reverse the Jensen’s inequality. Consequently:\n\\[\n\\begin{align*}\n\\log(\\frac{a^p}{p} + \\frac{b^q}{q}) &\\geq \\frac{1}{p}\\log a^p + \\frac{1}{q}\\log b^q\\\\\n&= \\frac{1}{p}\\cdot p \\log a + \\frac{1}{q}\\cdot q \\log b\\\\\n&= \\log (ab)\n\\end{align*}\n\\]\nSince \\(\\log x\\) is monotonic increasing,\n\\[\n\\frac{a^p}{p} + \\frac{b^q}{q} \\geq ab\n\\]\nThis closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#holders-inequality",
    "href": "posts/norms/index.html#holders-inequality",
    "title": "Norms",
    "section": "Holder’s inequality",
    "text": "Holder’s inequality\nWe can use Young’s inequality to prove the Holder’s inequality, named after the German mathematician Otto Ludwig Holder (1859-1937).\n\nTheorem 6 (Holder’s inequality) For any pair of vectors \\(\\bf{x},\\bf{y}\\in \\C^n\\), and for any positive real numbers satisfying \\(p\\) and \\(q\\), we have \\(\\frac{1}{p} + \\frac{1}{q} = 1\\) we have:\n\\[\n\\sum_{i=1}^{n}|x_i y_i| \\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}\n\\]\n\nProof.\nApply Young’s inequality to \\(a = \\frac{|x_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}}\\) and \\(b = \\frac{|y_i|}{\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}}\\). We get:\n\\[\n\\begin{align*}\n\\frac{|x_i||y_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}} &\\leq \\frac{1}{p} \\frac{|x_i|^p}{\\sum_{i=1}^n |x_i|^p} + \\frac{1}{q}\\frac{|y_i|^q}{\\sum_{i=1}^n |y_i|^q}\n\\end{align*}\n\\]\nSumming on both sides, we get:\n\\[\n\\begin{align*}\n\\frac{\\sum_{i=1}^n|x_i y_i|}{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}} &\\leq \\frac{1}{p} \\frac{\\sum_{i=1}^n |x_i|^p}{\\sum_{i=1}^n |x_i|^p} + \\frac{1}{q}\\frac{\\sum_{i=1}^n|y_i|^q}{\\sum_{i=1}^n |y_i|^q}\\\\\n&= \\frac{1}{p} + \\frac{1}{q}\\\\\n&= 1\\\\\n\\sum_{i=1}^n |x_i y_i| &\\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}\\left(\\sum_{i=1}^n |y_i|^q\\right)^{1/q}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#the-vector-p-norm",
    "href": "posts/norms/index.html#the-vector-p-norm",
    "title": "Norms",
    "section": "The vector \\(p\\)-norm",
    "text": "The vector \\(p\\)-norm\nThe vector \\(1\\)-norm and \\(2\\)-norm are special cases of the \\(p\\)-norm.\n\nDefinition 6 (\\(p\\)-norm) Given \\(p \\geq 1\\), the vector \\(p\\)-norm \\(\\norm{\\cdot}_p : \\C^n \\to \\R\\) is defined by :\n\\[\n\\norm{\\bf{x}}_p = \\left(\\sum_{i=1}^n |\\chi_i|^p\\right)^{1/p}\n\\]\n\n\nTheorem 7 The vector \\(p\\)-norm is a well-defined norm.\n\nProof.\nPositive semi-definite\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\bf{x}}_p &= \\left(\\sum_{i=1}^n |\\chi_i|^p \\right)^{1/p}\\\\\n&\\geq \\left(|\\chi_i|^p \\right)^{1/p}\\\\\n&= |\\chi_i| \\geq 0\n\\end{align*}\n\\]\nHomogeneity\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\alpha \\bf{x}}_p &= \\left(\\sum_{i=1}^n |\\alpha \\chi_i|^p \\right)^{1/p}\\\\\n&= \\left(\\sum_{i=1}^n |\\alpha|^p |\\chi_i|^p \\right)^{1/p}\\\\\n&= |\\alpha|\\left(\\sum_{i=1}^n |\\chi_i|^p \\right)^{1/p} &= |\\alpha|\\norm{\\bf{x}}_p\n\\end{align*}\n\\]\nTriangle Inequality\nDefine \\(\\frac{1}{q} := 1 - \\frac{1}{p}\\). \\(\\Longrightarrow (p-1)q = p\\).\nBy the Holder’s inequality: \\[\n\\begin{align*}\n\\sum_{i=1}^n |x_i||x_i + y_i|^{p-1} &\\leq \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\\\\\n\\sum_{i=1}^n |y_i||x_i + y_i|^{p-1} &\\leq \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\n\\end{align*}\n\\]\nSumming, we get:\n\\[\n\\begin{align*}\n\\sum_{i=1}^n |x_i + y_i|^{p} &\\leq \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\} \\left(\\sum_{i=1}^n |x_i + y_i|^{(p-1)q}\\right)^{1/q}\\\\\n&= \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\}\\left(\\sum_{i=1}^n |x_i + y_i|^{p}\\right)^{1-\\frac{1}{p}}\\\\\n\\Longrightarrow \\left(\\sum_{i=1}^n |x_i + y_i|^{p}\\right)^{1/p} &\\leq \\left\\{\\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}+ \\left(\\sum_{i=1}^n |y_i|^p\\right)^{1/p}\\right\\}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/norms/index.html#the-vector-infty-norm",
    "href": "posts/norms/index.html#the-vector-infty-norm",
    "title": "Norms",
    "section": "The vector \\(\\infty\\)-norm",
    "text": "The vector \\(\\infty\\)-norm\n\nDefinition 7 (\\(\\infty\\)-norm) The vector \\(\\infty\\)-norm, \\(\\norm{\\cdot}:\\C^n \\to \\R\\) is defined for \\(\\bf{x} \\in \\C^n\\) by:\n\\[\n\\norm{\\bf{x}}_\\infty = \\max\\{|\\chi_1|,|\\chi_2|,\\ldots,|\\chi_n|\\}\n\\]\nThe \\(\\infty\\)-norm simply measures how long the vector is by the magnitude of its largest entry.\n\n\nTheorem 8 The vector \\(\\infty\\)-norm is well-defined.\n\nProof.\nPositive semi-definiteness\nWe have:\n\\[\n\\norm{\\bf{x}}_{\\infty} = \\max_{1\\leq i \\leq n} |\\chi_i| \\geq |\\xi_i| \\geq 0\n\\]\nHomogeneity\nWe have:\n\\[\n\\norm{\\alpha \\bf{x}}_{\\infty} = \\max_{1\\leq i \\leq n}|\\alpha \\chi_i| =\\max_{1\\leq i \\leq n}|\\alpha|| \\chi_i| = |\\alpha| \\max_{1\\leq i \\leq n}|\\chi_i| = |\\alpha|\\norm{\\bf{x}}_{\\infty}\n\\]\nTriangle Inequality\n\\[\n\\begin{align*}\n\\norm{\\bf{x} + \\bf{y}}_\\infty &= \\max_{i=1}^m |\\chi_i + \\xi_i|\\\\\n&\\leq \\max_{i=1}^m (|\\chi_i| + |\\xi_i|)\\\\\n&\\leq \\max_{i=1}^m |\\chi_i| + \\max_{i=1}^m |\\xi_i|\\\\\n&= \\norm{\\bf{x}}_\\infty + \\norm{\\bf{y}}_\\infty\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/norms/index.html#equivalence-of-vector-norms",
    "href": "posts/norms/index.html#equivalence-of-vector-norms",
    "title": "Norms",
    "section": "Equivalence of vector norms",
    "text": "Equivalence of vector norms\nAs I was saying earlier, we often measure if a vector is small or large or the distance between two vectors by computing norms. It would be unfortunate, if a vector were small in one norm, yet large in another. Fortunately, the next theorem excludes this possibility.\n\nTheorem 9 (Equivalence of vector norms) Let \\(\\norm{\\cdot}_a:\\C^n \\to \\R\\) and \\(\\norm{\\cdot}_b:\\C^n\\to \\R\\) both be vector norms. Then there exist positive scalars \\(C_1\\) and \\(C_2\\) such that for \\(\\bf{x}\\in \\C^n\\),\n\\[\nC_1 \\norm{\\bf{x}}_b \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_b\n\\]\n\nProof.\nWe can prove equivalence of norms in four steps, the last which uses the extreme value theorem from Real Analysis.\n\nStep 1: It is sufficient to consider \\(\\norm{\\cdot}_b = \\norm{\\cdot}_1\\) (transitivity).\nWe will show that it is sufficient to prove that \\(\\norm{\\cdot}_a\\) is equivalent to \\(\\norm{\\cdot}_1\\) because norm equivalence is transitive: if two norms are equivalent to \\(\\norm{\\cdot}_1\\), then they are equivalent to each other. In particular, suppose both \\(\\norm{\\cdot}_a\\) and \\(\\norm{\\cdot}_{a'}\\) are equivalent to \\(\\norm{\\cdot}_1\\) for constants \\(0 \\leq C_1 \\leq C_2\\) and \\(0 \\leq C_1' \\leq C_2'\\) respectively:\n\\[\nC_1 \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_1\n\\]\nand\n\\[\nC_1' \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_{a'} \\leq C_2' \\norm{\\bf{x}}_1\n\\]\nThen, it immediately follows that:\n\\[\n\\norm{\\bf{x}}_{a'} \\leq C_2' \\norm{\\bf{x}}_1 \\leq \\frac{C_2'}{C_1} \\norm{\\bf{x}}_a\n\\]\nand\n\\[\n\\norm{\\bf{x}}_{a'} \\geq C_1' \\norm{\\bf{x}}_1 \\geq \\frac{C_1'}{C_2} \\norm{\\bf{x}}_a\n\\]\nand hence \\(\\norm{\\cdot}_a\\) and \\(\\norm{\\cdot}_{a'}\\) are equivalent. \\(\\blacksquare\\)\n\n\nStep 2: It is sufficient to consider only \\(\\bf{x}\\) with \\(\\norm{\\bf{x}}_1 = 1\\).\nWe wish to show that\n\\[\nC_1 \\norm{\\bf{x}}_1 \\leq \\norm{\\bf{x}}_a \\leq C_2 \\norm{\\bf{x}}_1\n\\]\nis true for all \\(\\bf{x} \\in V\\) for some \\(C_1\\), \\(C_2\\). It is trivially true for \\(\\bf{x}=\\bf{0}\\), so we only need to consider \\(\\bf{x}\\neq\\bf{0}\\), in which case, we can divide by \\(\\norm{\\bf{x}}_1\\), to obtain the condition:\n\\[\nC_1 \\leq \\norm{\\frac{\\bf{x}}{\\norm{\\bf{x}}_1 }}_a \\leq C_2\n\\]\nThe vector \\(\\bf{u} = \\frac{\\bf{x}}{\\norm{\\bf{x}}_1}\\) is a unit vector in the \\(1\\)-norm, \\(\\norm{\\bf{u}}_1 = 1\\). So, we can write:\n\\[\nC_1 \\leq \\norm{\\bf{u}}_a \\leq C_2\n\\]\nWe have the desired result. \\(\\blacksquare\\)\n\n\nStep 3: Any norm \\(\\norm{\\cdot}_a\\) is continuous under \\(\\norm{\\cdot}_1\\).\nWe wish to show that any norm \\(\\norm{\\cdot}_a\\) is a continuous function on \\(V\\) under the topology induced by \\(\\norm{\\cdot}_1\\). That is, we wish to show that for any \\(\\epsilon &gt; 0\\), there exists \\(\\delta &gt; 0\\), such that for all \\(\\norm{\\bf{x} - \\bf{c}}_1 &lt; \\delta\\), we have \\(\\norm{\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a}_1 &lt; \\epsilon\\).\nWe prove this into two steps. First, by the triangle inequality on \\(\\norm{\\cdot}_a\\), it follows that:\n\\[\n\\begin{align*}\n\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a &= \\norm{\\bf{c} + (\\bf{x} - \\bf{c})}_a - \\norm{\\bf{c}}_a \\\\\n&\\leq \\norm{\\bf{c}}_a + \\norm{(\\bf{x} - \\bf{c})}_a - \\norm{\\bf{c}}_a\\\\\n&= \\norm{(\\bf{x} - \\bf{c})}_a\n\\end{align*}\n\\]\nAnd\n\\[\n\\begin{align*}\n\\norm{\\bf{c}}_a - \\norm{\\bf{x}}_a &\\leq \\norm{(\\bf{x} - \\bf{c})}_a\n\\end{align*}\n\\]\nand hence:\n\\[\n|\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a| \\leq \\norm{(\\bf{x} - \\bf{c})}_a\n\\]\nSecond applying the triangle inequality again, and writing \\(\\bf{x} = \\sum_{i=1}^n \\alpha_i \\bf{e}_i\\) and \\(\\bf{c} = \\sum_{i=1}^n \\alpha_i' \\bf{e}_i\\) in our basis, we obtain:\n\\[\n\\begin{align*}\n\\norm{\\bf{x}-\\bf{c}}_a &= \\norm{\\sum_{i=1}^n (\\alpha_i - \\alpha_i')\\bf{e}_i}_a\\\\\n&\\leq \\sum_{i=1}^n \\norm{(\\alpha_i - \\alpha_i')\\bf{e}_i}_a & \\{ \\text{ Triangle Inequality }\\}\\\\\n&= \\sum_{i=1}^n |(\\alpha_i - \\alpha_i')|\\norm{\\bf{e}_i}_a \\\\\n&= \\norm{\\bf{x} - \\bf{c}}_1 \\left(\\max_i \\norm{\\bf{e}_i}_a \\right)\n\\end{align*}\n\\]\nTherefore, if we choose:\n\\[\n\\delta = \\frac{\\epsilon}{\\left(\\max_i \\norm{\\bf{e}_i}_a \\right)}\n\\]\nit immediate follows that:\n\\[\\begin{align*}\n\\norm{\\bf{x} - \\bf{c}}_1 &&lt; \\delta \\\\\n\\Longrightarrow |\\norm{\\bf{x}}_a - \\norm{\\bf{c}}_a| &\\leq \\norm{\\bf{x} - \\bf{c}}_a \\\\ &\\leq \\norm{\\bf{x} - \\bf{c}}_1 \\left(\\max_i \\norm{\\bf{e}_i}_a \\right) \\\\\n& \\leq \\frac{\\epsilon}{\\left(\\max_i \\norm{\\bf{e}_i}_a \\right)} \\left(\\max_i \\norm{\\bf{e}_i}_a \\right) = \\epsilon\n\\end{align*}\n\\]\nThis proves (uniform) continuity. \\(\\blacksquare\\)\n\n\nStep 4: The maximum and minimum of \\(\\norm{\\cdot}_a\\) on the unit ball\nLet \\(K:=\\{\\bf{u}:\\norm{\\bf{u}}_1 = 1\\}\\). Then, \\(K\\) is a compact set. Since \\(\\norm{\\cdot}_a\\) is continuous on \\(K\\), by the extreme value theorem, \\(\\norm{\\cdot}_a\\) must achieve a supremum and infimum on the set. So, for all \\(\\bf{u}\\) with \\(\\norm{\\bf{u}}_1 = 1\\), there exists \\(C_1,C_2 &gt; 0\\), such that:\n\\[ C_1 \\leq \\norm{\\bf{u}}_a \\leq C_2\\]\nas required by step 2. And we are done! \\(\\blacksquare\\)\n\n\nDeriving the constants \\(C_{1,\\infty}\\), \\(C_{\\infty,1}\\)\nLet’s write a python implementation of the various norms.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\ndef one_norm(x):\n    return np.sum(np.abs(x))\n\ndef two_norm(x):\n    return np.sqrt(np.sum(x**2))\n\ndef p_norm(x,p):\n    return np.pow(np.sum(np.abs(x)**p),1.0/p)\n\ndef infty_norm(x):\n    return np.max(np.abs(x))\n\ndef get_vectors_eq_norm_val(func, val, lower_bound, upper_bound):\n    x_1 = np.linspace(lower_bound, upper_bound, \n    int((upper_bound - lower_bound)*100 + 1))\n    x_2 = np.linspace(lower_bound, upper_bound, \n    int((upper_bound - lower_bound)*100 + 1))\n\n    pts = np.array(list(itertools.product(x_1, x_2)))\n    norm_arr = np.array(list(map(func, pts)))\n\n    pts_norm_list = list(zip(pts,norm_arr))\n\n    pts_with_norm_eq_val = []\n    for pt in pts_norm_list:\n        if pt[1] == val:\n            pts_with_norm_eq_val.append(pt[0])\n\n    return np.array(pts_with_norm_eq_val)\n\nNow, we can glean useful information by visualizing the set of points(vectors) with a given norm.\n\n\nShow the code\npts1 = get_vectors_eq_norm_val(\n    func=infty_norm, val=1.0, lower_bound=-1.0, upper_bound=1.0\n)\n\npts2 = get_vectors_eq_norm_val(\n    func=one_norm, val=2.0, lower_bound=-2.0, upper_bound=2.0\n)\n\nplt.grid(True)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\na = plt.scatter(pts1[:, 0], pts1[:, 1], s=2)\nb = plt.scatter(pts2[:, 0], pts2[:, 1], s=2)\n# c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)\n\nplt.legend(\n    (a, b), (r\"$||\\mathbf{x}||_\\infty = 1$\", r\"$||\\mathbf{x}||_1=2$\"), loc=\"lower left\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe blue rectangle represents all vectors \\(\\bf{x}\\in\\R^2\\) with unit \\(\\infty\\)-norm, \\(\\norm{\\bf{x}}_\\infty = 1\\). The orange rhombus represents all vectors \\(\\bf{x}\\) with \\(\\norm{\\bf{x}}_1 = 2\\). All points on or outside the blue square represent vectors \\(\\bf{y}\\), such that \\(\\norm{\\bf{y}}_\\infty \\geq 1\\). Hence, if \\(\\norm{\\bf{y}}_1 = 2\\), \\(\\norm{\\bf{y}}_\\infty \\geq 1\\).\nNow, pick any \\(\\bf{z}\\neq \\bf{0}\\). Then, \\(2\\norm{\\frac{\\bf{z}}{\\norm{\\bf{z}}_1}}_1 =2\\). Thus, \\(\\norm{\\frac{2\\bf{z}}{\\norm{\\bf{z}}_1}}_\\infty \\geq 1\\). So, it follows that if \\(\\bf{z}\\in\\R^2\\) is any arbitrary vector, \\(\\norm{\\bf{z}}_1 \\leq 2 \\norm{\\bf{z}}_\\infty\\).\nIn general, if \\(\\bf{x}\\in\\C^n\\), then:\n\\[\n\\begin{align*}\n\\norm{\\bf{x}}_1 &= \\sum_{i=1}^n |x_i|\\\\\n&\\leq \\sum_{i=1}^n \\max\\{|x_i|:i=1,2,\\ldots,n\\}\\\\\n&= n \\norm{\\bf{x}}_\\infty\n\\end{align*}\n\\]\nNext, in the below plot, the orange rhombus represents vectors \\(\\bf{x}\\in\\R^2\\), such that \\(\\normp{x}{1} = 1\\) and all points on or outside the orange rhombus are such that \\(\\normp{y}{1} \\geq 1\\). The blue square represents vectors \\(\\normp{y}{\\infty} = 1\\). Consequently, if \\(\\normp{y}{1} = 1\\), then \\(\\normp{y}{\\infty} \\leq \\normp{y}{1}\\). In general, if \\(\\bf{x}\\in C^n\\), we have:\n\\[\n\\begin{align*}\n\\normp{x}{\\infty} &= \\max\\{|x_1|,\\ldots,|x_n|\\}\\\\\n&\\leq \\sum_{i=1}^n |x_i|=\\normp{x}{1}\n\\end{align*}\n\\]\nPutting together, we have:\n\\[\n\\begin{align*}\n\\normp{x}{\\infty} \\leq C_{\\infty,1} \\normp{x}{1} \\\\\n\\normp{x}{1} \\leq C_{1,\\infty} \\normp{x}{\\infty}\n\\end{align*}\n\\]\nwhere \\(C_{\\infty,1} = 1\\) and \\(C_{1,\\infty}=n\\).\n\n\nShow the code\npts1 = get_vectors_eq_norm_val(\n    func=infty_norm, val=1.0, lower_bound=-1.0, upper_bound=1.0\n)\n\npts2 = get_vectors_eq_norm_val(\n    func=one_norm, val=1.0, lower_bound=-2.0, upper_bound=2.0\n)\n\nplt.grid(True)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\na = plt.scatter(pts1[:, 0], pts1[:, 1], s=2)\nb = plt.scatter(pts2[:, 0], pts2[:, 1], s=2)\n# c = plt.scatter(pts_with_unit_infty_norm[:,0],pts_with_unit_infty_norm[:,1],s=2)\n\nplt.legend(\n    (a, b), (r\"$||\\mathbf{x}||_\\infty = 1$\", r\"$||\\mathbf{x}||_1=1$\"), loc=\"lower left\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nDeriving the constants \\(C_{1,2}\\), \\(C_{2,1}\\)\nWe can also derive the constants \\(C_{1,2}\\) and \\(C_{2,1}\\). We have:\nLet \\(\\bf{x}\\in\\C^n\\) be an arbitrary vector. And let \\(\\bf{y}=(1+0i,\\ldots,1+0i)\\). By the Cauchy-Schwarz inequality,\n\\[\n\\begin{align*}\n\\sum_{i=1}^n |x_i| \\leq \\left(\\sum_{i=1}^n |x_i|^2\\right)^{1/2}\\sqrt{n}\n\\end{align*}\n\\]\nSo, our claim is \\(\\normp{x}{1} \\leq \\sqrt{n}\\normp{x}{2}\\).\nAlso, consider the vector \\(\\bf{v}=\\left(\\frac{1}{\\sqrt{n}},\\ldots,\\frac{1}{\\sqrt{n}}\\right)\\). \\(\\norm{\\bf{v}}_1 = \\sqrt{n}\\norm{\\bf{v}}_2\\). So, the bound is tight.\nMoreover:\n\\[\n\\begin{align*}\n\\normp{x}{2}^2 &= \\sum_{i=1}^n |x_i|^2 \\\\\n&\\leq \\sum_{i=1}^n |x_i|^2 + \\sum_{i \\neq j}|x_i||x_j|\\\\\n&= \\sum_{i=1}^n |x_i|^2 + \\sum_{i &lt; j}2|x_i||x_j|\\\\\n&= \\left(\\sum_{i=1}^n |x_i|\\right)^2\n\\end{align*}\n\\]\nSo, \\(\\normp{x}{2} \\leq \\normp{x}{1}\\). Consider the standard basis vector \\(\\bf{e}_1 = (1,0,0,\\ldots,0)\\). \\(\\norm{\\bf{e}_1}_2 = \\norm{\\bf{e}_1}_1\\). Hence, the bound is tight. We conclude that:\n\\[\n\\begin{align*}\n\\normp{x}{1} \\leq C_{1,2} \\normp{x}{2}\\\\\n\\normp{x}{2} \\leq C_{2,1} \\normp{x}{1}\n\\end{align*}\n\\]\nwhere \\(C_{1,2} = \\sqrt{n}\\) and \\(C_{2,1} = 1\\).\n\n\nDeriving the constants \\(C_{2,\\infty}\\) and \\(C_{\\infty,2}\\)\nLet \\(x \\in \\C^n\\). We have:\n\\[\n\\begin{align}\n\\norm{x}_2^2 & = \\sum_{i=0}^{n-1}|\\chi_i|^2\\\\\n&\\leq\\sum_{i=0}^{n-1} (\\max_{i=0}^{n-1}|\\chi_i|)^2\\\\\n&= n \\norm{x}_\\infty\n\\end{align}\n\\]\nSo, \\(\\norm{x}_2 \\leq \\sqrt{n} \\norm{x}_\\infty\\).\nMoreover, let \\(x = (1, 1, \\ldots, 1)^T\\). Then, \\(\\norm{x}_2 = \\sqrt{n}\\) and \\(\\norm{x}_\\infty = 1\\), so \\(\\norm{x}_2 = \\sqrt{n}\\norm{x}_\\infty\\). Hence, it is a tight inequality.\nAlso, we have:\n\\[\n\\begin{align*}\n\\norm{x}_\\infty^2 &= \\max \\{|\\chi_0|^2,|\\chi_1|^2,\\ldots,|\\chi_{n-1}^2|\\}\\\\\n&\\leq \\max \\{\\sum_{i=0}^{n-1}|\\chi_i|^2,\\sum_{i=0}^{n-1}|\\chi_i|^2,\\ldots,\\sum_{i=0}^{n-1}|\\chi_i|^2|\\}\\\\\n&= \\norm{x}_2^2\n\\end{align*}\n\\]\nMoreover, let \\(x = (1, 0)\\). Then, \\(\\norm{x}_2 = 1\\) and \\(\\norm{x}_\\infty = 1\\). So, \\(\\norm{x}_\\infty = \\norm{x}_2\\). Hence, the inequality is tight."
  },
  {
    "objectID": "posts/norms/index.html#matrix-norms",
    "href": "posts/norms/index.html#matrix-norms",
    "title": "Norms",
    "section": "Matrix Norms",
    "text": "Matrix Norms\nThe analysis of matrix algorithms requires the use of matrix norms. For example, the quality of a linear system solution may be poor, if the matrix of coefficients is nearly singular. To quantify the notion of singularity, we need a measure of the distance on the space of matrices. Matrix norms can be used to provide that measure.\n\nDefinitions\nSince \\(\\R^{m \\times n}\\) is isomorphic \\(\\R^{mn}\\), the definition of a matrix norm is equivalent to the definition of a vector norm. In particular, \\(f:\\R^{m \\times n} \\to \\R\\) is a matrix norm, if the following three properties holds:\n\\[\n\\begin{align*}\nf(A) \\geq 0, & & A \\in \\R^{m \\times n}\\\\\nf(A + B) \\leq f(A) + f(B), & & A,B \\in \\R^{m \\times n}\\\\\nf(\\alpha A) = |\\alpha|f(A), & & \\alpha \\in \\R, A \\in \\R^{m \\times n}\n\\end{align*}\n\\]\nThe most frequently used matrix norms in numerical linear algebra are the Frobenius norm and the \\(p\\)-norms.\n\nDefinition 8 (Frobenius Norm) The Frobenius norm \\(\\norm{\\cdot}_F : \\C^{m \\times n} \\to \\R\\) is defined for \\(A \\in \\C^{m \\times n}\\) by:\n\\[\n\\norm{A}_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n\\]\n\n\nTheorem 10 The Frobenius norm is a well-defined norm.\n\nProof.\nPositive Semi-definite\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_F &= \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\\\\\n&\\geq \\left( |a_{ij}|^2\\right)^{1/2} = |a_{ij}|\\\\\n&\\geq 0\n\\end{align*}\n\\]\nTriangle Inequality\nWe have:\n\\[\n\\begin{align*}\n\\norm{A + B}_F^2 &= \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij} + b_{ij}|^2 \\\\\n&\\leq \\sum_{i=1}^m \\sum_{j=1}^n \\left(|a_{ij}|^2 + |b_{ij}|^2 + 2|a_{ij}||b_{ij}|\\right)\\\\\n&= \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2 + \\sum_{i=1}^m \\sum_{j=1}^n |b_{ij}|^2 + 2\\sum_{i=1}^m \\sum_{j=1}^n|a_{ij}||b_{ij}|\\\\\n&\\leq \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2 + \\sum_{i=1}^m \\sum_{j=1}^n |b_{ij}|^2 + 2\\left(\\sum_{i=1}^m \\sum_{j=1}^n|a_{ij}|^2\\right)^{1/2}\\left(\\sum_{i=1}^m \\sum_{j=1}^n|b_{ij}|^2\\right)^{1/2} & \\{\\text{ Cauchy-Schwarz }\\}\\\\\n&= \\norm{A}_F^2 + \\norm{B}_F^2 + 2\\norm{A}_F \\norm{B}_F\\\\\n&= (\\norm{A}_F + \\norm{B}_F)^2\\\\\\\\\n\\Longrightarrow \\norm{A + B}_F &\\leq \\norm{A}_F + \\norm{B}_F\n\\end{align*}\n\\]\nHomogeneity\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\alpha A}_F &= \\left(\\sum_{i=1}^m \\sum_{j=1}^n |\\alpha a_{ij}|^2\\right)^{1/2}\\\\\n&=\\left(\\sum_{i=1}^m \\sum_{j=1}^n |\\alpha|^2 |a_{ij}|^2\\right)^{1/2}\\\\\n&= |\\alpha| \\norm{A}_F\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nDefinition 9 (Induced matrix norm) Let \\(\\norm{\\cdot}_\\mu : \\C^m \\to \\R\\) and \\(\\norm{\\cdot}_\\nu : \\C^n \\to R\\) be vector norms. Define \\(\\norm{\\cdot}_{\\mu,\\nu} : \\C^{m \\times n} \\to R\\) by:\n\\[\n\\norm{A}_{\\mu,\\nu} = \\sup_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu}\n\\]\nMatrix norms that are defined in this way are called induced matrix norms.\n\nLet us start by interpreting this. How large \\(A\\) is, as measured by \\(\\norm{A}_{\\mu,\\nu}\\) is defined as the most that \\(A\\) magnifies the length of non-zero vectors, where the length of the \\(\\bf{x}\\) is measured with the norm \\(\\norm{\\cdot}_\\nu\\) and the length of the transformed vector \\(A\\bf{x}\\) is measured with the norm \\(\\norm{\\cdot}_\\mu\\).\nTwo comments are in order. First,\n\\[\n\\begin{align*}\n\\sup_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} = \\sup_{\\bf{x} \\neq \\bf{0}} \\norm{A\\frac{\\bf{x}}{\\norm{\\bf{x}}_\\nu}}_\\mu = \\sup_{\\norm{\\bf{u}}_\\nu = 1} \\norm{A\\bf{u}}_\\mu\n\\end{align*}\n\\]\nSecond, it is not immediately obvious, that there is a vector \\(\\bf{x}\\) for which a supremum is attained. The fact is there is always such a vector \\(\\bf{x}\\). The \\(K=\\{\\bf{u}:\\norm{\\bf{u}}_\\nu = 1\\}\\) is a compact set, and \\(\\norm{\\cdot}_\\mu : \\C^m \\to \\R\\) is a continuous function. Continuous functions preserve compact sets. So, the supremum exists and further it belongs to \\(\\{A\\bf{x}:\\norm{\\bf{x}}_\\nu = 1\\}\\).\n\nTheorem 11 The induced matrix norm \\(\\norm{\\cdot}_{\\mu,\\nu} : \\C^{m \\times n} \\to \\R\\) is a well-defined norm.\n\nProof\nTo prove this, we merely check if the three conditions are met:\nLet \\(A,B \\in \\C^{m \\times n}\\) and \\(\\alpha \\in \\C\\) be arbitrarily chosen. Then:\nPositive definite\nLet \\(A \\neq 0\\). That means, at least one of the columns of \\(A\\) is not a zero-vector. Partition \\(A\\) by columns:\n\\[\n\\left[\n    \\begin{array}{c|c|c|c}\n        a_{1} & a_2 & \\ldots & a_{n}\n    \\end{array}\n\\right]\n\\]\nLet us assume that, it is the \\(j\\)-th column \\(a_j\\), that is non-zero. Let \\(\\bf{e}_j\\) be the column of \\(I\\)(the identity matrix) indexed with \\(j\\). Then:\n\\[\n\\begin{align*}\n\\norm{A}_{\\mu,\\nu} &= \\sup \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n&\\geq \\frac{\\norm{A\\bf{e}_j}_\\mu}{\\norm{\\bf{e}_j}_\\nu}\\\\\n&= \\frac{\\norm{a_j}_\\mu}{\\norm{\\bf{e}_j}_\\nu} & \\{ A\\bf{e}_j = a_j \\}\\\\\n&&gt; 0 & \\{ \\text{ we assumed } a_j \\neq \\bf{0}\\}\n\\end{align*}\n\\]\nHomogeneity\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\alpha A}_{\\mu,\\nu} &= \\sup_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{\\alpha A \\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n&= \\sup_{\\bf{x}\\neq \\bf{0}} \\frac{|\\alpha|\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Homogeneity of vector norm }\\norm{\\cdot}_\\mu\\}\\\\\n&= |\\alpha|\\sup_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Algebra }\\}\\\\\n&= |\\alpha|\\norm{A}_{\\mu,\\nu}\n\\end{align*}\n\\]\nTriangle Inequality\nWe have:\n\\[\n\\begin{align*}\n\\norm{A + B}_{\\mu,\\nu} &= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A + B) \\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Definition }\\}\\\\\n&= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x} + B\\bf{x})}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Distribute }\\}\\\\\n&\\leq \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu + \\norm{B\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} & \\{ \\text{ Triangle inequality for vector norms }\\}\\\\\n&= \\max_{\\bf{x}\\neq \\bf{0}} \\left(\\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} + \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} \\right) & \\{ \\text{ Algebra }\\}\\\\\n&= \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} + \\max_{\\bf{x}\\neq \\bf{0}} \\frac{\\norm{(A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\nu} \\\\\n&= \\norm{A}_{\\mu,\\nu} + \\norm{B}_{\\mu,\\nu} & \\{ \\text{ Definition }\\}\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nWhen \\(\\norm{\\cdot}_\\mu\\) and \\(\\norm{\\cdot}_\\nu\\) are the same norm, the induced norm becomes:\n\\[\n\\norm{A}_\\mu = \\max_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_\\mu}{\\norm{\\bf{x}}_\\mu}\n\\]\nor equivalently:\n\\[\n\\norm{A}_\\mu = \\max_{\\norm{\\bf{u}}_\\mu = 1} \\norm{A\\bf{u}}_\\mu\n\\]\n\nExample 1 Consider the vector \\(p\\)-norm \\(\\norm{\\cdot}_p:\\C^n \\to \\R\\) and let us denote the induced matrix norm \\(|||\\cdot|||:\\C^{m \\times n} \\to \\R\\) by \\(|||A||| = \\max_{\\bf{x}\\neq\\bf{0}}\\frac{\\norm{A\\bf{x}}_p}{\\norm{\\bf{x}}_p}\\). Prove that \\(|||\\bf{y}||| = \\norm{\\bf{y}}_p\\) for all \\(\\bf{y}\\in\\C^m\\).\n\nProof.\nWe have:\n\\[\n\\begin{align*}\n|||\\bf{y}||| &= \\frac{\\norm{\\bf{y}x}_p}{\\norm{x}_p} & \\{ \\text{ Definition }\\}\\\\\n&= \\frac{|x_1| \\norm{\\bf{y}}_p}{|x_1|} & \\{ x \\text{ has to be } 1 \\times 1, \\text{ a scalar }\\}\\\\\n&= \\norm{\\bf{y}}_p\n\\end{align*}\n\\]\nThe last example is important. One can view a vector \\(\\bf{y}\\in \\C^m\\) as an \\(m \\times 1\\) matrix. What this last exercise tells us is that regardless of whether we view \\(\\bf{y}\\) as a matrix or a vector, \\(\\norm{y}_p\\) is the same.\nWe already encountered the vector \\(p\\)-norms as an important class of vector norms. The matrix \\(p\\)-norm is induced by the corresponding vector norm.\n\nDefinition 10 (The matrix \\(p\\)-norm) For any vector \\(p\\)-norm, define the corresponding matrix \\(p\\)-norm \\(\\norm{\\cdot}_p : \\C^{m \\times n} \\to \\R\\) by:\n\\[\n\\norm{A}_p = \\max_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_p}{\\norm{\\bf{x}}_p} \\quad \\text{ or equivalently } \\quad \\norm{A}_p = \\max_{\\norm{\\bf{x}}_p = 1} \\norm{A\\bf{x}}_p\n\\]\n\nIn practice, the matrix \\(2\\)-norm is of great theoretical importance, but difficult to evaluate, except for special matrices. The \\(1\\)-norm, the \\(\\infty\\)-norm and Frobenius norms are straightforward and relatively cheap to compute.\nLet us instantiate the definition of the vector \\(p\\)-norm where \\(p=2\\), giving us a matrix norm induced by the vector \\(2\\)-norm or the Euclidean norm:\n\nDefinition 11 (The matrix \\(2\\)-norm) Define the matrix \\(2\\)-norm \\(\\norm{\\cdot}_2:\\C^{m \\times n} \\to \\R\\) by :\n\\[\n\\norm{A}_2 = \\max_{\\bf{x}\\neq\\bf{0}} \\frac{\\norm{A\\bf{x}}_2}{\\norm{\\bf{x}}_2} = \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{A\\bf{x}}_2\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe problem with the matrix \\(2\\)-norm is that it is hard to compute. In future posts, we shall find out that if \\(A\\) is a Hermitian matrix (\\(A = A^H\\)), then \\(\\norm{A}_2 = |\\lambda_1|\\) where \\(\\lambda_1\\) is the eigenvalue of \\(A\\) that is largest in magnitude.\nRecall from basic linear algebra, that computing eigenvalues involves computing the roots of polynomials, and for polynomials of degree three or greater, this is a non-trivial task. We shall see that the matrix \\(2\\)-norm plays an important part in theory, but less so in practical computation.\n\n\n\nExample 2 Show that:\n\\[\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2 = \\max(|d_1|,|d_2|)\n\\]\n\nSolution\nWe have:\n\\[\n\\begin{align*}\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2^2 &= \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}}_2^2 & \\{ \\text{ Definition }\\}\\\\\n&= \\max_{\\norm{\\bf{x}}_2 = 1}|d_1x_1|^2 + |d_2 x_2|^2\\\\\n&\\leq \\max_{\\norm{\\bf{x}}_2 = 1} [\\max(|d_1|,|d_2|)^2 |x_1|^2 + \\max(|d_1|,|d_2|)^2 |x_2|^2]\\\\\n&= \\max(|d_1|,|d_2|)^2 \\max_{\\norm{\\bf{x}}_2 = 1} (|x_1|^2 + |x_2|^2)\\\\\n&= \\max(|d_1|,|d_2|)^2\n\\end{align*}\n\\]\nMoreover, if we take \\(\\bf{x} = \\bf{e}_1\\) and \\(\\bf{x}=\\bf{e}_2\\), we get:\n\\[\n\\begin{align*}\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2^2  &= \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}}_2 & \\{ \\text{ Definition }\\}\\\\\n&\\geq  \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}}_2^2 \\\\\n&= |d_1|^2\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2^2  &= \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}}_2 & \\{ \\text{ Definition }\\}\\\\\n&\\geq  \\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}}_2 \\\\\n&= |d_2|^2\n\\end{align*}\n\\]\nConsequently,\n\\[\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2^2 \\geq \\max(|d_1|,|d_2|)^2\n\\]\nWe conclude that\n\\[\n\\norm{\\begin{bmatrix}\nd_1 & 0 \\\\\n0 & d_2\n\\end{bmatrix}}_2 = \\max(|d_1|,|d_2|)\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThe proof of the last example builds on a general principle: Showing that \\(\\max_{x \\in D} f(x) = \\alpha\\) for some function \\(f:D \\to \\R\\) can be broken down into showing that both:\n\\[\n\\max_{x \\in D} f(x) \\leq \\alpha\n\\]\nand\n\\[\n\\max_{x \\in D} f(x) \\geq \\alpha\n\\]\nIn turn, showing that \\(\\max_{x \\in D}f(x) \\geq \\alpha\\) can often be accomplished by showing that there exists a vector \\(y \\in D\\) such that \\(f(y) = \\alpha\\) since then\n\\[\n\\max_{x \\in D}f(x) \\geq f(y) = \\alpha\n\\]\nWe will use this technique in future proofs involving matrix norms.\n\n\n\nExercise 1 Let \\(D \\in C^{m \\times m}\\) be a diagonal matrix \\(diag(d_1,d_2,\\ldots,d_m)\\). Show that:\n\\[\n\\norm{D}_2 = \\max_{j=1}^{m} |d_j|\n\\]\n\nSolution.\nWe have:\n\\[\n\\begin{align*}\n\\norm{D}_2^2 &= \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{\n    \\begin{bmatrix}\n    d_1 \\\\\n    & d_2 \\\\\n    & & \\ddots\\\\\n    & & & d_m\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    x_1\\\\\n    x_2\\\\\n    \\vdots\\\\\n    x_m\n    \\end{bmatrix}\n}_2^2 \\{ \\text{ Definition }\\}\\\\\n&=\\max_{\\norm{\\bf{x}}_2 = 1} \\norm{\n    \\begin{bmatrix}\n    d_1 x_1\\\\\n    d_2 x_2\\\\\n    \\vdots\\\\\n    d_m x_m\n    \\end{bmatrix}\n}_2^2\\\\\n&= \\max_{\\norm{\\bf{x}}_2 = 1} \\sum_{j=1}^m |d_j x_j|^2\\\\\n&\\leq \\max_{\\norm{\\bf{x}}_2 = 1} \\sum_{j=1}^m \\max(|d_1|,\\ldots,|d_m|)^2 |x_j|^2\\\\\n&= \\max(|d_1|,\\ldots,|d_m|)^2 \\max_{\\norm{\\bf{x}}_2 = 1} \\sum_{j=1}^m |x_j|^2 \\\\\n&= \\max(|d_1|,\\ldots,|d_m|)^2\n\\end{align*}\n\\]\nMoreover, if we take take \\(\\bf{x} = \\bf{e}_j\\), the standard basis vector with its \\(j\\)-th coordinate equal to one, we find that\n\\[\n\\norm{D}_2^2 \\geq |d_j|^2\n\\]\nConsequently, \\(\\norm{D}_2^2 \\geq \\max(|d_1|,\\ldots,|d_m|)^2\\).\nThis closes the proof. \\(\\blacksquare\\)\n\nExercise 2 Let \\(\\bf{y}\\in\\C^m\\) and \\(\\bf{x} \\in \\C^n\\). Show that:\n\\[\n\\norm{\\bf{y}\\bf{x}^H}_2 = \\norm{\\bf{y}}_2 \\norm{\\bf{x}}_2\n\\]\n\nProof.\nFrom the Cauchy-Schwarz inequality, we know that:\n\\[\n|x^H z| \\leq \\norm{\\bf{x}^H}_2 \\norm{\\bf{z}}_2\n\\]\nNow, \\(\\bf{x}^H \\in \\C^{1 \\times n}\\) and \\(\\bf{z} \\in \\C^{n \\times 1}\\). So, \\(\\bf{x}^H \\bf{z} \\in \\C^{1 \\times 1}\\), and it is a scalar.\n\\[\n\\begin{align*}\n\\norm{\\bf{y}\\bf{x}^H}_2 &= \\max_{\\norm{\\bf{z}}_2 = 1} \\norm{\\bf{y}\\bf{x}^H \\bf{z}}_2 \\\\\n&= \\max_{\\norm{\\bf{z}}_2 = 1} |\\bf{x}^H \\bf{z}| \\norm{\\bf{y}}_2 \\{ \\bf{x}^H\\bf{z}\\text{ is scalar }\\}\\\\\n&\\leq \\max_{\\norm{\\bf{z}}_2 = 1} \\norm{\\bf{x}^H}_2 \\norm{\\bf{z}}_2 \\norm{\\bf{y}}_2 \\\\\n&= \\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2\n\\end{align*}\n\\]\nOn the other hand,\n\\[\n\\begin{align*}\n\\norm{\\bf{y}\\bf{x}^H}_2 &= \\max_{\\bf{z}\\neq \\bf{0}} \\frac{\\norm{\\bf{y}\\bf{x}^H \\bf{z}}_2}{\\norm{\\bf{z}}_2}\\\\\n&\\geq \\frac{\\norm{\\bf{y}\\bf{x}^H \\bf{x}}_2}{\\norm{\\bf{x}}_2} & \\{ \\text{ Specific }\\bf{z} \\}\\\\\n&= \\frac{\\norm{\\bf{y}\\norm{\\bf{x}}_2^2}_2}{\\norm{\\bf{x}}_2} & \\{ \\bf{x}^H \\bf{x} = \\norm{\\bf{x}}_2^2\\}\\\\\n&= \\norm{\\bf{y}}_2 \\norm{\\bf{x}}_2\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nExercise 3 Let \\(A \\in \\C^{m \\times n}\\) and \\(a_j\\) be its column indexed with \\(j\\). Prove that:\n\\[\n\\norm{a_j}_2 \\leq \\norm{A}_2\n\\]\n\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_2 &= \\max_{\\norm{\\bf{z}}_2 = 1} \\norm{A\\bf{z}}_2 \\\\\n&\\geq  \\norm{A\\bf{e}_j}_2\\\\\n&= \\norm{a_j}_2\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nExercise 4 Let \\(A \\in \\C^{m \\times n}\\). Prove that:\n\n\\[\n\\norm{A}_2 = \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A \\bf{x}|\n\\]\n\\[\n\\norm{A^H}_2 = \\norm{A}_2\n\\]\n\\[\n\\norm{A^H A}_2 = \\norm{A}_2^2\n\\]\n\n\nClaim.\n\\[\n\\norm{A}_2 = \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A \\bf{x}|\n\\]\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A \\bf{x}| &\\leq \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} \\norm{\\bf{y}^H}_2 \\norm{A\\bf{x}}_2 & \\{ \\text{ Cauchy-Schwarz }\\} \\\\\n&= \\max_{\\norm{\\bf{x}}_2 } \\norm{A\\bf{x}}_2\\\\\n&= \\norm{A}_2\n\\end{align*}\n\\]\nOn the other hand:\n\\[\n\\begin{align*}\n\\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A \\bf{x}| &\\geq \\max_{\\norm{\\bf{x}}_2 = 1} |\\left(\\frac{A\\bf{x}}{\\norm{A\\bf{x}}_2}\\right)^H A \\bf{x}| & \\{\\text{ Specific vector }\\}\\\\\n&= \\max_{\\norm{\\bf{x}}_2 = 1} \\frac{\\norm{A\\bf{x}}_2^2}{\\norm{A\\bf{x}}_2}\\\\\n&=\\max_{\\norm{\\bf{x}}_2 = 1} \\norm{A\\bf{x}}_2\\\\\n&= \\norm{A}_2\n\\end{align*}\n\\]\nWe have the desired result.\nClaim.\n\\[\n\\norm{A^H}_2 = \\norm{A}_2\n\\]\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\norm{A^H}_2^2 &= \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{A^H\\bf{x}}_2^2 \\\\\n&= \\max_{\\norm{\\bf{x}}_2 = 1} |(A^H \\bf{x})^H (A^H \\bf{x})|\n\\end{align*}\n\\]\nClaim.\n\\[\n\\norm{A^H}_2 = \\norm{A}_2\n\\]\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\norm{A^H}_2 &= \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A^H \\bf{x}| \\\\\n&= \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{x}^H A \\bf{y}| & \\{ |\\overline \\alpha| = |\\alpha| \\}\\\\\n&= \\max_{\\norm{\\bf{y}}_2 = 1} \\norm{A \\bf{y}}_2 \\\\\n&= \\norm{A}_2\n\\end{align*}\n\\]\nClaim\n\\[\n\\norm{A^H A}_2 = \\norm{A}_2^2\n\\]\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\norm{A^H A}_2 &= \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A^H A \\bf{x}|\\\\\n&\\leq \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} \\norm{\\bf{y}^H A^H}_2 \\norm{A\\bf{x}}_2 & \\{ \\text{ Cauchy-Schwarz }\\}\\\\\n&= \\max_{\\norm{\\bf{y}}_2 = 1} \\norm{A\\bf{y}}_2 \\max_{\\norm{\\bf{x}}_2 = 1} \\norm{A\\bf{x}}_2  & \\{ \\norm{A^H}_2 = \\norm{A}_2 \\}\\\\\n&= \\norm{A}_2^2\n\\end{align*}\n\\]\nMoreover:\n\\[\n\\begin{align*}\n\\norm{A^H A}_2 &= \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A^H A \\bf{x}|\\\\\n&\\geq \\max_{\\norm{\\bf{x}}_2 = 1}  |\\bf{x}^H A^H A \\bf{x}| \\{ \\text{ Restrict the choices of }\\bf{y}\\}\\\\\n&=  \\max_{\\norm{\\bf{x}}_2 = 1}  |(A\\bf{x})^H (A \\bf{x})| \\\\\n&=  \\max_{\\norm{\\bf{x}}_2 = 1}  \\norm{A\\bf{x}}_2^2\\\\\n&= \\norm{A}_2^2\n\\end{align*}\n\\]\n\nExercise 5 Partition\n\\[\nA = \\left[\n    \\begin{array}{c|c|c}\n        A_{1,1} & \\ldots & A_{1,N}\\\\\n        \\hline\n        \\vdots & & \\vdots\\\\\n        \\hline\n        A_{M,1} & \\ldots & A_{M,N}\n    \\end{array}\n\\right]\n\\]\nProve that \\(\\norm{A_{i,j}}_2 \\leq \\norm{A}_2\\).\n\nProof.\nBy definition,\n\\[\n\\begin{align*}\n\\norm{A_{i,j}}_2 = \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A_{i,j} \\bf{x}|\n\\end{align*}\n\\]\nSince \\(\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1\\) is a compact set, the above maximum exists. There exists \\(\\bf{w}_i\\) and \\(\\bf{v}_j\\), satisfying \\(\\norm{\\bf{w}_i}_2 = \\norm{\\bf{v}_j}_2 = 1\\) such that:\n\\[\n\\begin{align*}\n\\norm{A_{i,j}}_2 = |\\bf{w}_i^H A_{i,j} \\bf{v}_j|\n\\end{align*}\n\\]\nNext, we choose\n\\[\n\\bf{w} = \\left[\n    \\begin{array}{c}\n    0 \\\\\n    \\hline\n    0 \\\\\n    \\hline\n    \\vdots \\\\\n    \\hline\n    \\bf{w}_i\\\\\n    \\hline\n    0 \\\\\n    \\hline\n    \\vdots\\\\\n    0\n    \\end{array}\n\\right] \\quad\n\\bf{v} = \\left[\n    \\begin{array}{c}\n    0 \\\\\n    \\hline\n    0 \\\\\n    \\hline\n    \\vdots \\\\\n    \\hline\n    \\bf{v}_j\\\\\n    \\hline\n    0 \\\\\n    \\hline\n    \\vdots\\\\\n    0\n    \\end{array}\n\\right]\n\\]\nConsider:\n\\[\n\\begin{align*}\n\\norm{A}_2 &= \\max_{\\norm{\\bf{x}}_2 = \\norm{\\bf{y}}_2 = 1} |\\bf{y}^H A \\bf{x}|\\\\\n& \\geq |\\bf{w}^H A \\bf{v}|\\\\\n&= |\\bf{w}_j^H A_{i,j} \\bf{v}_i|\\\\\n&= \\norm{A_{i,j}}_2\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\n\nComputing the matrix \\(1\\)-norm and \\(\\infty\\)-norm\nThe matrix \\(1\\)-norm and the matrix \\(\\infty\\)-norm are of great importance, because, unlike the matrix \\(2\\)-norm, they are easy and relatively cheap to compute. The following exercises show how to practically compute the matrix \\(1\\)-norm and \\(\\infty\\)-norm.\n\nExercise 6 Let \\(A = \\C^{m \\times n}\\) and partition \\(A = [a_1 | a_2 | \\ldots | a_n]\\). Prove that\n\\[\n\\norm{A}_1 = \\max_{1 \\leq j \\leq n}\\norm{a_j}_1\n\\]\n\nProof\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_1 &= \\max_{\\norm{\\bf{x}}_1 = 1} \\norm{A\\bf{x}}_1 & \\{ \\text{ Definition }\\}\\\\\n&= \\max_{\\norm{\\bf{x}}_1 = 1} \\norm{a_1 x_1 + a_2 x_2 + \\ldots + a_n x_n}_1 & \\{ \\text{ Algebra }\\}\\\\\n&\\leq \\max_{\\norm{\\bf{x}}_1 = 1} \\norm{a_1 x_1}_1 + \\norm{a_2 x_2}_1 + \\ldots + \\norm{a_n x_n}_1 & \\{ \\text{ Triangle Inequality }\\}\\\\\n&= \\max_{\\norm{\\bf{x}}_1 = 1} |x_1| \\norm{a_1}_1 + |x_2| \\norm{a_2}_1 + \\ldots + |x_n| \\norm{a_n}_1  & \\{ \\text{ Homogeneity }\\}\\\\\n&= \\max_{\\norm{\\bf{x}}_1 = 1}  |x_1| (\\max_{1 \\leq j \\leq n} \\norm{a_j}_1) + |x_2| (\\max_{1 \\leq j \\leq n} \\norm{a_j}_1)+ \\ldots + |x_n| (\\max_{1 \\leq j \\leq n} \\norm{a_j}_1)\\\\\n&= \\max_{1 \\leq j \\leq n} \\norm{a_j}_1 \\max_{\\norm{\\bf{x}}_1 = 1} \\sum_{j=1}^n |x_j|\\\\\n&= \\max_{1 \\leq j \\leq n} \\norm{a_j}_1\n\\end{align*}\n\\]\nOn the other hand,\n\\[\n\\begin{align*}\n\\norm{A}_1  &= \\max_{\\norm{\\bf{x}}_1 = 1} \\norm{A\\bf{x}}_1 & \\{ \\text{ Definition }\\}\\\\\n&\\geq \\norm{A\\bf{e}_j}_1 & \\{ \\text{ Specific vector }\\}\\\\\n&= \\norm{a_j}_1\n\\end{align*}\n\\]\nThis concludes the proof. \\(\\blacksquare\\)\n\nExercise 7 Let \\(A = \\C^{m \\times n}\\) and partition\n\\[\nA = \\left[\n    \\begin{array}{c}\n        \\tilde{a}_0^T\\\\\n        \\hline\n        \\tilde{a}_1^T\\\\\n        \\hline\n        \\vdots\\\\\n        \\hline\n        \\tilde{a}_{m-1}^T\\\\\n    \\end{array}\n\\right]\n\\]\nProve that\n\\[\n\\norm{A}_\\infty = \\max_{0\\leq i &lt; m} \\norm{\\tilde{a}_i}_1 = \\max_{0 \\leq i &lt; m} (|\\alpha_{i,0}| + |\\alpha_{i,1}| + \\ldots + |\\alpha_{i,n-1}|)\n\\]\n\n*Notice that in this exercise, \\(\\tilde{a}_i\\) is really \\((\\tilde{a}_i^T)^T\\), since \\(\\tilde{a}_i^T\\) is the label for the \\(i\\)-th row of the matrix.\nProof.\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_\\infty &= \\max_{\\norm{\\bf{x}}_\\infty = 1} \\norm{A\\bf{x}}_\\infty & \\{ \\text{ Definition }\\} \\\\\n&= \\max_{\\norm{\\bf{x}}_\\infty = 1} \\norm{\\left[\n    \\begin{array}{c}\n        \\tilde{a}_0^T \\bf{x}\\\\\n        \\hline\n        \\tilde{a}_1^T \\bf{x}\\\\\n        \\hline\n        \\vdots\\\\\n        \\hline\n        \\tilde{a}_{m-1}^T \\bf{x}\\\\\n    \\end{array}\n\\right]}_\\infty & \\{ \\text{ Algebra }\\}\\\\\n&=  \\max_{\\norm{\\bf{x}}_\\infty = 1} \\max_{0 \\leq i &lt; m} |\\tilde{a}_i^T \\bf{x}|\\\\\n&=  \\max_{\\norm{\\bf{x}}_\\infty = 1} \\max_{0 \\leq i &lt; m} |\\alpha_{i,0}x_0 + \\ldots + \\alpha_{i,n-1}x_{n-1}|\\\\\n&\\leq  \\max_{\\norm{\\bf{x}}_\\infty = 1} \\max_{0 \\leq i &lt; m} \\left( |\\alpha_{i,0}x_0 | + \\ldots + |\\alpha_{i,n-1}x_{n-1}| \\right) & \\{ \\text{ Triangle Inequality }\\}\\\\\n&=  \\max_{\\norm{\\bf{x}}_\\infty = 1} \\max_{0 \\leq i &lt; m} \\left( |\\alpha_{i,0}||x_0 | + \\ldots + |\\alpha_{i,n-1}||x_{n-1}| \\right) & \\{ \\text{ Algebra }\\}\\\\\n&\\leq \\max_{\\norm{\\bf{x}}_\\infty = 1} \\max_{0 \\leq i &lt; m}\\left( |\\alpha_{i,0}|\\norm{\\bf{x}}_\\infty + \\ldots + |\\alpha_{i,n-1}|\\norm{\\bf{x}}_\\infty \\right) & \\{ |x_i| \\leq \\norm{\\bf{x}}_\\infty \\}\\\\\n&= \\max_{0 \\leq i &lt; m} ( |\\alpha_{i,0}| + \\ldots + |\\alpha_{i,n-1}|) \\max_{\\norm{\\bf{x}}_\\infty = 1} \\norm{\\bf{x}}_\\infty\\\\\n&= \\max_{0 \\leq i &lt; m} ( |\\alpha_{i,0}| + \\ldots + |\\alpha_{i,n-1}|)\\\\\n&= \\max_{0 \\leq i &lt; m}\n\\norm{\\tilde{a}_i}_1\n\\end{align*}\n\\]\nWe also want to show that \\(\\norm{A}_\\infty \\geq \\max_{0 \\leq i &lt; m} \\norm{\\tilde{a}_i}_1\\). Let \\(k\\) be such that \\(\\max_{0 \\leq i &lt; m}\\norm{\\tilde{a}_i}_1 = \\norm{\\tilde{a}_k}_1\\) and pick \\(\\bf{y} = \\left(\\begin{array}{c}\\psi_0\\\\ \\vdots\\\\ \\psi_{n-1}\\end{array}\\right)\\) so that \\(\\tilde{a}_k^T \\bf{y} = |\\alpha_{k,0}| + |\\alpha_{k,1}| + \\ldots + |\\alpha_{k,n-1}|=\\norm{\\tilde{a}_k}_1\\). This is a matter of picking \\(\\psi_j = |\\alpha_{k,j}|/\\alpha_{k,j}\\). Then, \\(|\\psi_j| = 1\\) and hence, \\(\\norm{\\bf{y}}_\\infty = 1\\) and \\(\\psi_j \\alpha_{k,j} = |\\alpha_{k,j}|\\). Then:\n\\[\n\\begin{align*}\n\\norm{A}_\\infty &= \\max_{\\norm{\\bf{x}}_\\infty = 1} \\norm{A\\bf{x}}_\\infty & \\{ \\text{ Definition }\\} \\\\\n&= \\max_{\\norm{\\bf{x}}_\\infty = 1} \\norm{\\left[\n    \\begin{array}{c}\n        \\tilde{a}_0^T \\bf{x}\\\\\n        \\hline\n        \\tilde{a}_1^T \\bf{x}\\\\\n        \\hline\n        \\vdots\\\\\n        \\hline\n        \\tilde{a}_{m-1}^T \\bf{x}\\\\\n    \\end{array}\n\\right]}_\\infty & \\{ \\text{ Expose rows }\\}\\\\\n&\\geq  \\norm{\\left[\n    \\begin{array}{c}\n        \\tilde{a}_0^T \\bf{y}\\\\\n        \\hline\n        \\tilde{a}_1^T \\bf{y}\\\\\n        \\hline\n        \\vdots\\\\\n        \\hline\n        \\tilde{a}_{m-1}^T \\bf{y}\\\\\n    \\end{array}\n\\right]}_\\infty & \\{ \\text{ Specific vector }\\}\\\\\n&\\geq |\\tilde{a}_k^T \\bf{y}|\\\\\n&= \\norm{\\tilde{a}_k}_1 \\\\\n&= \\max_{0 \\leq i &lt; m} \\norm{\\tilde{a}_i}_1\n\\end{align*}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nExercise 8 Fill out the following table:\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\nA & \\norm{A}_1 & \\norm{A}_\\infty & \\norm{A}_F & \\norm{A}_2\\\\\n\\hline\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\\\\\n\\hline\n\\begin{bmatrix}\n1 & 1 & 1\\\\\n1 & 1 & 1\\\\\n1 & 1 & 1\\\\\n1 & 1 & 1\n\\end{bmatrix}\\\\\n\\hline\n\\begin{bmatrix}\n0 & 1 & 0\\\\\n0 & 1 & 0\\\\\n0 & 1 & 0\n\\end{bmatrix}\\\\\n\\hline\n\\end{array}\n\\]\n\nSolution.\nLet\n\\[\nA = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nWe have, \\(\\norm{A}_1 = 1\\), \\(\\norm{A}_\\infty = 1\\), \\(\\norm{A}_F = \\sqrt{3}\\). Since this is a diagonal matrix, \\(\\norm{A}_2 = \\max_{0 \\leq i \\leq 2} |d_{i}|\\) = 1.\nNext, consider:\n\\[\n\\begin{bmatrix}\n1 & 1 & 1\\\\\n1 & 1 & 1\\\\\n1 & 1 & 1\\\\\n1 & 1 & 1\n\\end{bmatrix}\n\\]\nWe have, \\(\\norm{A}_1 = 4\\), \\(\\norm{A}_\\infty = 3\\), \\(\\norm{A}_F = \\sqrt{12}\\).\nNote that, we can write\n\\[\nA = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{bmatrix} [1, 1, 1, 1] = \\bf{x}\\bf{y}^H\n\\]\nwhere \\(\\bf{x} = \\bf{y} = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{bmatrix}\\). Using the property that, \\(\\norm{\\bf{x}\\bf{y}^H}_2 = \\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2\\), we have that, \\(\\norm{A}_2 = 4\\).\nFinally, if\n\\[\nA = \\begin{bmatrix}\n0 & 1 & 0\\\\\n0 & 1 & 0\\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\]\nwe find that \\(\\norm{A}_1 = 3\\), \\(\\norm{A}_\\infty = 1\\), \\(\\norm{A}_F = \\sqrt{3}\\). Finally, let \\(\\bf{x} = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}\\) and \\(\\bf{y} = \\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}\\). Then, \\(A = \\bf{x}\\bf{y}^H\\). So, \\(\\norm{A}_2 = \\norm{\\bf{x}}_2 \\norm{\\bf{y}}_2 = \\sqrt{3}\\).\n\n\nEquivalence of matrix norms\nWe saw that vector norms are equivalent in the sense that if a vector is small in one norm, it is small in all other norms and if it is large in one norm, it is large in all other norms. The same is true for matrix norms.\n\nTheorem 12 (Equivalence of matrix norms) Let \\(\\norm{\\cdot} : \\C^{m \\times n} \\to \\R\\) and \\(|||\\cdot|||:\\C^{m \\times n} \\to \\R\\) both be matrix norms. Then, there exist positive scalars \\(\\sigma\\) and \\(\\tau\\) such that for all \\(A \\in \\C^{m \\times n}\\)\n\\[\n\\sigma \\norm{A} \\leq |||A||| \\leq \\tau \\norm{A}\n\\]\n\nProof.\nThe proof again builds on the fact that the supremum over a compact set is achieved and can be replaced by the maximum. We will prove that there exists \\(\\tau\\) such that for all \\(A \\in \\C^{m \\times n}\\)\n\\[\n|||A||| \\leq \\tau \\norm{A}\n\\]\nLet \\(A \\in \\C^{m \\times n}\\) be an arbitrary matrix. Assume that \\(A \\neq 0\\) (the zero matrix). Then:\n\\[\n\\begin{align*}\n|||A||| &= \\frac{|||A|||}{\\norm{A}} \\cdot \\norm{A} & \\{\\text{Algebra}\\}\\\\\n&\\leq \\sup_{Z \\neq 0} \\left(\\frac{|||Z|||}{\\norm{Z}}\\right) \\norm{A} & \\{\\text{Definition of supremum}\\}\\\\\n&= \\sup_{Z \\neq 0} \\left(\\Biggl|\\Biggl|\\Biggl|\\frac{Z}{\\norm{Z}}\\Biggr|\\Biggr|\\Biggr|\\right) \\norm{A} & \\{\\text{Homogeneity}\\}\\\\\n&= \\left(\\sup_{\\norm{B} = 1} |||B||| \\right) \\norm{A} &\\{ \\text{change of variables }B=Z/\\norm{Z}\\}\\\\\n&= \\left(\\max_{\\norm{B}=1}|||B|||\\right) \\norm{A} & \\{\\text{the set }\\norm{B} = 1\\text{ is compact}\\}\n\\end{align*}\n\\]\nSo, we can choose \\(\\tau = \\max_{\\norm{B}=1} |||B|||\\).\nAlso, from the above proof, we deduce that, there exists \\(\\sigma\\) given by:\n\\[\n\\sigma = \\frac{1}{\\max_{|||B|||=1}||B||}\n\\]\nsuch that:\n\\[\n\\sigma \\norm{A} \\leq |||A|||\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nExercise 9 Given \\(A \\in \\C^{m \\times n}\\), show that \\(\\norm{A}_2 \\leq \\norm{A}_F\\). For what matrix, is the equality attained?\n\nSolution.\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_2^2 &= \\max_{\\norm{x}_2 = 1} \\norm{Ax}_2^2  & \\{\\text{Definition}\\}\\\\\n&= \\max_{\\norm{x}_2 = 1} \\norm{\\begin{bmatrix}\n\\sum_{j=0}^{n-1} a_{0,j} x_j \\\\\n\\sum_{j=0}^{n-1} a_{1,j} x_j \\\\\n\\vdots\\\\\n\\sum_{j=0}^{n-1} a_{m-1,j} x_j\n\\end{bmatrix}\n}_2^2\\\\\n&= \\max_{\\norm{x}_2 = 1} \\sum_{i=0}^{m-1} \\Biggl|\\sum_{j=0}^{n-1} a_{i,j} x_j\\Biggr|^2\\\\\n&\\leq \\max_{\\norm{x}_2 = 1} \\sum_{i=0}^{m-1} \\left(\\sum_{j=0}^{n-1} |a_{i,j} x_j|\\right)^2 & \\{\\text{Triangle Inequality}\\}\\\\\n&\\leq \\max_{\\norm{x}_2 = 1} \\sum_{i=0}^{m-1} \\left\\{\\left(\\sum_{j=0}^{n-1} |a_{i,j}|^2\\right)^{1/2} \\left(\\sum_{j=0}^{n-1}|x_j|^2\\right)^{1/2}\\right\\}^2 & \\{\\text{Cauchy-Schwarz}\\}\\\\\n&=\\max_{\\norm{x}_2 = 1} \\sum_{i=0}^{m-1} \\left(\\sum_{j=0}^{n-1} |a_{i,j}|^2\\right) \\left(\\sum_{j=0}^{n-1}|x_j|^2\\right) & \\{\\text{Simplify}\\}\\\\\n&=\\sum_{i=0}^{m-1} \\left(\\sum_{j=0}^{n-1} |a_{i,j}|^2\\right) & \\{\\norm{x}_2 = 1\\}\\\\\n&= \\norm{A}_F\n\\end{align*}\n\\]\nAlso, consider\n\\[A = \\begin{bmatrix}1 & 0 \\\\ 0 & 0\\end{bmatrix}\\]\nThen, \\(\\norm{A}_2 = \\norm{A}_F = 1\\). So, the inequality \\(\\norm{A}_2 \\leq \\norm{A}_F\\) is tight. This closes the proof. \\(\\blacksquare\\)\n\nExercise 10 Let \\(A \\in \\C^{m \\times n}\\). The following table summarizes the equivalences of various matrix norms:\n\\[\n\\begin{array}{c|c|c|c}\n& \\norm{A}_1 \\leq \\sqrt{m}\\norm{A}_2 & \\norm{A}_1 \\leq m \\norm{A}_\\infty & \\norm{A}_1 \\leq \\sqrt{m}\\norm{A}_F \\\\\n\\hline\n\\norm{A}_2 \\leq \\sqrt{n}\\norm{A}_1 & & \\norm{A}_2 \\leq \\sqrt{m}\\norm{A}_\\infty & \\norm{A}_2 \\leq \\norm{A}_F \\\\\n\\hline\n\\norm{A}_\\infty \\leq n \\norm{A}_1 & \\norm{A}_\\infty \\leq \\sqrt{n} \\norm{A}_2 & & \\norm{A}_\\infty \\leq \\sqrt{n}\\norm{A}_F\\\\\n\\hline\n\\norm{A}_F \\leq \\sqrt{n} \\norm{A}_1 & \\norm{A}_F \\leq \\tau \\norm{A}_2 & \\norm{A}_F \\leq \\sqrt{m}\\norm{A}_\\infty\n\\end{array}\n\\]\nFor each, prove the inequality, including that it is a tight inequality for some nonzero \\(A\\). (Skip \\(\\norm{A}_F \\leq \\tau \\norm{A}_2\\), we revisit it in a later post)\n\nSolution.\nClaim. Our claim is that \\(\\norm{A}_1 \\leq \\sqrt{m} \\norm{A}_2\\).\nPartition \\(A = [a_0 | a_1 | \\ldots | a_{n-1}]\\).\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_1 &= \\max_{0 \\leq j &lt; n} \\norm{a_j}_1 & \\{\\text{Definition}\\}\\\\\n&= \\max_{0 \\leq j &lt; n} \\norm{\n    \\begin{bmatrix}\n    \\alpha_{0,j}\\\\\n    \\alpha_{1,j}\\\\\n    \\vdots\\\\\n    \\alpha_{m-1,j}\n    \\end{bmatrix}\n}_1\\\\\n&= \\max_{0 \\leq j &lt; n} \\sum_{i=0}^{m-1}|\\alpha_{i,j}| \\cdot |1|\\\\\n&= \\max_{0 \\leq j &lt; n} \\left(\\sum_{i=0}^{m-1}|\\alpha_{i,j}|^2\\right)^{1/2} \\left(\\sum_{i=0}^{m-1}|1|^2\\right)^{1/2} & \\{\\text{Cauchy-Schwarz}\\}\\\\\n&= \\max_{0 \\leq j &lt; n} \\norm{a_j}_2 \\sqrt{m}\\\\\n&= \\max_{0 \\leq j &lt; n} \\norm{A}_2 \\sqrt{m} & \\{\\norm{A_{i,j}}_2 \\leq \\norm{A}_2\\}\\\\\n&= \\sqrt{m} \\norm{A}_2\n\\end{align*}\n\\]\nMoreover, consider the matrix\n\\[\nA = \\begin{bmatrix}\n1 & 0 \\\\\n1 & 0\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0\n\\end{bmatrix}\n\\]\nWe have, \\(\\norm{A}_2 = \\sqrt{2}\\) and \\(\\norm{A}_1 = 2\\), so \\(\\norm{A}_1 = \\sqrt{2}\\norm{A}_2\\). Thus, the inequality is tight. This closes the proof.\nClaim. Our claim is that \\(\\norm{A}_1 \\leq m \\norm{A}_\\infty\\).\nSolution.\nWe have:\n\\[\n\\begin{align}\n\\norm{A}_1 &= \\max_{0 \\leq j &lt; n} \\norm{a_j}_1 & \\{\\text{Definition}\\}\\\\\n&= \\max_{0 \\leq j &lt; n} \\sum_{i=0}^{m-1}|\\alpha_{i,j}|\\\\\n&\\leq \\sum_{j=0}^{n-1} \\sum_{i=0}^{m-1} |\\alpha_{i,j}| = \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} |\\alpha_{i,j}| \\\\\n&=\\sum_{i=0}^{m-1} \\norm{\\tilde{a}_i^T}_1\\\\\n&= m \\max_{0 \\leq i &lt; m} \\norm{\\tilde{a}_i^T}_1\\\\\n&= m \\norm{A}_\\infty\n\\end{align}\n\\]\nAgain, consider the matrix\n\\[\nA = \\begin{bmatrix}\n1 & 0\\\\\n1 & 0\n\\end{bmatrix}\n\\]\n\\(\\norm{A}_1 = 2\\) and \\(\\norm{A}_\\infty = 1\\), so \\(\\norm{A}_1 = 2 \\norm{A}_\\infty\\). Hence, the inequality is tight. This closes the proof.\nClaim. Our claim is that \\(\\norm{A}_1 \\leq \\sqrt{m}\\norm{A}_F\\).\nSolution.\nWe have shown that:\n\\[\n\\begin{align}\n\\norm{A}_1 &\\leq \\sqrt{m}\\norm{A}_2 \\\\\n\\norm{A}_2 &\\leq \\norm{A}_F\n\\end{align}\n\\]\nSo, we deduce that \\(\\norm{A}_1 \\leq \\sqrt{m}\\norm{A}_F\\). Moreover, consider\n\\[\nA = \\sqrt{2}I\n\\]\nThen, \\(\\norm{A}_1 = \\sqrt{2}\\) and \\(\\norm{A}_F = 2\\), so \\(\\norm{A}_1 = \\sqrt{2}\\norm{A}_F\\). Hence, the inequality is tight.\nClaim. Our claim is that \\(\\norm{A}_2 \\leq \\sqrt{n}\\norm{A}_1\\).\nSolution.\nWe have:\n\\[\n\\begin{align}\n\\norm{A}_2 &= \\max_{x \\neq 0} \\frac{\\norm{Ax}_2}{\\norm{x}_2} & \\{\\text{Definition}\\}\\\\\n&= \\max_{x \\neq 0} \\frac{\\norm{Ax}_1}{\\norm{x}_2}  & \\{\\norm{z}_2 \\leq \\norm{z}_1\\} \\\\\n&= \\max_{x \\neq 0} \\frac{\\norm{Ax}_1}{\\frac{1}{\\sqrt{n}}\\norm{x}_1}  & \\{\\norm{z}_1 \\leq \\sqrt{n}\\norm{z}_2\\} \\\\\n&= \\sqrt{n}\\norm{A}_1\n\\end{align}\n\\]\nAgain, consider the matrix \\(A = [1 | 1| \\ldots | 1]\\). Then, \\(\\norm{A}_2 = \\sqrt{n}\\) and \\(\\norm{A}_1 = 1\\). So, \\(\\norm{A}_2 = \\sqrt{n}\\norm{A}_1\\).\nClaim. Our claim is that \\(\\norm{A}_2 \\leq \\sqrt{m} \\norm{A}_\\infty\\).\nSolution.\n\\[\n\\begin{align*}\n\\norm{A}_2 &= \\max_{x \\neq 0} \\frac{\\norm{Ax}_2}{\\norm{x}_2} &\\{\\text{Definition}\\}\\\\\n&=\\max_{x \\neq 0} \\frac{\\norm{\\begin{bmatrix}\\tilde{a}_0^T \\\\ \\tilde{a}_1^T \\\\ \\vdots \\\\ \\tilde{a}_{m-1}^T\\end{bmatrix}x}_2}{\\norm{x}_2} &\\{\\text{Expose rows}\\}\\\\\n&=\\max_{x \\neq 0} \\frac{\\norm{\\begin{bmatrix}\\tilde{a}_0^T x \\\\ \\tilde{a}_1^T x\\\\ \\vdots \\\\ \\tilde{a}_{m-1}^T x\\end{bmatrix}}_2}{\\norm{x}_2} &\\{\\text{Algebra}\\}\\\\\n&\\leq \\max_{x \\neq 0} \\frac{\\sqrt{m}\\norm{Ax}_\\infty}{\\norm{x}_2} &\\{\\norm{z}_2 \\leq \\sqrt{n}\\norm{z}_\\infty\\}\\\\\n& \\leq \\max_{x \\neq 0} \\frac{\\sqrt{m}\\norm{Ax}_\\infty}{\\norm{x}_\\infty} &\\{\\norm{z}_\\infty \\leq \\norm{z}_2\\}\\\\\n&= \\sqrt{m} \\norm{A}_\\infty\n\\end{align*}\n\\]\nMoreover, consider the matrix\n\\[\nA = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1\n\\end{bmatrix}\n\\]\nWe have \\(\\norm{A}_2 = \\sqrt{m}\\), \\(\\norm{A}_\\infty = 1\\), so \\(\\norm{A}_2 = \\sqrt{m}\\norm{A}_\\infty\\). Hence, the inequality is tight.\nClaim. Our claim is that \\(\\norm{A}_\\infty \\leq n \\norm{A}_1\\).\nSolution.\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_\\infty &= \\max_{x \\neq 0} \\frac{\\norm{Ax}_\\infty}{\\norm{x}_\\infty} & \\{\\text{Definition}\\}\\\\\n&\\leq  \\max_{x \\neq 0} \\frac{\\norm{Ax}_1}{\\norm{x}_\\infty} & \\{\\norm{x}_\\infty \\leq \\norm{x}_1\\}\\\\\n&\\leq  \\max_{x \\neq 0} \\frac{\\norm{Ax}_1}{\\frac{1}{n}\\norm{x}_1} & \\{\\norm{x}_1 \\leq n \\norm{x}_\\infty\\}\\\\\n&= n \\norm{A}_1\n\\end{align*}\n\\]\nMoreover, let \\(A = [1 | 1 | \\ldots | 1]\\). Then, \\(\\norm{A}_\\infty = n\\) and \\(\\norm{A}_1 = 1\\), so \\(\\norm{A}_\\infty = n \\norm{A}_1\\). Hence, the inequality is tight.\nClaim. Our claim is that \\(\\norm{A}_\\infty \\leq \\sqrt{n} \\norm{A}_2\\).\nSolution.\nWe have:\n\\[\n\\begin{align*}\n\\norm{A}_\\infty &= \\max_{x \\neq 0} \\frac{\\norm{Ax}_\\infty}{\\norm{x}_\\infty} & \\{\\text{Definition}\\}\\\\\n&\\leq  \\max_{x \\neq 0} \\frac{\\norm{Ax}_2}{\\norm{x}_\\infty} & \\{\\norm{x}_\\infty \\leq \\norm{x}_2\\}\\\\\n&\\leq  \\max_{x \\neq 0} \\frac{\\norm{Ax}_2}{\\frac{1}{\\sqrt{n}}\\norm{x}_2} & \\{\\norm{x}_2 \\leq \\sqrt{n} \\norm{x}_\\infty\\}\\\\\n&= \\sqrt{n} \\norm{A}_2\n\\end{align*}\n\\]\nMoreover, let \\(A = [1|1|\\ldots|1]\\)\nThen, \\(\\norm{A}_\\infty = n\\), \\(\\norm{A}_2 = \\sqrt{n}\\) and \\(\\norm{A}_\\infty = \\sqrt{n} \\norm{A}_2\\). So, the bound is tight.\nClaim. Our claim is that \\(\\norm{A}_\\infty \\leq \\sqrt{n}\\norm{A}_F\\).\nSolution. This is true since \\(\\norm{A}_\\infty \\leq \\sqrt{n}\\norm{A}_2\\) and \\(\\norm{A}_2 \\leq \\norm{A}_F\\).\nLet \\(A = [1 | 1 | \\ldots | 1]\\). Then, \\(\\norm{A}_\\infty = n\\) and \\(\\norm{A}_F = \\sqrt{n}\\). So, \\(\\norm{A}_\\infty = \\sqrt{n}\\norm{A}_F\\). The bound is tight.\nClaim. Our claim is that \\(\\norm{A}_F \\leq \\sqrt{n} \\norm{A}_1\\).\nSolution.\nWe have:\n\\[\n\\begin{align}\n\\norm{A}_F^2 &= \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} |\\alpha_{i,j}|^2 & \\{\\text{Definition}\\}\\\\\n&= \\sum_{j=0}^{n-1} \\sum_{i=0}^{m-1} |\\alpha_{i,j}|^2 \\\\\n&\\leq \\sum_{j=0}^{n-1} \\left(\\sum_{i=0}^{m-1} |\\alpha_{i,j}| \\right)^2 \\\\\n&= \\sum_{j=0}^{n-1} \\norm{a_j}_1^2 \\\\\n&\\leq \\sum_{j=0}^{n-1} \\max_{j=0}^{n-1} \\norm{a_j}_1^2 \\\\\n&= n \\norm{A}_1^2\n\\end{align}\n\\]\nConsequently, \\(\\norm{A}_F \\leq \\sqrt{n}\\norm{A}_1\\). Let \\(A = [1 |1 | \\ldots| 1]\\). Then, \\(\\norm{A}_F = \\sqrt{n}\\) and \\(\\norm{A}_1 = 1\\), so \\(\\norm{A}_F = \\sqrt{n}\\norm{A}_1\\). Hence, the bound is tight.\nClaim. Our claim is that \\(\\norm{A}_F \\leq \\sqrt{m} \\norm{A}_\\infty\\).\nSolution.\nWe have:\n\\[\n\\begin{align}\n\\norm{A}_F^2 &= \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} |\\alpha_{i,j}|^2 & \\{\\text{Definition}\\}\\\\\n&\\leq \\sum_{i=0}^{m-1} \\left(\\sum_{j=0}^{n-1} |\\alpha_{i,j}| \\right)^2 \\\\\n&= \\sum_{i=0}^{m-1} \\norm{\\tilde{a}_i^T}_1^2 \\\\\n&\\leq \\sum_{j=0}^{n-1} \\max_{i=0}^{m-1} \\norm{\\tilde{a}_i^T}_1^2 \\\\\n&= m \\norm{A}_\\infty^2\n\\end{align}\n\\]\nConsequently, \\(\\norm{A}_F \\leq \\sqrt{m}\\norm{A}_\\infty\\). Let \\(A = [1, 1, \\ldots, 1]^T\\). Then, \\(\\norm{A}_F = \\sqrt{m}\\) and \\(\\norm{A}_\\infty = 1\\), so \\(\\norm{A}_F = \\sqrt{m}\\norm{A}_1\\). Hence, the bound is tight.\n\n\nSub-multiplicative norms\nThere are a number of properties that we would like a matrix norm to have(but not all matrix norms do). Given a matrix norm \\(\\norm{\\cdot} : \\C^{m \\times n} \\to \\R\\), we may ask the following question. Do there exist vector norms \\(\\norm{\\cdot}_\\mu : C^m \\to \\R\\) and \\(\\norm{\\cdot}:\\C^n \\to R\\), such that the matrix norm is an upper bound on how much the non-zero vector \\(x\\) is stretched? That is, the following inequality is satisfied:\n\\[\n\\frac{\\norm{Ax}_\\mu}{\\norm{x}_\\nu} \\leq \\norm{A}\n\\]\nor equivalently\n\\[\n\\norm{Ax}_\\mu \\leq \\norm{A} \\norm{x}_\\nu\n\\]\nwhere this second formulation has the benefit that it also holds if \\(x = 0\\).\n\nDefinition 12 (Subordinate matrix norm) A matrix norm \\(\\norm{\\cdot}:\\C^{m \\times n} \\to \\R\\) is said to be subordinate to vector norms \\(\\norm{\\cdot}_\\mu :\\C^m \\to \\R\\) and \\(\\norm{\\cdot}_\\nu : \\C^n \\to \\R\\), if for all, \\(x \\in \\C^n\\),\n\\[\n\\norm{Ax}_\\mu \\leq \\norm{A} \\norm{x}_\\nu\n\\]\nIf \\(\\norm{\\cdot}_\\mu\\) and \\(\\norm{\\cdot}_\\nu\\) are the same norm (but perhaps for different \\(m\\) and \\(n\\)), then \\(\\norm{\\cdot}\\) is said to be subordinate to the given vector norm.\n\n\nExercise 11 Prove that the matrix \\(2\\)-norm is subordinate to the vector \\(2\\)-norm.\n\nProof.\nLet \\(A \\in C^{m \\times n}\\) and let \\(x \\in \\C^n\\). Assume that \\(x \\neq 0\\), for if \\(x = 0\\), then the inequality \\(\\norm{Ax}_2 \\leq \\norm{A}_2 \\norm{x}_2\\) is vacuously true.\nWe have:\n\\[\n\\begin{align}\n\\norm{Ax}_2&= \\left(\\frac{\\norm{Ax}_2}{\\norm{x}_2}\\right) \\norm{x}_2 & \\{x \\neq 0\\} \\\\\n&\\leq \\left(\\max_{y \\neq 0} \\frac{\\norm{Ay}_2}{\\norm{y}_2}\\right)\\norm{x}_2\\\\\n&= \\norm{A}_2 \\norm{x}_2\n\\end{align}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nExercise 12 Prove that the Frobenius norm is subordinate to the vector \\(2\\)-norm.\n\nProof.\nWe are interested to prove the claim that, \\((\\forall A \\in \\C^{m\\times n})(\\forall x \\in \\C^n)\\):\n\\[ \\norm{Ax}_2 \\leq \\norm{A}_F \\norm{x}_2 \\]\nAgain, without loss of generality, we have:\n\\[\n\\begin{align}\n\\norm{Ax}_2^2 &= \\norm{\n    \\begin{bmatrix}\n        \\sum_{j=0}^{n-1}\\alpha_{0,j} x_j \\\\\n        \\sum_{j=0}^{n-1}\\alpha_{1,j} x_j \\\\\n        \\vdots\n        \\sum_{j=0}^{n-1}\\alpha_{m-1,j} x_j\n    \\end{bmatrix}\n}_2^2 & \\{\\text{Definition}\\}\\\\\n&= \\sum_{i=0}^{m-1} \\Biggl| \\sum_{j=0}^{n-1}\\alpha_{i,j} x_j \\Biggr|^2\\\\\n&= \\sum_{i=0}^{m-1} \\left(\\sum_{j=0}^{n-1}|\\alpha_{i,j} x_j| \\right)^2 & \\{\\text{Triangle Inequality}\\}\\\\\n&\\leq \\sum_{i=0}^{m-1} \\left[\\left(\\sum_{j=0}^{n-1}|\\alpha_{i,j}|^2 \\right)  \\left(\\sum_{j=0}^{n-1} |x_j|^2\\right)\\right] & \\{\\text{Cauchy-Schwarz}\\}\\\\\n&= \\left(\\sum_{j=0}^{n-1} |x_j|^2\\right) \\left(\\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1}|\\alpha_{i,j}|^2 \\right)   & \\{\\text{Algebra}\\}\\\\\n&= \\norm{A}_F^2 \\norm{x}_2^2\n\\end{align}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nTheorem 13 Induced matrix norms, \\(\\norm{\\cdot}_{\\mu,\\nu} : \\C^{m \\times n} \\to \\R\\) are subordinate to the norms, \\(\\norm{\\cdot}_\\mu\\) and \\(\\norm{\\cdot}_\\nu\\) that induce them.\n\nProof.\nWithout loss of generality, assume that \\(x \\neq 0\\), otherwise the proposition is vacuously true.\nWe have:\n\\[\n\\begin{align}\n\\norm{Ax}_\\mu &= \\frac{\\norm{Ax}_\\mu}{\\norm{x}_\\nu} \\norm{x}_\\nu \\\\\n&\\leq \\left(\\max_{x \\neq 0} \\frac{\\norm{Ax}_\\mu}{\\norm{x}_\\nu} \\right) \\norm{x}_\\nu \\\\\n&= \\left(\\max_{y \\neq 0} \\frac{\\norm{Ay}_\\mu}{\\norm{y}_\\nu} \\right) \\norm{x}_\\nu \\\\\n&= \\norm{A} \\norm{x}_\\nu\n\\end{align}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\n\nCorollary 1 Any matrix \\(p\\)-norm is subordinate to the corresponding vector norm.\n\nProof.\nWithout the loss of generality, assume that \\(x \\neq 0\\). If \\(x = 0\\), the proposition is vacuously true.\nWe have:\n\\[\n\\begin{align}\n\\norm{Ax}_p &= \\left(\\frac{\\norm{Ax}_p}{\\norm{x}_p} \\right) \\norm{x}_p & \\{ x \\neq 0\\}\\\\\n&\\leq  \\left(\\max_{x \\neq 0} \\frac{\\norm{Ax}_p}{\\norm{x}_p} \\right) \\norm{x}_p\\\\\n&=  \\left(\\max_{y \\neq 0}\\frac{\\norm{Ay}_p}{\\norm{y}_p} \\right) \\norm{x}_p\\\\\n&= \\norm{A}_p \\norm{x}_p\n\\end{align}\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nAnother desirable property that not all norms have is that:\n\\[\n\\norm{AB} \\leq \\norm{A} \\norm{B}\n\\]\nThis requires the given norm to be defined for all matrix sizes.\n\nDefinition 13 (Consistent matrix norm) A matrix norm \\(\\norm{\\cdot} : \\C^{m \\times n} \\to \\R\\) is said to be consistent matrix norm if it is defined for all \\(m\\) and \\(n\\), using the same formula for all \\(m\\) and \\(n\\).\n\n\nDefinition 14 (Submultiplicative matrix norm) A consistent matrix norm \\(\\norm{\\cdot} : \\C^{m \\times n} \\to \\R\\) is said to be submultiplicative if it satisfies:\n\\[\n\\norm{AB} \\leq \\norm{A} \\norm{B}\n\\]\n\n\nTheorem 14 Let \\(\\norm{\\cdot} : \\C^n \\to \\R\\) be a vector norm defined for all \\(n\\). Define the corresponding induced matrix norm as:\n\\[\n\\norm{A} = \\max_{x \\neq 0} \\frac{\\norm{Ax}}{\\norm{x}} = \\max_{\\norm{x} = 1} \\norm{Ax}\n\\]\nThen, for any \\(A \\in \\C^{m \\times k}\\) and \\(B^{k \\times n}\\), the inequality \\(\\norm{AB} \\leq \\norm{A} \\norm{B}\\) holds.\n\nIn other words, induced matrix norms are submultiplicative.\nProof.\nWe have:\n\\[\n\\begin{align}\n\\norm{AB} &= \\max_{\\norm{x}=1} \\norm{ABx} & \\{\\text{Definition}\\}\\\\\n&= \\max_{\\norm{x}=1} \\norm{A(Bx)} & \\{\\text{Associativity}\\}\\\\\n&\\leq \\max_{\\norm{x}=1} \\norm{A} \\norm{Bx} & \\{\\text{Subordinate property}\\}\\\\\n&\\leq \\max_{\\norm{x}=1} \\norm{A} \\norm{B} \\norm{x} & \\{\\text{Subordinate property}\\}\\\\\n&= \\norm{A} \\norm{B} & \\{\\norm{x}=1\\}\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/move-semantics/index.html",
    "href": "posts/move-semantics/index.html",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "",
    "text": "To understand the basic principles of move semantics, let’s look at the execution of a small piece of code. I’ve written a toy Vector class. I choose the manage the memory myself, so I will follow the rule of three. I will supply a copy-constructor, copy-assignment operator and a destructor. I have also overloaded operator+() to support element-wise addition of two vectors.\n\n\nAssume that we have the following program:\n//basics/copy_semantics.cpp\n#include &lt;iostream&gt;\n#include &lt;stdexcept&gt;\n#include &lt;initializer_list&gt;\n\n\ntemplate &lt;typename T&gt;\nclass Vector {\nprivate:\n    int capacity_;\n    int size_;\n    T* ptr_;\n\npublic:\n    Vector() :capacity_{ 0 }, size_{ 0 }, ptr_{ nullptr } {}\n    Vector(int size) : capacity_{ size }, ptr_{ new T[size] }, size_{ size } {}\n    Vector(int size, T data) : Vector(size) {\n        for (int i{ 0 }; i &lt; size; ++i)\n            ptr_[i] = data;\n    }\n\n    Vector(std::initializer_list&lt;T&gt; list) {\n        clear();\n        for (const T& elem : list)\n            push_back(elem);\n    }\n\n    //Destructor\n    ~Vector()\n    {\n        clear();\n    }\n\n    //Copy constructor\n    Vector(const Vector& v)\n    {\n        if (this == &v)\n            return;\n\n        capacity_ = v.capacity_;\n        size_ = v.size_;\n        ptr_ = new T[v.size_];\n\n        for (int i{ 0 }; i &lt; v.size_; ++i)\n            ptr_[i] = v.ptr_[i];\n    }\n\n    //Copy assignment operator\n    Vector&lt;T&gt;& operator=(const Vector&lt;T&gt;& v)\n    {\n        if (this != &v)\n        {\n            delete[] ptr_;\n            ptr_ = nullptr;\n\n            capacity_ = v.capacity_;\n            size_ = v.size_;\n            ptr_ = new T[capacity_];\n\n            for (int i{ 0 }; i &lt; v.size_; ++i)\n                ptr_[i] = v.ptr_[i];\n        }\n\n        return *this;\n    }\n\n    T& operator[](int i)\n    {\n        if (i &gt;= 0 && i &lt; size_)\n            return ptr_[i];\n        else\n            throw std::out_of_range(\"Index out of bounds.\");\n    }\n\n    T& operator[](int i) const\n    {\n        if (i &gt;= 0 && i &lt; size_)\n            return ptr_[i];\n        else\n            throw std::out_of_range(\"Index out of bounds.\");\n    }\n\n    void reserve(int size)\n    {\n        if (size_ &lt; capacity_) return;\n\n        if (ptr_ == nullptr)\n        {\n            size_ = 0;\n            capacity_ = 0;\n        }\n\n        T* bufferNew = new T[size];\n        unsigned int l_size = std::min(capacity_, size);\n        for (int i{ 0 }; i &lt; l_size; ++i)\n        {\n            bufferNew[i] = ptr_[i];\n        }\n\n        if (ptr_ != nullptr)\n            delete[] ptr_;\n\n        ptr_ = bufferNew;\n        capacity_ = size;\n    }\n\n    void clear()\n    {\n        if (ptr_ != nullptr)\n            delete[] ptr_;\n        ptr_ = nullptr;\n        size_ = 0;\n        capacity_ = 0;\n    }\n\n    int size() const\n    {\n        return size_;\n    }\n\n    int capacity()\n    {\n        return capacity_;\n    }\n\n    void push_back(const T& elem)\n    {\n        if (size_ &gt;= capacity_) {\n            reserve(capacity_ + 5); // Double the capacity\n        }\n\n        ptr_[size_++] = elem;\n    }\n\n    void pop_back()\n    {\n        --size_;\n    }\n\n    T front()\n    {\n        if (size_ &gt; 0)\n            return ptr_[0];\n        else\n            throw std::out_of_range(\"Index out of bounds.\");\n    }\n\n    T back()\n    {\n        if (size_ &gt; 0)\n            return ptr_[size_ - 1];\n        else\n            throw std::out_of_range(\"Index out of bounds.\");\n    }\n\n    T* getRawPointer()\n    {\n        return ptr_;\n    }\n};\n\ntemplate &lt;typename T&gt;\nVector&lt;T&gt; operator+(const Vector&lt;T&gt;& v1, const Vector&lt;T&gt;& v2)\n{\n    if (v1.size() != v2.size())\n        throw std::logic_error(\"Vector lengths must be equal.\");\n    Vector&lt;T&gt; result;\n    for (int i{ 0 }; i &lt; v1.size(); ++i)\n        result.push_back(v1[i] + v2[i]);\n\n    return result;\n}\n\nVector&lt;Vector&lt;double&gt;&gt; createAndInsert()\n{\n    Vector&lt;Vector&lt;double&gt;&gt; pts;\n    pts.reserve(3);\n    Vector&lt;double&gt; x{ 1.0, 1.0 };\n    pts.push_back(x);\n    pts.push_back(x + x);\n    pts.push_back(x);\n    return pts;\n}\n\nint main()\n{\n    Vector&lt;Vector&lt;double&gt;&gt; result = createAndInsert();\n    return 0;\n}\nCompiler Explorer\nLet us look at the individual steps of the program (inspecting both stack and the heap) when we compile this program with a C++ compiler.\nFirst in main, we create the empty vector pts which will be used to store points in the euclidean space \\(\\mathbf{R}^2\\):\nVector&lt;Vector&lt;double&gt;&gt; pts;\nwhich is placed on the stack as an object that has size_ = 0, capacity_ = 0 and no memory allocated for elements.\nThen, we call\npts.reserve(3);\nThis allocates memory for 3 elements on the heap. The member pts_-&gt;capacity_ equals 3, pts-&gt;size_ equals 0 and pts_-&gt;ptr_ contains the address to heap block. The allocated memory is not initialized, because the number of elements is still 0.\nThen, we create a \\(2\\)-tuple to hold the cartesian coordinates of a point \\((1.0,1.0)\\). We create a Vector&lt;double&gt; initialized to {1.0,1.0}. Essentially, we create an object x on the stack with its members x-&gt;size_ = 2, x-&gt;capacity_ = 5 and a pointer x-&gt;ptr_ containing the address of newly allocated memory on the heap for 5 elements. Further, x-&gt;ptr_[0]=1.0, x-&gt;ptr_[1]=1.0.\nVector&lt;double&gt; x{1.0, 1.0};\nAfter this statement, the program has the following state: we have two objects on the stack : pts and x. Both of them have memory allocated on the heap.\n\n\n\nCheckpoint #1\n\n\nThe next step is the command to insert x into the pts vector.\npts.push_back(x);\nMy toy Vector class is said to have value semantics, which means it creates copies of the values passed to it. As a result, we get a first element in the vector, which is a full(deep) copy of the passed value/object x:\n\n\n\nCheckpoint #2\n\n\nThe current state is that we have a vector pts and two copies of x={1.0,1.0}, one of which is the first element in pts.\nLet’s now look at the next statement, which creates a new temporary vector and again inserts it into the pts vector:\npts.push_back(x + x);\nThis statement is performed in three steps:\nStep 1. We create a temporary Vector&lt;double&gt; object x + x.\n\n\n\nStep #1\n\n\nStep 2. x+x is a temporary. The Vector&lt;T&gt;::push_back(const T&) function accepts a reference-to-const as an argument. Since x+x is a temporary, it cannot be modified and binds to a reference-to-const. Moreover, being a temporary object, it is likely to die soon. Referencing it extends the lifetime of the temporary x + x={2.0,2.0}.\nNow, the statement pts_[size++] = elem will invoke the copy-assignment operator on the yet uninitialized second element pts[1] which is of type Vector&lt;double&gt;. This will force a full (deep) copy of x + x={2.0,2.0}. At this time, two copies of {2.0,2.0} exist on the heap. One of these is assigned to pts[1].\n\n\n\nStep #2\n\n\nStep 3. When push_back(const T&) returns, the temporary x + x will die and its destructor is called and the memory allocated on the heap is freed. You can see this on cppinsights.\nOur code is clearly not performing well: we create a copy of the temporary x + x and destroy the source of the copy immediately afterwards, which means we unnecessarily allocate and free memory that we could have just moved from source to the copy.\n\n\n\nStep #3\n\n\nWith the next statement, again we insert x into pts:\npts.push_back(x)\nAgain, pts copies x.\n\n\n\nCheckpoint #3\n\n\nThis is also something to improve. Because the value of x is no longer needed, some optimization could use the memory of x as the memory for the new element instead.\nAt the end of createAndInsert() we come to the return statement:\nreturn pts;\nHere, the behaviour of the program is a bit more complicated. We return by value (the return type is not a reference), which should be a copy of the value in the return statement. Creating a copy of pts means that we have create a deep copy of the whole vector with all of its elements. Thus, we have to allocate heap memory for the array of elements in the pts and heap memory for the value of each 2-tuple. Here, we would have to allocate memory 4 times.\nHowever, since at the same time pts is destroyed because we leave the scope where it is declared, the compiler is allowed to perform named return value optimization (NRVO). This means that the compiler can generate code so that pts is used as the return value.\nLet us assume that we have the named return value optimization. In that case, at the end of the return statement, pts simply becomes the return value and the destructor of x is called, which frees the memory allocated when it was declared.\n\n\n\nReturn statement\n\n\nFinally, we come to the assignment of the return value to result:\nresult = createAndInsert()\nHere, we really get behavior that can be improved: the usual assignment operator has the goal of giving result the same value as the source value that is assigned. In general, any source(assigned) value should not be modified and should independent from the object that the value was assigned to. So, the assignment operator will create a deep-copy of the whole return value:\n\n\n\nReturn statement\n\n\nHowever, right after that we no longer need the temporary return value and we destroy it:\nAgain, we create a copy of a temporary object and destroy the source of the copy immediately afterwards, which means that we again unnecessarily allocate and free memory. This time it applies to four allocations\nFor the state of the program after this assignment in main(), we allocated memory numerous times and released it. Unnecessary memory allocations were caused by:\n\nInserting a temporary object into pts.\nInserting an object into pts where we no longer need the value.\nAssigning a temporary vector with all its elements.\n\nWe can more or less avoid these performance pennalties."
  },
  {
    "objectID": "posts/move-semantics/index.html#motivation-for-move-semantics",
    "href": "posts/move-semantics/index.html#motivation-for-move-semantics",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "",
    "text": "To understand the basic principles of move semantics, let’s look at the execution of a small piece of code. I’ve written a toy Vector class. I choose the manage the memory myself, so I will follow the rule of three. I will supply a copy-constructor, copy-assignment operator and a destructor. I have also overloaded operator+() to support element-wise addition of two vectors.\n\n\nAssume that we have the following program:\n//basics/copy_semantics.cpp\n#include &lt;iostream&gt;\n#include &lt;stdexcept&gt;\n#include &lt;initializer_list&gt;\n\n\ntemplate &lt;typename T&gt;\nclass Vector {\nprivate:\n    int capacity_;\n    int size_;\n    T* ptr_;\n\npublic:\n    Vector() :capacity_{ 0 }, size_{ 0 }, ptr_{ nullptr } {}\n    Vector(int size) : capacity_{ size }, ptr_{ new T[size] }, size_{ size } {}\n    Vector(int size, T data) : Vector(size) {\n        for (int i{ 0 }; i &lt; size; ++i)\n            ptr_[i] = data;\n    }\n\n    Vector(std::initializer_list&lt;T&gt; list) {\n        clear();\n        for (const T& elem : list)\n            push_back(elem);\n    }\n\n    //Destructor\n    ~Vector()\n    {\n        clear();\n    }\n\n    //Copy constructor\n    Vector(const Vector& v)\n    {\n        if (this == &v)\n            return;\n\n        capacity_ = v.capacity_;\n        size_ = v.size_;\n        ptr_ = new T[v.size_];\n\n        for (int i{ 0 }; i &lt; v.size_; ++i)\n            ptr_[i] = v.ptr_[i];\n    }\n\n    //Copy assignment operator\n    Vector&lt;T&gt;& operator=(const Vector&lt;T&gt;& v)\n    {\n        if (this != &v)\n        {\n            delete[] ptr_;\n            ptr_ = nullptr;\n\n            capacity_ = v.capacity_;\n            size_ = v.size_;\n            ptr_ = new T[capacity_];\n\n            for (int i{ 0 }; i &lt; v.size_; ++i)\n                ptr_[i] = v.ptr_[i];\n        }\n\n        return *this;\n    }\n\n    T& operator[](int i)\n    {\n        if (i &gt;= 0 && i &lt; size_)\n            return ptr_[i];\n        else\n            throw std::out_of_range(\"Index out of bounds.\");\n    }\n\n    T& operator[](int i) const\n    {\n        if (i &gt;= 0 && i &lt; size_)\n            return ptr_[i];\n        else\n            throw std::out_of_range(\"Index out of bounds.\");\n    }\n\n    void reserve(int size)\n    {\n        if (size_ &lt; capacity_) return;\n\n        if (ptr_ == nullptr)\n        {\n            size_ = 0;\n            capacity_ = 0;\n        }\n\n        T* bufferNew = new T[size];\n        unsigned int l_size = std::min(capacity_, size);\n        for (int i{ 0 }; i &lt; l_size; ++i)\n        {\n            bufferNew[i] = ptr_[i];\n        }\n\n        if (ptr_ != nullptr)\n            delete[] ptr_;\n\n        ptr_ = bufferNew;\n        capacity_ = size;\n    }\n\n    void clear()\n    {\n        if (ptr_ != nullptr)\n            delete[] ptr_;\n        ptr_ = nullptr;\n        size_ = 0;\n        capacity_ = 0;\n    }\n\n    int size() const\n    {\n        return size_;\n    }\n\n    int capacity()\n    {\n        return capacity_;\n    }\n\n    void push_back(const T& elem)\n    {\n        if (size_ &gt;= capacity_) {\n            reserve(capacity_ + 5); // Double the capacity\n        }\n\n        ptr_[size_++] = elem;\n    }\n\n    void pop_back()\n    {\n        --size_;\n    }\n\n    T front()\n    {\n        if (size_ &gt; 0)\n            return ptr_[0];\n        else\n            throw std::out_of_range(\"Index out of bounds.\");\n    }\n\n    T back()\n    {\n        if (size_ &gt; 0)\n            return ptr_[size_ - 1];\n        else\n            throw std::out_of_range(\"Index out of bounds.\");\n    }\n\n    T* getRawPointer()\n    {\n        return ptr_;\n    }\n};\n\ntemplate &lt;typename T&gt;\nVector&lt;T&gt; operator+(const Vector&lt;T&gt;& v1, const Vector&lt;T&gt;& v2)\n{\n    if (v1.size() != v2.size())\n        throw std::logic_error(\"Vector lengths must be equal.\");\n    Vector&lt;T&gt; result;\n    for (int i{ 0 }; i &lt; v1.size(); ++i)\n        result.push_back(v1[i] + v2[i]);\n\n    return result;\n}\n\nVector&lt;Vector&lt;double&gt;&gt; createAndInsert()\n{\n    Vector&lt;Vector&lt;double&gt;&gt; pts;\n    pts.reserve(3);\n    Vector&lt;double&gt; x{ 1.0, 1.0 };\n    pts.push_back(x);\n    pts.push_back(x + x);\n    pts.push_back(x);\n    return pts;\n}\n\nint main()\n{\n    Vector&lt;Vector&lt;double&gt;&gt; result = createAndInsert();\n    return 0;\n}\nCompiler Explorer\nLet us look at the individual steps of the program (inspecting both stack and the heap) when we compile this program with a C++ compiler.\nFirst in main, we create the empty vector pts which will be used to store points in the euclidean space \\(\\mathbf{R}^2\\):\nVector&lt;Vector&lt;double&gt;&gt; pts;\nwhich is placed on the stack as an object that has size_ = 0, capacity_ = 0 and no memory allocated for elements.\nThen, we call\npts.reserve(3);\nThis allocates memory for 3 elements on the heap. The member pts_-&gt;capacity_ equals 3, pts-&gt;size_ equals 0 and pts_-&gt;ptr_ contains the address to heap block. The allocated memory is not initialized, because the number of elements is still 0.\nThen, we create a \\(2\\)-tuple to hold the cartesian coordinates of a point \\((1.0,1.0)\\). We create a Vector&lt;double&gt; initialized to {1.0,1.0}. Essentially, we create an object x on the stack with its members x-&gt;size_ = 2, x-&gt;capacity_ = 5 and a pointer x-&gt;ptr_ containing the address of newly allocated memory on the heap for 5 elements. Further, x-&gt;ptr_[0]=1.0, x-&gt;ptr_[1]=1.0.\nVector&lt;double&gt; x{1.0, 1.0};\nAfter this statement, the program has the following state: we have two objects on the stack : pts and x. Both of them have memory allocated on the heap.\n\n\n\nCheckpoint #1\n\n\nThe next step is the command to insert x into the pts vector.\npts.push_back(x);\nMy toy Vector class is said to have value semantics, which means it creates copies of the values passed to it. As a result, we get a first element in the vector, which is a full(deep) copy of the passed value/object x:\n\n\n\nCheckpoint #2\n\n\nThe current state is that we have a vector pts and two copies of x={1.0,1.0}, one of which is the first element in pts.\nLet’s now look at the next statement, which creates a new temporary vector and again inserts it into the pts vector:\npts.push_back(x + x);\nThis statement is performed in three steps:\nStep 1. We create a temporary Vector&lt;double&gt; object x + x.\n\n\n\nStep #1\n\n\nStep 2. x+x is a temporary. The Vector&lt;T&gt;::push_back(const T&) function accepts a reference-to-const as an argument. Since x+x is a temporary, it cannot be modified and binds to a reference-to-const. Moreover, being a temporary object, it is likely to die soon. Referencing it extends the lifetime of the temporary x + x={2.0,2.0}.\nNow, the statement pts_[size++] = elem will invoke the copy-assignment operator on the yet uninitialized second element pts[1] which is of type Vector&lt;double&gt;. This will force a full (deep) copy of x + x={2.0,2.0}. At this time, two copies of {2.0,2.0} exist on the heap. One of these is assigned to pts[1].\n\n\n\nStep #2\n\n\nStep 3. When push_back(const T&) returns, the temporary x + x will die and its destructor is called and the memory allocated on the heap is freed. You can see this on cppinsights.\nOur code is clearly not performing well: we create a copy of the temporary x + x and destroy the source of the copy immediately afterwards, which means we unnecessarily allocate and free memory that we could have just moved from source to the copy.\n\n\n\nStep #3\n\n\nWith the next statement, again we insert x into pts:\npts.push_back(x)\nAgain, pts copies x.\n\n\n\nCheckpoint #3\n\n\nThis is also something to improve. Because the value of x is no longer needed, some optimization could use the memory of x as the memory for the new element instead.\nAt the end of createAndInsert() we come to the return statement:\nreturn pts;\nHere, the behaviour of the program is a bit more complicated. We return by value (the return type is not a reference), which should be a copy of the value in the return statement. Creating a copy of pts means that we have create a deep copy of the whole vector with all of its elements. Thus, we have to allocate heap memory for the array of elements in the pts and heap memory for the value of each 2-tuple. Here, we would have to allocate memory 4 times.\nHowever, since at the same time pts is destroyed because we leave the scope where it is declared, the compiler is allowed to perform named return value optimization (NRVO). This means that the compiler can generate code so that pts is used as the return value.\nLet us assume that we have the named return value optimization. In that case, at the end of the return statement, pts simply becomes the return value and the destructor of x is called, which frees the memory allocated when it was declared.\n\n\n\nReturn statement\n\n\nFinally, we come to the assignment of the return value to result:\nresult = createAndInsert()\nHere, we really get behavior that can be improved: the usual assignment operator has the goal of giving result the same value as the source value that is assigned. In general, any source(assigned) value should not be modified and should independent from the object that the value was assigned to. So, the assignment operator will create a deep-copy of the whole return value:\n\n\n\nReturn statement\n\n\nHowever, right after that we no longer need the temporary return value and we destroy it:\nAgain, we create a copy of a temporary object and destroy the source of the copy immediately afterwards, which means that we again unnecessarily allocate and free memory. This time it applies to four allocations\nFor the state of the program after this assignment in main(), we allocated memory numerous times and released it. Unnecessary memory allocations were caused by:\n\nInserting a temporary object into pts.\nInserting an object into pts where we no longer need the value.\nAssigning a temporary vector with all its elements.\n\nWe can more or less avoid these performance pennalties."
  },
  {
    "objectID": "posts/move-semantics/index.html#copy-elison",
    "href": "posts/move-semantics/index.html#copy-elison",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "Copy elison",
    "text": "Copy elison\nCopy-elison is based on the fact that the compiler is allowed to follow the as-if rule. The compiler is allowed to generate any code which has the same effect as the code you told it to add. The standard actually says, that if the compiler is told to copy something, but the copy is not really necessary, because the original is not going to be used again, then the compiler is allowed to elide(omit) the copy. The compiler is allowed to elide copies, where the results are as-if copies were made.\nConsider the function below:\n\n#include &lt;cmath&gt;\ndouble discountFactor(double r, double t){\n    double result = exp(-r * t);\n    return result;\n}\n\nint main()\n{\n    double df {discountFactor(0.05, 1.00)};\n    return 0;\n}\nHow many parameters are passed to the function discountFactor(double, double)? C++ programmers answer \\(2\\), assembly-language programmers answer \\(3\\). Why? At a low-level, when we have a return-value, we have to tell the generated code, where to put the return value. The function is passed the address where the results should be written.\nAlright, so this is what’s going on. Our function main() is going to call discountFactor(double, double) in order to populate a local df. The stack frame for the function main() looks like this:\n\n\n\nStack frame for main()\n\n\nNow, we are going to call the function discountFactor(double, double). When we call discountFactor(double, double), we have to create the stack-frame for discountFactor(double, double).\n\n\n\nCall to discountFactor()\n\n\nOkay, now we execute the function discountFactor(double, double) and now the return value is now stored directly at the address given by &df.\n\n\n\nReturn statement\n\n\nSo, this is going to elide the copy. This form of copy elison is called Return Value Optimization(RVO). The calling function allocates space for the return value on the stack, and passes the address of that memory to the callee. The callee can then construct a return value directly into that space, which eliminates the need to copy from the inside to the outside.\nAlso, although the compiler is normally required to make a copy when a function parameter is passed by value (so modifications to the parameter inside the function can’t affect the caller), it is allowed to elide the copy, when the source is a temporary.\nvoid f(std::string a)\n{\n    int b{123};\n    //some code\n    return;\n}\n\nvoid g()\n{\n    f(std::string(\"A\"));\n    std::vector&lt;int&gt; y;\n}\nThis is how it actually works. We are going to create our temporary - the string \"A\" in the place, where we would have actually copied it, that is, in the local variable a in the stack frame of f(std::string)."
  },
  {
    "objectID": "posts/move-semantics/index.html#copy-and-swap-idiom",
    "href": "posts/move-semantics/index.html#copy-and-swap-idiom",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "Copy and swap idiom",
    "text": "Copy and swap idiom\nAny class which manages resources (a wrapper like a smart pointer) needs to implement The Big Three. While the goals of the copy constructor and destructor are straightforward, the copy-assignment operator is arguably the most nuanced and difficult. What pitfalls need to be avoided?\nConsider our naive implementation of the assignment operator:\n//Copy assignment operator\nVector&lt;T&gt;& operator=(const Vector&lt;T&gt;& v)\n{\n    if (this != &v)     //(1)\n    {\n        delete[] ptr_;  //(2)\n        ptr_ = nullptr;\n\n        capacity_ = v.capacity_;    //(2)\n        size_ = v.size_;            //(2)\n        ptr_ = new T[capacity_];    //(2)\n\n        for (int i{ 0 }; i &lt; v.size_; ++i)\n            ptr_[i] = v.ptr_[i];                //(3)\n    }\n\n    return *this;\n}\nWhile this manages the heap memory without leaks, it suffers from three problems, marked sequentially in the code as (n).\n\nThe first is the self-assignment test. This check is an easy way to prevent us from running needless code on self-assignment. But, in all other cases, it merely serves to slow down the program and acts as noise in the code; self-assignment rarely occurs, so most of the time this check is a waste.\nThe seond is, it only provides a basic exception guarantee. If new T[capacity_] fails, *this will have been modified. Namely, the size_ and capacity_ are wrong and the old data referenced by ptr_ is gone! For a strong exception guarantee, we need something akin to:\n\n//Copy assignment operator\nVector&lt;T&gt;& operator=(const Vector&lt;T&gt;& v)\n{\n    if (this != &v)     //(1)\n    {\n        // get the new data ready, before we replace the old data\n        int newCapacity = v.capacity_;\n        int newSize = v.size_;\n        T* newPtr_ = new T[newCapacity]();  //(2)\n\n        for (int i{ 0 }; i &lt; v.size_; ++i)\n            newPtr_[i] = v.ptr_[i];         //(3)\n\n        //replace the old data (all are non-throwing)\n        delete[] ptr_;\n        size_ = newSize;       \n        capacity_ = newCapacity_;           \n        ptr_ = newPtr;                      \n    }\n\n    return *this;\n}\n\nOur code has expanded! This leads us to the third problem: code duplication.\n\nIn our case, the core of it is only two lines (the allocation and the copy), but with more complex resources, this code bloat can be quite a hassle. What if my class manages more than one resource?\nThe copy-and-swap idiom is the solution, and elegantly assists the assignment operator in achieving two things: avoiding code duplication and providing a strong exception guarantee.\nWhile, the rule-of-three successfully entails our copy-constructor, assignment and destructor, it should really be called The Big Three and A Half: any time your class manages a resource, it also makes sense to provide a swap function.\nA swap function is a non-throwing function that swaps two objects of a class member-for-member.\nWe need to add swap functionality to our class, and we do that as follows:\nvoid swap(Vector& other) noexcept\n{\n    std::swap(size_, other.size_);\n    std::swap(capacity_,other.capacity_);\n    std::swap(ptr_, other.ptr_);\n}\n\nfriend void swap(Vector& lhs, Vector& rhs) noexcept{\n    std::swap(lhs.size_, rhs.size_);\n    std::swap(lhs.capacity_, rhs.capacity_);\n    std::swap(lhs.ptr_, rhs.ptr_);\n}\nNow, this is extremely efficient, it merely swaps pointers and sizes rather than allocating and copying entire arrays. We are now ready to implement the copy-and-swap idiom.\nOur assignment operator is:\nVector& operator=(Vector other) \n{\n    std::swap(*this, other);    //(2)\n    return *this;\n}\n\nWhy does it work?\nWe first notice an important choice : the parameter argument is taken by-value. While one could just as easily do the following(and indeed many naive implementations of the idiom do):\nVector& operator=(const Vector& other)\n{\n    Vector temp {other}; \n    std::swap(*this, temp);\n    return *this;\n}\nWe lose an important optimization opportunity - if a temporary is passed, the compiler will not perform copy elison. Not only that, but this choice is critical in C++ 11, which is discussed later. (On a general note, a remarkably useful guideline is as follows: if you’re going to make a copy of something in a function, let the compiler do it in the parameter list).\nEither way, this method of obtaining our resource is key to eliminating code duplication: we get to use the code from the copy-constructor to make the copy, and never need to repeat any bit of it. Now that, the copy is made, we are ready to swap.\nObserve that upon entering the function, the new data is already allocated, copied and ready to be used. This is what gives us a strong exception guarantee: we won’t even enter the function if the construction of the copy fails, and therefore it’s not possible to alter the state of *this. The assignment operator guarantees that the operations call will be fully rolled back in case of an error, as if the error never happened.\nConceptually, it works by using the copy-constructor’s functionality to create a local copy of the data, then takes the copied data with a swap() function swapping the old data with the new data.\nTo summarize: in order to use the copy-and-swap idiom, we need three things: a working copy-constructor, working destructor (both are the basis of any wrapper, so should be complete anyway), and a swap function."
  },
  {
    "objectID": "posts/move-semantics/index.html#value-categories",
    "href": "posts/move-semantics/index.html#value-categories",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "Value Categories",
    "text": "Value Categories\nIn C++, every expression is either an lvalue or an rvalue. Consider an object that owns some resources(file-descriptors, sockets, memory buffer).\n\nAn lvalue denotes an object whose resources cannot be reused. The object is an lvalue, if you can’t take the guts(resources) out of this object and donate it to someone else. lvalues include expressions that designate objects by their name. For example, in the expression double y = f(x), y is an lvalue. Moreover, lvalues have persistent storage and an identifiable memory address. For instance, if I declare std::vector&lt;double&gt; v{1.0,2.0,3.0,4.0,5.0};, then v[0] is an lvalue.\nAn rvalue denotes an object whose resources can be reused. The object is an rvalue, if you can take the guts(resources) out of it and donate it to another object. rvalues typically include temporary objects as they can’t manipulated at the place they are created and are likely to be destroyed soon. For instance, if declare int x = 5;, 5 is an rvalue. Moreover, in the statement double y = f(x);, the expression f(x) is an rvalue."
  },
  {
    "objectID": "posts/move-semantics/index.html#moving-data",
    "href": "posts/move-semantics/index.html#moving-data",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "Moving data",
    "text": "Moving data\nAs seen earlier, C++ sometimes performs unnecessary copying.\nVector&lt;double&gt; x;\nx = Vector&lt;double&gt;({\n        1.0, 2.0, 3.0, 4.0, 5.0, \n        6.0, 7.0, 8.0, 9.0, 10.0\n    });\ncppinsights produces the following annotations:\nVector&lt;double&gt; x = Vector&lt;double&gt;();\nconst double __temporary179_5[10] = {\n    1.0, 2.0, 3.0, 4.0, 5.0, \n    6.0, 7.0, 8.0, 9.0, 10.0\n};\nconst Vector&lt;double&gt; __temporary179_6 = Vector&lt;double&gt;(Vector&lt;double&gt;(std::initializer_list&lt;double&gt;{__temporary179_5, 10}));\nx.operator=(__temporary179_6);\n__temporary179_6.~Vector();\n/* __temporary179_5 // lifetime ends here */\nIn the above code snippet, the temporary vector of reals \\(\\{1.0,2.0,3.0,\\ldots,10.0\\}\\) is copied element-wise to x and then destroyed immediately after. We’ve wasted a lot of energy in deep-copying.\nSimilarly, appending to a full vector causes much copying before the append. That is not what we want to do.\nWhat we really want to do is, transfer the contents of __temporary19_6 vector to x in a very simple way. Firstly, we copy the pointers; we cannot stop there, because at this point there are two Vector&lt;T&gt; objects owning the same memory resource.\n\n\n\nStep 1. Copy the pointers\n\n\nThe second step is, of course to set the pointers of the temporary vector to nullptr.\n\n\n\nStep 2. Zero out the members of __temp\n\n\nThat looks great and this is cheap! We are doing the minimum amount of work to transfer the contents of the temporary into x.\nAt the end of the assignment operation, the temporary goes out of scope and the vector \\(\\{1,2,3,\\ldots,10\\}\\) is in x. How do we implement this logic programmatically?\nIn addition to the copy-constructor, we write a move constructor. A move constuctor simply moves the data by taking ownership of the pointer that refers to the data, leaving the data itself where it resides.\n// move constructor\nVector(Vector&& src) noexcept\n{\n    // Just swap the memory pointers\n    std::swap(src, *this);\n}\n\nrvalue references in detail\nThe constructor takes an argument of the type rvalue reference. rvalue references are declared two ampersands. lvalues bind to lvalue references. When taking a reference to a temporary object, an rvalue, you have two choices. rvalues can bind to:\n\nA const lvalue reference.\nA mutable rvalue reference.\n\nconst std::string& r1 {\"hello\"};    \nstd::string& r2 {\"world\"};\n\nconst Vector&lt;int&gt;& r3 {1,2,3,4,5};\nVector&lt;int&gt;&& r4{6,7,8,9,10};\nAll these references have the semantics of - we can steal/modify the resources of the object we refer to, provided the state of the object remains a valid state. Technically, these semantics are not checked by compilers, so we can modify an rvalue reference as we can do with any non-const object of the type. We might also decide not to modify the value.\nstd::string&& r1 = \"hello\";    \nr1 += \"world\"; \n\nVector&lt;int&gt;&& r2 {1,2,3,4,5};\nr2.push_back(6);\nAnd it’s a logic error to take a mutable lvalue reference to a temporary, so this is disallowed in the language:\n// std::string& r1 = \"hello\";    //error: this is not possible\n// r1 += \"world\"; \n// Vector&lt;int&gt;& r2 {1,2,3,4,5};\n// r2.push_back(6);\nAssigning a temporary to a reference extends the lifetime of the temporary so that it matches the lifetime of the reference. So, this is legal:\nint main()\n{\n    {\n        const std::string& s = foo();\n        std::cout &lt;&lt; s &lt;&lt; std::endl;    //the temporary to which s refers is still alive\n    }\n    //but now it's destroyed\n    return 0;    \n}\nAnd so is this:\nstd::string foo(){ return \"foo\";};      //function that returns a string\n\nvoid bar(std::string&& s){              // extends the lifetime as before\n    std::cout &lt;&lt; s; \n}    \n\nint main()\n{\n    bar(foo());     \n    return 0;\n}\n\n\nrvalue references as parameters\nWhen we declare a parameter to be an rvalue reference, it has exactly the behavior and semantics as introduced above:\n\nThe parameter can only bind to a temporary object or an rvalue.\nAccording to the semantics of rvalue references:\n\nThe caller claims that it is no longer interested in the object. Therefore, you can steal the guts of object, take ownership of its resources.\nHowever, the caller might still be interested in using the object. Therefore, any modification should keep the referenced object in a valid state."
  },
  {
    "objectID": "posts/move-semantics/index.html#stdmove",
    "href": "posts/move-semantics/index.html#stdmove",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "std::move()",
    "text": "std::move()\nHey, this is cool! Why don’t we apply these ideas to the below example?\nVector&lt;int&gt; v1 {1, 2, 3, 4, 5};\nVector&lt;int&gt; v2 {v1};\nWell, in this case, we would have a problem. v1 has a name, it has a persistent storage location and a memory address, it is an lvalue. You can’t steal the contents of v1.\nBut, we can do something about this. If indeed you are interested to transfer the contents of v1 into v2, then all we need to do is use std::move.\nVector&lt;int&gt; v1 {1, 2, 3, 4, 5};\nVector&lt;int&gt; v2 {std::move(v1)};\nstd::move() is a function that you can think of as performing an unconditional cast of its argument to an rvalue reference. std::move(v1) marks v1 to be movable. It does not physically move anything. It signals, that the object v1 may be moved from.\nIf you have an lvalue, an object for which the lifetime does not end when you use it, you can mark it with std::move() to express I no longer need this object here. std::move does not move; it only sets a temporary marker in the context where the expression is used:\nvoid foo1(const std::string& lr);    //binds to the passed object without modifying it\nvoid foo1(std::string&& rv);         //binds to the passed object and might steal/modify its contents\n\nstd::string s{\"hello\"};\n\nfoo1(s);                             //calls the first foo(), s keeps its value\nfoo1(std::move(s));                  //calls the second foo(), s might lose its value\n                                     //semantically s no longer legal to access\nObjects marked with std::move() can still be passed to a function that takes an ordinary const lvalue reference. Consider another code snippet:\nvoid foo2(const std::string& lr);   //binds to the passed object without modifying it\n                                    //no other overload of foo2()\nfoo2(s);                            // calls foo2(), s keeps its value\nfoos(std::move(s))                  // calls foo2(), s keeps its value because we know that\n                                    // foo2() can't modify or take ownership of the contents of s.\nSemantically, s is still legal to access after the execution of the last line. Because there’s overload of foo2(const std::string&&), there is no ways its contents can be modified or transferred.\nNote that, an object marked with std::move() cannot be passed to a non-const lvalue reference.\n\nHeader file for std::move()\nstd::move() is defined as a function in the C++ standard library. To use it, you have to include the header file &lt;utility&gt; where it is defined:\n\n\nImplementation of std::move()\nstd::move() is nothing but a static_cast to an rvalue reference. You can achieve the same effect by calling static_cast manually as follows:\nfoo(static_cast&lt;decltype(obj)&&&gt;(obj));     //same effect foo(std::move(obj))\nTherefore, we could also write:\nstd::string s{\"hello\"};\nfoo(static_cast&lt;std::string&&&gt;(s));"
  },
  {
    "objectID": "posts/move-semantics/index.html#moved-from-objects",
    "href": "posts/move-semantics/index.html#moved-from-objects",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "Moved-from objects",
    "text": "Moved-from objects\nAfter a std::move(), moved-from objects are not (partially) destroyed. They are still valid objects for which at least the destructor will be called. However, they should also be valid in the sense that they have a consistent state and all operations work as expected. The only thing you do not know is their contents.\n\nValid but unspecified state\nThe C++ standard library guarantees that moved-from objects are in a valid but unspecified state. Consider the following code:\nstd::string s{\"hello\"};\nstd::vector&lt;std::string&gt; coll{};\ncoll.push_back(std::move(s));\nAfter passing s with std::move() you can ask for the number of characters, print out the value, or even assign a new value. However, you cannot print the first character or any other character without checking the number of characters first:\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;string&gt;\n#include &lt;utility&gt;\n\nint main()\n{\n    std::string s{\"hello\"};\n    std::vector&lt;std::string&gt; coll{};\n    coll.push_back(std::move(s));   //keeps in a valid but unclear state\n    std::cout &lt;&lt; \"Contents of s : \" &lt;&lt; s &lt;&lt; \"\\n\";     //ok (don't know which value is written)\n    std::cout &lt;&lt; \"size : \" &lt;&lt; s.size() &lt;&lt; \"\\n\"; //ok (rites the number of characters)\n    // std::cout &lt;&lt; \"[0] = \" &lt;&lt; s[0] &lt;&lt; \"\\n\"; //error (potentially undefined behavior)\n    s = \"new value\";    // ok\n    return 0;\n}\nCompiler Explorer\nstdout\nContents of s : \nsize : 0\n\n\nReusing moved-from objects\nWe might wonder why moved-from objects are still valid objects and are not (partially) destroyed. The reason is that there are useful applications of move semantics, where it makes sense to use moved-from objects again.\nFor example, consider code where we read chunks of data from a network socket or read strings line-by-line from a file stream and move them into a vector:\nstd::vector&lt;std::string&gt; allRows;\nstd::string row;\n\nwhile(std::getline(myStream, row)){ //read next line into row\n    allRows.push_back(std::move(row));  //and move it to somewhere\n}\nEach time after we read a line into row, we use std::move() to move the value of row into the vector of all rows. Then, std::getline() uses the moved-from object row again to read the next line into it.\nAs a second example, consider a generic function that swaps two values:\ntemplate &lt;typename T&gt;\nvoid swap(T& a, T& b)\n{\n    T tmp{std::move(a)};\n    a = std::move(b);       //assign new value to moved-from a\n    b = std::move(temp);    //assign new value to moved-from b\n}\nHere, we move the value of a into a temporary object to be able t move-assign the value of b afterwards. The moved-from object b then receives the value of tmp, which is the former value of a.\nCode like this is used in sorting algorithms for example, sorting a vector of buy/sell orders in the order book by the bid/ask prices.\n\n\nMove assignments of objects to themselves\nThe rule that moved-from objects are in a valid but unspecified state usually also applies to objects after a direct or indirect self-move.\nFor example, after the following statement, the object x is usually valid without its value being known:\nx = std::move(x);   //afterwards x is valid but has an unclear value"
  },
  {
    "objectID": "posts/move-semantics/index.html#the-canonical-move-constructor-and-move-assignment-operator",
    "href": "posts/move-semantics/index.html#the-canonical-move-constructor-and-move-assignment-operator",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "The canonical move constructor and move assignment operator",
    "text": "The canonical move constructor and move assignment operator\nConsider the below Widget class as an example. The canonical move constructor and move assignment operators are written as follows:\nclass Widget{\n    private:\n        int i;\n        std::string s{};\n        int* resource;            // Owning pointer\n\n    public:\n    // Move constructor\n    Widget(Widget&& rhs) noexcept //Phase 1: member-wise swap\n        : i {std::move(rhs.i)}\n        , s {std::move(rhs.s)}\n        , resource{std::move(rhs.resource)}\n    {\n        rhs.resource = nullptr; // Phase 2: reset the move-from object\n    }\n\n    // Move assignment operator\n    Widget& operator=(Widget&& src)\n    {\n        delete resource;            //Phase 1: Cleanup\n        std::swap(src, *this);      //Phase 2: Member-wise move\n        src-&gt;resource = nullptr;    //Phase 3: Reset\n        return *this;\n    }\n\n    Widget::Widget& operator=(Widget src);\n}\nAn owning-pointer such int* is special, and it has to be dealt with separately.\nRaw pointers are bad (especially owning raw pointers). In this case, the declaration doesn’t indicate whether it points to an element or an array.\nIf instead, we have a smart-pointer, then what I can do is omit is phase 2.\nclass Widget{\n    private:\n        int i;\n        std::string s{};\n        int* resource;      \n\n    public:\n    // Move constructor\n    Widget(Widget&& rhs) noexcept //Phase 1: member-wise swap\n        : i {std::move(rhs.i)}\n        , s {std::move(rhs.s)}\n        , resource{std::move(rhs.resource)}\n    {}\n\n    // Move assignment operator\n    Widget& operator=(Widget&& src)\n    {\n        std::swap(src, *this); \n        return *this;\n    }\n\n    Widget::Widget& operator=(Widget src);\n}\nI would like to show you one more thing. The canonical copy assignment operator also doubles up as a move-assignment operator.\n// Copy/Move assignment operator\nWidget::Widget& operator=(Widget src)  //Copy/move constructor called \n{\n    std::swap(src, *this); \n    return *this;\n}\n\nint main()\n{\n    Widget w1(5,\"hello\", new int(10)),\n    Widget w2 = w1;     //copy/move assignment operator called\n    Widget w3 = std::move(w1);  //copy/move assignment operator called\n}\nIn the assignment statement Widget w2 = w1;, first the copy constructor is called and the contents of w1 are copied to src, before the control enters the body of operator=(Widget). Whereas the assignment statement Widget w3 = std::move(w1) results in the invocation of the move constructor and the contents of w1 are transferred to w3 before we execute the body of the assignment operator."
  },
  {
    "objectID": "posts/move-semantics/index.html#avoiding-unnecessary-stdmove",
    "href": "posts/move-semantics/index.html#avoiding-unnecessary-stdmove",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "Avoiding unnecessary std::move()",
    "text": "Avoiding unnecessary std::move()\nAs we saw, returning a local object by value automatically uses move semantics if supported. However, to be safe, programmers might try to force this with an explicit std::move():\nstd::string foo()\n{\n    std::string s;\n    // do something\n    // ...\n    return std::move(s);    //Bad, don't do this\n}\nRemember that std::move() is just a static_cast to an rvalue reference. Therefore, std::move is an expression that yields the type std::string&&. However, this no longer matches the return type and therefore disables return value optimization, which usually allows the returned object to be used as a return value. For types where move semantics is not implemented, this might even force the copying of the return value instead of just using the returned object as the return value."
  },
  {
    "objectID": "posts/move-semantics/index.html#value-categories-in-detail",
    "href": "posts/move-semantics/index.html#value-categories-in-detail",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "Value categories in detail",
    "text": "Value categories in detail\nTo compile an expression or statement it does not only matter whether the involved types fit. For example, you cannot assign an int to an int, when on the left hand side of the assignment, an int literal is used.\nint i{};\ni = 88;     //Ok\n//88 = i;   //Error\nFor this reason, each expression in C++ has a value category. Besides the type, the value category is essential to decide what you can do with an expression.\n\nValue categories since C++11\n\n\n\nValue Categories\n\n\nWe have the following primary categories:\n\nlvalue (Locator Value)\nprvalue (Pure Readable Value)\nxvalue (Expiring Value)\n\nThe composite categories are: - glvalue (generalized lvalue) as a common term for lvalue or xvalue - rvalue as a common term for xvalue or prvalue\nIntuitively, it’s easy to understand the primary value categories, if you look at the following diagram:\n\n\n\nValue Categories\n\n\nFor example,\nclass X{\n};\n\nX v;\nconst X c;\n\nf(v);   //passes a modifiable lvalue\nf(c);   //passes a non-modifiable lvalue\nf(X()); //passes a prvalue (old syntax of creating a temporary)\nf(X{}); //passes a prvalue (new syntax of creating a temporary)\nf(std::move(v));  //passes an xvalue\nRoughly speaking, as a rule of thumb:\n\nAll names used as expressions are lvalues.\nAll string literals used as expressions are lvalues.\nAll non-string literals used as expressions are prvalues.\nAll temporaries without a name (especially objects returned by value) are prvalues.\nAll objects marked with a std::move are xvalues.\n\n\n\nCopy Elison since C++17\nC++17 added mandates to the standard, formally known as :\n\nGuaranteed copy elison\nGuraranteed return value optimization\nCopy evasion\n\nIf, in an initialization of an object, when the initializer expression is prvalue of the same class type as the variable type, copy elison is guaranteed.\n#include &lt;iostream&gt;\n\nclass T{\n    public:\n    T(){ std::cout &lt;&lt; \"c'tor T()\\n\";}\n    T(const T& t){ std::cout &lt;&lt; \"c'tor T(const T& t)\\n\";}\n    T(T&& t){ std::cout &lt;&lt; \"c'tor T(T&& t)\\n\";}\n};\nT x{T{}};\nIn C++17, this is equivalent to T x{};. The default constructor is invoked only once.\nSimilarly, if, in a return statement the operand is a prvalue of the same class type as the function return type, copy elison is guaranteed.\nT func()\n{\n    return T{};\n}\n\nT x{func()}; //Only one default construction of T allowed here\nUnder the rules of C++17, under the hood, a prvalue will be used only as unmaterialized recipe of an object, until actual materialization is required.\nA prvalue is an expression whose evaluation initializes/materializes an object. This is called as temporary materialization conversion.\nclass  T{\n    public:\n    T(){\n        std::cout &lt;&lt; \"c'tor T()\\n\";\n    }\n    //delete move and copy constructors\n    T(const T& other) = delete;\n    T(T&& other) = delete;\n}\n\nT make(){\n    //Creates the first temporary (pre C++17)\n    return T{};\n}\n\nint main(){\n    // Construct the second temporary (pre C++17)\n    // Copy/move temporary into N using the = operator (pre C++17)\n    T t = make();\n    return 0;\n}\nPre C++17, the function make() would construct a temporary within its scope. This temporary would then be copied/moved into another temporary within the main scope. Finally, the operator= would build t via copy/move construction. All of this temporary business would be elided by (RVO) by any decent compiler resulting in make() constructing a single object within t. However, this elison is somewhat optional, so the compiler must also demand that copy and move constructors exist, just in case. The above code does not compile with any pre C++17 compiler.\nPost C++17, make() creates an object of type T within t. Avoiding excessive use of temporary objects is now a language feature and the reliance on compiler optimization is removed. The above code does compiler with a post C++17 compiler.\n\n\nValue Categories since C++17\nC++17 has the same value categories but clarified the semantic meaning of the value categories as described in the figure above.\nThe key approach for explaining value categories now is that in general, we have two major kinds of expressions:\n\nglvalues: expressions for locations of long-living objects or functions.\nprvalues: expressions for short-living values for initializations.\n\nAn xvalue is then considered a special location, representing a (long-living) object, whose resources/values are no longer needed.\nLoosely speaking, prvalues themselves do not exist somewhere in memory, they do not denote objects. They are used for initialization. In C++17, prvalues are not moved from. It doesn’t make sense to talk about whether you can steal it’s resources."
  },
  {
    "objectID": "posts/move-semantics/index.html#perfect-forwarding",
    "href": "posts/move-semantics/index.html#perfect-forwarding",
    "title": "A hitchhiker’s guide to move semantics and perfect forwarding",
    "section": "Perfect Forwarding",
    "text": "Perfect Forwarding\n\nMotivation for perfect forwarding\nConsider a function that declares the parameter as rvalue reference:\nvoid f(std::string&& s)\n{\n    // Do something\n    std::cout &lt;&lt; \"string s : \" &lt;&lt; s &lt;&lt; \"\\n\";\n}\nAs we’ve learned, we can only pass rvalues to this function:\nstd::string s{\"hello\"};\n// f(s);                // Error : passing an lvalue to an rvalue ref\nf(std::move(s));        // okay, passing an xvalue\nf(std::string(\"world\"));// okay, passing a prvalue\nHowever, when we use the parameter s inside the function f(std::string&&), we are dealing with an object that has a name. This means that we use s as an lvalue. We can do only what we are allowed to do with an lvalue. This means that we cannot call our function recursively.\nvoid f(std::string&& s)\n{\n    // Do something\n    std::cout &lt;&lt; \"string s : \" &lt;&lt; s &lt;&lt; \"\\n\";\n    // f(s);    // Error: passing an lvalue to an rvalue reference\n}\nWe have to mark s with std::move() again:\nvoid f(std::string&& s)\n{\n    // Do something\n    std::cout &lt;&lt; \"string s : \" &lt;&lt; s &lt;&lt; \"\\n\";\n    f(std::move(s));    // Ok, passing an xvalue\n}\nThis is the formal specification of the rule that move semantics is not passed through.\nTo forward an object that is passed with move semantics to a function, it not only has to be bound to an rvalue reference; you have to use std::move() again to forward its move semantics to another function.\nFor example:\nclass X{\n    // ...\n};\n\n//forward declarations\nvoid foo(const X&);     //for constant values (read-only access)\nvoid foo(X&);           //for variable values (out parameters)\nvoid foo(X&&);          //for values that are no longer used(move semantics)\nWe have the following rules when calling these functions:\nX v;\nconst X c;\n\nfoo(v);     //calls foo(X&)\nfoo(c);     //calls foo(const X&)\nfoo(X{});   //calls foo(X&&)\nfoo(std::move(v))   //calls foo(X&&)\nfoo(std::move(c))   //calls foo(const X&)\nNow, assume that we want to call foo() for the same arguments indirectly via a helper function callFoo(). That helper function would also need the three overloads.\nvoid callFoo(const X& arg){     //arg binds to all const objects\n    foo(arg);                   //calls foo(const X&)\n}\n\nvoid callFoo(X& arg)            //arg binds to lvalues\n{\n    foo(arg);                   //calls foo(&)\n}\n\nvoid callFoo(X&& arg){          //arg binds to rvalues\n    foo(std::move(arg));        //needs std::move() to call foo(X&&)\n}\nIn all cases, arg is used as an lvalue (being an object with a name). The first version forwards it as a const object, but the other two cases implement two different ways to forward the non-const argument.\n\nArguments declared as lvalue references (that bind to objects that do not have move semantics) are passed as they are.\nArguments declared as rvalue references (that bind to objects that have move semantics) are passed with std::move.\n\nThis allows us to forward move semantics perfectly: for any argument that is passed with move semantics, we keep the move semantics; but we do not add move semantics when we get an argument that does not have it.\nOnly with this implementation is the use of callFoo to call foo transparent.\nX v;\nconst X c;\n\ncallFoo(v);     //calls foo(X&)\ncallFoo(c);     //calls foo(const X&)\ncallFoo(X{});   //calls foo(X&&)\ncallFoo(std::move(v))   //calls foo(X&&)\ncallFoo(std::move(c))   //calls foo(const X&)\nRemember that an rvalue passed to an rvalue reference becomes an lvalue when used, which means that we need std::move() to pass it as an rvalue again. However, we cannot use std::move() everywhere. For the other overloads, using std::move() would call the overload of foo() for rvalue references when an lvalue is passed.\nFor perfect forwarding in generic code, we would always need all these overloads for each parameter. To support all combinations, this means having \\(3^2 = 9\\) overloads for \\(2\\) generic arguments and \\(3^3 = 27\\) overloads for \\(3\\) generic arguments.\nTherefore, C++11 introduced a special way to perfectly forward without any overloads but still keeping the type and the value category.\n\n\nImplementing perfect forwarding\nTo avoid overloading functions for parameters with different value categories, C++ introduced the mechanism of perfect forwarding. You need three things:\n\nTake the call parameter as a pure rvalue reference (delcared with && but without const or volatile)\nThe type of the parameter has to be a template parameter of the function.\nWhen forwarding the parameter to another function, use a helper function called std::forward&lt;&gt;() which is declared in &lt;utility&gt;.\n\nYou have to implement a function that perfectly forwards an argument as follows:\ntemplate&lt;typename T&gt;\nvoid callFoo(T&& arg)\n{\n    foo(std::forward&lt;T&gt;(arg));\n}\nstd::forward&lt;&gt;() is defined as a conditional std::move(), so that we get the same behavior as the three (or four) overloads of callFoo() above:\n\nIf we pass an rvalue to arg, we have the same effect as calling foo(std::move(arg)).\nIf we pass an lvalue to arg, we have the same effect as calling foo(arg).\n\nWhat exactly is happening here, is pretty tricky and needs a careful explanation.\n\n\nUniversal and Forwarding references\nFirst note that we declare arg as an rvalue reference parameter:\ntemplate&lt;typename T&gt;\nvoid callFoo(T&& arg)\nThis might give the impression that the rules of rvalue references apply. However, that is not the case. An rvalue reference (not qualified with const or volatile) of a function template parameter does not follow the rules of ordinary rvalue references. It is a different thing.\n\nTwo terms : Universal and Forwarding Reference\nSuch a reference is called a universal reference. Unfortunately, there is also another term for it that is mainly used in the C++ standard: forwarding reference. There is no difference between these two terms, it is just that we have a historical mess here with two established terms that mean the same thing. Both terms describe basic aspects of universal/forwarding references:\n\nThey can universally bind to objects of all types(const and non-const) and value categories.\nThey are usually used to forward arguments; but note that this is not the only use (one reason for me to prefer the term universal reference)\n\n\n\nUniversal references bind to all value categories\nThe important feature of universal references is that they can bind to objects and expressions of any value category:\ntemplate&lt;typename T&gt;\nvoid callFoo(T&& arg);  //arg is universal/forwarding reference\n\nX v;\nconst X c;\ncallFoo(v);             //ok\ncallFoo(c);             //ok\ncallFoo(X{});           //ok\ncallFoo(std::move(v));  //ok\ncallFoo(std::move(c));  //ok\nIn addition, they preserve the const-ness and value category (whether we have an rvalue or an lvalue) of the object they are bound to:\n#include &lt;iostream&gt;\n#include &lt;type_traits&gt;\nclass Widget{};\n\ntemplate &lt;typename T&gt;\nvoid f(T&& arg){\n    std::cout &lt;&lt; std::boolalpha;\n\n    if (std::is_same&lt;T&&, Widget&&gt;::value)\n    {\n        std::cout &lt;&lt; \"T&& = Widget&\\n\";\n    }\n    else if (std::is_same&lt;T&&, const Widget&&gt;::value)\n    {\n        std::cout &lt;&lt; \"T&& = const Widget&\\n\";\n    }\n    else if(std::is_same&lt;T&&, Widget&&&gt;::value)\n    {\n        std::cout &lt;&lt; \"T&& = Widget&&\\n\";\n    }\n    else if(std::is_same&lt;T&&,const Widget&&&gt;::value)\n    {\n        std::cout &lt;&lt; \"T&& = const Widget&&\\n\";\n    }\n}\n\nint main()\n{\n    Widget w;\n    const Widget cw;\n\n    std::cout &lt;&lt; \"Calling f(w)\\n\"; \n    f(w);\n    std::cout &lt;&lt; \"Calling f(cw)\\n\"; \n    f(cw);\n    std::cout &lt;&lt; \"Calling f(std::move(w))\\n\"; \n    f(std::move(w));\n    std::cout &lt;&lt; \"Calling f(std::move(cw))\\n\"; \n    f(std::move(cw));\n    return 0;\n}\nCompiler Explorer\nstdout:\nCalling f(w)\nT&& = Widget&\nCalling f(cw)\nT&& = const Widget&\nCalling f(std::move(w))\nT&& = Widget&&\nCalling f(std::move(cw))\nT&& = const Widget&&\nBy rule, the template parameter type T, is deduced to be:\n\nAn lvalue reference if we pass an lvalue.\nAn rvalue reference if we pass to an rvalue.\n\nNote that, a generic rvalue reference that is qualified with const (or volatile) is not a universal reference.\nThe rules of reference collapsing are now applied:\n\nWidget& && becomes Widget&\nWidget&& & becomes Widget&\nWidget& && becomes Widget &\nWidget&& && becomes Widget&&\n\nIn the function call f( w ), we are passing an lvalue, so the template parameter T is deduced to be an lvalue reference, Widget&. Therefore, T&& is Widget& && and by the rules of reference collapsing, this collapses to Widget&.\nSimilarly, in the function call f( Widget{} ), we are passing an rvalue, so the template parameter T is deduced to be an rvalue reference, Widget&&. Therefore, T&& is Widget&& && which collapses to Widget&&.\n\n\nstd::forward&lt;&gt;()\nstd::forward&lt;&gt;() conditionally casts its input into an rvalue reference.\n\nIf the given input expression is an lvalue, it is cast to an lvalue reference.\nIf the given input expression is an rvalue, it is cast to an rvalue reference.\n\nstd::forward does not forward anything.\nA really cool use-case of perfect forwarding is the std::make_unique() function. std::make_unique&lt;T&gt;() must invoke the underlying constructor. However, whilst doing so, it must preserve the const-ness and the value category of the arguments passed to the constructor.\nHere is a quick code snippet:\n// Let's say that we would like to implement the make_unique()\n// function that invokes the underlying constructor - either move\n// or copy based on the arguments\n#include &lt;memory&gt;\n#include &lt;iostream&gt;\n\nclass X{\n    public:\n        X(){}\n        X(const X& x){ std::cout &lt;&lt; \"copy c'tor\\n\";}\n        X(X&& x) { std::cout &lt;&lt; \"move c'tor\\n\"; }\n};\n\n\ntemplate&lt;typename T, typename... Args&gt;\nstd::unique_ptr&lt;T&gt; make_unique(Args&&... args){\n    T* ptr_t = new T(std::forward&lt;Args&gt;(args)...);\n    return std::unique_ptr&lt;T&gt;(ptr_t);\n}\n\nint main()\n{\n    X x1{};\n    std::unique_ptr&lt;X&gt; x2{make_unique&lt;X&gt;(x1)};  //calls X(const X&)\n    std::unique_ptr&lt;X&gt; x3{make_unique&lt;X&gt;(X{})}; //calls X(X&&)\n    return 0;\n}"
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "Let’s start with the definition of Ito processes.\n\nDefinition 1 (Ito Process) Let \\((B(t):t\\geq0)\\) be a standard brownian motion defined on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). An Ito process \\((X(t):t\\geq0)\\) is of the form:\n\\[\\begin{aligned}\nX(t) & =X(0)+\\int_{0}^{t}V(s)dB(s)+\\int_{0}^{t}D(s)ds\n\\end{aligned} \\tag{1}\\]\nwhere \\((V(t),t\\geq0)\\) and \\((D(t),t\\geq0)\\) are two adapted processes for which the integrals make sense in the sense of Ito and Riemann. We refer to \\((V(t):t\\geq0)\\) as the local volatility and to \\((D(t):t\\geq0)\\) as the local drift.\n\nWe will often denote an Ito process \\((X(t):t\\geq0)\\) in differential form as:\n\\[\\begin{aligned}\ndX(t) & =D(t)dt+V(t)dB(t)\n\\end{aligned} \\tag{2}\\]\nThis form makes no rigorous sense; when we write it, we mean Equation 1. Nevertheless, the differential equation has two great advantages:\n(1) It gives some intuition on what drives the variation of \\(X(t)\\). On one hand, there is a contribution of the Brownian increments which are modulated by the volatility \\(V(t)\\). On the other hand, there is a smoother contribution coming from the time variation which is modulated by the drift \\(D(t)\\).\n(2) The differential notation has computational power. In particular, evaluating Ito’s formula is reduced to computing differentials, as in classical calculus, but by doing it upto the second order.\nAn important class of Ito processes is given by processes for which the volatility and the drift are simply functions of the position of the process.\n\nDefinition 2 Let \\((B(t):t\\geq0)\\) be a standard Brownian motion. An Ito process \\((X(t):t\\geq0)\\) of the form\n\\[\\begin{aligned}\ndX(t) & =\\mu(X(t))dt+\\sigma(X(t))dB(t),\\quad X(0)=x\n\\end{aligned} \\tag{3}\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are functions from \\(\\mathbf{R}\\) to \\(\\mathbf{R}\\), is called a time-homogenous diffusion.\n\n\nDefinition 3 An Ito-process \\((Y(t),t\\geq0)\\) of the form:\n\\[\\begin{aligned}\n{1}\ndY(t) & =\\mu(t,X(t))dt+\\sigma(t,X(t))dB(t)\\quad Y(0)=y\n\\end{aligned} \\tag{4}\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are now functions \\([0,\\infty)\\times\\mathbf{R}\\to\\mathbf{R}\\) is called a time-inhomogenous diffusion.\n\nThe equations above are called stochastic differential equations (SDE) of the respective process \\((X(t))\\) and \\((Y(t))\\).\nIn other words, a diffusion \\((X(t),t\\geq 0)\\) is an Ito process whose local volatility \\(V(t)\\) and local drift \\(D(t)\\) at time \\(t\\) depend only on the position of the process at time \\(t\\) and possibly on the time \\(t\\) itself. It cannot depend on the path of the process before time \\(t\\) or on the explicit values of the driving Brownian motion at that time (which is not the process \\(X(t)\\) itself). The class of diffusions, and of the Ito processes in general, constitutes a huge collection of stochastic processes for stochastic modelling.\nNote that an SDE is a generalization of ordinary differential equations or ODEs. Indeed, if there were no randomness, that is, no Brownian motion, the SDE would be reduced to\n\\[\\begin{aligned}\ndX(t) & =\\mu(X(t))dt\n\\end{aligned}\\]\nThis can be written for \\(X(t)=f(t)\\) as:\n\\[\\begin{aligned}\n\\frac{df}{dt} & =\\mu(f)\n\\end{aligned}\\]\nThis is a first-order ordinary differential equation. It governs the deterministic evolution of the function \\(X(t)=f(t)\\) in time. An SDE adds a random term to this evolution that is formally written as:\n\\[\\begin{aligned}\n\\frac{dX}{dt} & =\\mu(X(t))+\\sigma(X(t))\\frac{dB(t)}{dt}\n\\end{aligned}\\]\nWe know very well, that Brownian motion is not differentiable; hence the above is not well-defined. The ill-defined term \\(dB(t)/dt\\) is sometimes called white noise. However, equation Equation 3 is well-defined in the sense of the Ito process. These types of equations are well-suited to model phenomena with intrinsic randomness.\nHere are some examples of diffusions:\n\nExample 1 (Brownian Motion with a drift). If we take \\(X(t)=\\sigma B(t)+\\mu t\\) for some \\(\\sigma&gt;0\\) and \\(\\mu\\in\\mathbf{R}\\), then we can write \\(X(t)\\) as:\n\\[\\begin{aligned}\nX(t) & =\\int_{0}^{t}\\sigma dB(t)+\\int_{0}^{t}\\mu dt,\\quad X(0)=0\n\\end{aligned}\\]\nIn the differential form this becomes\n\\[\\begin{aligned}\ndX(t) & =\\mu dt+\\sigma dB(t)\n\\end{aligned}\\]\nIn this case, the local drift and the local volatility are constant.\n\n\nExample 2 (Geometric Brownian Motion). We consider the process \\(S(t)=\\exp((\\mu-\\sigma^{2}/2)t+\\sigma B(t))\\). To find the stochastic differential equation, we apply the Ito’s Lemma to\n\\[\\begin{aligned}\nf(t,x) & =\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)\n\\end{aligned}\\]\nWe have:\n\\[\\begin{aligned}\ndf(t,x) & =\\left((\\mu-\\sigma^{2}/2)+\\frac{1}{2}\\sigma^{2}\\right)\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)dt+\\sigma\\exp((\\mu-\\sigma^{2}/2)t+\\sigma x)dB(t)\\\\\n& =\\mu S(t)dt+\\sigma S(t)dB(t)\n\\end{aligned}\\]\nNote that the local drift and the local volatility are now proportional to the position. So, the higher \\(S(t)\\), the higher the volatility and drift.\n\n\nExample 3 (Any smooth function of Brownian motion). Ito’s formula gurarantees that any smooth function \\(f(t,B(t))\\) of time and a Brownian motion is an Ito process with volatility \\(V(t)=\\partial_{t}f(t,B(t))\\) and drift \\(D(t)=\\partial_{x}f(t,B(t))+\\frac{1}{2}\\partial_{xx}f(t,B(t))\\). We will see in further ahead, that, in general, any reasonable function of an Ito process remains an Ito process.\n\n\nExample 4 (An Ito process that is not a diffusion) Consider the process\n\\[\\begin{aligned}\nX(t) & =\\int_{0}^{t}B^{2}(s)dB(s)\n\\end{aligned}\\]\nThis is an Ito process with local volatility \\(V(t)=B(t)^{2}\\) and local drift \\(D(t)=0\\). However, it is not a diffusion, because the local volatility is not an explicit function of \\(X(t)\\).\nIt turns out that the Brownian bridge is a time-inhomogenous diffusion and that the Ornstein-Uhlenbeck process is a time-homogenous diffusion. To understand these examples, we need to extend Ito’s formula to Ito processes.\n\n\n\nThe first step towards a general Ito’s formula is the quadratic variation of an Ito process.\n\nProposition 1 (Quadratic variation of an Ito process.) Let \\((B(t),t\\geq0)\\) be a standard Brownian motion and \\((X(t):t\\geq0)\\) be an Ito process of the form \\(dX(t)=V(t)dB(t)+D(t)dt\\). Then, the quadratic variation of the process \\((X(t):t\\geq0)\\) is:*\n\\[\\begin{aligned}\n&lt;X,X&gt;_{t} & =\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(X(t_{j+1})-X(t_{j}))^{2}=\\int_{0}^{t}V(s)^{2}ds\n\\end{aligned} \\tag{5}\\]\nfor any partition \\((t_{j},j\\leq n)\\) of \\([0,t]\\), where the limit is in probability.\n\n\nRemark 1. Note that the quadratic variation is increasing in \\(t\\), but it is not deterministic in general! \\(V_t\\) is a random variable. The quadratic variation is a smooth stochastic process. (It is differentiable) Observe that we recover the quadratic variation for the Brownian motion for \\(V(t)=1\\) as expected. We also notice that the formula follows easily from the rules of Ito Calculus, thereby showing the consistency of the theory. Indeed we have:\n\\[\\begin{aligned}\nd&lt;X,X&gt;_{t} & =(dX(t))^{2}=(V(t)dB(t)+D(t)dt)^{2}\\\\\n& =V(t)^{2}(dB(t))^{2}+2V(t)D(t)dB(t)\\cdot dt+D^{2}(t)(dt)^{2}\\\\\n& =V(t)^{2}dt\n\\end{aligned}\\]\n\n\nProof. Proof. The proof is involved, but it reviews some important concepts of stochastic calculus. We prove the case when the process \\(V\\) is in \\(\\mathcal{L}_{c}^{2}(T)\\) for some \\(T&gt;0\\). We write \\(I(t)=\\int_{0}^{t}V(s)dB(s)\\) and \\(R(t)=\\int_{0}^{t}D(s)ds\\). We first show that only the Ito integral contributes to the quadratic variation and the Riemann integral does not contribute, so that:\n\\[\\begin{aligned}\n&lt;X,X&gt;_{t} & =&lt;I,I&gt;_{t}\n\\end{aligned} \\tag{6}\\]\nWe have that the increment square of \\(X(t)\\) is:\n\\[\\begin{aligned}\n(X(t_{j+1})-X(t_{j}))^{2} & =(I(t_{j+1})-I(t_{j}))^{2}+2(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j}))+(R(t_{j+1})-R(t_{j}))^{2}\n\\end{aligned}\\]\nThe Cauchy-Schwarz inequality implies :\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j})) & \\leq\\left(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\right)^{1/2}\\left(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\right)^{1/2}\n\\end{aligned}\\]\nTherefore, to prove equation Equation 5, it suffices to show that \\(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\to 0\\) almost surely. Since \\(D(s)\\) is an almost surely continuous process, the stochastic process \\(R(t)=\\int_{0}^{t}D(s)ds\\) has continuous paths with probability \\(1\\). Therefore:\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2} & =\\max_{1\\leq j\\leq n}|R(t_{j+1})-R(t_{j})|\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))\n\\end{aligned}\\]\nSince, \\(R(t)\\) is continuous on the compact set \\([0,t]\\), it is uniformly continuous a.s. So, as \\(|t_{j+1}-t_{j}|\\to 0\\), by uniform continuity it follows that \\(\\max|R(t_{j+1})-R(t_{j})|\\to 0\\) a.s.\nIt remains to prove that \\(&lt;I,I&gt;_{t}=\\int_{0}^{t}V(s)^{2}ds\\). We first prove the case when \\(V\\in\\mathcal{S}(T)\\) is a simple adapted process. Consider a partition \\((t_{j}:j\\leq n)\\) of \\([0,t]\\). Without loss of generality, we can suppose that \\(V\\) is constant on each \\([t_{j},t_{j+1})\\) by refining the partition. We then have:\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2} & =\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\n\\end{aligned}\\]\nNow, we have seen in the proof of Ito’s formula that \\(\\mathbb{E}\\left[\\left\\{ \\sum_{j=0}^{n-1}V(t_{j})^{2}((B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\\right\\} ^{2}\\right]\\to 0\\), so \\(\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\\) approaches \\(\\sum_{j=0}^{n-1}V(t_{j})^{2}(t_{j+1}-t_{j})\\) in the mean square sense. As the mesh size becomes finer, the \\(L^{2}\\)-limit is \\(\\int_{0}^{t}V(t)^{2}dt\\).\nThe case \\(V\\in\\mathcal{L}_{c}^{2}(T)\\) is proved by approximating \\(V\\) by a simple process in \\(\\mathcal{S}(T)\\). More precisely, we can find a simple process \\(V^{(\\epsilon)}(t)\\) that is \\(\\epsilon\\)-close to \\(V\\) in the sense:\n\\[\\begin{aligned}\n||I^{(\\epsilon)}-I|| & =||\\int V^{\\epsilon}dB(t)-\\int VdB(t)||=\\int_{0}^{t}\\mathbb{E}[(V^{(\\epsilon)}(t)-V(t))^{2}]ds&lt;\\epsilon\n\\end{aligned} \\tag{7}\\]\nTo prove the claim, we need to show that for \\(t\\leq T\\),\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right] & \\to0\\quad\\text{as }\\quad n\\to\\infty\n\\end{aligned}\\]\n\\(L^{1}\\)-convergence implies convergence in probability of the sequence \\(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\). We now introduce the \\(V^{(\\epsilon)}(t)\\) approximation inside the absolute value as well as its corresponding integral \\(I^{(\\epsilon)}(t)=\\int_{0}^{t}V^{(\\epsilon)}(s)ds\\). By the triangle inequality, we have:\n\\[\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right]\\nonumber \\\\\n= & \\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}+(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right.\\nonumber \\\\\n& +\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\Biggr]\\nonumber \\\\\n\\leq & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}\\right|\\right]+\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right|\\right]\\\\\n& +\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right]\\nonumber\n\\end{aligned} \\tag{8}\\]\nWe show that the first and third terms converge uniformly and that the second term goes to \\(0\\) as \\(n\\to\\infty\\).\nThe second term goes to \\(0\\) as \\(n\\to\\infty\\) by the argument for simple processes.\n\\(&lt;I^{(\\epsilon)},I^{(\\epsilon)}&gt;_{t}=\\int_{0}^{t}V^{(\\epsilon)}(s)^{2}ds\\).\nFor the third term, the linearity of the integral and the Cauchy Schwarz inequality (applied to \\(\\mathbb{E}\\int_{0}^{t}\\)) imply that it is:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right] & \\leq\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)-V(s))^{2}ds\\Biggr|\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)+V(s))^{2}ds\\Biggr|\\right]^{1/2}\n\\end{aligned}\\]\nThe first factor is smaller than the square root of \\(\\epsilon\\) by Equation 7, whereas the second factor is bounded.\nThe first term in equation Equation 8 is handled similarly. The linearity of the Ito integral and the Cauchy-Schwarz inequality applied to \\(\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}\\cdot\\right)\\right]\\) give that the first term is:\n\\[\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s))^{2}dB(s)-\\int_{t_{j}}^{t_{j+1}}(V^{\\epsilon}(s))^{2}dB(s)\\right|\\right]\\\\\n= & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right|\\right]\\\\\n\\leq & \\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right)^{2}\\Biggr|\\right]^{1/2}\n\\end{aligned}\\]\nBy Ito isometry, the first factor in the above expression can be simplified:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2} & =\\sum_{j=0}^{n-1}\\mathbb{E}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\\\\n& =\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)-V^{\\epsilon}(s))^{2}]ds\n\\end{aligned}\\]\nBy Equation 7, this factor is smaller than \\(\\epsilon\\). The second factor equals \\(\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)+V^{\\epsilon}(s))^{2}]ds\\) by Ito-isometry and is uniformly bounded. This concludes the proof of the proposition. \\(\\blacksquare\\)\n\nNote that quadratic variation \\(&lt;I,I&gt;_{t}=\\int(V(s))^{2}ds\\) is computed path-by-path and hence the result is random. On the other the variance of the Ito integral \\(Var(I(t))=\\mathbb{E}[I_{t}^{2}]=\\int\\mathbb{E}[V_{s}^{2}]ds\\) is the mean value of all possible paths of the quadratic variation and hence is non-random. We are now ready to state Ito’s formula for Ito processes. We write the result in differential form for conciseness.\n\nTheorem 1 (Ito’s formula for Ito processes) Let \\((B(t):t\\geq0)\\) be a standard brownian motion, and let \\((X(t):t\\geq0)\\) be an Ito process of the form \\(dX(t)=V(t)dB(t)+D(t)dt\\). Consider a function \\(f(t,x)\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R})\\). Then we have with probability one for all \\(t\\leq T\\):*\n\\[\\begin{aligned}\ndf(t,X(t)) & =\\partial_{x}f(t,X(t))dX(t)+\\partial_{t}f(t,X(t))dt+\\frac{1}{2}\\partial_{xx}f(t,X(t))d&lt;X,X&gt;_{t}\n\\end{aligned}\\]\nThis can also be written as:\n\\[\\begin{aligned}\ndf(t,X(t))= & \\partial_{x}f(t,X(t))V(t)dB(t)+\\left[\\partial_{x}f(t,X(t))D(t)+\\partial_{t}f(t,X(t))+\\frac{1}{2}(V(t))^{2}\\partial_{xx}f(t,X(t))\\right]dt\n\\end{aligned}\\]\n\nThe proof of the Theorem 1 is again a Taylor approximation with the form of the quadratic variation of the process. We will omit it.\n\nExample 5 (Ornstein-Uhlenbeck Process). Consider the Ornstein-Uhlenbeck process \\((Y(t):t\\geq0)\\):\n\\[\\begin{aligned}\nY(t) & =Y(0)e^{-t}+e^{-t}\\int_{0}^{t}e^{s}dB(s)\n\\end{aligned}\\]\nNote that this process is an explicit function of \\(t\\) and of the Ito process \\(X(t)=Y(0)+\\int_{0}^{t}e^{s}dB(s)\\). Indeed, we have:\n\\[\\begin{aligned}\nY(t) & =e^{-t}X(t)\n\\end{aligned}\\]\nLet \\(f(t,x)=e^{-t}x\\). Then, \\(f_{x}(t,x)=e^{-t}\\), \\(f_{xx}(t,x)=0\\) and \\(f_{t}(t,x)=-e^{-t}x\\). So, by Ito’s lemma,\n\\[\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}dX(t)\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}(e^{t}dB(t))\\nonumber \\\\\ndY(t) & =-Y(t)dt+dB(t)\n\\end{aligned} \\tag{9}\\]\nThis is the SDE for the Ornstein Uhlenbeck process.\nThe SDE has a very nice interpretation: The drift is positive if \\(Y(t)&lt;0\\) and negative if \\(Y(t)&gt;0\\). Moreover, the drift is proportional to the position (exactly like a spring pulling the process back to the \\(x\\)-axis following the Hooke’s law!). This is the mechanism that ensures that the process does not venture too far from \\(0\\) and is eventually stationary.\nThe SDE Equation 9 is now easily generalized by adding two parameters for the volatility and the drift:\n\\[\\begin{aligned}\ndY(t) & =-kY(t)dt+\\sigma dB(t),\\quad k\\in\\mathbf{R},\\sigma&gt;0\n\\end{aligned} \\tag{10}\\]\nIt is not hard to check that the solution to the SDE is:\n\\[\\begin{aligned}\nY(t) & =Y(0)e^{-kt}+e^{-kt}\\int_{0}^{t}e^{ks}\\sigma dB(s)\n\\end{aligned} \\tag{11}\\]\n\n\nExercise 1 The Ornstein-Uhlenbeck process with parameters. Use the Ito’s formula to show that the Equation 11 is the solution to the Ornstein-Uhlenbeck SDE Equation 10.\n\nSolution.\nLet \\(X(t)=Y(0)+\\int_{0}^{t}e^{ks}\\sigma dB(s)\\), so \\(dX(t)=e^{kt}\\sigma dB(t)\\). Then, \\(Y(t)=e^{-kt}X(t)\\). Let \\(f(t,x)=e^{-kt}x\\). Then, by Ito’s formula:\n\\[\\begin{aligned}\ndf(t,x) & =-ke^{-kt}X(t)dt+e^{-kt}dX(t)\\\\\ndY(t) & =-kY(t)dt+e^{-kt}e^{kt}\\sigma dB(t)\\\\\ndY(t) & =-kY(t)dt+\\sigma dB(t)\n\\end{aligned}\\]\nThe latest version of Ito’s formula is another useful tool for producing martingales from a function of an Ito process. We start with two examples generalizing martingales for Brownian motion.\n\nExample 6 (A generalization of \\((B(t))^{2}-t\\)). Let \\((V(t):t\\leq T)\\) be a process in \\(\\mathcal{L}_{c}^{2}(T)\\). Consider an Ito process \\((X(t):t\\leq T)\\) given by \\(dX(t)=V(t)dB(t)\\). Note that \\(((X(t))^{2}:t\\leq T)\\) is a submartingale by Jensen’s inequality, since \\(\\mathbb{E}[X^{2}(t)|\\mathcal{F}_{s}]\\geq(\\mathbb{E}[X(t)|\\mathcal{F}_{s})^{2}=X^{2}(s)\\). We show that the compensated process\n\\[\\begin{aligned}\nM(t) & =X^{2}(t)-\\int_{0}^{t}V^{2}(s)ds,\\quad t\\leq T\n\\end{aligned}\\]\nis a martingale for the Brownian filtration. (This is another instance of the Doob-Meyer decomposition). By the Ito’s formula for \\(f(x)=x^{2}\\), we have:\n\\[\\begin{aligned}\ndf(x) & =f_{x}(X(t)dX(t)+\\frac{1}{2}f_{xx}(X(t))d&lt;X,X&gt;_{t}\\\\\n& =2X(t)dX(t)+(V(t))^{2}dt\\\\\ndf(X(t)) & =2X(t)V(t)dB(t)+(V(t))^{2}dt\n\\end{aligned}\\]\nIn Integral form this implies:\n\\[\\begin{aligned}\n(X(t))^{2} & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)+\\int_{0}^{t}(V(s))^{2}ds\\\\\nM(t)=(X(t))^{2}-\\int_{0}^{t}(V(s))^{2}ds & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)\n\\end{aligned}\\]\nWe conclude that \\((M(t):t\\leq T)\\) is a martingale, provided \\(X(t)V(t)\\in L_{c}^{2}(T)\\).\nThere is another more direct way to prove that \\((M(t):t\\leq T)\\) is a martingale whenever \\((V(t):t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\). This is by using increments: for \\(t'&lt;t\\leq T\\),\n\\[\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =\\mathbb{E}[(X_{t}+(X_{t'}-X_{t}))^{2}|\\mathcal{F}_{t}]\\\\\n& =\\mathbb{E}[X_{t}^{2}+2X_{t}(X_{t'}-X_{t})+(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\\\\\n& =X_{t}^{2}+2X_{t}\\mathbb{E}[X_{t'}-X_{t}|\\mathcal{F}_{t}]+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}\\]\nSince \\((X_{t}:t\\geq0)\\) is a martingale, \\(\\mathbb{E}[(X_{t'}-X_{t})|M_{t}]=0\\), so the middle term equals zero and we are left with:\n\\[\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =X_{t}^{2}+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}\\]\nBy conditional Ito Isometry,\n\\[\\begin{aligned}\n\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}] & =\\int_{0}^{t'}V_{s}^{2}ds-\\int_{0}^{t}V_{s}^{2}ds=\\int_{t}^{t'}V_{s}^{2}ds\n\\end{aligned}\\]\n\n\nExample 7 (A generalization of the geometric Brownian motion). Let \\(\\sigma(t)\\) be a continuous, deterministic function such that \\(|\\sigma(t)|\\leq1\\), \\(t\\in[0,T]\\). The process\n\\[\\begin{aligned}\nM(t) & =\\exp\\left(\\int_{0}^{t}\\sigma(s)dB(s)-\\frac{1}{2}\\int_{0}^{t}\\sigma^{2}(s)ds\\right),\\quad t\\leq T\n\\end{aligned}\\]\nis a martingale for the Brownian filtration. To see this, note that we can write \\(M(t)=f(t,X(t))\\) where \\(f(t,x)=\\exp(x-\\frac{1}{2}\\int\\sigma^{2}(s)ds)\\) and \\(X(t)=\\int_{0}^{t}\\sigma(s)dB(s)\\), so \\(dX(t)=\\sigma(t)dB(t)\\). Ito’s formula gives:\n\\[\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t)dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\\\\ndM(t) & =-\\frac{1}{2}\\sigma^{2}(t)M(t)dt+M(t)\\sigma(t)dB(t)+\\frac{1}{2}M(t)\\sigma^{2}(t)dt\\\\\n& =M(t)\\sigma(t)dB(t)\\\\\nM(t) & =M(0)+\\int_{0}^{t}M(s)\\sigma(s)dB(s)\n\\end{aligned}\\]\nObserve also that:\n\\[\\begin{aligned}\n\\mathbb{E}[M_{t}^{2}] & =e^{-\\int_{0}^{t}\\sigma^{2}(s)ds}\\mathbb{E}[e^{2\\int_{0}^{t}\\sigma(s)dB(s)}]=e^{\\int_{0}^{t}\\sigma^{2}(s)ds}\n\\end{aligned}\\]\nsince \\(\\int_{0}^{t}\\sigma(s)dB(s)\\) is a Gaussian random variable with mean \\(0\\) and variance \\(\\int_{0}^{t}\\sigma^{2}(s)ds\\).\nWe conclude from the equation that \\((M(t),t\\geq0)\\) is a martingale.\n\n\nExample 8 (Martingales of Geometric Brownian Motion). Let\n\\[\\begin{aligned}\nS(t) & =S(0)\\exp(\\sigma B(t)-\\sigma^{2}t/2)\n\\end{aligned}\\]\nbe a geometric brownian motion. We find a PDE satisfied by \\(f(t,x)\\) for \\(f(t,S(t))\\) to be a martingale. It suffices to apply Ito’s formula of Theorem 1. We get:\n\\[\\begin{aligned}\ndf(t,S(t)) & =f_{t}(t,S(t))dt+f_{x}(t,S(t))dS(t)+\\frac{1}{2}f_{xx}(t,S(t))dS(t)\\cdot dS(t)\n\\end{aligned}\\]\nNow note from the earlier result that \\(dS(t)=S(t)\\sigma dB(t)\\). So, \\(dS(t)\\cdot dS(t)=\\frac{1}{2}\\sigma^{2}(S(t))^{2}dt\\). So,\n\\[\\begin{aligned}\ndf(t,S(t)) & =\\left\\{ \\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\sigma^{2}(S(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right\\} dt+\\sigma S(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}\\]\nFinally, the PDE for \\(f(t,x)\\) is obtained by setting the factor in front of \\(dt\\) to \\(0\\), because we want \\(f\\) to be a martingale process. It is important to keep in mind, that the PDE should always be written in terms of the time variable \\(t\\) and the space variable \\(x\\). Therefore, the PDE of \\(f\\) as a function of time and space is:\n\\[\\begin{aligned}\n\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(t,x)+\\frac{\\partial f}{\\partial t}(t,x) & =0\n\\end{aligned}\\]\nNo more randomness appears in the PDE!\n\nHere is a specific case where we can apply the Ito’s formula to construct martingales of Ito processes.\n\nExample 9 Consider the process given by the SDE:\n\\[\\begin{aligned}\ndX(t) & =X(t)dB(t),\\quad X(0)=2\n\\end{aligned}\\]\nLet’s find a PDE for which \\(f(t,X(t))\\) is a martingale for the Brownian filtration. We have by Ito’s formula that:\n\\[\\begin{aligned}\ndf(t,X(t)) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\\\\n& =\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}(X(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+X(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}\\]\nSetting the drift term to \\(0\\) gives the PDE:\n\\[\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}} & =0\n\\end{aligned}\\]\nIt is then easy to check that \\(X(t)\\) is a martingale and so is \\(t+\\log(X(t))^{2}\\), since the functions \\(f(t,x)=x\\) and \\(f(t,x)=t+\\log x^{2}\\) satisfy the PDE. However, the process \\(tX(t)\\) is not, as the function \\(f(t,x)=xt\\) is not a solution of the PDE.\n\n\n\n\nIto’s formula can be generalized to several Ito processes. Let’s start by stating an example of a function of two Ito processes. Such a function \\(f(x_{1},x_{2})\\) will be a function of two space variables. Not surprisingly, it needs to have two derivatives in each variable and they need to be a continuous function; we need \\(f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})\\).\n\nTheorem 2 (Ito’s formula for many Ito processes) Let \\((X(t):t\\geq0)\\) and (Y(t):t)$ be two Ito processes of the form:\n\\[\\begin{aligned}\ndX(t) & =V(t)dB(t)+D(t)dt\\nonumber \\\\\ndY(t) & =U(t)dB(t)+R(t)dt\n\\end{aligned} \\tag{12}\\]\nwhere \\((B(t):t\\geq0)\\) is a standard Brownian motion. Then, for \\(f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})\\), we have:\n\\[\\begin{aligned}\ndf(X(t),Y(t)) & =f_{x}(X(t),Y(t))dX(t)+f_{y}(X(t),Y(t))dY(t)+\\frac{1}{2}f_{xx}(X(t),Y(t))d&lt;X,X&gt;_{t}\\\\\n& +f_{xy}(X(t),Y(t))d&lt;X,Y&gt;_{t}+\\frac{1}{2}f_{yy}(X(t),Y(t))d&lt;Y,Y&gt;_{t}\n\\end{aligned}\\]\n\nThe idea of the proof is the same as in Theorem 1 : Taylor’s expansion and quadratic variation, together with the cross-variation of two processes.\n\\[\\begin{aligned}\ndX(t)\\cdot dY(t) & =(V(t)dB(t)+D(t)dt)(U(t)dB(t)+R(t)dt)\\\\\n& =U(t)V(t)dt\n\\end{aligned}\\]\n\nExample 10 (Product Rule) An important example of this formula is Ito’s product rule. Let \\(X(t)\\) and \\(Y(t)\\) be as in Equation 12. Then:\n\\[\\begin{aligned}\nd(X(t)Y(t)) & =Y(t)dX(t)+X(t)dY(t)+dX(t)\\cdot dY(t)\n\\end{aligned}\\]\n\n\nExercise 2 Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\((B_{t}:t\\geq0)\\) be a standard brownian motion. Using integration by parts, show that\n\\[\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}\\]\nand prove that \\(\\int_{0}^{t}B(s)ds\\sim\\mathcal{N}(0,t^{3}/3)\\).\nIs\n\\[\\begin{aligned}\nX(t) & =\\begin{cases}\n0 & t=0\\\\\n\\frac{\\sqrt{3}}{t}\\int_{0}^{t}B(s)ds & t&gt;0\n\\end{cases}\n\\end{aligned}\\]\na standard Wiener process?\nSolution.\nClearly, \\(B(s,\\omega)\\) is a random variable and the Riemann integral \\(\\int_0^t B(s,\\omega)ds\\) depends on the sample path \\(\\omega\\). So, \\((\\int_0^t B_s ds, t\\leq T)\\) is a stochastic process. Using integration by parts:\n\\[\\begin{aligned}\n\\int u\\left(\\frac{dv}{ds}\\right)ds & =uv-\\int v\\left(\\frac{du}{ds}\\right)ds\n\\end{aligned}\\]\nWe set \\(u=B(s)\\) and \\(dv/ds=1\\). Then:\n\\[\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =sB(s)|_{0}^{t}-\\int_{0}^{t}sdB(s)\\\\\n& =tB(t)-\\int_{0}^{t}sdB(s)\\\\\n& =\\int_{s=0}^{s=t}tdB(s)-\\int_{0}^{t}sdB(s)\\\\\n& =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}\\]\nThus, \\(\\int_{0}^{t}B(s)ds\\) is a Gaussian random variable with:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\int_{0}^{t}B(s)ds\\right] & =\\mathbb{E}\\left[\\int_{0}^{t}(t-s)dB(s)\\right]\\\\\n& =0\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\int_{0}^{t}B(s)ds\\right)^{2}\\right] & =\\int_{0}^{t}(t-s)^{2}ds\\\\\n& =\\left.\\frac{(t-s)^{3}}{-3}\\right|_{0}^{t}\\\\\n& =\\frac{t^{3}}{3}\n\\end{aligned}\\]\nThus, using the properties of Ito Integral, \\(\\int_{0}^{t}B(s)ds=\\int_{0}^{t}(t-s)dB(s)\\) is a martingale. Now the quadratic variation \\(&lt;M,M&gt;_{t}=0\\), and this can be a bit tricky. Remember, \\(\\left\\langle \\int_{0}^{t}f(s,B_{s})dB(s),\\int_{0}^{t}f(s,B_{s})dB(s)\\right\\rangle =\\int_{0}^{t}f^{2}(s,B_{s})ds\\) if and only if \\(f\\) is a function of the time \\(s\\) and the position of the Brownian motion \\(B(s)\\). Since, \\(f\\) is a function of \\(t\\) as well, this rule cannot be applied.\nBy first principles, we can show that, the quadratic variation is indeed \\(0\\): \\[\\begin{aligned}\n\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(I(t_{j+1})-I(t_{j})\\right)^{2}\\right] & =\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})^{2}\\right]\\\\\n& =\\lim_{n\\to\\infty}\\max_{1\\leq j\\leq n}|t_{j+1}-t_{j}|\\cdot\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})\\right]\n\\end{aligned}\\]\nSince the paths of \\(B_{t}\\) are continuous, so are the paths \\(B_{t}^{2}\\) on the compact interval \\([0,t]\\). So, \\((B_{s}^{2},s\\in[0,t])\\) is uniformly bounded. Thus, the expectation term is bounded. As \\(n\\to\\infty\\), the mesh size approaches zero, and consequently the quadratic variation approaches zero.\n\n\nExample 11 Let \\(X_{t}=\\int_{0}^{t}B_{s}dB_{s}\\) and \\(Y_{t}=\\int_{0}^{t}B_{s}^{2}dB_{s}\\). Is \\((X_{t}Y_{t},t\\geq0)\\) a martingale?\n\nSolution.\nBy Ito’s product rule, we have:\n\\[\\begin{aligned}\nd(X_{t}Y_{t}) & =X_{t}dY_{t}+Y_{t}dX_{t}+dX_{t}\\cdot dY_{t}\\\\\n& =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+(B_{s}dB_{s})\\cdot(B_{s}^{2}dB_{s})\\\\\n& =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+B_{s}^{3}dt\\\\\nX_{t}Y_{t} & =X_{0}Y_{0}+\\int_{0}^{t}X_{t}B_{s}^{2}dB_{s}+\\int_{0}^{t}Y_{t}B_{s}dB_{s}+\\int_{0}^{t}B_{s}^{3}dt\n\\end{aligned}\\]\nThe term in \\(dt\\) is not zero, so the product cannot be a martingale.\n\nExample 12 (A generalization of Geometric Brownian Motion). Consider \\((\\int_{0}^{t}V_{s}dB_{s},t\\geq0)\\) an Ito process. Define the positive process:\n\\[\\begin{aligned}\nM_{t} & =\\exp\\left(\\int_{0}^{t}V_{s}dB_{s}-\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\right),\\quad t\\geq0\n\\end{aligned} \\tag{13}\\]\n\nSolution.\nIto’s formula applied to the processes \\(X_{t}=\\int_{0}^{t}V_{s}dB_{s}\\) and \\(Y_{t}=\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\) with the function \\(f(x,y)=e^{x-y}\\) yields:\n\\[\\begin{aligned}\ndf(x,y) & =f_{x}(X_{t},Y_{t})dX_{t}+f_{y}(X_{t},Y_{t})dY_{t}\\\\\n& +\\frac{1}{2}f_{xx}(X_{t},Y_{t})dX_{t}\\cdot dX_{t}+\\frac{1}{2}f_{yy}(X_{t},Y_{t})dY_{t}\\cdot dY_{t}\\\\\n& +f_{xy}(X_{t},Y_{t})dX_{t}\\cdot dY_{t}\n\\end{aligned}\\]\nNow, all first and second order derivatives are \\(\\partial_{x}(e^{x-y})=M_{t}\\), \\(\\partial_{y}(e^{x-y})=-e^{x-y}=-M_{t}\\), \\(\\partial_{xx}(e^{x-y})=M_{t}\\)\n\\(dX_{t}=V_{t}dB_{t}\\). \\(dY_{t}=\\frac{1}{2}V_{t}^{2}dt\\).\n\\(dX_{t}\\cdot dX_{t}=V_{t}^{2}dt\\), \\(dX_{t}\\cdot dY_{t}=0\\),\n\\(dY_{t}\\cdot dY_{t}=0\\).\nConsequently, we have:\n\\[\\begin{aligned}\ndM_{t} & =M_{t}V_{t}dB_{t}-\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n& +\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n& =M_{t}V_{t}dB_{t}\n\\end{aligned}\\]\nThus, \\((M_{t},t\\geq0)\\) is a martingale.\n\nExercise 3 (Generalized Ito Integral). Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\((B_{t}:t\\geq0)\\) be a standard brownian motion. Given that \\(f\\) is a simple process, show that:\n\\[\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})dB_{s} & =B_{t}f(t,B_{t})-\\int_{0}^{t}\\left[B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{s}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds\\\\\n& -\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})ds & =tf(t,B_{t})-\\int_{0}^{t}s\\left[\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\n\nSolution.\nI suppress \\((t,B_{t})\\) for simplicity. Applying the product rule to \\(B_{t}f\\), we get:\n\\[\\begin{aligned}\nd(B_{t}f) & =fdB_{t}+B_{t}df+dB_{t}\\cdot df\\\\\n& =fdB_{t}+B_{t}\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& +dB_{t}\\cdot\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& =fdB_{t}+\\left(B_{t}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{t}\\right)dt+B_{t}\\frac{\\partial f}{\\partial x}dB_{t}\\\\\nB_{t}f & =\\int_{0}^{t}fdB_{s}+\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds+\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fdB_{s} & =B_{t}f-\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds-\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nApplying product rule to \\(tf(t,B_{t})\\), we get:\n\\[\\begin{aligned}\nd(tf) & =fdt+tdf+dt\\cdot df\\\\\n& =fdt+t\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& +dt\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& =fdt+t\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+t\\frac{\\partial f}{\\partial x}dB_{t}\\\\\ntf & =\\int_{0}^{t}fds+\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds+\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fds & =tf-\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nThe following example will be important when we discuss the Girsanov theorem.\n\nTheorem 3 (Ito’s formula for many Ito processes) Let \\(X_t = (X_t^{(j)},j\\leq d,j\\in\\mathbf{N})\\) be Ito processes constructed on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) and \\(f \\in C^{1,2}([0,T]\\times \\mathbb{R}^d)\\). Then for \\(t \\in [0,T]\\), we have:\n\\[\ndf(t,X_t) = \\partial_t f(t,X_t) dt + \\sum_{j=1}^{d} \\partial_{x_j} f(t,X_t) dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} \\sum_{k=1}^d \\partial_{x_j x_k}^2 f(t,X_t) dX_t^{(j)}\\cdot dX_t^{(k)}\n\\]\n\n\n\n\nThe good news is that it is not too hard to implement iterative schemes to sample paths of a diffusion. Consider \\((X_t,t\\leq T)\\) a solution to the SDE:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t) dt\n\\]\nTo keep the notation to a minimum, we consider a time-homogeonous diffusion. For a partition \\((t_j,j\\leq n)\\) of \\([0,T]\\) with \\(t_n = T\\), consider the increment\n\\[\nX_{t_{j+1}} - X_{t_j} = \\int_{t_j}^{t_{j+1}} \\sigma(X_s) dB(s) + \\int_{t_j}^{t_{j+1}} \\mu(X_s) ds\n\\tag{14}\\]\nNote that, if \\(\\sigma\\) and \\(\\mu\\) are smooth functions, we can apply Ito’s formula to \\(\\sigma(X_s)\\) and \\(\\mu(X_s)\\) for \\(s\\in (t_j,t_{j+1}]\\)! We get:\n\\[\n\\begin{align*}\n\\sigma(X_s) &= \\sigma(X_{t_j}) + \\int_{t_j}^{s}\\sigma'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\sigma''(X_u)(dX_u)^2\\\\\n\\mu(X_s) &= \\mu(X_{t_j}) + \\int_{t_j}^{s}\\mu'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\mu''(X_u)(dX_u)^2\n\\end{align*}\n\\tag{15}\\]\nNow, we can approximate the increment in Equation 14 at different levels of precision by considering a different estimate for Equation 15.\n\nExample 13 (Euler-Maruyama Scheme). This scheme consists of taking \\(\\sigma(X_s) \\approx \\sigma(X_{t_j})\\) and \\(\\mu(X_s) \\approx \\mu(X_{t_j})\\) for \\(s \\in [t_j,t_{j+1})\\) in Equation 15. Putting this back in Equation 14, we get:\n\\[\nX_{t_{j+1}} - X_{t_j} \\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j)\n\\tag{16}\\]\nThe process \\(X_t\\) can then be constructed recursively on the discrete set \\((t_j,j \\leq n)\\) as follows:\n\\[\n\\begin{align*}\nX_0 &= x\\\\\nX_{t_1} &= (X_{t_1} - X_0) + X_0\\\\\nX_{t_2} &= (X_{t_2} - X_{t_1}) + (X_{t_1} - X_0) + X_0\\\\\n\\vdots\\\\\nX_{t_n} &= \\sum_{j=0}^{n-1}(X_{t_{j+1}} - X_{t_j}) + X_0\n\\end{align*}\n\\]\n\n\nExample 14 (Milstein Scheme). In this scheme, we go an order further for the approximation of the volatility in Equation 15 and consider also the integral in \\(dX_u\\). We take \\(dX_u = \\sigma(X_{t_j})dB_u\\). We then express \\(\\sigma(X_s)\\) in Equation 15 as:\n\\[\n\\begin{align*}\n\\sigma(X_s) &\\approx \\sigma(X_{t_j}) + \\int_{t_j}^{s} \\sigma'(X_{t_j}) (\\sigma(X_{t_j})dB_u)\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{s}  dB_u\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) (B_s - B_{t_j})\n\\end{align*}\n\\]\nIf we put this back in Equation 14, we get:\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{t_{j+1}} (B(s) - B(t_j))dB(s)\n\\end{align*}\n\\tag{17}\\]\nNow, consider the function \\(f(x)=x^2\\). We have:\n\\[\n\\begin{align*}\nf_x(x) &= 2x\\\\\nf_{xx}(x) &= 2\n\\end{align*}\n\\]\nSo, we get:\n\\[\nB^2(t_{j+1}) - B^2(t_j) = 2 \\int_{t_j}^{t_{j+1}} B(s) dB(s) + \\frac{1}{2} \\int_{t_j}^{t_{j+1}} (2) ds\n\\]\nThus,\n\\[\n\\int_{t_j}^{t_{j+1}} B(s) dB(s) = \\frac{1}{2}[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)]\n\\tag{18}\\]\nSubstituting Equation 18 into Equation 17, we get the Milstein approximation:\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+\\sigma'(X_{t_j}) \\sigma(X_{t_j})  \\left[\\frac{1}{2}((B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)) -  B_{t_j}(B_{t_{j+1}}-B_{t_j})\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j) -  2B_{t_j}B_{t_{j+1}}+2B_{t_j}^2\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n\\]\nThus, under the Milstein scheme\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n\\tag{19}\\]\nThe recursive nature of these two schemes makes them easy to implement numerically. The Milstein scheme is not much more costly to implement as it contains only one more term than the Euler scheme.\n\n\n\nLet’s code an SIVP class and an abstract base class Solver.\n\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Optional\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\n# More descriptive type hints\nT = np.ndarray\nX = np.ndarray\n\n@dataclass\nclass SIVP:\n    \"\"\"\n    An abstraction for a stochastic initial value problem\n    \"\"\"\n    t_start : float\n    t_end : float\n    initial_condition: float\n\n    drift: Callable[[float, np.ndarray], np.ndarray]\n    vol: Callable[[float, np.ndarray], np.ndarray]\n    dvol_dx: Optional[Callable[[float, np.ndarray], np.ndarray]] = None\n\n\n\n@dataclass\nclass Solver(ABC):\n    \"\"\"\n    An abstract base class for all numerical schemes\n    \"\"\"\n\n    num_steps: int = 100\n    num_paths: int = 100\n\n    def __post_init__(self):\n        self.iter = 0\n\n        # x_values is a matrix of shape [num_paths,num_steps]\n        self.x_values = np.zeros((self.num_paths, self.num_steps + 1))\n        self.step_size = 1.0 / self.num_steps\n\n        # gaussian increments\n        self.brownian_increments = np.sqrt(self.step_size) * np.random.standard_normal(\n            size=(self.num_paths, self.num_steps)\n        )\n        self.brownian = np.cumsum(self.brownian_increments, axis=1)\n        self.brownian = np.concatenate(\n            [np.zeros(shape=(self.num_paths, 1)), self.brownian], axis=1\n        )\n\n    @abstractmethod\n    def iterate(self, sivp: SIVP) -&gt; X:\n        \"\"\"\n        Compute the next iterate X(n+1)\n        \"\"\"\n\n    def solve(self, sivp: SIVP) -&gt; (T, X):\n        \"\"\"\n        Solve the SIVP\n        \"\"\"\n        self.x_values[:, 0] = np.full(\n            shape=(self.num_paths,), fill_value=sivp.initial_condition\n        )\n        while self.iter &lt; self.num_steps:\n            self.x_values[:, self.iter + 1] = self.iterate(sivp)\n            self.iter += 1\n\n        times = np.linspace(sivp.t_start, sivp.t_end, self.num_steps + 1)\n        return times, self.x_values\n\n    def reset(self):\n        self.__post_init__()\n\nThe EulerMaruyama and Milstein methods will derive from the abstract base class Solver.\n\n@dataclass\nclass EulerMaruyama(Solver):\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n\n    \"\"\"\n\n    def iterate(self, sivp: SIVP) -&gt; X:\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n        current_time = self.iter * self.step_size\n        mu_n = sivp.drift(current_time, self.x_values[:, self.iter])\n        sigma_n = sivp.vol(current_time, self.x_values[:, self.iter])\n        delta_x = (\n            mu_n * self.step_size + sigma_n * self.brownian_increments[:, self.iter]\n        )\n        return self.x_values[:, self.iter] + delta_x\n\n\n@dataclass\nclass Milstein(Solver):\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n    + 0.5 * sigma(n,X_n) * sigma'(n,X_n) * ((B(n+1) - B(n))**2 - (t_{n+1} - t_n))\n\n    \"\"\"\n\n    def iterate(self, sivp: SIVP):\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n        current_time = self.iter * self.step_size\n        mu_n = sivp.drift(current_time, self.x_values[:, self.iter])\n        sigma_n = sivp.vol(current_time, self.x_values[:, self.iter])\n        dvol_dx_n = sivp.dvol_dx(current_time, self.x_values[:, self.iter])\n\n        delta_x = (\n            mu_n * self.step_size\n            + sigma_n * self.brownian_increments[:, self.iter]\n            + 0.5\n            * sigma_n\n            * dvol_dx_n\n            * (self.brownian_increments[:, self.iter] ** 2 - self.step_size)\n        )\n\n        return self.x_values[:, self.iter] + delta_x\n\n\nExercise 4 (Simulating SDEs) Simulate 100 paths for the following diffusions given by their SDEs on \\([0,1]\\) using the Euler-Maruyama scheme and the Milstein scheme for a discretization of \\(0.01\\).\n\nGeometric Brownian Motion:\n\n\\[\ndS_t = S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)S_t dt, \\quad S_0 = 1\n\\]\nfor \\(\\mu=-1/2\\), \\(\\mu=-2\\), and \\(\\mu=0\\)\n\nOrnstein-Uhlenbeck process:\n\n\\[\ndX_t = -X_t dt + dB_t, \\quad X_0 = 1\n\\]\n\nThe diffusion:\n\n\\[\ndX_t = \\sqrt{1+X_t^2}dB_t + \\sin X_t dt, \\quad X_0 = 0\n\\]\n\nSolution.\nWe can now use EulerMaruyama and Milstein solvers and generate some sample paths.\n\n\nShow the code\ndef plot_trajectories(ts, xs, title):\n    plt.xlabel(r'Time $t$')\n    plt.ylabel(r'$X(t)$')\n    plt.title(title)\n    plt.grid(True)\n    plt.plot(ts, xs.transpose())\n    plt.show()\n\n# mu = -0.5 case\neuler = EulerMaruyama(num_paths=10, num_steps=100)\n\ngbm_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=1.0,\n    drift=lambda t, s_t: np.zeros(10),\n    vol=lambda t, s_t: s_t,\n    dvol_dx=lambda t, s_t: np.ones(10),\n)\n\n\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=-0.5$')\n\n# mu = -2 case\ngbm_sde.drift = lambda t, s_t : -1.5 * s_t\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=-2.0$')\n\n# mu = 0 case\ngbm_sde.drift = lambda t, s_t : 0.5 * s_t\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=0.0$')\n\n# ornstein-uhlenbeck\nornstein_uhlenbeck_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=1.0,\n    drift = lambda t, s_t : -s_t,\n    vol = lambda t, s_t : np.ones(10),\n    dvol_dx = lambda t, s_t : np.zeros(10)\n)\n\nts, xs = euler.solve(ornstein_uhlenbeck_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Ornstein-Uhlenbeck')\n\n# diffusion\ndiffusion_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=0.0,\n    drift = lambda t, s_t : np.sin(s_t),\n    vol = lambda t, s_t : np.sqrt(1+s_t**2),\n    dvol_dx = lambda t, s_t : s_t / np.sqrt(1 + s_t**2) \n)\n\nts,xs = euler.solve(diffusion_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of the diffusion')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is of course possible go beyond the Milstein scheme to improve the approximation. However, it turns out that the above schemes already converge quite rapidly to the process itself. To see this, consider the mean absolute error between the approximation \\(X_T^{(n)}\\) at time \\(T\\) and \\(X_T\\). Suppose that the approximation \\(X^{(n)}\\) is obtained for a partition with discretization \\(t_{j+1}-t_j = 1/n\\). It is possible to show that for the Euler scheme, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}[|X_T^{(n)} - X_T|] \\leq \\frac{C}{\\sqrt{n}}\n\\end{align*}\n\\]\nwhereas for the Milstein scheme\n\\[\n\\begin{align*}\n\\mathbb{E}[|X_T^{(n)} - X_T|] \\leq \\frac{C'}{n}\n\\end{align*}\n\\]\nfor some constants \\(C,C'&gt;0\\). Note that the mean error between the two process must be the worst at the last point, since the errors add up.\n\n\n\n\nAs for the differential equations in standard calculus, SDEs play an important role in modelling stochastic phenomena. To model a trajectory \\(X_t, t\\leq T\\), it suffices to write down the variation due to the deterministic change \\(\\mu(X_t)dt\\) for some function \\(\\mu(X_t)\\) and the variation due to local fluctuations \\(\\sigma(X_t)dB_t\\) for some function \\(\\sigma(X_t)\\). Here, we assume that the local drift and volatility are time-homogenous for simplicity. This gives the SDE:\n\\[\ndX_t = \\mu(X_t) dt + \\sigma(X_t) dB_t\n\\]\nDo we get one nice Ito process for any choice of functions \\(\\sigma\\) and \\(\\mu\\)? The short answer is no. Here are sufficient conditions for the existence of a unique process.\n\nTheorem 4 (Existence and Uniqueness of solutions to SDE) Consider the SDE\n\\[\ndX_t = \\mu(X_t) dt  + \\sigma(X_t)dB_t, \\quad X_0 = x, \\quad t\\in [0,T]\n\\]\nIf the functions \\(\\sigma\\) and \\(\\mu\\) grow not faster than \\(Kx^2\\) for some \\(K&gt;0\\) and are differentiable with bounded derivatives on \\(\\mathbb{R}^1\\), then there exists a unique solution \\((X_t,t\\in[0,T])\\) to the SDE. In other words, there exists a continuous process \\((X_t, t \\leq T)\\) adapted to the filtration of the Brownian motion given by:\n\\[\nX_t = x + \\int_0^t \\mu(X_s) ds + \\int_0^t \\sigma(X_s) dB_s, \\quad t\\leq T\n\\]\n\n\nExample 15 Consider the SDE :\n\\[\ndX_t = \\sqrt{1 + X_t^2}dB_t + \\sin X_t dt\n\\tag{20}\\]\nThere exists a unique diffusion process \\((X_t,t\\geq 0)\\) that is a solution of this SDE. To see this, we verify the conditions of the Theorem 4. We have:\n\\[\n\\sigma(x) = \\sqrt{1 + x^2}, \\quad \\mu(x) = \\sin x\n\\]\nClearly, these functions satisfy the growth condition since \\(\\mu\\) is bounded and \\(\\sigma\\) grows like \\(|x|\\) for large \\(x\\). As for the derivatives, we have \\(\\sigma'(x)=\\frac{x}{\\sqrt{1+x^2}}\\) and \\(\\mu'(x)=\\cos x\\). The two derivatives are bounded.\n\nThe assumptions of Theorem 4 are not too surprising, since similar ones are found in the classical case of ODE. For example, if we have the ODE\n\\[\n\\frac{dX_t}{dt} = X_t^{1/2}, \\quad X_0 = 0\n\\]\nthen clearly \\(X_t = 0\\) for all \\(t\\) is a solution. But, we also have by integrating that:\n\\[\nX_t = \\frac{t^2}{4}, t \\geq 0\n\\]\nTherefore, the uniqueness breaks down. Note, that the function \\(\\mu(x)=\\sqrt{x}\\) does not have bounded derivatives at \\(0\\). Similarly, consider the ODE:\n\\[\n\\frac{dX_t}{dt} = e^{X_t}, X_0 = 0\n\\]\nHere, the function \\(\\mu(x)\\) grows much faster than \\(x^2\\). The solution of the ODE is by integrating\n\\[\n\\begin{align*}\n-(e^{-X_t} - 1) &= t\\\\\ne^{-X_t} &= (1 - t)\\\\\n-X_t &= \\log(1 - t)\\\\\nX_t &= \\log \\left(\\frac{1}{1-t}\\right)\n\\end{align*}\n\\]\nThe solution explodes at \\(t=1\\). The same phenomenon may occur for SDE; that is the process will almost surely go \\(\\infty\\) in finite time. These times are called explosion times. Note that it is possible to consider the paths of the diffusion upto these explosion times. It is important to keep in mind that the conditions of Theorem 4 are sufficient, but not necessary. In particular, it is sometimes possible to explicitly construct a diffusion whose local volatility and drift do not satisfy the conditions. Let’s look at two important instances of such diffusions.\n\nLet \\(B_t\\) be a brownian motion in \\(\\mathbb{R}^d\\) for \\(d&gt;1\\). Consider the process giving the distance at time \\(t\\) of \\(B_t\\) to the origin; that is:\n\\[\nR_t = ||B_t|| = \\left(\\sum_{j=1}^d B_t^{(j)}\\right)^2\n\\]\n(For \\(d=1\\), this is simply \\(|B_t|\\). Ito’s formula cannot be applied in this case, since the absolute value is not differentiable at the origin.) In higher dimensions, the function is smooth enough as long as we stay away from the origin. This is not a problem as long as \\(R_0 &gt; 0\\).\n\nLet’s find out the SDE that this process satisfies.\nLet \\(r=f(x_1,x_2,\\ldots,x_d)=\\sqrt{\\sum_{j=1}^d x_j^2}\\). We have:\n\\[\\begin{align*}\n\\frac{\\partial r}{\\partial x_j} &= \\frac{x_j}{\\sqrt{\\sum_{j=1}^d x_j^2}}\\\\\n\\frac{\\partial^2 r}{\\partial x_j^2} &= \\frac{||x|| - x_j \\cdot \\frac{x_j}{||x||}}{||x||^2}\\\\\n&= \\frac{||x||^2 - x_j^2}{||x||^3}\n\\end{align*}\\]\nBy Ito’s formula,\n\\[\\begin{align*}\ndR_t &= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\sum_{j=1}^{d} \\frac{R_t^2 - (B_t^{(j)})^2}{R_t^3}\\\\\n&= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\frac{d R_t^2 - R_t^2}{R_t^3}\\\\\n&= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\frac{(d-1)}{R_t}\n\\end{align*}\\]\nWe can define \\(dW_t = \\sum_{j=1}^{d} \\frac{B_t^{(j)}}{R_t} dB_t^{(j)}\\). Then, \\(dR_t = dW_t + \\frac{d-1}{2R_t}dt\\)\nIt turns out that \\((W_t,t \\geq 0)\\) is a standard Brownian motion by Levy’s characterization theorem. This is the subject of the next section. The SDE shows that \\(dR_t\\) is a diffusion. The SDE makes sense for any real number \\(d &gt; 1\\), not only integers. Moreover, the SDE is well-defined since \\(R_t\\) is never equal to \\(0\\). However, the SDE does not satisfy the assumption of the existence and uniqueness of the solution of SDEs since \\(1/x\\) diverges at \\(0\\). The solution to the SDE still exists since we constructed it! We sample paths of this process in numerical project.\n\nExample 16 (The Cox-Ingersoll-Ross (CIR) model.) Consider the SDE:\n\\[\ndS_t = \\sigma \\sqrt{S_t} dW_t + (a - bS_t)dt, \\quad S_0 &gt; 0\n\\tag{21}\\]\nfor some parameters \\(a,b &gt; 0\\) where \\((W_t,t \\geq 0)\\) is a standard Brownian motion. The local volatility \\(\\sigma(x)=\\sigma \\sqrt{x}\\) does not have a bounded derivative close to \\(0\\), since \\(\\sigma'(x)=\\frac{\\sigma}{2\\sqrt{x}}\\). We will nevertheless construct a diffusion that is a solution to the SDE. Consider independent Ornstein-Uhlenbeck processes \\(X_t^{(j)}, j \\leq d\\), with SDE:\n\\[\ndX_t^{(j)} = \\frac{-b}{2}X_t^{(j)}dt + \\frac{\\sigma}{2}dB_t^{(j)}, \\quad X_0^{(j)} &gt; 0\n\\]\nwhere \\(B_t = (B_t^{(j)},j\\leq d)\\) is a Brownian motion in \\(\\mathbb{R}^d\\). We consider the process\n\\[\nS_t = \\sum_{j \\leq d} (X_t^{(j)})^2\n\\]\nClearly, \\(S_t\\) is nongegative for all \\(t \\geq 0\\) by design, so \\(\\sqrt{S_t}\\) is well-defined. Let’s compute the SDE of the process. By Ito’s formula, we have:\n\\[\n\\begin{align*}\ndS_t &= 2\\sum_{j=1}^{d}X_t^{(j)} dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} 2 (dX_t^{(j)})^2\\\\\n&=2\\sum_{j=1}^{d}X_t^{(j)} dX_t^{(j)} + \\frac{\\sigma^2}{4}\\sum_{j=1}^{d} dt \\\\\n&=2\\sum_{j=1}^{d}X_t^{(j)} \\left(\\frac{-b}{2}X_t^{(j)}dt + \\frac{\\sigma}{2}dB_t^{(j)}\\right) + \\frac{d\\sigma^2}{4} dt\\\\\n&=\\left[\\left(\\sum_{j=1}^d (-b)(X_t^{(j)})^2\\right) + \\frac{d\\sigma^2}{4}\\right] dt + \\sigma \\sum_{j=1}^{d} X_t^{(j)} dB_t^{(j)}\\\\\n&=\\sigma \\sum_{j=1}^{d} X_t^{(j)} dB_t^{(j)} + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n&= \\sigma \\sqrt{S_t} \\sum_{j=1}^{d} \\frac{X_t^{(j)}}{\\sqrt{S_t}} dB_t^{(j)}  + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n&= \\sigma \\sqrt{S_t} dW_t  + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n\\end{align*}\n\\]\nwhere we have defined \\(dW_t = \\sum_{j=1}^d \\frac{X_t^{(j)}}{\\sqrt{S_t}} dB_t^{(j)}\\). It turns out that the process \\((W_t,t\\geq 0)\\) is a standard brownian motion by Levy’s characterization theorem. If we accept this for a moment, we have the SDE:\n\\[\ndS_t = \\sigma \\sqrt{S_t} dW_t + \\left(\\frac{d \\sigma^2}{4} - bS_t\\right) dt, \\quad S_0 &gt; 0\n\\tag{22}\\]\nThis is a time-homogenous diffusion called the Cox-Ingersoll-Ross (CIR) process. Again, notice that there are no issues with square root, since \\(S_t\\) is positive by construction! The SDE also makes sense if replace \\(d\\sigma^2/4\\) by a parameter \\(a\\) in Equation 21 as long as \\(a \\geq \\frac{d\\sigma^2}{4}\\). This process is important for interest rates and stochastic volatility models.\nNote that the local drift is \\(a - bS_t, a\\ge \\frac{d\\sigma^2}{4}\\). This can be written as:\n\\[\\begin{align*}\na - bS_t = b\\left(\\frac{b}{a} - S_t \\right)\n\\end{align*}\\]\nThis means that the local drift is negative if \\(S_t &gt; \\frac{a}{b}\\) and it is positive if \\(S_t &lt; \\frac{a}{b}\\). So, the SDE exhibits the same phenomenon as for the SDE of the Ornstein-Uhlenbeck process in Example 5. In particular we should expect that for \\(t\\) large, the process should fluctuate around the mean value \\(\\frac{a}{b}\\). The CIR model is therefore an example of a mean-reverting process. More generally, this hints to the fact, that the process is stationary in the long run.\n\n\n\n\nWe know very well by now, that an Ito integral is continuous martingale with respect to the Brownian filtration, whenever the integrand is in \\(\\mathcal{L}_c^2(T)\\). What can we say about the converse? In other words, if we have a martingale with respect to the Brownian filtration, can it be expressed as an Ito integral for some integrand \\((V_t,t\\leq T)\\)? Amazingly, the answer to this question is yes!\n\nTheorem 5 (Martingale Representation Theorem.) Let \\((B_t,t \\geq 0)\\) be a Brownian motion with filtration \\((\\mathcal{F}_t,t\\geq 0)\\) on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). Consider a martingale \\((M_t,t\\leq T)\\) with respect to this filtration. Then, there exists an adapted process \\((V_t,t \\leq T)\\) such that:\n\\[\nM_t = M_0 + \\int_0^t V_s dB_s, \\quad t \\leq T\n\\tag{23}\\]\nOne striking fact of the result is that \\((M_t,t\\leq T)\\) ought to be continuous. In other words, we cannot construct a process with a jump that is a martingale adapted to Brownian motion!\nInstead of the proving the theorem, we will see how the result is not too surprising with stronger assumptions. Instead of supposing that \\(M_t\\) is \\(\\mathcal{F}_t\\) measurable, take that \\(M_t\\) is \\(\\sigma(B_t)\\)-measurable. In other words, \\(M_t=h(B_t)\\) for some function \\(h\\). In the case where \\(h\\) is smooth, then it is clear by Ito’s formula that the representation Equation 23 holds with \\(V_s = h'(B_s)\\).\n\nAn important consequence of Theorem 5 is a third definition of Brownian motion.\n\nTheorem 6 (One-dimensional Levy’s Characterization theorem) Let \\((M_t,t\\in [0,T])\\) be a continuous martingale with respect to the filtration \\((\\mathcal{F}_t,t \\leq T)\\) with \\(M_0 = 0\\) and with quadratic variation \\(&lt;M&gt;_t = t\\). Then, \\((M_t,t\\leq T)\\) is a standard brownian motion.\n\nProof.\nWe first need to show that \\(M_t - M_s \\sim \\mathcal{N}(0,t - s)\\) or using the characteristic function approach, we need to show that \\(f_{(M_t - M_s)}(\\theta)=\\mathbb{E}[e^{\\theta (M_t - M_s)}] = e^{\\frac{1}{2}\\theta^2 (t-s)}\\) for constant \\(\\theta\\).\nLet \\(f(t,x) = e^{\\theta x - \\frac{1}{2}\\theta^2 t}\\).\nBy Ito’s formula, we have:\n\\[\\begin{align*}\ndf(t,M_t) &= \\theta e^{\\theta M_t - \\frac{1}{2}\\theta^2 t} dM_t + \\left(-\\frac{1}{2}\\theta^2 + \\frac{1}{2} \\theta^2 \\right)e^{\\theta M_t - \\frac{1}{2}\\theta^2 t}dt\\\\\n&= \\theta f(t,M_t) dM_t\n\\end{align*}\\]\nIntegrating on both sides, we get:\n\\[\\begin{align*}\nf(t,M_t) - f(s,M_s) &= \\theta \\int_0^t f(u,M_u) dM_u\n\\end{align*}\\]\nThe ito integral \\(\\int_0^t f(s,M_s) dM_s\\) is well-defined and its expectation is \\(0\\). Hence, applying expectation operator on both sides, we get:\n\\[\\begin{align*}\n\\mathbb{E}[f(t,M_t) - f(s,M_s)] &= \\theta \\mathbb{E} \\left[ \\int_0^t f(s,M_s) dM_s\\right]\\\\\n&= 0\n\\end{align*}\\]\nSince \\(e^{x} \\neq 0\\) for all \\(x\\), dividing by \\(f(s,M_s)\\), we get:\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\frac{f(t,M_t)}{f(s,M_s)}\\right] &= 1\\\\\n\\mathbb{E}\\left[\\frac{e^{\\theta M_t - \\frac{1}{2}\\theta^2 t}}{e^{\\theta M_s - \\frac{1}{2}\\theta^2 s}}\\right] &= 1\\\\\n\\mathbb{E}\\left[e^{\\theta(M_t - M_s)}\\right] &= e^{\\frac{1}{2}\\theta^2(t-s)}\n\\end{align*}\\]\nwhich is the moment generating function for the normal distribution with mean zero and variance \\(t-s\\). So, \\(M_t - M_s \\sim \\mathcal{N}(0, t - s)\\).\nFurther, consider \\(t_1 \\leq t_2 \\leq t_3\\). We have:\n\\[\\begin{align*}\n\\mathbb{E}[(M_{t_3} - M_{t_2})(M_{t_2} - M_{t_1})] &= \\mathbb{E}[\\mathbb{E}[(M_{t_3} - M_{t_2})(M_{t_2} - M_{t_1})|\\mathcal{F}_{t_2}]] \\\\\n&= \\mathbb{E}[(M_{t_2} - M_{t_1})\\mathbb{E}[(M_{t_3} - M_{t_2})|\\mathcal{F}_{t_2}]] \\\\\n&= \\mathbb{E}[(M_{t_2} - M_{t_1})(M_{t_2} - M_{t_2})] \\\\\n&= 0\n\\end{align*}\\]\nwhere \\(\\mathbb{E}[M_{t_3} - M_{t_2}|\\mathcal{F}_{t_2}] = 0\\) follows from the fact that \\((M_t,t\\geq 0)\\) is a martingale. Consequently, \\(M_{t_3} - M_{t_2} \\perp M_{t_2} - M_{t_1}\\) and non-overlapping increments are independent.\nMoreover, \\(M(0) = 0\\). So, \\((M_t,t\\geq 0)\\) is a standard brownian motion. This closes the proof. \\(\\blacksquare\\)\n\n\n\n\nExercise 5 Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and \\((W_t,t\\geq 0)\\) be a standard Wiener process. Find the SDE for the random process \\(X_t = W_t^n, n \\in \\mathbb{Z}^{+}\\).\nShow that\n\\[\n\\mathbb{E}[W_t^n] = \\frac{1}{2}n(n-1) \\int_0^t \\mathbb{E}\\left[W_s^{(n-2)}\\right] ds\n\\]\nand using mathematical induction prove that:\n\\[\n\\mathbb{E}[W_t^n] = \\begin{cases}\n\\frac{n! t^{n/2}}{2^{n/2} \\left(\\frac{n}{2}\\right)!}, & n=2,4,6,\\ldots\\\\\n0, & n=1,3,5,\\ldots\n\\end{cases}\n\\]\n\nSolution."
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#itos-formula.",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#itos-formula.",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "The first step towards a general Ito’s formula is the quadratic variation of an Ito process.\n\nProposition 1 (Quadratic variation of an Ito process.) Let \\((B(t),t\\geq0)\\) be a standard Brownian motion and \\((X(t):t\\geq0)\\) be an Ito process of the form \\(dX(t)=V(t)dB(t)+D(t)dt\\). Then, the quadratic variation of the process \\((X(t):t\\geq0)\\) is:*\n\\[\\begin{aligned}\n&lt;X,X&gt;_{t} & =\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(X(t_{j+1})-X(t_{j}))^{2}=\\int_{0}^{t}V(s)^{2}ds\n\\end{aligned} \\tag{5}\\]\nfor any partition \\((t_{j},j\\leq n)\\) of \\([0,t]\\), where the limit is in probability.\n\n\nRemark 1. Note that the quadratic variation is increasing in \\(t\\), but it is not deterministic in general! \\(V_t\\) is a random variable. The quadratic variation is a smooth stochastic process. (It is differentiable) Observe that we recover the quadratic variation for the Brownian motion for \\(V(t)=1\\) as expected. We also notice that the formula follows easily from the rules of Ito Calculus, thereby showing the consistency of the theory. Indeed we have:\n\\[\\begin{aligned}\nd&lt;X,X&gt;_{t} & =(dX(t))^{2}=(V(t)dB(t)+D(t)dt)^{2}\\\\\n& =V(t)^{2}(dB(t))^{2}+2V(t)D(t)dB(t)\\cdot dt+D^{2}(t)(dt)^{2}\\\\\n& =V(t)^{2}dt\n\\end{aligned}\\]\n\n\nProof. Proof. The proof is involved, but it reviews some important concepts of stochastic calculus. We prove the case when the process \\(V\\) is in \\(\\mathcal{L}_{c}^{2}(T)\\) for some \\(T&gt;0\\). We write \\(I(t)=\\int_{0}^{t}V(s)dB(s)\\) and \\(R(t)=\\int_{0}^{t}D(s)ds\\). We first show that only the Ito integral contributes to the quadratic variation and the Riemann integral does not contribute, so that:\n\\[\\begin{aligned}\n&lt;X,X&gt;_{t} & =&lt;I,I&gt;_{t}\n\\end{aligned} \\tag{6}\\]\nWe have that the increment square of \\(X(t)\\) is:\n\\[\\begin{aligned}\n(X(t_{j+1})-X(t_{j}))^{2} & =(I(t_{j+1})-I(t_{j}))^{2}+2(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j}))+(R(t_{j+1})-R(t_{j}))^{2}\n\\end{aligned}\\]\nThe Cauchy-Schwarz inequality implies :\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))(R(t_{j+1})-R(t_{j})) & \\leq\\left(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\right)^{1/2}\\left(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\right)^{1/2}\n\\end{aligned}\\]\nTherefore, to prove equation Equation 5, it suffices to show that \\(\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2}\\to 0\\) almost surely. Since \\(D(s)\\) is an almost surely continuous process, the stochastic process \\(R(t)=\\int_{0}^{t}D(s)ds\\) has continuous paths with probability \\(1\\). Therefore:\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))^{2} & =\\max_{1\\leq j\\leq n}|R(t_{j+1})-R(t_{j})|\\sum_{j=0}^{n-1}(R(t_{j+1})-R(t_{j}))\n\\end{aligned}\\]\nSince, \\(R(t)\\) is continuous on the compact set \\([0,t]\\), it is uniformly continuous a.s. So, as \\(|t_{j+1}-t_{j}|\\to 0\\), by uniform continuity it follows that \\(\\max|R(t_{j+1})-R(t_{j})|\\to 0\\) a.s.\nIt remains to prove that \\(&lt;I,I&gt;_{t}=\\int_{0}^{t}V(s)^{2}ds\\). We first prove the case when \\(V\\in\\mathcal{S}(T)\\) is a simple adapted process. Consider a partition \\((t_{j}:j\\leq n)\\) of \\([0,t]\\). Without loss of generality, we can suppose that \\(V\\) is constant on each \\([t_{j},t_{j+1})\\) by refining the partition. We then have:\n\\[\\begin{aligned}\n\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2} & =\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\n\\end{aligned}\\]\nNow, we have seen in the proof of Ito’s formula that \\(\\mathbb{E}\\left[\\left\\{ \\sum_{j=0}^{n-1}V(t_{j})^{2}((B(t_{j+1})-B(t_{j}))^{2}-(t_{j+1}-t_{j})\\right\\} ^{2}\\right]\\to 0\\), so \\(\\sum_{j=0}^{n-1}V(t_{j})^{2}(B(t_{j+1})-B(t_{j}))^{2}\\) approaches \\(\\sum_{j=0}^{n-1}V(t_{j})^{2}(t_{j+1}-t_{j})\\) in the mean square sense. As the mesh size becomes finer, the \\(L^{2}\\)-limit is \\(\\int_{0}^{t}V(t)^{2}dt\\).\nThe case \\(V\\in\\mathcal{L}_{c}^{2}(T)\\) is proved by approximating \\(V\\) by a simple process in \\(\\mathcal{S}(T)\\). More precisely, we can find a simple process \\(V^{(\\epsilon)}(t)\\) that is \\(\\epsilon\\)-close to \\(V\\) in the sense:\n\\[\\begin{aligned}\n||I^{(\\epsilon)}-I|| & =||\\int V^{\\epsilon}dB(t)-\\int VdB(t)||=\\int_{0}^{t}\\mathbb{E}[(V^{(\\epsilon)}(t)-V(t))^{2}]ds&lt;\\epsilon\n\\end{aligned} \\tag{7}\\]\nTo prove the claim, we need to show that for \\(t\\leq T\\),\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right] & \\to0\\quad\\text{as }\\quad n\\to\\infty\n\\end{aligned}\\]\n\\(L^{1}\\)-convergence implies convergence in probability of the sequence \\(\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}\\). We now introduce the \\(V^{(\\epsilon)}(t)\\) approximation inside the absolute value as well as its corresponding integral \\(I^{(\\epsilon)}(t)=\\int_{0}^{t}V^{(\\epsilon)}(s)ds\\). By the triangle inequality, we have:\n\\[\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-\\int_{0}^{t}(V(s))^{2}ds\\right|\\right]\\nonumber \\\\\n= & \\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}+(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right.\\nonumber \\\\\n& +\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\Biggr]\\nonumber \\\\\n\\leq & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I(t_{j+1})-I(t_{j}))^{2}-(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}\\right|\\right]+\\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}(I^{(\\epsilon)}(t_{j+1})-I^{(\\epsilon)}(t_{j}))^{2}-\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds\\right|\\right]\\\\\n& +\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right]\\nonumber\n\\end{aligned} \\tag{8}\\]\nWe show that the first and third terms converge uniformly and that the second term goes to \\(0\\) as \\(n\\to\\infty\\).\nThe second term goes to \\(0\\) as \\(n\\to\\infty\\) by the argument for simple processes.\n\\(&lt;I^{(\\epsilon)},I^{(\\epsilon)}&gt;_{t}=\\int_{0}^{t}V^{(\\epsilon)}(s)^{2}ds\\).\nFor the third term, the linearity of the integral and the Cauchy Schwarz inequality (applied to \\(\\mathbb{E}\\int_{0}^{t}\\)) imply that it is:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s))^{2}ds-\\int_{0}^{t}(V(s))^{2}ds\\Biggr|\\right] & \\leq\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)-V(s))^{2}ds\\Biggr|\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\int_{0}^{t}(V^{(\\epsilon)}(s)+V(s))^{2}ds\\Biggr|\\right]^{1/2}\n\\end{aligned}\\]\nThe first factor is smaller than the square root of \\(\\epsilon\\) by Equation 7, whereas the second factor is bounded.\nThe first term in equation Equation 8 is handled similarly. The linearity of the Ito integral and the Cauchy-Schwarz inequality applied to \\(\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}\\cdot\\right)\\right]\\) give that the first term is:\n\\[\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s))^{2}dB(s)-\\int_{t_{j}}^{t_{j+1}}(V^{\\epsilon}(s))^{2}dB(s)\\right|\\right]\\\\\n= & \\mathbb{E}\\left[\\left|\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right|\\right]\\\\\n\\leq & \\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2}\\mathbb{E}\\left[\\Biggl|\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)+V^{\\epsilon}(s))dB(s)\\right)^{2}\\Biggr|\\right]^{1/2}\n\\end{aligned}\\]\nBy Ito isometry, the first factor in the above expression can be simplified:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\right]^{1/2} & =\\sum_{j=0}^{n-1}\\mathbb{E}\\left(\\int_{t_{j}}^{t_{j+1}}(V(s)-V^{\\epsilon}(s))dB(s)\\right)^{2}\\\\\n& =\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)-V^{\\epsilon}(s))^{2}]ds\n\\end{aligned}\\]\nBy Equation 7, this factor is smaller than \\(\\epsilon\\). The second factor equals \\(\\sum_{j=0}^{n-1}\\int_{t_{j}}^{t_{j+1}}\\mathbb{E}[(V(s)+V^{\\epsilon}(s))^{2}]ds\\) by Ito-isometry and is uniformly bounded. This concludes the proof of the proposition. \\(\\blacksquare\\)\n\nNote that quadratic variation \\(&lt;I,I&gt;_{t}=\\int(V(s))^{2}ds\\) is computed path-by-path and hence the result is random. On the other the variance of the Ito integral \\(Var(I(t))=\\mathbb{E}[I_{t}^{2}]=\\int\\mathbb{E}[V_{s}^{2}]ds\\) is the mean value of all possible paths of the quadratic variation and hence is non-random. We are now ready to state Ito’s formula for Ito processes. We write the result in differential form for conciseness.\n\nTheorem 1 (Ito’s formula for Ito processes) Let \\((B(t):t\\geq0)\\) be a standard brownian motion, and let \\((X(t):t\\geq0)\\) be an Ito process of the form \\(dX(t)=V(t)dB(t)+D(t)dt\\). Consider a function \\(f(t,x)\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R})\\). Then we have with probability one for all \\(t\\leq T\\):*\n\\[\\begin{aligned}\ndf(t,X(t)) & =\\partial_{x}f(t,X(t))dX(t)+\\partial_{t}f(t,X(t))dt+\\frac{1}{2}\\partial_{xx}f(t,X(t))d&lt;X,X&gt;_{t}\n\\end{aligned}\\]\nThis can also be written as:\n\\[\\begin{aligned}\ndf(t,X(t))= & \\partial_{x}f(t,X(t))V(t)dB(t)+\\left[\\partial_{x}f(t,X(t))D(t)+\\partial_{t}f(t,X(t))+\\frac{1}{2}(V(t))^{2}\\partial_{xx}f(t,X(t))\\right]dt\n\\end{aligned}\\]\n\nThe proof of the Theorem 1 is again a Taylor approximation with the form of the quadratic variation of the process. We will omit it.\n\nExample 5 (Ornstein-Uhlenbeck Process). Consider the Ornstein-Uhlenbeck process \\((Y(t):t\\geq0)\\):\n\\[\\begin{aligned}\nY(t) & =Y(0)e^{-t}+e^{-t}\\int_{0}^{t}e^{s}dB(s)\n\\end{aligned}\\]\nNote that this process is an explicit function of \\(t\\) and of the Ito process \\(X(t)=Y(0)+\\int_{0}^{t}e^{s}dB(s)\\). Indeed, we have:\n\\[\\begin{aligned}\nY(t) & =e^{-t}X(t)\n\\end{aligned}\\]\nLet \\(f(t,x)=e^{-t}x\\). Then, \\(f_{x}(t,x)=e^{-t}\\), \\(f_{xx}(t,x)=0\\) and \\(f_{t}(t,x)=-e^{-t}x\\). So, by Ito’s lemma,\n\\[\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}dX(t)\\nonumber \\\\\ndY(t) & =-Y(t)dt+e^{-t}(e^{t}dB(t))\\nonumber \\\\\ndY(t) & =-Y(t)dt+dB(t)\n\\end{aligned} \\tag{9}\\]\nThis is the SDE for the Ornstein Uhlenbeck process.\nThe SDE has a very nice interpretation: The drift is positive if \\(Y(t)&lt;0\\) and negative if \\(Y(t)&gt;0\\). Moreover, the drift is proportional to the position (exactly like a spring pulling the process back to the \\(x\\)-axis following the Hooke’s law!). This is the mechanism that ensures that the process does not venture too far from \\(0\\) and is eventually stationary.\nThe SDE Equation 9 is now easily generalized by adding two parameters for the volatility and the drift:\n\\[\\begin{aligned}\ndY(t) & =-kY(t)dt+\\sigma dB(t),\\quad k\\in\\mathbf{R},\\sigma&gt;0\n\\end{aligned} \\tag{10}\\]\nIt is not hard to check that the solution to the SDE is:\n\\[\\begin{aligned}\nY(t) & =Y(0)e^{-kt}+e^{-kt}\\int_{0}^{t}e^{ks}\\sigma dB(s)\n\\end{aligned} \\tag{11}\\]\n\n\nExercise 1 The Ornstein-Uhlenbeck process with parameters. Use the Ito’s formula to show that the Equation 11 is the solution to the Ornstein-Uhlenbeck SDE Equation 10.\n\nSolution.\nLet \\(X(t)=Y(0)+\\int_{0}^{t}e^{ks}\\sigma dB(s)\\), so \\(dX(t)=e^{kt}\\sigma dB(t)\\). Then, \\(Y(t)=e^{-kt}X(t)\\). Let \\(f(t,x)=e^{-kt}x\\). Then, by Ito’s formula:\n\\[\\begin{aligned}\ndf(t,x) & =-ke^{-kt}X(t)dt+e^{-kt}dX(t)\\\\\ndY(t) & =-kY(t)dt+e^{-kt}e^{kt}\\sigma dB(t)\\\\\ndY(t) & =-kY(t)dt+\\sigma dB(t)\n\\end{aligned}\\]\nThe latest version of Ito’s formula is another useful tool for producing martingales from a function of an Ito process. We start with two examples generalizing martingales for Brownian motion.\n\nExample 6 (A generalization of \\((B(t))^{2}-t\\)). Let \\((V(t):t\\leq T)\\) be a process in \\(\\mathcal{L}_{c}^{2}(T)\\). Consider an Ito process \\((X(t):t\\leq T)\\) given by \\(dX(t)=V(t)dB(t)\\). Note that \\(((X(t))^{2}:t\\leq T)\\) is a submartingale by Jensen’s inequality, since \\(\\mathbb{E}[X^{2}(t)|\\mathcal{F}_{s}]\\geq(\\mathbb{E}[X(t)|\\mathcal{F}_{s})^{2}=X^{2}(s)\\). We show that the compensated process\n\\[\\begin{aligned}\nM(t) & =X^{2}(t)-\\int_{0}^{t}V^{2}(s)ds,\\quad t\\leq T\n\\end{aligned}\\]\nis a martingale for the Brownian filtration. (This is another instance of the Doob-Meyer decomposition). By the Ito’s formula for \\(f(x)=x^{2}\\), we have:\n\\[\\begin{aligned}\ndf(x) & =f_{x}(X(t)dX(t)+\\frac{1}{2}f_{xx}(X(t))d&lt;X,X&gt;_{t}\\\\\n& =2X(t)dX(t)+(V(t))^{2}dt\\\\\ndf(X(t)) & =2X(t)V(t)dB(t)+(V(t))^{2}dt\n\\end{aligned}\\]\nIn Integral form this implies:\n\\[\\begin{aligned}\n(X(t))^{2} & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)+\\int_{0}^{t}(V(s))^{2}ds\\\\\nM(t)=(X(t))^{2}-\\int_{0}^{t}(V(s))^{2}ds & =(X(0))^{2}+2\\int_{0}^{t}X(s)V(s)dB(s)\n\\end{aligned}\\]\nWe conclude that \\((M(t):t\\leq T)\\) is a martingale, provided \\(X(t)V(t)\\in L_{c}^{2}(T)\\).\nThere is another more direct way to prove that \\((M(t):t\\leq T)\\) is a martingale whenever \\((V(t):t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\). This is by using increments: for \\(t'&lt;t\\leq T\\),\n\\[\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =\\mathbb{E}[(X_{t}+(X_{t'}-X_{t}))^{2}|\\mathcal{F}_{t}]\\\\\n& =\\mathbb{E}[X_{t}^{2}+2X_{t}(X_{t'}-X_{t})+(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\\\\\n& =X_{t}^{2}+2X_{t}\\mathbb{E}[X_{t'}-X_{t}|\\mathcal{F}_{t}]+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}\\]\nSince \\((X_{t}:t\\geq0)\\) is a martingale, \\(\\mathbb{E}[(X_{t'}-X_{t})|M_{t}]=0\\), so the middle term equals zero and we are left with:\n\\[\\begin{aligned}\n\\mathbb{E}[X_{t'}^{2}|\\mathcal{F}_{t}] & =X_{t}^{2}+\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}]\n\\end{aligned}\\]\nBy conditional Ito Isometry,\n\\[\\begin{aligned}\n\\mathbb{E}[(X_{t'}-X_{t})^{2}|\\mathcal{F}_{t}] & =\\int_{0}^{t'}V_{s}^{2}ds-\\int_{0}^{t}V_{s}^{2}ds=\\int_{t}^{t'}V_{s}^{2}ds\n\\end{aligned}\\]\n\n\nExample 7 (A generalization of the geometric Brownian motion). Let \\(\\sigma(t)\\) be a continuous, deterministic function such that \\(|\\sigma(t)|\\leq1\\), \\(t\\in[0,T]\\). The process\n\\[\\begin{aligned}\nM(t) & =\\exp\\left(\\int_{0}^{t}\\sigma(s)dB(s)-\\frac{1}{2}\\int_{0}^{t}\\sigma^{2}(s)ds\\right),\\quad t\\leq T\n\\end{aligned}\\]\nis a martingale for the Brownian filtration. To see this, note that we can write \\(M(t)=f(t,X(t))\\) where \\(f(t,x)=\\exp(x-\\frac{1}{2}\\int\\sigma^{2}(s)ds)\\) and \\(X(t)=\\int_{0}^{t}\\sigma(s)dB(s)\\), so \\(dX(t)=\\sigma(t)dB(t)\\). Ito’s formula gives:\n\\[\\begin{aligned}\ndf(t,x) & =f_{t}(t,X(t)dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\\\\ndM(t) & =-\\frac{1}{2}\\sigma^{2}(t)M(t)dt+M(t)\\sigma(t)dB(t)+\\frac{1}{2}M(t)\\sigma^{2}(t)dt\\\\\n& =M(t)\\sigma(t)dB(t)\\\\\nM(t) & =M(0)+\\int_{0}^{t}M(s)\\sigma(s)dB(s)\n\\end{aligned}\\]\nObserve also that:\n\\[\\begin{aligned}\n\\mathbb{E}[M_{t}^{2}] & =e^{-\\int_{0}^{t}\\sigma^{2}(s)ds}\\mathbb{E}[e^{2\\int_{0}^{t}\\sigma(s)dB(s)}]=e^{\\int_{0}^{t}\\sigma^{2}(s)ds}\n\\end{aligned}\\]\nsince \\(\\int_{0}^{t}\\sigma(s)dB(s)\\) is a Gaussian random variable with mean \\(0\\) and variance \\(\\int_{0}^{t}\\sigma^{2}(s)ds\\).\nWe conclude from the equation that \\((M(t),t\\geq0)\\) is a martingale.\n\n\nExample 8 (Martingales of Geometric Brownian Motion). Let\n\\[\\begin{aligned}\nS(t) & =S(0)\\exp(\\sigma B(t)-\\sigma^{2}t/2)\n\\end{aligned}\\]\nbe a geometric brownian motion. We find a PDE satisfied by \\(f(t,x)\\) for \\(f(t,S(t))\\) to be a martingale. It suffices to apply Ito’s formula of Theorem 1. We get:\n\\[\\begin{aligned}\ndf(t,S(t)) & =f_{t}(t,S(t))dt+f_{x}(t,S(t))dS(t)+\\frac{1}{2}f_{xx}(t,S(t))dS(t)\\cdot dS(t)\n\\end{aligned}\\]\nNow note from the earlier result that \\(dS(t)=S(t)\\sigma dB(t)\\). So, \\(dS(t)\\cdot dS(t)=\\frac{1}{2}\\sigma^{2}(S(t))^{2}dt\\). So,\n\\[\\begin{aligned}\ndf(t,S(t)) & =\\left\\{ \\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\sigma^{2}(S(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right\\} dt+\\sigma S(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}\\]\nFinally, the PDE for \\(f(t,x)\\) is obtained by setting the factor in front of \\(dt\\) to \\(0\\), because we want \\(f\\) to be a martingale process. It is important to keep in mind, that the PDE should always be written in terms of the time variable \\(t\\) and the space variable \\(x\\). Therefore, the PDE of \\(f\\) as a function of time and space is:\n\\[\\begin{aligned}\n\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(t,x)+\\frac{\\partial f}{\\partial t}(t,x) & =0\n\\end{aligned}\\]\nNo more randomness appears in the PDE!\n\nHere is a specific case where we can apply the Ito’s formula to construct martingales of Ito processes.\n\nExample 9 Consider the process given by the SDE:\n\\[\\begin{aligned}\ndX(t) & =X(t)dB(t),\\quad X(0)=2\n\\end{aligned}\\]\nLet’s find a PDE for which \\(f(t,X(t))\\) is a martingale for the Brownian filtration. We have by Ito’s formula that:\n\\[\\begin{aligned}\ndf(t,X(t)) & =f_{t}(t,X(t))dt+f_{x}(t,X(t))dX(t)+\\frac{1}{2}f_{xx}(t,X(t))d&lt;X,X&gt;_{t}\\\\\n& =\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}(X(t))^{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+X(t)\\frac{\\partial f}{\\partial x}dB(t)\n\\end{aligned}\\]\nSetting the drift term to \\(0\\) gives the PDE:\n\\[\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}x^{2}\\frac{\\partial^{2}f}{\\partial x^{2}} & =0\n\\end{aligned}\\]\nIt is then easy to check that \\(X(t)\\) is a martingale and so is \\(t+\\log(X(t))^{2}\\), since the functions \\(f(t,x)=x\\) and \\(f(t,x)=t+\\log x^{2}\\) satisfy the PDE. However, the process \\(tX(t)\\) is not, as the function \\(f(t,x)=xt\\) is not a solution of the PDE."
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#multivariate-extension.",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#multivariate-extension.",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "Ito’s formula can be generalized to several Ito processes. Let’s start by stating an example of a function of two Ito processes. Such a function \\(f(x_{1},x_{2})\\) will be a function of two space variables. Not surprisingly, it needs to have two derivatives in each variable and they need to be a continuous function; we need \\(f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})\\).\n\nTheorem 2 (Ito’s formula for many Ito processes) Let \\((X(t):t\\geq0)\\) and (Y(t):t)$ be two Ito processes of the form:\n\\[\\begin{aligned}\ndX(t) & =V(t)dB(t)+D(t)dt\\nonumber \\\\\ndY(t) & =U(t)dB(t)+R(t)dt\n\\end{aligned} \\tag{12}\\]\nwhere \\((B(t):t\\geq0)\\) is a standard Brownian motion. Then, for \\(f\\in\\mathcal{C}^{2\\times2}(\\mathbf{R}\\times\\mathbf{R})\\), we have:\n\\[\\begin{aligned}\ndf(X(t),Y(t)) & =f_{x}(X(t),Y(t))dX(t)+f_{y}(X(t),Y(t))dY(t)+\\frac{1}{2}f_{xx}(X(t),Y(t))d&lt;X,X&gt;_{t}\\\\\n& +f_{xy}(X(t),Y(t))d&lt;X,Y&gt;_{t}+\\frac{1}{2}f_{yy}(X(t),Y(t))d&lt;Y,Y&gt;_{t}\n\\end{aligned}\\]\n\nThe idea of the proof is the same as in Theorem 1 : Taylor’s expansion and quadratic variation, together with the cross-variation of two processes.\n\\[\\begin{aligned}\ndX(t)\\cdot dY(t) & =(V(t)dB(t)+D(t)dt)(U(t)dB(t)+R(t)dt)\\\\\n& =U(t)V(t)dt\n\\end{aligned}\\]\n\nExample 10 (Product Rule) An important example of this formula is Ito’s product rule. Let \\(X(t)\\) and \\(Y(t)\\) be as in Equation 12. Then:\n\\[\\begin{aligned}\nd(X(t)Y(t)) & =Y(t)dX(t)+X(t)dY(t)+dX(t)\\cdot dY(t)\n\\end{aligned}\\]\n\n\nExercise 2 Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\((B_{t}:t\\geq0)\\) be a standard brownian motion. Using integration by parts, show that\n\\[\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}\\]\nand prove that \\(\\int_{0}^{t}B(s)ds\\sim\\mathcal{N}(0,t^{3}/3)\\).\nIs\n\\[\\begin{aligned}\nX(t) & =\\begin{cases}\n0 & t=0\\\\\n\\frac{\\sqrt{3}}{t}\\int_{0}^{t}B(s)ds & t&gt;0\n\\end{cases}\n\\end{aligned}\\]\na standard Wiener process?\nSolution.\nClearly, \\(B(s,\\omega)\\) is a random variable and the Riemann integral \\(\\int_0^t B(s,\\omega)ds\\) depends on the sample path \\(\\omega\\). So, \\((\\int_0^t B_s ds, t\\leq T)\\) is a stochastic process. Using integration by parts:\n\\[\\begin{aligned}\n\\int u\\left(\\frac{dv}{ds}\\right)ds & =uv-\\int v\\left(\\frac{du}{ds}\\right)ds\n\\end{aligned}\\]\nWe set \\(u=B(s)\\) and \\(dv/ds=1\\). Then:\n\\[\\begin{aligned}\n\\int_{0}^{t}B(s)ds & =sB(s)|_{0}^{t}-\\int_{0}^{t}sdB(s)\\\\\n& =tB(t)-\\int_{0}^{t}sdB(s)\\\\\n& =\\int_{s=0}^{s=t}tdB(s)-\\int_{0}^{t}sdB(s)\\\\\n& =\\int_{0}^{t}(t-s)dB(s)\n\\end{aligned}\\]\nThus, \\(\\int_{0}^{t}B(s)ds\\) is a Gaussian random variable with:\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\int_{0}^{t}B(s)ds\\right] & =\\mathbb{E}\\left[\\int_{0}^{t}(t-s)dB(s)\\right]\\\\\n& =0\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\int_{0}^{t}B(s)ds\\right)^{2}\\right] & =\\int_{0}^{t}(t-s)^{2}ds\\\\\n& =\\left.\\frac{(t-s)^{3}}{-3}\\right|_{0}^{t}\\\\\n& =\\frac{t^{3}}{3}\n\\end{aligned}\\]\nThus, using the properties of Ito Integral, \\(\\int_{0}^{t}B(s)ds=\\int_{0}^{t}(t-s)dB(s)\\) is a martingale. Now the quadratic variation \\(&lt;M,M&gt;_{t}=0\\), and this can be a bit tricky. Remember, \\(\\left\\langle \\int_{0}^{t}f(s,B_{s})dB(s),\\int_{0}^{t}f(s,B_{s})dB(s)\\right\\rangle =\\int_{0}^{t}f^{2}(s,B_{s})ds\\) if and only if \\(f\\) is a function of the time \\(s\\) and the position of the Brownian motion \\(B(s)\\). Since, \\(f\\) is a function of \\(t\\) as well, this rule cannot be applied.\nBy first principles, we can show that, the quadratic variation is indeed \\(0\\): \\[\\begin{aligned}\n\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}\\left(I(t_{j+1})-I(t_{j})\\right)^{2}\\right] & =\\lim_{n\\to\\infty}\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})^{2}\\right]\\\\\n& =\\lim_{n\\to\\infty}\\max_{1\\leq j\\leq n}|t_{j+1}-t_{j}|\\cdot\\mathbb{E}\\left[\\sum_{j=0}^{n-1}B_{t_{j}}^{2}(t_{j+1}-t_{j})\\right]\n\\end{aligned}\\]\nSince the paths of \\(B_{t}\\) are continuous, so are the paths \\(B_{t}^{2}\\) on the compact interval \\([0,t]\\). So, \\((B_{s}^{2},s\\in[0,t])\\) is uniformly bounded. Thus, the expectation term is bounded. As \\(n\\to\\infty\\), the mesh size approaches zero, and consequently the quadratic variation approaches zero.\n\n\nExample 11 Let \\(X_{t}=\\int_{0}^{t}B_{s}dB_{s}\\) and \\(Y_{t}=\\int_{0}^{t}B_{s}^{2}dB_{s}\\). Is \\((X_{t}Y_{t},t\\geq0)\\) a martingale?\n\nSolution.\nBy Ito’s product rule, we have:\n\\[\\begin{aligned}\nd(X_{t}Y_{t}) & =X_{t}dY_{t}+Y_{t}dX_{t}+dX_{t}\\cdot dY_{t}\\\\\n& =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+(B_{s}dB_{s})\\cdot(B_{s}^{2}dB_{s})\\\\\n& =X_{t}B_{s}^{2}dB_{s}+Y_{t}B_{s}dB_{s}+B_{s}^{3}dt\\\\\nX_{t}Y_{t} & =X_{0}Y_{0}+\\int_{0}^{t}X_{t}B_{s}^{2}dB_{s}+\\int_{0}^{t}Y_{t}B_{s}dB_{s}+\\int_{0}^{t}B_{s}^{3}dt\n\\end{aligned}\\]\nThe term in \\(dt\\) is not zero, so the product cannot be a martingale.\n\nExample 12 (A generalization of Geometric Brownian Motion). Consider \\((\\int_{0}^{t}V_{s}dB_{s},t\\geq0)\\) an Ito process. Define the positive process:\n\\[\\begin{aligned}\nM_{t} & =\\exp\\left(\\int_{0}^{t}V_{s}dB_{s}-\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\right),\\quad t\\geq0\n\\end{aligned} \\tag{13}\\]\n\nSolution.\nIto’s formula applied to the processes \\(X_{t}=\\int_{0}^{t}V_{s}dB_{s}\\) and \\(Y_{t}=\\frac{1}{2}\\int_{0}^{t}V_{s}^{2}ds\\) with the function \\(f(x,y)=e^{x-y}\\) yields:\n\\[\\begin{aligned}\ndf(x,y) & =f_{x}(X_{t},Y_{t})dX_{t}+f_{y}(X_{t},Y_{t})dY_{t}\\\\\n& +\\frac{1}{2}f_{xx}(X_{t},Y_{t})dX_{t}\\cdot dX_{t}+\\frac{1}{2}f_{yy}(X_{t},Y_{t})dY_{t}\\cdot dY_{t}\\\\\n& +f_{xy}(X_{t},Y_{t})dX_{t}\\cdot dY_{t}\n\\end{aligned}\\]\nNow, all first and second order derivatives are \\(\\partial_{x}(e^{x-y})=M_{t}\\), \\(\\partial_{y}(e^{x-y})=-e^{x-y}=-M_{t}\\), \\(\\partial_{xx}(e^{x-y})=M_{t}\\)\n\\(dX_{t}=V_{t}dB_{t}\\). \\(dY_{t}=\\frac{1}{2}V_{t}^{2}dt\\).\n\\(dX_{t}\\cdot dX_{t}=V_{t}^{2}dt\\), \\(dX_{t}\\cdot dY_{t}=0\\),\n\\(dY_{t}\\cdot dY_{t}=0\\).\nConsequently, we have:\n\\[\\begin{aligned}\ndM_{t} & =M_{t}V_{t}dB_{t}-\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n& +\\frac{1}{2}M_{t}V_{t}^{2}dt\\\\\n& =M_{t}V_{t}dB_{t}\n\\end{aligned}\\]\nThus, \\((M_{t},t\\geq0)\\) is a martingale.\n\nExercise 3 (Generalized Ito Integral). Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\((B_{t}:t\\geq0)\\) be a standard brownian motion. Given that \\(f\\) is a simple process, show that:\n\\[\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})dB_{s} & =B_{t}f(t,B_{t})-\\int_{0}^{t}\\left[B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{s}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds\\\\\n& -\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nand\n\\[\\begin{aligned}\n\\int_{0}^{t}f(s,B_{s})ds & =tf(t,B_{t})-\\int_{0}^{t}s\\left[\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right]ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\n\nSolution.\nI suppress \\((t,B_{t})\\) for simplicity. Applying the product rule to \\(B_{t}f\\), we get:\n\\[\\begin{aligned}\nd(B_{t}f) & =fdB_{t}+B_{t}df+dB_{t}\\cdot df\\\\\n& =fdB_{t}+B_{t}\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& +dB_{t}\\cdot\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& =fdB_{t}+\\left(B_{t}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}B_{t}\\right)dt+B_{t}\\frac{\\partial f}{\\partial x}dB_{t}\\\\\nB_{t}f & =\\int_{0}^{t}fdB_{s}+\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds+\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fdB_{s} & =B_{t}f-\\int_{0}^{t}\\left(B_{s}\\frac{\\partial f}{\\partial t}+\\frac{\\partial f}{\\partial x}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}B_{s}\\right)ds-\\int_{0}^{t}B_{s}\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nApplying product rule to \\(tf(t,B_{t})\\), we get:\n\\[\\begin{aligned}\nd(tf) & =fdt+tdf+dt\\cdot df\\\\\n& =fdt+t\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& +dt\\left(\\frac{\\partial f}{\\partial t}dt+\\frac{\\partial f}{\\partial x}dB_{t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}(dB_{t})^{2}\\right)\\\\\n& =fdt+t\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)dt+t\\frac{\\partial f}{\\partial x}dB_{t}\\\\\ntf & =\\int_{0}^{t}fds+\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds+\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\\\\\n\\int_{0}^{t}fds & =tf-\\int_{0}^{t}s\\left(\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\frac{\\partial^{2}f}{\\partial x^{2}}\\right)ds-\\int_{0}^{t}s\\frac{\\partial f}{\\partial x}dB_{s}\n\\end{aligned}\\]\nThe following example will be important when we discuss the Girsanov theorem.\n\nTheorem 3 (Ito’s formula for many Ito processes) Let \\(X_t = (X_t^{(j)},j\\leq d,j\\in\\mathbf{N})\\) be Ito processes constructed on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) and \\(f \\in C^{1,2}([0,T]\\times \\mathbb{R}^d)\\). Then for \\(t \\in [0,T]\\), we have:\n\\[\ndf(t,X_t) = \\partial_t f(t,X_t) dt + \\sum_{j=1}^{d} \\partial_{x_j} f(t,X_t) dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} \\sum_{k=1}^d \\partial_{x_j x_k}^2 f(t,X_t) dX_t^{(j)}\\cdot dX_t^{(k)}\n\\]"
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#numerical-simulation-of-sdes",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#numerical-simulation-of-sdes",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "The good news is that it is not too hard to implement iterative schemes to sample paths of a diffusion. Consider \\((X_t,t\\leq T)\\) a solution to the SDE:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t) dt\n\\]\nTo keep the notation to a minimum, we consider a time-homogeonous diffusion. For a partition \\((t_j,j\\leq n)\\) of \\([0,T]\\) with \\(t_n = T\\), consider the increment\n\\[\nX_{t_{j+1}} - X_{t_j} = \\int_{t_j}^{t_{j+1}} \\sigma(X_s) dB(s) + \\int_{t_j}^{t_{j+1}} \\mu(X_s) ds\n\\tag{14}\\]\nNote that, if \\(\\sigma\\) and \\(\\mu\\) are smooth functions, we can apply Ito’s formula to \\(\\sigma(X_s)\\) and \\(\\mu(X_s)\\) for \\(s\\in (t_j,t_{j+1}]\\)! We get:\n\\[\n\\begin{align*}\n\\sigma(X_s) &= \\sigma(X_{t_j}) + \\int_{t_j}^{s}\\sigma'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\sigma''(X_u)(dX_u)^2\\\\\n\\mu(X_s) &= \\mu(X_{t_j}) + \\int_{t_j}^{s}\\mu'(X_u) dX_u + \\frac{1}{2}\\int_{t_j}^{s} \\mu''(X_u)(dX_u)^2\n\\end{align*}\n\\tag{15}\\]\nNow, we can approximate the increment in Equation 14 at different levels of precision by considering a different estimate for Equation 15.\n\nExample 13 (Euler-Maruyama Scheme). This scheme consists of taking \\(\\sigma(X_s) \\approx \\sigma(X_{t_j})\\) and \\(\\mu(X_s) \\approx \\mu(X_{t_j})\\) for \\(s \\in [t_j,t_{j+1})\\) in Equation 15. Putting this back in Equation 14, we get:\n\\[\nX_{t_{j+1}} - X_{t_j} \\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j)\n\\tag{16}\\]\nThe process \\(X_t\\) can then be constructed recursively on the discrete set \\((t_j,j \\leq n)\\) as follows:\n\\[\n\\begin{align*}\nX_0 &= x\\\\\nX_{t_1} &= (X_{t_1} - X_0) + X_0\\\\\nX_{t_2} &= (X_{t_2} - X_{t_1}) + (X_{t_1} - X_0) + X_0\\\\\n\\vdots\\\\\nX_{t_n} &= \\sum_{j=0}^{n-1}(X_{t_{j+1}} - X_{t_j}) + X_0\n\\end{align*}\n\\]\n\n\nExample 14 (Milstein Scheme). In this scheme, we go an order further for the approximation of the volatility in Equation 15 and consider also the integral in \\(dX_u\\). We take \\(dX_u = \\sigma(X_{t_j})dB_u\\). We then express \\(\\sigma(X_s)\\) in Equation 15 as:\n\\[\n\\begin{align*}\n\\sigma(X_s) &\\approx \\sigma(X_{t_j}) + \\int_{t_j}^{s} \\sigma'(X_{t_j}) (\\sigma(X_{t_j})dB_u)\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{s}  dB_u\\\\\n&= \\sigma(X_{t_j}) + \\sigma'(X_{t_j}) \\sigma(X_{t_j}) (B_s - B_{t_j})\n\\end{align*}\n\\]\nIf we put this back in Equation 14, we get:\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\sigma'(X_{t_j}) \\sigma(X_{t_j}) \\int_{t_j}^{t_{j+1}} (B(s) - B(t_j))dB(s)\n\\end{align*}\n\\tag{17}\\]\nNow, consider the function \\(f(x)=x^2\\). We have:\n\\[\n\\begin{align*}\nf_x(x) &= 2x\\\\\nf_{xx}(x) &= 2\n\\end{align*}\n\\]\nSo, we get:\n\\[\nB^2(t_{j+1}) - B^2(t_j) = 2 \\int_{t_j}^{t_{j+1}} B(s) dB(s) + \\frac{1}{2} \\int_{t_j}^{t_{j+1}} (2) ds\n\\]\nThus,\n\\[\n\\int_{t_j}^{t_{j+1}} B(s) dB(s) = \\frac{1}{2}[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)]\n\\tag{18}\\]\nSubstituting Equation 18 into Equation 17, we get the Milstein approximation:\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+\\sigma'(X_{t_j}) \\sigma(X_{t_j})  \\left[\\frac{1}{2}((B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j)) -  B_{t_j}(B_{t_{j+1}}-B_{t_j})\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}}^2 - B_{t_j}^2)-(t_{j+1}-t_j) -  2B_{t_j}B_{t_{j+1}}+2B_{t_j}^2\\right]\\\\\n&= \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n\\]\nThus, under the Milstein scheme\n\\[\n\\begin{align*}\nX_{t_{j+1}} - X_{t_j} &\\approx \\sigma(X_{t_j})(B_{t_{j+1}}-B_{t_j}) + \\mu(X_{t_j})(t_{j+1} - t_j) \\\\\n&+ \\frac{\\sigma'(X_{t_j}) \\sigma(X_{t_j})}{2}  \\left[(B_{t_{j+1}} - B_{t_j})^2-(t_{j+1}-t_j)\\right]\n\\end{align*}\n\\tag{19}\\]\nThe recursive nature of these two schemes makes them easy to implement numerically. The Milstein scheme is not much more costly to implement as it contains only one more term than the Euler scheme.\n\n\n\nLet’s code an SIVP class and an abstract base class Solver.\n\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Optional\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\n# More descriptive type hints\nT = np.ndarray\nX = np.ndarray\n\n@dataclass\nclass SIVP:\n    \"\"\"\n    An abstraction for a stochastic initial value problem\n    \"\"\"\n    t_start : float\n    t_end : float\n    initial_condition: float\n\n    drift: Callable[[float, np.ndarray], np.ndarray]\n    vol: Callable[[float, np.ndarray], np.ndarray]\n    dvol_dx: Optional[Callable[[float, np.ndarray], np.ndarray]] = None\n\n\n\n@dataclass\nclass Solver(ABC):\n    \"\"\"\n    An abstract base class for all numerical schemes\n    \"\"\"\n\n    num_steps: int = 100\n    num_paths: int = 100\n\n    def __post_init__(self):\n        self.iter = 0\n\n        # x_values is a matrix of shape [num_paths,num_steps]\n        self.x_values = np.zeros((self.num_paths, self.num_steps + 1))\n        self.step_size = 1.0 / self.num_steps\n\n        # gaussian increments\n        self.brownian_increments = np.sqrt(self.step_size) * np.random.standard_normal(\n            size=(self.num_paths, self.num_steps)\n        )\n        self.brownian = np.cumsum(self.brownian_increments, axis=1)\n        self.brownian = np.concatenate(\n            [np.zeros(shape=(self.num_paths, 1)), self.brownian], axis=1\n        )\n\n    @abstractmethod\n    def iterate(self, sivp: SIVP) -&gt; X:\n        \"\"\"\n        Compute the next iterate X(n+1)\n        \"\"\"\n\n    def solve(self, sivp: SIVP) -&gt; (T, X):\n        \"\"\"\n        Solve the SIVP\n        \"\"\"\n        self.x_values[:, 0] = np.full(\n            shape=(self.num_paths,), fill_value=sivp.initial_condition\n        )\n        while self.iter &lt; self.num_steps:\n            self.x_values[:, self.iter + 1] = self.iterate(sivp)\n            self.iter += 1\n\n        times = np.linspace(sivp.t_start, sivp.t_end, self.num_steps + 1)\n        return times, self.x_values\n\n    def reset(self):\n        self.__post_init__()\n\nThe EulerMaruyama and Milstein methods will derive from the abstract base class Solver.\n\n@dataclass\nclass EulerMaruyama(Solver):\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n\n    \"\"\"\n\n    def iterate(self, sivp: SIVP) -&gt; X:\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n        current_time = self.iter * self.step_size\n        mu_n = sivp.drift(current_time, self.x_values[:, self.iter])\n        sigma_n = sivp.vol(current_time, self.x_values[:, self.iter])\n        delta_x = (\n            mu_n * self.step_size + sigma_n * self.brownian_increments[:, self.iter]\n        )\n        return self.x_values[:, self.iter] + delta_x\n\n\n@dataclass\nclass Milstein(Solver):\n    \"\"\"\n    Numerical solver for a stochastic differential equation(SDE) using\n    the Euler-Maruyama method.\n\n    Consider an SDE of the form :\n\n    dX_t = mu(t,X_t) dt + sigma(t,X_t) dB_t\n\n    with initial condition X_0 = x_0\n\n    The solution to the SDE can be computed using the increments\n\n    X_{n+1} - X_n = mu(n,X_n)(t_{n+1}-t_n) + sigma(n,X_n)(B(n+1)-B(n))\n    + 0.5 * sigma(n,X_n) * sigma'(n,X_n) * ((B(n+1) - B(n))**2 - (t_{n+1} - t_n))\n\n    \"\"\"\n\n    def iterate(self, sivp: SIVP):\n        \"\"\"\n        Generate the next iterate X(n+1)\n        \"\"\"\n        current_time = self.iter * self.step_size\n        mu_n = sivp.drift(current_time, self.x_values[:, self.iter])\n        sigma_n = sivp.vol(current_time, self.x_values[:, self.iter])\n        dvol_dx_n = sivp.dvol_dx(current_time, self.x_values[:, self.iter])\n\n        delta_x = (\n            mu_n * self.step_size\n            + sigma_n * self.brownian_increments[:, self.iter]\n            + 0.5\n            * sigma_n\n            * dvol_dx_n\n            * (self.brownian_increments[:, self.iter] ** 2 - self.step_size)\n        )\n\n        return self.x_values[:, self.iter] + delta_x\n\n\nExercise 4 (Simulating SDEs) Simulate 100 paths for the following diffusions given by their SDEs on \\([0,1]\\) using the Euler-Maruyama scheme and the Milstein scheme for a discretization of \\(0.01\\).\n\nGeometric Brownian Motion:\n\n\\[\ndS_t = S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)S_t dt, \\quad S_0 = 1\n\\]\nfor \\(\\mu=-1/2\\), \\(\\mu=-2\\), and \\(\\mu=0\\)\n\nOrnstein-Uhlenbeck process:\n\n\\[\ndX_t = -X_t dt + dB_t, \\quad X_0 = 1\n\\]\n\nThe diffusion:\n\n\\[\ndX_t = \\sqrt{1+X_t^2}dB_t + \\sin X_t dt, \\quad X_0 = 0\n\\]\n\nSolution.\nWe can now use EulerMaruyama and Milstein solvers and generate some sample paths.\n\n\nShow the code\ndef plot_trajectories(ts, xs, title):\n    plt.xlabel(r'Time $t$')\n    plt.ylabel(r'$X(t)$')\n    plt.title(title)\n    plt.grid(True)\n    plt.plot(ts, xs.transpose())\n    plt.show()\n\n# mu = -0.5 case\neuler = EulerMaruyama(num_paths=10, num_steps=100)\n\ngbm_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=1.0,\n    drift=lambda t, s_t: np.zeros(10),\n    vol=lambda t, s_t: s_t,\n    dvol_dx=lambda t, s_t: np.ones(10),\n)\n\n\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=-0.5$')\n\n# mu = -2 case\ngbm_sde.drift = lambda t, s_t : -1.5 * s_t\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=-2.0$')\n\n# mu = 0 case\ngbm_sde.drift = lambda t, s_t : 0.5 * s_t\nts, xs = euler.solve(gbm_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Geometric Brownian Motion, $\\mu=0.0$')\n\n# ornstein-uhlenbeck\nornstein_uhlenbeck_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=1.0,\n    drift = lambda t, s_t : -s_t,\n    vol = lambda t, s_t : np.ones(10),\n    dvol_dx = lambda t, s_t : np.zeros(10)\n)\n\nts, xs = euler.solve(ornstein_uhlenbeck_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of Ornstein-Uhlenbeck')\n\n# diffusion\ndiffusion_sde = SIVP(\n    t_start=0.0,\n    t_end=1.0,\n    initial_condition=0.0,\n    drift = lambda t, s_t : np.sin(s_t),\n    vol = lambda t, s_t : np.sqrt(1+s_t**2),\n    dvol_dx = lambda t, s_t : s_t / np.sqrt(1 + s_t**2) \n)\n\nts,xs = euler.solve(diffusion_sde)\neuler.reset()\n\nplot_trajectories(ts, xs, r'10 sample paths of the diffusion')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is of course possible go beyond the Milstein scheme to improve the approximation. However, it turns out that the above schemes already converge quite rapidly to the process itself. To see this, consider the mean absolute error between the approximation \\(X_T^{(n)}\\) at time \\(T\\) and \\(X_T\\). Suppose that the approximation \\(X^{(n)}\\) is obtained for a partition with discretization \\(t_{j+1}-t_j = 1/n\\). It is possible to show that for the Euler scheme, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}[|X_T^{(n)} - X_T|] \\leq \\frac{C}{\\sqrt{n}}\n\\end{align*}\n\\]\nwhereas for the Milstein scheme\n\\[\n\\begin{align*}\n\\mathbb{E}[|X_T^{(n)} - X_T|] \\leq \\frac{C'}{n}\n\\end{align*}\n\\]\nfor some constants \\(C,C'&gt;0\\). Note that the mean error between the two process must be the worst at the last point, since the errors add up."
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#existence-and-uniqueness-of-sdes",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#existence-and-uniqueness-of-sdes",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "As for the differential equations in standard calculus, SDEs play an important role in modelling stochastic phenomena. To model a trajectory \\(X_t, t\\leq T\\), it suffices to write down the variation due to the deterministic change \\(\\mu(X_t)dt\\) for some function \\(\\mu(X_t)\\) and the variation due to local fluctuations \\(\\sigma(X_t)dB_t\\) for some function \\(\\sigma(X_t)\\). Here, we assume that the local drift and volatility are time-homogenous for simplicity. This gives the SDE:\n\\[\ndX_t = \\mu(X_t) dt + \\sigma(X_t) dB_t\n\\]\nDo we get one nice Ito process for any choice of functions \\(\\sigma\\) and \\(\\mu\\)? The short answer is no. Here are sufficient conditions for the existence of a unique process.\n\nTheorem 4 (Existence and Uniqueness of solutions to SDE) Consider the SDE\n\\[\ndX_t = \\mu(X_t) dt  + \\sigma(X_t)dB_t, \\quad X_0 = x, \\quad t\\in [0,T]\n\\]\nIf the functions \\(\\sigma\\) and \\(\\mu\\) grow not faster than \\(Kx^2\\) for some \\(K&gt;0\\) and are differentiable with bounded derivatives on \\(\\mathbb{R}^1\\), then there exists a unique solution \\((X_t,t\\in[0,T])\\) to the SDE. In other words, there exists a continuous process \\((X_t, t \\leq T)\\) adapted to the filtration of the Brownian motion given by:\n\\[\nX_t = x + \\int_0^t \\mu(X_s) ds + \\int_0^t \\sigma(X_s) dB_s, \\quad t\\leq T\n\\]\n\n\nExample 15 Consider the SDE :\n\\[\ndX_t = \\sqrt{1 + X_t^2}dB_t + \\sin X_t dt\n\\tag{20}\\]\nThere exists a unique diffusion process \\((X_t,t\\geq 0)\\) that is a solution of this SDE. To see this, we verify the conditions of the Theorem 4. We have:\n\\[\n\\sigma(x) = \\sqrt{1 + x^2}, \\quad \\mu(x) = \\sin x\n\\]\nClearly, these functions satisfy the growth condition since \\(\\mu\\) is bounded and \\(\\sigma\\) grows like \\(|x|\\) for large \\(x\\). As for the derivatives, we have \\(\\sigma'(x)=\\frac{x}{\\sqrt{1+x^2}}\\) and \\(\\mu'(x)=\\cos x\\). The two derivatives are bounded.\n\nThe assumptions of Theorem 4 are not too surprising, since similar ones are found in the classical case of ODE. For example, if we have the ODE\n\\[\n\\frac{dX_t}{dt} = X_t^{1/2}, \\quad X_0 = 0\n\\]\nthen clearly \\(X_t = 0\\) for all \\(t\\) is a solution. But, we also have by integrating that:\n\\[\nX_t = \\frac{t^2}{4}, t \\geq 0\n\\]\nTherefore, the uniqueness breaks down. Note, that the function \\(\\mu(x)=\\sqrt{x}\\) does not have bounded derivatives at \\(0\\). Similarly, consider the ODE:\n\\[\n\\frac{dX_t}{dt} = e^{X_t}, X_0 = 0\n\\]\nHere, the function \\(\\mu(x)\\) grows much faster than \\(x^2\\). The solution of the ODE is by integrating\n\\[\n\\begin{align*}\n-(e^{-X_t} - 1) &= t\\\\\ne^{-X_t} &= (1 - t)\\\\\n-X_t &= \\log(1 - t)\\\\\nX_t &= \\log \\left(\\frac{1}{1-t}\\right)\n\\end{align*}\n\\]\nThe solution explodes at \\(t=1\\). The same phenomenon may occur for SDE; that is the process will almost surely go \\(\\infty\\) in finite time. These times are called explosion times. Note that it is possible to consider the paths of the diffusion upto these explosion times. It is important to keep in mind that the conditions of Theorem 4 are sufficient, but not necessary. In particular, it is sometimes possible to explicitly construct a diffusion whose local volatility and drift do not satisfy the conditions. Let’s look at two important instances of such diffusions.\n\nLet \\(B_t\\) be a brownian motion in \\(\\mathbb{R}^d\\) for \\(d&gt;1\\). Consider the process giving the distance at time \\(t\\) of \\(B_t\\) to the origin; that is:\n\\[\nR_t = ||B_t|| = \\left(\\sum_{j=1}^d B_t^{(j)}\\right)^2\n\\]\n(For \\(d=1\\), this is simply \\(|B_t|\\). Ito’s formula cannot be applied in this case, since the absolute value is not differentiable at the origin.) In higher dimensions, the function is smooth enough as long as we stay away from the origin. This is not a problem as long as \\(R_0 &gt; 0\\).\n\nLet’s find out the SDE that this process satisfies.\nLet \\(r=f(x_1,x_2,\\ldots,x_d)=\\sqrt{\\sum_{j=1}^d x_j^2}\\). We have:\n\\[\\begin{align*}\n\\frac{\\partial r}{\\partial x_j} &= \\frac{x_j}{\\sqrt{\\sum_{j=1}^d x_j^2}}\\\\\n\\frac{\\partial^2 r}{\\partial x_j^2} &= \\frac{||x|| - x_j \\cdot \\frac{x_j}{||x||}}{||x||^2}\\\\\n&= \\frac{||x||^2 - x_j^2}{||x||^3}\n\\end{align*}\\]\nBy Ito’s formula,\n\\[\\begin{align*}\ndR_t &= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\sum_{j=1}^{d} \\frac{R_t^2 - (B_t^{(j)})^2}{R_t^3}\\\\\n&= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\frac{d R_t^2 - R_t^2}{R_t^3}\\\\\n&= \\sum_{j=1}^{d} \\frac{B_t^{j}}{R_t} dB_t^{(j)} + \\frac{1}{2} \\frac{(d-1)}{R_t}\n\\end{align*}\\]\nWe can define \\(dW_t = \\sum_{j=1}^{d} \\frac{B_t^{(j)}}{R_t} dB_t^{(j)}\\). Then, \\(dR_t = dW_t + \\frac{d-1}{2R_t}dt\\)\nIt turns out that \\((W_t,t \\geq 0)\\) is a standard Brownian motion by Levy’s characterization theorem. This is the subject of the next section. The SDE shows that \\(dR_t\\) is a diffusion. The SDE makes sense for any real number \\(d &gt; 1\\), not only integers. Moreover, the SDE is well-defined since \\(R_t\\) is never equal to \\(0\\). However, the SDE does not satisfy the assumption of the existence and uniqueness of the solution of SDEs since \\(1/x\\) diverges at \\(0\\). The solution to the SDE still exists since we constructed it! We sample paths of this process in numerical project.\n\nExample 16 (The Cox-Ingersoll-Ross (CIR) model.) Consider the SDE:\n\\[\ndS_t = \\sigma \\sqrt{S_t} dW_t + (a - bS_t)dt, \\quad S_0 &gt; 0\n\\tag{21}\\]\nfor some parameters \\(a,b &gt; 0\\) where \\((W_t,t \\geq 0)\\) is a standard Brownian motion. The local volatility \\(\\sigma(x)=\\sigma \\sqrt{x}\\) does not have a bounded derivative close to \\(0\\), since \\(\\sigma'(x)=\\frac{\\sigma}{2\\sqrt{x}}\\). We will nevertheless construct a diffusion that is a solution to the SDE. Consider independent Ornstein-Uhlenbeck processes \\(X_t^{(j)}, j \\leq d\\), with SDE:\n\\[\ndX_t^{(j)} = \\frac{-b}{2}X_t^{(j)}dt + \\frac{\\sigma}{2}dB_t^{(j)}, \\quad X_0^{(j)} &gt; 0\n\\]\nwhere \\(B_t = (B_t^{(j)},j\\leq d)\\) is a Brownian motion in \\(\\mathbb{R}^d\\). We consider the process\n\\[\nS_t = \\sum_{j \\leq d} (X_t^{(j)})^2\n\\]\nClearly, \\(S_t\\) is nongegative for all \\(t \\geq 0\\) by design, so \\(\\sqrt{S_t}\\) is well-defined. Let’s compute the SDE of the process. By Ito’s formula, we have:\n\\[\n\\begin{align*}\ndS_t &= 2\\sum_{j=1}^{d}X_t^{(j)} dX_t^{(j)} + \\frac{1}{2}\\sum_{j=1}^{d} 2 (dX_t^{(j)})^2\\\\\n&=2\\sum_{j=1}^{d}X_t^{(j)} dX_t^{(j)} + \\frac{\\sigma^2}{4}\\sum_{j=1}^{d} dt \\\\\n&=2\\sum_{j=1}^{d}X_t^{(j)} \\left(\\frac{-b}{2}X_t^{(j)}dt + \\frac{\\sigma}{2}dB_t^{(j)}\\right) + \\frac{d\\sigma^2}{4} dt\\\\\n&=\\left[\\left(\\sum_{j=1}^d (-b)(X_t^{(j)})^2\\right) + \\frac{d\\sigma^2}{4}\\right] dt + \\sigma \\sum_{j=1}^{d} X_t^{(j)} dB_t^{(j)}\\\\\n&=\\sigma \\sum_{j=1}^{d} X_t^{(j)} dB_t^{(j)} + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n&= \\sigma \\sqrt{S_t} \\sum_{j=1}^{d} \\frac{X_t^{(j)}}{\\sqrt{S_t}} dB_t^{(j)}  + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n&= \\sigma \\sqrt{S_t} dW_t  + \\left[\\frac{d\\sigma^2}{4} - b S_t\\right] dt \\\\\n\\end{align*}\n\\]\nwhere we have defined \\(dW_t = \\sum_{j=1}^d \\frac{X_t^{(j)}}{\\sqrt{S_t}} dB_t^{(j)}\\). It turns out that the process \\((W_t,t\\geq 0)\\) is a standard brownian motion by Levy’s characterization theorem. If we accept this for a moment, we have the SDE:\n\\[\ndS_t = \\sigma \\sqrt{S_t} dW_t + \\left(\\frac{d \\sigma^2}{4} - bS_t\\right) dt, \\quad S_0 &gt; 0\n\\tag{22}\\]\nThis is a time-homogenous diffusion called the Cox-Ingersoll-Ross (CIR) process. Again, notice that there are no issues with square root, since \\(S_t\\) is positive by construction! The SDE also makes sense if replace \\(d\\sigma^2/4\\) by a parameter \\(a\\) in Equation 21 as long as \\(a \\geq \\frac{d\\sigma^2}{4}\\). This process is important for interest rates and stochastic volatility models.\nNote that the local drift is \\(a - bS_t, a\\ge \\frac{d\\sigma^2}{4}\\). This can be written as:\n\\[\\begin{align*}\na - bS_t = b\\left(\\frac{b}{a} - S_t \\right)\n\\end{align*}\\]\nThis means that the local drift is negative if \\(S_t &gt; \\frac{a}{b}\\) and it is positive if \\(S_t &lt; \\frac{a}{b}\\). So, the SDE exhibits the same phenomenon as for the SDE of the Ornstein-Uhlenbeck process in Example 5. In particular we should expect that for \\(t\\) large, the process should fluctuate around the mean value \\(\\frac{a}{b}\\). The CIR model is therefore an example of a mean-reverting process. More generally, this hints to the fact, that the process is stationary in the long run."
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#martingale-representation-and-levys-characterization",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#martingale-representation-and-levys-characterization",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "We know very well by now, that an Ito integral is continuous martingale with respect to the Brownian filtration, whenever the integrand is in \\(\\mathcal{L}_c^2(T)\\). What can we say about the converse? In other words, if we have a martingale with respect to the Brownian filtration, can it be expressed as an Ito integral for some integrand \\((V_t,t\\leq T)\\)? Amazingly, the answer to this question is yes!\n\nTheorem 5 (Martingale Representation Theorem.) Let \\((B_t,t \\geq 0)\\) be a Brownian motion with filtration \\((\\mathcal{F}_t,t\\geq 0)\\) on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). Consider a martingale \\((M_t,t\\leq T)\\) with respect to this filtration. Then, there exists an adapted process \\((V_t,t \\leq T)\\) such that:\n\\[\nM_t = M_0 + \\int_0^t V_s dB_s, \\quad t \\leq T\n\\tag{23}\\]\nOne striking fact of the result is that \\((M_t,t\\leq T)\\) ought to be continuous. In other words, we cannot construct a process with a jump that is a martingale adapted to Brownian motion!\nInstead of the proving the theorem, we will see how the result is not too surprising with stronger assumptions. Instead of supposing that \\(M_t\\) is \\(\\mathcal{F}_t\\) measurable, take that \\(M_t\\) is \\(\\sigma(B_t)\\)-measurable. In other words, \\(M_t=h(B_t)\\) for some function \\(h\\). In the case where \\(h\\) is smooth, then it is clear by Ito’s formula that the representation Equation 23 holds with \\(V_s = h'(B_s)\\).\n\nAn important consequence of Theorem 5 is a third definition of Brownian motion.\n\nTheorem 6 (One-dimensional Levy’s Characterization theorem) Let \\((M_t,t\\in [0,T])\\) be a continuous martingale with respect to the filtration \\((\\mathcal{F}_t,t \\leq T)\\) with \\(M_0 = 0\\) and with quadratic variation \\(&lt;M&gt;_t = t\\). Then, \\((M_t,t\\leq T)\\) is a standard brownian motion.\n\nProof.\nWe first need to show that \\(M_t - M_s \\sim \\mathcal{N}(0,t - s)\\) or using the characteristic function approach, we need to show that \\(f_{(M_t - M_s)}(\\theta)=\\mathbb{E}[e^{\\theta (M_t - M_s)}] = e^{\\frac{1}{2}\\theta^2 (t-s)}\\) for constant \\(\\theta\\).\nLet \\(f(t,x) = e^{\\theta x - \\frac{1}{2}\\theta^2 t}\\).\nBy Ito’s formula, we have:\n\\[\\begin{align*}\ndf(t,M_t) &= \\theta e^{\\theta M_t - \\frac{1}{2}\\theta^2 t} dM_t + \\left(-\\frac{1}{2}\\theta^2 + \\frac{1}{2} \\theta^2 \\right)e^{\\theta M_t - \\frac{1}{2}\\theta^2 t}dt\\\\\n&= \\theta f(t,M_t) dM_t\n\\end{align*}\\]\nIntegrating on both sides, we get:\n\\[\\begin{align*}\nf(t,M_t) - f(s,M_s) &= \\theta \\int_0^t f(u,M_u) dM_u\n\\end{align*}\\]\nThe ito integral \\(\\int_0^t f(s,M_s) dM_s\\) is well-defined and its expectation is \\(0\\). Hence, applying expectation operator on both sides, we get:\n\\[\\begin{align*}\n\\mathbb{E}[f(t,M_t) - f(s,M_s)] &= \\theta \\mathbb{E} \\left[ \\int_0^t f(s,M_s) dM_s\\right]\\\\\n&= 0\n\\end{align*}\\]\nSince \\(e^{x} \\neq 0\\) for all \\(x\\), dividing by \\(f(s,M_s)\\), we get:\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\frac{f(t,M_t)}{f(s,M_s)}\\right] &= 1\\\\\n\\mathbb{E}\\left[\\frac{e^{\\theta M_t - \\frac{1}{2}\\theta^2 t}}{e^{\\theta M_s - \\frac{1}{2}\\theta^2 s}}\\right] &= 1\\\\\n\\mathbb{E}\\left[e^{\\theta(M_t - M_s)}\\right] &= e^{\\frac{1}{2}\\theta^2(t-s)}\n\\end{align*}\\]\nwhich is the moment generating function for the normal distribution with mean zero and variance \\(t-s\\). So, \\(M_t - M_s \\sim \\mathcal{N}(0, t - s)\\).\nFurther, consider \\(t_1 \\leq t_2 \\leq t_3\\). We have:\n\\[\\begin{align*}\n\\mathbb{E}[(M_{t_3} - M_{t_2})(M_{t_2} - M_{t_1})] &= \\mathbb{E}[\\mathbb{E}[(M_{t_3} - M_{t_2})(M_{t_2} - M_{t_1})|\\mathcal{F}_{t_2}]] \\\\\n&= \\mathbb{E}[(M_{t_2} - M_{t_1})\\mathbb{E}[(M_{t_3} - M_{t_2})|\\mathcal{F}_{t_2}]] \\\\\n&= \\mathbb{E}[(M_{t_2} - M_{t_1})(M_{t_2} - M_{t_2})] \\\\\n&= 0\n\\end{align*}\\]\nwhere \\(\\mathbb{E}[M_{t_3} - M_{t_2}|\\mathcal{F}_{t_2}] = 0\\) follows from the fact that \\((M_t,t\\geq 0)\\) is a martingale. Consequently, \\(M_{t_3} - M_{t_2} \\perp M_{t_2} - M_{t_1}\\) and non-overlapping increments are independent.\nMoreover, \\(M(0) = 0\\). So, \\((M_t,t\\geq 0)\\) is a standard brownian motion. This closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/ito-processes-and-stochastic-diff-eqs/index.html#exercise-problems",
    "href": "posts/ito-processes-and-stochastic-diff-eqs/index.html#exercise-problems",
    "title": "Ito Processes and Stochastic Differential Equations",
    "section": "",
    "text": "Exercise 5 Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and \\((W_t,t\\geq 0)\\) be a standard Wiener process. Find the SDE for the random process \\(X_t = W_t^n, n \\in \\mathbb{Z}^{+}\\).\nShow that\n\\[\n\\mathbb{E}[W_t^n] = \\frac{1}{2}n(n-1) \\int_0^t \\mathbb{E}\\left[W_s^{(n-2)}\\right] ds\n\\]\nand using mathematical induction prove that:\n\\[\n\\mathbb{E}[W_t^n] = \\begin{cases}\n\\frac{n! t^{n/2}}{2^{n/2} \\left(\\frac{n}{2}\\right)!}, & n=2,4,6,\\ldots\\\\\n0, & n=1,3,5,\\ldots\n\\end{cases}\n\\]\n\nSolution."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html",
    "href": "posts/gaussian-discriminant-analysis/index.html",
    "title": "Classification Algorithms",
    "section": "",
    "text": "Let’s say that you have input data \\(\\mathcal{D}=\\{(\\mathbf{x}_i,y_i): i=1,2,\\ldots,N\\}\\), and suppose that \\(y_i \\in \\{0,1\\}\\). Each input has \\(D\\)-features, so \\(\\mathbf{x}_i \\in \\mathbf{R}^D\\). You want to classify an arbitrary feature vector \\(\\mathbf{x}\\) into \\(y=0\\) or \\(y=1\\).\nOne way to build a classifier is to learn the joint probability distribution \\(p(\\mathbf{x},y)\\) and then to condition on \\(\\mathbf{x}\\), thereby deriving \\(p(y=c|\\mathbf{x})\\). This is called the generative approach. An alternative approach is to fit a model of the form \\(p(y|\\mathbf{x})\\) directly. This is called the discriminative approach."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#discriminative-versus-generative-models",
    "href": "posts/gaussian-discriminant-analysis/index.html#discriminative-versus-generative-models",
    "title": "Classification Algorithms",
    "section": "",
    "text": "Let’s say that you have input data \\(\\mathcal{D}=\\{(\\mathbf{x}_i,y_i): i=1,2,\\ldots,N\\}\\), and suppose that \\(y_i \\in \\{0,1\\}\\). Each input has \\(D\\)-features, so \\(\\mathbf{x}_i \\in \\mathbf{R}^D\\). You want to classify an arbitrary feature vector \\(\\mathbf{x}\\) into \\(y=0\\) or \\(y=1\\).\nOne way to build a classifier is to learn the joint probability distribution \\(p(\\mathbf{x},y)\\) and then to condition on \\(\\mathbf{x}\\), thereby deriving \\(p(y=c|\\mathbf{x})\\). This is called the generative approach. An alternative approach is to fit a model of the form \\(p(y|\\mathbf{x})\\) directly. This is called the discriminative approach."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#logistic-regression",
    "href": "posts/gaussian-discriminant-analysis/index.html#logistic-regression",
    "title": "Classification Algorithms",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe sigmoid function \\(sigm(x)\\) is defined as:\n\\[\\begin{align*}\nsigm(x) = \\frac{e^x}{1+e^x} \\tag{1}\n\\end{align*}\\]\nThe logistic regression models the class posterior probability as:\n\\[\\begin{align*}\np(y=1|\\mathbf{x}) =sigm(\\mathbf{w}^T \\mathbf{x}) = \\frac{e^{\\mathbf{w}^T \\mathbf{x}}}{1 + e^{\\mathbf{w}^T \\mathbf{x}}} \\tag{2}\n\\end{align*}\\]\nRe-arranging, we can write:\n\\[\\begin{align*}\n\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})} &= e^{\\mathbf{w}^T \\mathbf{x}}\\\\\n\\log \\left(\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})}\\right) &= \\mathbf{w}^T \\mathbf{x} \\tag{3}\n\\end{align*}\\]\nThe quantity \\(\\frac{p(y=1|\\mathbf{x})}{1 - p(y=1|\\mathbf{x})}\\) is called the odds and can take on any value between \\(0\\) and \\(\\infty\\). Odds are traditionally used instead of probabilities to express chances of winning in horse-racing and casino games such as roulette.\nThe left-hand side is called log odds or logit. In the simplest case of \\(D=1\\) predictor, the equation (3) becomes:\n\\[\\begin{align*}\n\\log \\left(\\frac{p(y_i = 1|x_i,\\mathbf{w})}{1 - p(y_i = 1|x_i,\\mathbf{w})}\\right) &= w_0 + w_1 x_i \\tag{4}\n\\end{align*}\\]\n\nLikelihood\nThe likelihood of all the data is:\n\\[\\begin{align*}\nL(w_0,w_1) &= \\prod_{i=1}^{N} p(y_i|\\mathbf{x}_i) \\\\\n&= \\prod_{i=1}^{N} p(y_i=1|\\mathbf{x}_i)^{I(y_i=1)} p(y_i=0|\\mathbf{x}_i)^{I(y_i=0)} \\\\\n&= \\prod_{i=1}^{N} p(y_i=1|\\mathbf{x}_i)^{I(y_i=1)} \\cdot [1 - p(y_i=1|\\mathbf{x}_i)]^{I(y_i=0)} \\tag{5}\n\\end{align*}\\]\nWe seek estimates for \\(w_0\\) and \\(w_1\\), such that the predicted class probabilities \\(\\hat{p}(y_i = 1|x_i)\\) and \\(\\hat{p}(y_i = 0|x_i)\\) are as close as possible to the observed class labels. So, we try to maximize the likelihood function \\(L\\)."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#linear-discriminant-analysis",
    "href": "posts/gaussian-discriminant-analysis/index.html#linear-discriminant-analysis",
    "title": "Classification Algorithms",
    "section": "Linear Discriminant Analysis",
    "text": "Linear Discriminant Analysis\nLet \\(c\\) be an arbitrary class label. By the Bayes formula,\n\\[\\begin{align*}\np(y=c|\\mathbf{x}) &= \\frac{p(\\mathbf{x},y=c)}{p(\\mathbf{x})} \\\\\n&= \\frac{p(\\mathbf{x}|y=c) \\cdot p(y=c)}{\\sum_{c=1}^{C} p(\\mathbf{x}|y=c) \\cdot p(y=c)} \\tag{6}\n\\end{align*}\\]\nThe LDA is a generative classifier that models the class conditional distribution \\(p(\\mathbf{x}|y=c)\\) and the class prior \\(p(y=c)\\) and applies the Bayes rule to derive \\(p(y=c|\\mathbf{x})\\).\nLDA makes the following assumptions:\n\nThe prior follows a Bernoulli distribution.\n\n\\[\\begin{align*}\np(y=y_i) = \\phi^{y_i} (1 - \\phi)^{(1-y_i)}\n\\end{align*}\\]\n\nThe data from class \\(c\\) is a \\(D\\)-dimensional multivariate gaussian distribution. We have:\n\n\\[\\begin{align*}\np(\\mathbf{x}|y=c) = \\mathcal{N}(\\mathbf{\\mu}_c,\\mathbf{\\Sigma}) \\tag{8}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\np(\\mathbf{x}|y=c) &= \\frac{1}{(2\\pi)^{D/2} |\\det \\mathbf{\\Sigma}|^{1/2}} \\exp \\left[-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_c)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_c) \\right] \\tag{9}\n\\end{align*}\\]\n\nLikelihood\nThe likelihood of all the data is:\n\\[\\begin{align*}\nL(\\phi,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_C,\\mathbf{\\Sigma}) &= \\prod_{i=1}^{N} p(\\mathbf{x}_i,y_i)\\\\\n&=\\prod_{i=1}^{N} p(\\mathbf{x}_i|y_i)\\cdot p(y=y_i) \\tag{10}\n\\end{align*}\\]\n\n\nLog-Likelihood\nThe log-likelihood function \\(l\\) is:\n\\[\\begin{align*}\nl(\\phi,\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_C,\\mathbf{\\Sigma}) = \\log L &= \\sum_{i=1}^{N} \\log p(\\mathbf{x}_i|y_i) + \\sum_{i=1}^{N} \\log p(y=y_i) \\tag{11}\n\\end{align*}\\]\nFor simplicity let’s assume we have \\(C=2\\) classes. Then, the above sum can be written as:\n\\[\\begin{align*}\nl(\\phi,\\mathbf{\\mu}_0,\\mathbf{\\mu}_1,\\mathbf{\\Sigma}) &= \\sum_{i=1}^{N} I(y_i=1)\\log p(\\mathbf{x}_i|y=1) + \\sum_{i=1}^{N} I(y_i = 0)\\log p(\\mathbf{x}_i|y=0) \\\\ &+ \\sum_{i=1}^{N} I(y_i=1) \\log p(y=y_i) + \\sum_{i=1}^{N} I(y_i=0) \\log p(y=y_i) \\tag{12}\n\\end{align*}\\]\n\n\nMLE Estimate for \\(\\phi\\)\nThe first two terms of the log-likelihood function \\(l\\) are not a function of \\(\\phi\\). Taking the partial derivative of \\(l\\) with respect to \\(\\phi\\) on both sides, we are left with:\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\phi} &= \\frac{\\partial}{\\partial \\phi}\\left[\\sum_{i=1}^{N}I(y_i = 1) y_i\\log \\phi + \\sum_{i=1}^{N} I(y_i=0)(1-y_i)\\log(1-\\phi)\\right]\\\\\n&= \\sum_{i=1}^{N} I(y_i = 1) \\frac{y_i}{\\phi} + \\sum_{i=1}^{N} I(y_i=0) (1-y_i)\\frac{-1}{1-\\phi}\\\\\n&= \\sum_{i=1}^{N} I(y_i = 1) \\frac{1}{\\phi} - \\sum_{i=1}^{N} I(y_i=0) \\frac{1}{1-\\phi} \\tag{13}\n\\end{align*}\\]\nEquating \\(\\frac{\\partial l}{\\partial \\phi}\\) to zero:\n\\[\\begin{align*}\n\\sum_{i=1}^{N} I(y_i = 1) \\frac{1}{\\phi} &= \\sum_{i=1}^{N} I(y_i=0) \\frac{1}{1-\\phi}\\\\\n(1-\\phi)\\sum_{i=1}^{N} I(y_i = 1) &= \\phi\\sum_{i=1}^{N} I(y_i=0)\\\\\n\\sum_{i=1}^{N} I(y_i = 1) &= \\phi\\sum_{i=1}^{N} I(y_i=0) + \\phi\\sum_{i=1}^{N} I(y_i=1)\\\\\n\\sum_{i=1}^{N} I(y_i = 1) &= \\phi \\cdot N \\\\\n\\hat{\\phi} &= \\frac{\\sum_{i=1}^{N} I(y_i = 1)}{N} \\tag{14}\n\\end{align*}\\]\n\n\nMLE Estimate for \\(\\mu_c\\)\nFirst, note that:\n\\[\\begin{align*}\n\\log p(\\mathbf{x}_i|y=1) = -\\frac{D}{2}\\log(2\\pi) - \\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|) - \\frac{1}{2}(\\mathbf{x}_i - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x}_i - \\mathbf{\\mu}_1) \\tag{15}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mu_1} &= -\\frac{1}{2}\\sum_{i=1}^{N} I(y_i = 1)\\frac{\\partial}{\\partial \\mu_1}[(\\mathbf{x}_i - \\mu_1)^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu_1)] \\tag{16}\n\\end{align*}\\]\nWe know that, \\(\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^T A \\mathbf{x}) = 2A \\mathbf{x}\\).\nConsequently,\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mu_1} &= -\\mathbf{\\Sigma}^{-1}\\sum_{i=1}^{N} I(y_i = 1) (\\mathbf{x}_i - \\mu_1) \\tag{17}\n\\end{align*}\\]\nEquating \\(\\frac{\\partial l}{\\partial \\mu_1} = 0\\), we have:\n\\[\\begin{align*}\n\\hat{\\mu}_1 &= \\frac{\\sum_{i=1}^{N}I(y_i = 1) \\mathbf{x}_i}{\\sum_{i=1}^{N}I(y_i = 1)} \\tag{18}\n\\end{align*}\\]\nIn general, for a class \\(c\\), we have:\n\\[\\begin{align*}\n\\hat{\\mu}_c &= \\frac{\\sum_{i=1}^{N}I(y_i = c) \\mathbf{x}_i}{\\sum_{i=1}^{N}I(y_i = c)} \\tag{19}\n\\end{align*}\\]\n\n\nTraces and Determinants\nDefinition. The trace of a square matrix \\(A\\) is defined to the sum of the diagonal elements \\(a_{ii}\\) of \\(A\\)\n\\[\\begin{align*}\ntr(A) = \\sum_i a_{ii} \\tag{20}\n\\end{align*}\\]\nClaim. (Cyclic property) Let \\(A,B,C\\) be arbitrary matrices whose dimensions are conformal and are such that the product \\(ABC\\) (and therefore the other two products) is a square matrix. Then, the trace is invariant under cyclic permutations of matrix products:\n\\[\\begin{align*}\ntr(ABC) = tr(BCA) = tr(CAB) \\tag{21}\n\\end{align*}\\]\nProof.\nWe have:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} (ABC)_{ii} \\tag{22}\n\\end{align*}\\]\nThe \\((i,i)\\) element of \\(ABC\\) must be the inner product of the \\(i\\)-th row of \\(A\\) and the \\(i\\)-th column of \\(BC\\). So:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j A_{ij} (BC)_{ji} \\tag{23}\n\\end{align*}\\]\nThe \\((j,i)\\) element of \\(BC\\) must be the inner product of the \\(j\\)-th row of \\(B\\) and the \\(i\\)-th column of \\(C\\). So:\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j A_{ij} \\sum_{k} B_{jk} C_{ki} \\\\\n&= \\sum_{i} \\sum_j \\sum_{k} A_{ij}  B_{jk} C_{ki} \\tag{24}\n\\end{align*}\\]\nBut, this can be re-written as\n\\[\\begin{align*}\ntr(ABC) &= \\sum_{i} \\sum_j \\sum_{k} A_{ij}  B_{jk} C_{ki} \\\\\n&= \\sum_j \\sum_k B_{jk} \\sum_i C_{ki} A_{ij} \\\\\n&= \\sum_j \\sum_k B_{jk} (CA)_{kj} \\\\\n&= \\sum_j (BCA)_{jj} \\\\\n&= tr(BCA) \\tag{25}\n\\end{align*}\\]\nSimilarly, it can be shown that \\(tr(BCA) = tr(CAB)\\). This closes the proof.\nClaim. Let \\(A\\) and \\(B\\) be matrices. Then,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} tr(BA) = B^T \\tag{26}\n\\end{align*}\\]\nProof.\nWe have:\n\\[\\begin{align*}\ntr(BA) &= \\sum_i (BA)_{ii} \\\\\n&= \\sum_i \\sum_j B_{ij} A_{ji}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\left[\\frac{\\partial}{\\partial A} tr(BA)\\right]_{(i,j)} = \\frac{\\partial}{\\partial a_{ij}} tr(BA) = B_{ji}\n\\end{align*}\\]\nThis closes the proof.\nClaim. Let \\(A\\) be a square matrix. Then:\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} \\log (\\det A) = (A^{-1})^T \\tag{27}\n\\end{align*}\\]\nProof.\nRecall that:\n\\[\\begin{align*}\n\\det A = \\sum_{j} a_{ij} C_{ij}\n\\end{align*}\\]\nwhere \\(C_{ij}\\) is the cofactor obtained after removing the \\(i\\)-th row and \\(j\\)-th column of \\(A\\). Thus,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial a_{ij}}\\det A = C_{ij}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A}\\det A = C\n\\end{align*}\\]\nwhere \\(C\\) is the cofactor matrix of \\(A\\). We know that \\(C = (adj A)^T\\), where \\(adj A\\) is the adjugate of \\(A\\). Moreover, \\(A^{-1} = \\frac{1}{|\\det A|} adj (A)\\). Therefore,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial A} \\log (\\det A) &= \\frac{1}{|\\det A|} \\frac{\\partial}{\\partial A}\\det A \\\\\n&= \\frac{1}{|\\det A|} C \\\\\n&= \\frac{1}{|\\det A|} (adj A)^T \\\\\n&= \\left(\\frac{1}{|\\det A|} adj A\\right)^T \\\\\n&= (A^{-1})^T\n\\end{align*}\\]\n\n\nMLE Estimate for the covariance matrix \\(\\mathbf{\\Sigma}\\)\nSince \\(\\mathbf{x}^T A \\mathbf{x}\\) is a scalar, \\(\\mathbf{x}^T A \\mathbf{x} = tr(\\mathbf{x}^T A \\mathbf{x})\\). We have:\n\\[\\begin{align*}\n\\mathbf{x}^T A \\mathbf{x} &= tr(\\mathbf{x}^T A \\mathbf{x}) = tr(A \\mathbf{x} \\mathbf{x}^T) = tr(\\mathbf{x} \\mathbf{x}^T A)\n\\end{align*}\\]\nWe have:\n\\[\\begin{align*}\nl(\\phi,\\mu_c,\\mathbf{\\Sigma}) &= \\sum_{i=1}^{N} \\log p(\\mathbf{x}_i|y_i) + \\sum_{i=1}^{N} \\log p(y=y_i) \\\\\n&= -\\frac{ND}{2} \\log(2\\pi) - \\frac{N}{2} \\log(|\\det \\mathbf{\\Sigma}|) \\\\\n&- \\frac{1}{2}\\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x}_i - \\mu_{y_i}) \\\\\n&+ \\sum_{i=1}^{N} \\log p(y=y_i)\\\\\n&= -\\frac{ND}{2} \\log(2\\pi) + \\frac{N}{2} \\log(|\\det \\mathbf{\\Sigma}^{-1}|) \\\\\n&- \\frac{1}{2}\\sum_{i=1}^{N} tr[(\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T \\mathbf{\\Sigma}^{-1}]  \\\\\n&+ \\sum_{i=1}^{N} \\log p(y=y_i)\n\\end{align*}\\]\nDifferentiating both sides with respect to \\(\\mathbf{\\Sigma}^{-1}\\), get:\n\\[\\begin{align*}\n\\frac{\\partial l}{\\partial \\mathbf{\\Sigma}^{-1}} &= \\frac{N}{2} \\mathbf{\\Sigma} - \\frac{1}{2}\\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T\n\\end{align*}\\]\nConsequently, we have:\n\\[\\begin{align*}\n\\hat{\\mathbf{\\Sigma}}_{mle} &= \\frac{1}{N} \\sum_{i=1}^{N} (\\mathbf{x}_i - \\mu_{y_i}) (\\mathbf{x}_i - \\mu_{y_i})^T \\tag{28}\n\\end{align*}\\]\n\n\nDecision boundary\nLet’s again consider the binary classification problem with \\(C=2\\) classes. The decision boundary is the line or the hyperplane that separates the part of the space where the probability that the point belongs to class \\(1\\) is larger than \\(50\\) percent from the part where the probability that the point belongs to class \\(2\\) is larger than \\(50\\) percent.\nThe decision boundary is given by \\(p(y=1|\\mathbf{x}) = p(y=0|\\mathbf{x})\\). Since these probabilities involve an exponent, it’s convenient to take logarithms on both sides. This results in:\n\\[\\begin{align*}\n\\cancel{-\\frac{D}{2}\\log(2\\pi)} - \\cancel{\\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|)} - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) = \\\\\n\\cancel{-\\frac{D}{2}\\log(2\\pi)} - \\cancel{\\frac{1}{2} \\log(|\\det \\mathbf{\\Sigma}|)} - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu}_0)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0) \\tag{29}\n\\end{align*}\\]\nSimplifying, we have:\n\\[\\begin{align*}\n(\\mathbf{x} - \\mathbf{\\mu}_1)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) &= (\\mathbf{x} - \\mathbf{\\mu}_0)^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0) \\tag{30}\n\\end{align*}\\]\n\\[\\begin{align*}\n(\\mathbf{x}^T - \\mathbf{\\mu}_1^T) \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_1) &= (\\mathbf{x}^T - \\mathbf{\\mu}_0^T) \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_0)\\\\\n\\cancel{\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}} - \\mathbf{x}^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_1 - \\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\mathbf{\\mu}_1^T \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_1 &= \\cancel{\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} \\mathbf{x}} - \\mathbf{x}^T \\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_0 - \\mathbf{\\mu}_0^T \\mathbf{\\Sigma}^{-1} \\mathbf{x} + \\mathbf{\\mu}_0^T \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_0\n\\end{align*}\\]\nNote that, \\(\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0)\\) is a scalar, so \\(\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0) = (\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0))^T\\). So, we get:\n\\[\\begin{align*}\n2\\mathbf{x}^T \\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0) = \\underbrace{\\mu_1^T \\mathbf{\\Sigma}^{-1} \\mu_1 - \\mu_0^T \\mathbf{\\Sigma}^{-1} \\mu_0}_{\\text{constant}} \\tag{31}\n\\end{align*}\\]\nThis is the equation of the decision boundary. This is a linear projection of the vector \\(\\mathbf{x}\\) onto the \\(\\mathbf{\\Sigma}^{-1} (\\mu_1 - \\mu_0)\\) direction. Whenever this projection equals to this constant, we are on the decision boundary; when it’s larger than this threshold, it’s class \\(1\\) and when it’s smaller it’s class \\(2\\). So, the decision boundary is just a line perpendicular to this vector and crossing it in the point that corresponds to this threshold.\nTo make it clear, the fact that the decision boundary is linear follows from our assumption that the covariances are the same."
  },
  {
    "objectID": "posts/gaussian-discriminant-analysis/index.html#quadratic-discriminant-analysis-qda",
    "href": "posts/gaussian-discriminant-analysis/index.html#quadratic-discriminant-analysis-qda",
    "title": "Classification Algorithms",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\nLDA assumes that the data within each class \\(c\\) are drawn from a multivariate Gaussian distribution with a class-specific mean vector \\(\\mathbf{\\mu}_c\\) and a covariance matrix that common to all \\(C\\) classes. Quadratic Discriminant Analysis (QDA) classifier assumes that the observations from each class are drawn from a Gaussian distribution and each class has its own mean vector \\(\\mathbf{\\mu}_c\\) and covariance matrix \\(\\mathbf{\\Sigma}_c\\).\n\\[\\begin{align*}\np(y=c|\\mathbf{x}) = \\frac{1}{(2\\pi)^{D/2} |\\det \\mathbf{\\Sigma_c}|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{x} - \\mu_c)^T \\mathbf{\\Sigma}_c^{-1}(\\mathbf{x} - \\mu_c)\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html",
    "href": "posts/exploring-option-greeks/index.html",
    "title": "Exploring Option Greeks",
    "section": "",
    "text": "I derived the Black-Scholes formula for European style vanilla FX options in a previous post here. The Black-Scholes model \\(Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma)\\) equipped with a single flat volatility parameter \\(\\sigma\\) produces option prices which are NOT consistent with the observed market prices of FX options across different strikes and maturities.\nAlthough, the BS model suffers many flaws, it is still often used, at least for quoting purposes. Since all of the other inputs into the model - market data variables such as the stock price \\(S_0\\), the domestic depo rate \\(r_{DOM}\\), the foreign depo rate \\(r_{FOR}\\), and the parameters such as option strike \\(K\\), the time-to-maturity \\(T\\), can be either seen in the market or are known constants, we can easily solve for the value \\(\\sigma_{\\text{imp}}\\) of the parameter \\(\\sigma\\) such that:\n\\[Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma_{\\text{imp}}) = V_{\\text{market}}\\]\nThis value \\(\\sigma_{\\text{imp}}\\) implied from the market price of the option is called the implied volatility.\nThus, although the BS model suffers from flaws, it is mainly used as a quote converter. In the FX options market, option prices are quoted in terms of implied volatilities. The BS formula is used to convert implied vols \\(\\sigma_{\\text{imp}}\\) to prices and vice versa. The delta hedge to be exchanged between counterparties is calculated according to the BS formula, and this is also true for the Vega hedge of various exotic options. In many cases, the model is also used to run trading books.\nIn this note, I explore various delta conventions and derive the greeks. Check out FX Vol smile by Wyestup! The entire concept of the FX volatility smile is based on the parametrization with respect to delta."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html#introduction.",
    "href": "posts/exploring-option-greeks/index.html#introduction.",
    "title": "Exploring Option Greeks",
    "section": "",
    "text": "I derived the Black-Scholes formula for European style vanilla FX options in a previous post here. The Black-Scholes model \\(Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma)\\) equipped with a single flat volatility parameter \\(\\sigma\\) produces option prices which are NOT consistent with the observed market prices of FX options across different strikes and maturities.\nAlthough, the BS model suffers many flaws, it is still often used, at least for quoting purposes. Since all of the other inputs into the model - market data variables such as the stock price \\(S_0\\), the domestic depo rate \\(r_{DOM}\\), the foreign depo rate \\(r_{FOR}\\), and the parameters such as option strike \\(K\\), the time-to-maturity \\(T\\), can be either seen in the market or are known constants, we can easily solve for the value \\(\\sigma_{\\text{imp}}\\) of the parameter \\(\\sigma\\) such that:\n\\[Bl(S_0,K,T,r_{DOM},r_{FOR},\\sigma_{\\text{imp}}) = V_{\\text{market}}\\]\nThis value \\(\\sigma_{\\text{imp}}\\) implied from the market price of the option is called the implied volatility.\nThus, although the BS model suffers from flaws, it is mainly used as a quote converter. In the FX options market, option prices are quoted in terms of implied volatilities. The BS formula is used to convert implied vols \\(\\sigma_{\\text{imp}}\\) to prices and vice versa. The delta hedge to be exchanged between counterparties is calculated according to the BS formula, and this is also true for the Vega hedge of various exotic options. In many cases, the model is also used to run trading books.\nIn this note, I explore various delta conventions and derive the greeks. Check out FX Vol smile by Wyestup! The entire concept of the FX volatility smile is based on the parametrization with respect to delta."
  },
  {
    "objectID": "posts/exploring-option-greeks/index.html#quote-style-conversions.",
    "href": "posts/exploring-option-greeks/index.html#quote-style-conversions.",
    "title": "Exploring Option Greeks",
    "section": "Quote style conversions.",
    "text": "Quote style conversions.\nIn FX markets, options are quoted in one of 4 quote styles - domestic per foreign (d/f), percentage foreign (%f), percentage domestic (%d) and foreign per domestic (f/d).\nThe standard Black-Scholes formula is:\n\\[\n\\begin{align*}\nV_{d/f} &= \\omega [S_0 e^{-r_{FOR} T} \\Phi(d_{+}) - K e^{-r_{DOM}T} \\Phi(d_{-})\\\\\n&= \\omega e^{-r_{DOM}T}[F \\Phi(d_{+}) - K  \\Phi(d_{-})]\n\\end{align*}\n\\]\n\nImplementing the Bl Calculator and Option Greeks.\nimport numpy as np\nfrom scipy.stats import norm\nfrom enum import Enum\nimport datetime as dt\n\nclass CallPut(Enum):\n    CALL_OPTION = 1\n    PUT_OPTION = -1\n\nclass BlackCalculator:\n    \"\"\"Implements the Black formula to price a vanilla option\"\"\"\n    def __init__(\n        self,\n        s_t : float,\n        strike : float,\n        today : float,\n        expiry : float,\n        r_dom : float,\n        r_for : float,\n        sigma : float            \n    )\n        self._s_t = s_t\n        self._strike = strike\n        self._today = today\n        self._expiry = expiry\n        self._r_dom = r_dom\n        self._r_for = r_for\n        self._sigma = sigma\n\n    def at_the_money_forward(\n        self,\n    ) -&gt; float :\n        \"\"\"Computes the at-the-money forward\"\"\"\n\n        foreign_df = np.exp(self._r_for * (expiry - today))\n        domestic_df = np.exp(self._r_dom * (expiry - today))\n        fwd_points = foreign_df / domestic_df\n        return self._s_t * fwd_points \n            \n    def d_plus(S_t,K,t,T,r_DOM,r_FOR,sigma):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        return (np.log(F/K) + (T-t)*(sigma**2)/2)/(sigma * np.sqrt(T - t))\n\n    def d_minus(S_t,K,t,T,r_DOM,r_FOR,sigma):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        return (np.log(F/K) - (T-t)*(sigma**2)/2)/(sigma * np.sqrt(T - t))\n\n    def pv(S_t,K,t,T,r_DOM,r_FOR,sigma, CCY1Notional,callPut):\n        F = at_the_money_forward(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        omega = callPut.value\n        d_plus = dPlus(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        d_minus = dMinus(S_t,K,t,T,r_DOM,r_FOR,sigma)\n        domesticDF = np.exp(-r_DOM*(T-t))\n        \n        undiscountedPrice = omega* (F * norm.cdf(omega * d_plus) - K * norm.cdf(omega * d_minus))\n        pv = domesticDF * undiscountedPrice * CCY1Notional\n        return pv"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "",
    "text": "The following post is going to derive the least squares estimate of the coefficients of linear regression. Our data consists of \\(p\\) predictors or features \\(X_1,\\ldots,X_p\\) and a response \\(Y\\), and there are \\(n\\) observations in our dataset. Assume that the data arises from the real world model:\n\\[\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\ldots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\ldots \\\\\n\\epsilon_n\n\\end{bmatrix} \\tag{1}\n\\end{align}\\]\nor in matrix notation,\n\\[Y = X \\beta + \\epsilon \\tag{2}\\]\n\n\nThe real world model in equation (1) is called the population regression line.\nIn statistics, we quite often do not know the population mean \\(\\mu\\), but we try to estimate it using the sample mean \\(\\hat{\\mu}\\).\nIn a similar vein, we do not know the true values of the regression coefficients \\(\\beta_1,\\beta_2,\\ldots,\\beta_p\\). Instead, we estimate them from the data as \\(\\hat{\\beta_1},\\hat{\\beta_2},\\ldots,\\hat{\\beta_p}\\).\nSo, our linear regression model would predict an outcome:\n\\[\\hat{Y} = \\hat{\\beta_1}X_1 + \\hat{\\beta_2} X_2 + \\ldots +\\hat{\\beta_p} X_p \\tag{3}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#introduction",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#introduction",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "",
    "text": "The following post is going to derive the least squares estimate of the coefficients of linear regression. Our data consists of \\(p\\) predictors or features \\(X_1,\\ldots,X_p\\) and a response \\(Y\\), and there are \\(n\\) observations in our dataset. Assume that the data arises from the real world model:\n\\[\\begin{align}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\ldots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1p} \\\\\nx_{21} & x_{22} & \\ldots & x_{2p} \\\\\n\\vdots \\\\\nx_{n1} & x_{n2} & \\ldots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\ldots \\\\\n\\epsilon_n\n\\end{bmatrix} \\tag{1}\n\\end{align}\\]\nor in matrix notation,\n\\[Y = X \\beta + \\epsilon \\tag{2}\\]\n\n\nThe real world model in equation (1) is called the population regression line.\nIn statistics, we quite often do not know the population mean \\(\\mu\\), but we try to estimate it using the sample mean \\(\\hat{\\mu}\\).\nIn a similar vein, we do not know the true values of the regression coefficients \\(\\beta_1,\\beta_2,\\ldots,\\beta_p\\). Instead, we estimate them from the data as \\(\\hat{\\beta_1},\\hat{\\beta_2},\\ldots,\\hat{\\beta_p}\\).\nSo, our linear regression model would predict an outcome:\n\\[\\hat{Y} = \\hat{\\beta_1}X_1 + \\hat{\\beta_2} X_2 + \\ldots +\\hat{\\beta_p} X_p \\tag{3}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#residual-sum-of-squares",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#residual-sum-of-squares",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Residual sum of squares",
    "text": "Residual sum of squares\nThe difference between the observed response value and the predicted response value is called as the residual.\nWe define the residual sum of squares as:\n\\[\\begin{align*}\n(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta})&= (Y' - \\hat{\\beta}' X')(Y - X\\hat{\\beta})\\\\\n&= Y'Y - Y'X \\hat{\\beta} - \\hat{\\beta}' X' Y + \\hat{\\beta}'X'X\\hat{\\beta}\n\\end{align*}\\]\nThe \\(j\\)-th column of \\(Y'X\\) is \\(\\sum_{i=1}^{n}y_i x_{ij}\\) and therefore the product \\(Y'X\\hat{\\beta}\\) equals \\(\\sum_{j=1}^{p}\\sum_{i=1}^{n}y_i x_{ij}\\hat{\\beta_j}\\). But, \\((x_{ij}) = (x_{ji})^T\\). The same sum can be re-written \\(\\sum_{i=1}^{n}\\sum_{j=1}^{p}\\hat{\\beta_j} x_{ji}^T y_i\\). Thus, \\(\\hat{\\beta}' X' Y = Y' X \\hat{\\beta}\\).\nConsequently,\n\\[\\begin{align*}\n(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta})&= Y'Y - 2Y'X \\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta} \\tag{4}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-i",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-i",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Aside proof I",
    "text": "Aside proof I\nClaim. Let \\(A \\in \\mathbf{R}^{m \\times n}\\) be a rectangular matrix and \\(\\vec{x}\\) be a vector of \\(n\\) elements and let \\(\\vec{y}\\) be the matrix-vector product:\n\\[\\vec{y} = A \\vec{x}\\]\nThen,\n\\[\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A\\]\nProof.\nLet \\(A_1,\\ldots,A_n\\) be the columns of \\(A\\). Then,\n\\[\\begin{align*}\n\\vec{y} &= [A_1, A_2, \\ldots, A_n] \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\\\\n&= A_1 x_1 + A_2 x_2 + \\ldots + A_n x_n\n\\end{align*}\\]\nThus,\n\\[\\frac{\\partial \\vec{y}}{\\partial x_i} = A_i\\]\nConsequently,\n\\[\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-ii",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#aside-proof-ii",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Aside proof II",
    "text": "Aside proof II\nClaim. Consider the quadratic form \\(Q(\\vec{x}) = \\vec{x}^T A^T A \\vec{x}\\). Then, we have:\n\\[\\frac{\\partial Q}{\\partial \\vec{x}} = 2A^T A\\vec{x}\\]\nProof.\nThe matrix \\(K = A^T A\\) is symmetric, since \\((A^T A)^T = A^T (A^T)^T = A^T A\\). So, \\(Q = \\vec{x}^T K \\vec{x}\\). Now, let \\(A = (A_1, A_2, \\ldots, A_n)\\) in the block form, \\(A_j\\) denotes the \\(j\\)-th column of \\(A\\). Thus, \\(A \\vec{x} =\\sum_j A_j x_j\\). and \\(\\vec{x}^T A^T = \\sum_j A_j x_j\\) as well. So, \\(Q = \\left(\\sum_j A_j x_j\\right)^2\\). Consequently,\n\\[\\begin{align}\n\\frac{\\partial Q}{\\partial x_j} &= 2 A_j \\left(\\sum_{j} A_j x_j\\right)\n\\end{align}\\]\nThus,\n\\[\\begin{align}\n\\frac{\\partial Q}{\\partial \\vec{x}} &= 2 \\begin{bmatrix}A_1 \\\\ A_2 \\\\ \\vdots \\\\\nA_n\\end{bmatrix} \\left(\\sum_{j} A_j x_j\\right) \\\\\n&= 2 A^T A \\vec{x}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#least-squares-estimate",
    "href": "posts/derivation-for-the-least-squares-estimate-of-beta-/index.html#least-squares-estimate",
    "title": "Derivation of the Least Squares Estimate Beta in Linear Regression",
    "section": "Least squares estimate",
    "text": "Least squares estimate\nWe proceed with minimizing the RSS expression in equation (4). Taking derivatives with respect to the vector \\(\\hat{\\beta}\\) on both sides, and equating to zero, we have:\n\\[\\begin{align*}\n\\frac{\\partial (RSS)}{\\hat{\\beta}}&= - 2Y'X + 2X'X\\hat{\\beta} = 0 \\\\\nX^T X \\hat{\\beta} &= Y^T X \\\\\n\\hat{\\beta} &= (X^T X)^{-1} Y^T X\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html",
    "href": "posts/cox-ingersoll-ross-model/index.html",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "",
    "text": "The short rate under the CIR model has the dynamics:\n\\[dr_t = \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t}dB_t\\]\nFor a moment, if we drop the stochastic term, and merely consider the first order linear ODE \\(\\frac{dr_t}{dt} + \\kappa r_t = \\kappa \\theta\\), the integrating factor for this differential equation is \\(e^{\\int \\kappa dt} = e^{\\kappa t}\\). Multiplying both sides by the integrating factor, we have:\n\\[\\begin{align*}\ne^{\\kappa t} dr_t &= \\kappa(\\theta - r_t) e^{\\kappa t}dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\ne^{\\kappa t} dr_t + r_t e^{\\kappa t}dt &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\nd(e^{\\kappa t} r_t) &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\n\\int_{0}^{t} d(e^{\\kappa s} r_s) &= \\theta \\kappa\\int_{0}^{t}  e^{\\kappa s} ds + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\n[e^{\\kappa s} r_s]_{0}^{t} &= \\kappa \\theta \\left[\\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t} + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s\\\\\ne^{\\kappa t}r_t - r_0 &= \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\ne^{\\kappa t} r_t &= r_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\nr_t &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\n\\end{align*}\\]\nThe mean is given by:\n\\[\\begin{align*}\n\\mathbf{E}[r_t] &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n\\end{align*}\\]\nThe random variable \\(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\) has mean \\(0\\) and variance:\n\\[\\begin{align*}\n\\mathbf{E}\\left[\\left(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\right)^2\\right] &= \\sigma^2 \\int_{0}^{t}e^{-2\\kappa(t-s)} \\mathbf{E}[r_s] ds \\\\\n&= \\sigma^2 e^{-2\\kappa t}\\int_{0}^{t}e^{2\\kappa s} \\left(r_0 e^{-\\kappa s} + \\theta(1-e^{-\\kappa s})\\right) ds\\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\int_{0}^{t} e^{\\kappa s} ds + \\sigma^2 \\theta e^{-2\\kappa t} \\int_{0}^{t}(e^{2\\kappa s}-e^{\\kappa s}) ds \\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\left[\\frac{e^{\\kappa s}}{\\kappa} \\right]_{0}^{t} +\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t} (e^{\\kappa t} - 1)+\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{2e^{\\kappa s}}{2\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa} e^{-2\\kappa t}(e^{2\\kappa t} - 2e^{\\kappa t} - (1 - 2))\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa}e^{-2\\kappa t} (1 + e^{2\\kappa t} - 2e^{\\kappa t})\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} (e^{-\\kappa t} - e^{-2\\kappa t})+\\frac{\\sigma^2 \\theta}{2\\kappa} (1 - e^{-\\kappa t})^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html#short-rate-dynamics-mean-and-variance",
    "href": "posts/cox-ingersoll-ross-model/index.html#short-rate-dynamics-mean-and-variance",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "",
    "text": "The short rate under the CIR model has the dynamics:\n\\[dr_t = \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t}dB_t\\]\nFor a moment, if we drop the stochastic term, and merely consider the first order linear ODE \\(\\frac{dr_t}{dt} + \\kappa r_t = \\kappa \\theta\\), the integrating factor for this differential equation is \\(e^{\\int \\kappa dt} = e^{\\kappa t}\\). Multiplying both sides by the integrating factor, we have:\n\\[\\begin{align*}\ne^{\\kappa t} dr_t &= \\kappa(\\theta - r_t) e^{\\kappa t}dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\ne^{\\kappa t} dr_t + r_t e^{\\kappa t}dt &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\nd(e^{\\kappa t} r_t) &= \\kappa e^{\\kappa t}\\theta dt + \\sigma e^{\\kappa t}\\sqrt{r_t} dB_t \\\\\n\\int_{0}^{t} d(e^{\\kappa s} r_s) &= \\theta \\kappa\\int_{0}^{t}  e^{\\kappa s} ds + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\n[e^{\\kappa s} r_s]_{0}^{t} &= \\kappa \\theta \\left[\\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t} + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s\\\\\ne^{\\kappa t}r_t - r_0 &= \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\ne^{\\kappa t} r_t &= r_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_{0}^{t}  e^{\\kappa s}\\sqrt{r_s} dB_s \\\\\nr_t &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\n\\end{align*}\\]\nThe mean is given by:\n\\[\\begin{align*}\n\\mathbf{E}[r_t] &= r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n\\end{align*}\\]\nThe random variable \\(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\) has mean \\(0\\) and variance:\n\\[\\begin{align*}\n\\mathbf{E}\\left[\\left(\\sigma \\int_{0}^{t}  e^{-\\kappa (t-s)}\\sqrt{r_s} dB_s\\right)^2\\right] &= \\sigma^2 \\int_{0}^{t}e^{-2\\kappa(t-s)} \\mathbf{E}[r_s] ds \\\\\n&= \\sigma^2 e^{-2\\kappa t}\\int_{0}^{t}e^{2\\kappa s} \\left(r_0 e^{-\\kappa s} + \\theta(1-e^{-\\kappa s})\\right) ds\\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\int_{0}^{t} e^{\\kappa s} ds + \\sigma^2 \\theta e^{-2\\kappa t} \\int_{0}^{t}(e^{2\\kappa s}-e^{\\kappa s}) ds \\\\\n&= \\sigma^2 r_0 e^{-2\\kappa t} \\left[\\frac{e^{\\kappa s}}{\\kappa} \\right]_{0}^{t} +\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{e^{\\kappa s}}{\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t} (e^{\\kappa t} - 1)+\\sigma^2 \\theta e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa} - \\frac{2e^{\\kappa s}}{2\\kappa}\\right]_{0}^{t}\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa} e^{-2\\kappa t}(e^{2\\kappa t} - 2e^{\\kappa t} - (1 - 2))\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} e^{-2\\kappa t}(e^{\\kappa t} - 1)+\\frac{\\sigma^2 \\theta}{2\\kappa}e^{-2\\kappa t} (1 + e^{2\\kappa t} - 2e^{\\kappa t})\\\\\n&= \\frac{\\sigma^2 r_0}{\\kappa} (e^{-\\kappa t} - e^{-2\\kappa t})+\\frac{\\sigma^2 \\theta}{2\\kappa} (1 - e^{-\\kappa t})^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/cox-ingersoll-ross-model/index.html#naive-python-implementation",
    "href": "posts/cox-ingersoll-ross-model/index.html#naive-python-implementation",
    "title": "Cox-Ingersoll-Ross (CIR) model",
    "section": "Naive python implementation",
    "text": "Naive python implementation\n\nCIRProcess class\nThe class CIRProcess is designed as an engine to generate sample paths of the CIR process.\n\nimport math\nfrom dataclasses import dataclass\n\nimport joypy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nfrom tqdm import tqdm\n\n\n@dataclass\nclass CIRProcess:\n    \"\"\"An engine for generating sample paths of the Cox-Ingersoll-Ross process\"\"\"\n\n    kappa: float\n    theta: float\n    sigma: float\n    step_size: float\n    total_time: float\n    r_0: float\n\n    def generate_paths(self, paths: int):\n        \"\"\"Generate sample paths\"\"\"\n        num_steps = int(self.total_time / self.step_size)\n        dz = np.random.standard_normal((paths, num_steps))\n        r_t = np.zeros((paths, num_steps))\n        zero_vector = np.full(paths, self.r_0)\n        prev_r = zero_vector\n        for i in range(num_steps):\n            r_t[:, i] = (\n                prev_r\n                + self.kappa * np.subtract(self.theta, prev_r) * self.step_size\n                + self.sigma\n                * np.sqrt(np.abs(prev_r))\n                * math.sqrt(self.step_size)\n                * dz[:, i]\n            )\n\n            prev_r = r_t[:, i]\n\n        return r_t\n\n\n\nSample Paths\nWe generate \\(N=10\\) paths of the CIR process.\n\n\nShow the code\ncir_process = CIRProcess(\n    kappa=3,\n    r_0=9,\n    sigma=0.5,\n    step_size=10e-3,\n    theta=3,\n    total_time=1.0,\n)\n\nnum_paths = 10\n\npaths = cir_process.generate_paths(num_paths)\n\nt = np.linspace(0.01, 1.0, 100)\n\nplt.grid(True)\nplt.xlabel(r\"Time $t$\")\nplt.ylabel(r\"$R(t)$\")\nplt.title(r\"$N=10$ paths of the Cox-Ingersoll-Ross process\")\nfor path in paths:\n    plt.plot(t, path)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nEvolution of the distribution.\nThe evolution of the distribution with time can be visualized.\n\n\nShow the code\n# TODO: - this is where slowness lies, generating paths is a brezze\n\n# Wrap the paths 2d-array in a dataframe\npaths_tr = paths.transpose()\n# Take 20 samples at times t=0.05, 0.10, 0.15, ..., 1.0 along each path\nsamples = paths_tr[4::5]\n# Reshape in a 1d column-vector\nsamples_arr = samples.reshape(num_paths * 20)\nsamples_df = pd.DataFrame(samples_arr, columns=[\"values\"])\nsamples_df[\"time\"] = [\n    \"t=\" + str((int(i / num_paths) + 1) / 20) for i in range(num_paths * 20)\n]\n\n# TODO: end\n\nfig, ax = joypy.joyplot(\n    samples_df,\n    by=\"time\",\n    colormap=cm.autumn_r,\n    column=\"values\",\n    grid=\"y\",\n    kind=\"kde\",\n    range_style=\"own\",\n    tails=10e-3,\n)\nplt.vlines(\n    [cir_process.theta, cir_process.r_0],\n    -0.2,\n    1,\n    color=\"k\",\n    linestyles=\"dashed\",\n)\nplt.show()"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html",
    "href": "posts/coding-a-neural-network-layer/index.html",
    "title": "Coding a neural network layer",
    "section": "",
    "text": "In 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called artificial neurons as building blocks.\nIn it’s most simple form, the neuron consists of :\n\ndendrites, which receive the information from other neurons\nsoma, which processes the information\nsynapse, transmits the output of this neuron\naxon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows.\nDefinition. An artificial neuron with weights \\(w_1,\\ldots,w_n \\in \\mathbf{R}\\), bias \\(b\\in\\mathbf{R}\\) and an activation function \\(\\rho:\\mathbf{R} \\to \\mathbf{R}\\) is defined as the scalar-valued function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) given by:\n\\[\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\\]\nwhere \\(\\mathbf{w} = (w_1,\\ldots,w_n)\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\).\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the \\(\\text{inputs} \\cdot \\text{ weights}\\) flow into our neuron, they are summed and a bias, another trainable parameter is added.\nSay, we have an input \\(x_1\\) and weight \\(w_1\\), then the output \\(y_1 = w_1 x_1\\) is a straight-line with slope \\(w_1\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe bias offsets the overall function.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\n\n\nLet us now look at some examples of activation functions.\nThe heaviside function is defined as:\n\\[\\begin{align*}\n\\rho(x) &=\n\\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nThe sigmoid function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#introduction",
    "href": "posts/coding-a-neural-network-layer/index.html#introduction",
    "title": "Coding a neural network layer",
    "section": "",
    "text": "In 1943, McCulloch and Pitts introduced artificial intelligence to the world. Their idea was to develop an algorithmic approach to mimic the functionality of the human brain. Due to the structure of the brain consisting of a net of neurons, they introduced the so-called artificial neurons as building blocks.\nIn it’s most simple form, the neuron consists of :\n\ndendrites, which receive the information from other neurons\nsoma, which processes the information\nsynapse, transmits the output of this neuron\naxon, point of connection to other neurons\n\nConsequently, a mathematical definition of an artificial neuron is as follows.\nDefinition. An artificial neuron with weights \\(w_1,\\ldots,w_n \\in \\mathbf{R}\\), bias \\(b\\in\\mathbf{R}\\) and an activation function \\(\\rho:\\mathbf{R} \\to \\mathbf{R}\\) is defined as the scalar-valued function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) given by:\n\\[\\begin{align*}\nf(x_1,\\ldots,x_n) = \\rho \\left(\\sum_{i=1}^{n}w_i x_i + b\\right) = \\rho(\\mathbf{w}^T \\mathbf{x}+b) \\tag{1}\n\\end{align*}\\]\nwhere \\(\\mathbf{w} = (w_1,\\ldots,w_n)\\) and \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\).\nA single neuron by itself is useless, but when combined with hundreds or thousands(or many more) of other neurons, the interconnectivity can approximate any complex function and frequently outperforms any other machine learning methods.\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,5}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=30 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n}\n\\foreach \\i in {1,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=green!30] (Output-\\i) at (9.0,-\\i * 2) {\\large $\\hat{y}_\\i$};\n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,2}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,...,5}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\foreach \\i in {1,...,5}\n{\n    \\foreach \\j in {1,2}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nDense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use. Once all of the \\(\\text{inputs} \\cdot \\text{ weights}\\) flow into our neuron, they are summed and a bias, another trainable parameter is added.\nSay, we have an input \\(x_1\\) and weight \\(w_1\\), then the output \\(y_1 = w_1 x_1\\) is a straight-line with slope \\(w_1\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=blue]{x};\n\\addlegendentry{\\(f(x)=x\\)}\n\\addplot[color=red]{2*x};\n\\addlegendentry{\\(f(x)=2x\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe bias offsets the overall function.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{x+1};\n\\addlegendentry{\\(f(x)=x+1\\)}\n\\addplot[color=gray]{x-1};\n\\addlegendentry{\\(f(x)=x-1\\)}\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\n\n\nLet us now look at some examples of activation functions.\nThe heaviside function is defined as:\n\\[\\begin{align*}\n\\rho(x) &=\n\\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nThe sigmoid function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\frac{1}{1+e^{-x}}\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{1/(1+exp(-x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe Rectifiable Linear Unit (ReLU) function is defined as:\n\\[\\begin{align*}\n\\rho(x) &= \\max(0,x)\n\\end{align*}\\]\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}\n\\addplot[color=black]{max(0,x)};\n\\end{axis}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#coding-a-layer-with-3-neurons",
    "href": "posts/coding-a-neural-network-layer/index.html#coding-a-layer-with-3-neurons",
    "title": "Coding a neural network layer",
    "section": "Coding a layer with 3-neurons",
    "text": "Coding a layer with 3-neurons\nLet’s code a simple layer with \\(n=3\\) neurons.\n\ninputs = [1, 2, 3, 2.5]\nweights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = []\n\n# For each neuron\nfor neuron_weights, neuron_bias in zip(weights, biases):\n    # zeroed output of the neuron\n    neuron_output = 0.0\n    # for each input and weight to the neuron\n    for input, weight in zip(inputs, neuron_weights):\n        # multiply this input with the associated weight\n        # and add to the neuron's output variable\n        neuron_output += input * weight\n    # Add bias\n    neuron_output += neuron_bias\n    # Put the neuron's result to the layer's output list\n    layer_outputs.append(neuron_output)\n\nprint(layer_outputs)\n\n[4.8, 1.21, 2.385]\n\n\nWe can achieve the same results as in our pure Python implementation of multiplying each component in our input vector \\(\\mathbf{x}\\) and weights vector \\(\\mathbf{w}\\) element-wise, by taking an inner product \\(\\mathbf{w} \\cdot \\mathbf{x}\\).\n\nimport numpy as np\n\ninputs = [1, 2, 3, 2.5]\nweights = [\n    [0.2, 0.8, -0.5, 1.0], \n    [0.5, -0.91, 0.26, -0.5], \n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\n# Output of the current layer\nlayer_outputs = np.dot(weights, inputs) + biases\n\nprint(layer_outputs)\n\n[4.8   1.21  2.385]\n\n\nTo train, neural networks tend to receive data in batches. So far, the example input data has only one sample (or observation) of various features called a feature set instance:\nsample = [1, 2, 3, 2.5]\nOften, neural networks expect to take in many samples at a time. One reason is its faster to train in batches in parallel processing. Also, if you fit on one sample at a time, you’re highly likely to keep fitting to that individual sample, rather than slowly producing general tweaks to the weights and biases that fit the entire dataset. Fitting or training in batches gives you a higher chance of making more meaningful changes to weights and biases."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#a-layer-of-neurons-and-a-batch-of-data",
    "href": "posts/coding-a-neural-network-layer/index.html#a-layer-of-neurons-and-a-batch-of-data",
    "title": "Coding a neural network layer",
    "section": "A layer of neurons and a batch of data",
    "text": "A layer of neurons and a batch of data\nCurrently, the weights matrix looks as follows:\n\\[\\begin{align*}\nW = \\begin{bmatrix}\n0.2 & 0.8 & -0.5 & 1.0 \\\\\n0.5 & -0.91 & 0.26 & -0.5 \\\\\n-0.26 & -0.27 & 0.17 & 0.87\n\\end{bmatrix}\n\\end{align*}\\]\nAnd say, that we have a batch of inputs:\n\\[\\begin{align*}\nX = \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 3.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\end{align*}\\]\nWe need to take the inner products \\((1.0, 2.0, 3.0, 3.5) \\cdot (0.2, 0.8, -0.5, 1.0)\\), \\((2.0, 5.0, -1.0, 2.0) \\cdot (0.2, 0.8, -0.5, 1.0)\\) and \\((-1.5, 2.7, 3.3, -0.8) \\cdot (0.2, 0.8, -0.5, 1.0)\\) for the first neuron.\nWe need to take the inner products \\((1.0, 2.0, 3.0, 3.5) \\cdot (0.5, -0.91, 0.26, -0.5)\\), \\((2.0, 5.0, -1.0, 2.0) \\cdot (0.5, -0.91, 0.26, -0.5)\\) and \\((-1.5, 2.7, 3.3, -0.8) \\cdot (0.5, -0.91, 0.26, -0.5)\\) for the second neuron.\nAnd so forth.\nConsider the matrix product \\(XW^T\\):\n\\[\\begin{align*}\nXW^T &= \\begin{bmatrix}\n1.0 & 2.0 & 3.0 & 2.5 \\\\\n2.0 & 5.0 & -1.0 & 2.0\\\\\n-1.5 & 2.7 & 3.3 & -0.8\n\\end{bmatrix}\n\\begin{bmatrix}\n0.2 & 0.5 & -0.26 \\\\\n0.8 & -0.91 & -0.27 \\\\\n-0.5 & 0.26 & 0.17 \\\\\n1.0 & -0.5 & 0.87\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n2.8 & -1.79 & 1.885 \\\\\n6.9 & -4.81 & -0.3 \\\\\n-0.59 & -1.949 & -0.474\n\\end{bmatrix}\n\\end{align*}\\]\n\nimport numpy as np\n\nX = [\n    [1.0, 2.0, 3.0, 2.5],\n    [2.0, 5.0, -1.0, 2.0],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nW = [\n    [0.2, 0.8, -0.5, 1.0],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nnp.dot(X,np.array(W).T)\n\narray([[ 2.8  , -1.79 ,  1.885],\n       [ 6.9  , -4.81 , -0.3  ],\n       [-0.59 , -1.949, -0.474]])\n\n\nSo, we can process a batch of inputs as:\n\nlayer_outputs = np.dot(X,np.array(W).T) + biases\nprint(layer_outputs)\n\n[[ 4.8    1.21   2.385]\n [ 8.9   -1.81   0.2  ]\n [ 1.41   1.051  0.026]]\n\n\nThe second argument for np.dot() is going to be our transposed weights. Before, we were computing the neuron output using a single sample of data, but now we’ve taken a step forward where we model the layer behavior on a batch of data."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#adding-layers",
    "href": "posts/coding-a-neural-network-layer/index.html#adding-layers",
    "title": "Coding a neural network layer",
    "section": "Adding Layers",
    "text": "Adding Layers\nThe neural network we have built is becoming more respectable, but at the moment, we have only one layer. Neural networks become deep when they have \\(2\\) or more hidden layers. At the moment, we have just one layer, which is effectively an output layer. Why we want two or more hidden layers will become apparent later on. Currently, we have no hidden layers. A hidden layer isn’t an input or output layer; as the scientist, you see the data as they are handed to the input layer and the resulting data from the output layer. Layers between these endpoints have values that we don’t necessarily deal with, and hence the name “hidden”. Don’t let this name convince you that you can’t access these values, though. You will often use them to diagnose issues or improve your neural network. To explore this concept, let’s add another layer to this neural network, and for now, let’s assume that these two layers that we’re going to have will be hidden layers, and we just coded our output layer yet.\nBefore we add another layer, let’s think about what’s coming. In the case of the first layer, we can see that we have an input with \\(4\\) features.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nSamples(feature set data) get fed through the input, which does not change it in any way, to our first hidden layer, which we can see has \\(3\\) sets of weights with \\(4\\) values each.\nEach of those \\(3\\) unique weight sets is associated with its distinct neuron. Thus, since we have \\(3\\) weight sets, we have \\(3\\) neurons in the first hidden layer. Each neuron has a unique set of weights, of which we have \\(4\\) (as there are \\(4\\) inputs to this layer), which is why our initial weights have a shape of \\((3,4)\\).\nNow we wish to add another layer. To do that, we must make sure that the expected input to that layer matches the previous layer’s output. We have set the number of neurons in a layer by setting how many weights and biases we have. The previous layer’s influence on weight sets for the current layer is that each weight set needs to have a separate weight per input. This means a distinct weight per neuron from the previous layer (or feature if we’re talking the input). The previous layer has \\(3\\) weight sets and \\(3\\) biases, so we know it has \\(3\\) neurons. This then means, for the next layer, we can have as many weight sets as we want (because this is how many neurons this new layer will have), but each of those weight sets must have \\(3\\) discrete weights.\nTo create this new layer, we are going to copy and paste our weights and biases to weights2 and biases2, and change their values to new made up sets. Here’s an example:\n\ninputs = [\n    [1, 2, 3, 2.5],\n    [2.0, 5.0, -1.0, 2],\n    [-1.5, 2.7, 3.3, -0.8]\n]\n\nweights = [\n    [0.2, 0.8, -0.5, 1],\n    [0.5, -0.91, 0.26, -0.5],\n    [-0.26, -0.27, 0.17, 0.87]\n]\n\nbiases = [2, 3, 0.5]\n\nweights2 = [\n    [0.1, -0.14, 0.5],\n    [-0.5, 0.12, -0.33],\n    [-0.44, 0.73, -0.13]\n]\n\nbiases2 = [-1, 2, -0.5]\n\nNext, we will now call the outputs layer1_outputs.\n\nlayer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n\nAs previously stated, inputs to the layers are either inputs from the actual dataset you’re training with, or outputs from a previous layer. That’s why we defined \\(2\\) versions of weights and biases, but only one of inputs.\n\nlayer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n\nAt this point, our neural network could be visually represented as:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {1,2,...,4}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input-\\i) at (0,-\\i * 2) {\\large $x_\\i$};\n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,3}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=-10 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n% Connect neurons In-Hidden1\n\\foreach \\i in {1,...,4}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Input-\\i) -- (Hidden1-\\j);   \n    }\n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {1,...,3}\n{\n    \\foreach \\j in {1,...,3}\n    {\n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\j);   \n    }\n}\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#training-data",
    "href": "posts/coding-a-neural-network-layer/index.html#training-data",
    "title": "Coding a neural network layer",
    "section": "Training Data",
    "text": "Training Data\nNext, rather than hand-typing in random data, we’ll use a function that can create non-linear data. What do we mean by non-linear? Linear data can be fit or represented by a straight line. Non-linear data cannot be represented well by a straight line.\nWe shall use the python package nnfs to create data. You can install it with\npip install nnfs\nYou typically don’t generate training data from a package like nnfs for your neural networks. Generating a dataset this way is purely for convenience at this stage. I shall also use this package to ensure repeatability.\n\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\nThe nnfs.init() does three things: it sets the random seed to \\(0\\) by default, creates a float32 dtype default and overrides the original dot product from numpy. All of these are meant to ensure repeatable results for following along.\n\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\n\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:,0], X[:,1])\nplt.show()\n\n\n\n\n\n\n\n\nThe spiral_data function allows us to create a dataset with as many classes as we want. The function has parameters to choose the number of classes and the number of points/observations per class in the resulting non-linear dataset.\nIf you trace from the center, you can determine all \\(3\\) classes separately, but this is a very challenging problem for a machine learning classifier to solve. Adding color to the chart makes this more clear:\n\nplt.scatter(X[:,0],X[:,1],c=y,cmap='brg')\nplt.show()\n\n\n\n\n\n\n\n\nKeep in mind that the neural network will not be aware of the color differences as the data have no class encodings. This is only made as an instruction for you. In the data above, each dot is an observation, that is, it’s coordinates are the samples that form the dataset. The classification for the dot has to do with which spiral it is a part of, depicted by red, blue or green color."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#dense-layer-class",
    "href": "posts/coding-a-neural-network-layer/index.html#dense-layer-class",
    "title": "Coding a neural network layer",
    "section": "Dense Layer Class",
    "text": "Dense Layer Class\nNow that we no longer need to hand-type our data, we should create something similar for our various types of neural network layers. So far, we’ve only used what’s called a dense or fully-connected layer. These layers are commonly referred to as dense layers in papers, literature and code, but you will see them called fully-connected or fc for short in the code I write. Our dense layer class begins with two methods:\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        # Initialize weights and biases\n        pass # using pass statement as a placeholder\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from inputs, weights and biases\n        pass # using pass statement as a placeholder\n\nWeights are often initialized randomly for a model, but not always. If you wish to load a pre-trained model, you will initialize the parameters to whatever that pretrained model finished with. It’s also possible that, even for a new model, you have some other initialization rules besides random. From now, we’ll stick with random initialization. Next, we have the forward method. When we pass data through a model from beginning to end, this is called a forward pass. Just like everything else, this is not the only way to do things. You can have the data loop back around and do other interesting things. We’ll keep it usual and perform a regular forward pass.\nTo continue the LayerDense class code, let’s add the random initialization of weights and biases:\n#Layer initialization\ndef __init__(self,n_inputs, n_neurons):\n    self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n    self.biases = np.zeros((1,n_neurons))\nHere, we are setting the weights to be random and the biases to be \\(0\\). Note that, we are initializing weights to be a matrix of dimensions \\(n_{inputs} \\times n_{neurons}\\), rather than \\(n_{neurons} \\times n_{inputs}\\). We’re doing this ahead instead of transposing everytime we perform a forward pass, as explained in the previous chapter.\nWe initialize the biases to zero, because with many samples containing values of \\(0\\), it will ensure that a neuron fires initially. The most common initialization for biases is zero. This will vary depending on our use-case and is just one of the many things we can tweak when trying to improve results. One situation where we might want to try something else is with what’s called dead neurons.\nImagine our step function again:\n\\[\\begin{align*}\ny = \\begin{cases}\n1, & x &gt; 0\\\\\n0, & x \\leq 0\n\\end{cases}\n\\end{align*}\\]\nIt’s possible for \\(\\text{weights} \\cdot \\text{inputs} + \\text{biases}\\) not to meet the threshold of the step function, which means the neuron will output a zero. On its own, this is not a big issue, but it becomes a problem if this happens to this neuron for every one of the input samples (it’ll become clear why once we learn about backpropogation). So, then this neuron’s \\(0\\) output is the input to another neuron. Any weight multiplied by zero will be zero. With an increasing number of neurons outputting \\(0\\), more inputs to the next neurons will be zeros, rendering the network essentially non-trainable or dead.\nOn to our forward method now.\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1,n_neurons))\n\n    def forward(self,inputs):\n        self.output = np.dot(inputs,self.weights) + self.biases\n\nWe are now ready to make use of this new class instead of hardcoded calculations, so let’s generate some data using the discussed dataset creation method and use our new layer to perform a forward pass:\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a dense layer with 2 input features and 3 output values\ndense1 = DenseLayer(2, 3)\n\n# Perform a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Let's see the output of the first few samples\nprint(dense1.output[:5])\n\n[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n [-1.11171044e-04 -5.11007493e-05 -1.12099799e-04]\n [ 2.99257295e-06 -2.69126613e-04 -1.45165104e-04]\n [ 8.95101766e-05 -4.30442247e-04 -1.68079801e-04]\n [-3.49893759e-04 -3.07208364e-04 -4.33002861e-04]]"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#activation-functions-1",
    "href": "posts/coding-a-neural-network-layer/index.html#activation-functions-1",
    "title": "Coding a neural network layer",
    "section": "Activation Functions",
    "text": "Activation Functions\nWe use activation functions because if the activation function itself is non-linear, it allows for neural networks with two or more layers to map non-linear functions. We’ll see how this works. In general, your neural network will have \\(2\\) types of activation functions. The first will be the activation function used in hidden layers, and the second will be used in the output layer. Usually, the activation function used for hidden neurons will be all the same for all of them, but it doesn’t have to.\n\nWhy use activation functions?\nLet’s discuss why we use activation functions in the first place? In most cases, for a neural network to fit a non-linear function, we need it to contain two or more hidden layers and we need those hidden layers to use a non-linear activation function.\nWhile there are certainly problems in life that are linear in nature, for example, trying to figure out the cost of some number of shirts, and we know the cost of an individual shirt, then the equation to calculate the price of any number of those products is a linear equation; other problems in life are not so simple.\nMany interesting and hard problems are non-linear. The main attraction of neural networks has to do with their ability to solve non-linear problems. If we allow only linear activation functions in a neural network, the output will just be a linear transformation of the input, which is not enough to form a universal function approximator.\nFor simplicity, suppose a neural network has \\(2\\) hidden layers with \\(1\\) neuron each.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Input) at (0,0) {\\large $x_1$};\n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50\n        ] (Hidden1) at (3.0,0) {\\large $h_1^{(1)}$};\n        \n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50\n        ] (Hidden2) at (6.0,0) {\\large $h_1^{(2)}$};\n\n    \\node[circle, \n        minimum size = 15mm,\n        fill=red!30\n        ] (Output) at (9.0,0) {\\large $\\hat{y}_1$};        \n        \n\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $w_1$};\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (Hidden2) node [midway,above]  {\\large $w_2$};\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden2) -- (Output);\n    \\draw[-&gt;, shorten &gt;=1pt] (3.0, -2.0) node [below] {\\large $b_1$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (6.0, -2.0) node [below] {\\large $b_2$} -- (Hidden2);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\n\\hat{y}_1 &= h_1^{(2)} \\\\\n&= w_2 h_1^{(1)} + b_2 \\\\\n&= w_2 (w_1 x_1 + b_1) + b_2 \\\\\n&= w_2 w_1 x_1 + (w_2 b_1 + b_2)\n\\end{align*}\\]\nSo, \\(\\hat{y}_1\\) is a linear function of the inputs, no matter, what values we choose for weights and biases.\nThe composition of linear functions is linear. No matter what we do, however many layers we have, or neurons we have in each layer, this network can only model linear functions."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-a-pair-of-neurons",
    "href": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-a-pair-of-neurons",
    "title": "Coding a neural network layer",
    "section": "ReLU Activation in a pair of Neurons",
    "text": "ReLU Activation in a pair of Neurons\nIt is less obvious how, with a barely non-linear activation function, like the rectified linear activation function, we can suddenly model non-linear relationships and functions. Let’s start with a single neuron. We’ll begin with both a weight of zero and a bias of zero:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $0.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.00$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nIn this case, no matter what input we pass, the output of this neuron will always be \\(0\\), because the weight is \\(0\\) and the bias is \\(0\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid]\n\\addplot[color=blue,thick]{0};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nLet’s set the weight to be \\(1.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.00$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nNow, it just looks like the basic rectified linear function. No surprises yet!\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick]{max(x,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nNow, let’s set the bias to \\(0.50\\):\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWe can see that in this case, with a single neuron, the bias offsets the overall function’s activation point horizontally.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nBy increasing bias, we’re making this neuron activate earlier. What happens when we negate the weight to \\(-1.0\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWith a negative weight and this single neuron, the function has become a question of when this neuron deactivates.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(-x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWhat happens if modify the weight to \\(-2.00\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n    \n    \\node[] (w) at (-3,0) {};\n    \\node[] (b) at (0,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w) -- (Input) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b) node [below] {\\large $0.50$} -- (Input);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe neuron now deactivates at \\(0.25\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2.0,ymax=2.0,xmin=-2.0,xmax=2.0]\n\\addplot[color=blue,thick,samples=100]{max(-2*x+0.50,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nUpto this point, we’ve seen how we can use the bias to offset the function horizontally, and the weight to influence the slope of the activation. Moreover, we’re also able to control whether the function is one for determining where the neuron activates or deactivates. What happens when we have, rather than just one neuron, a pair of neurons? For example, let’s pretend that we have two hidden layers of \\(1\\) neuron each. Thinking back to the \\(y=x\\) activation function, we unsurprisingly discovered that a linear activation function produced linear results no matter what chain of neurons we made. Let’s see what happens with the rectified linear function for the activation.\nWe’ll begin with the last values for the first neuron and a weight of \\(1.00\\) and a bias of \\(0.00\\) for the second neuron.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $0.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nAs we can see so far, there’s no change. This is because the second neuron’s bias is doing no offsetting, and the second neuron’s weight is just multiplying the output by \\(1\\), so there’s no change. Let’s try to adjust the second neuron’s bias now:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=6,ytick={-1,0,...,6}]\n\\addplot[color=blue,thick,samples=100]{max(max(-x+0.50,0),0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nLet’s try to adjust the second neuron’s bias now:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nNow, we see some fairly interesting behavior. The bias of the second neuron indeed shifted the overall function but, rather than shifting it horizontally, it shifted vertically.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=6,ytick={-1,0,...,6}]\n\\addplot[color=blue,thick,samples=100]{max(max(-x+0.50,0)+1.00,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWhat then might happen, if we make the \\(2\\)nd neuron’s weight \\(-2\\) rather than \\(1\\)?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nSomething exciting has occurred! What we have here is a neuron that has both an activation and a deactivation point. Now, the output after these two neurons will be variable, so long as it is inside of some specific range. So, basically if both neurons are activated then we actually sort of see this influence on the value. Otherwise, if both neurons aren’t activated, then the output is just a static value.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-1,ymax=2,ytick={-1,0,...,2},xmin=-2,xmax=2]\n\\addplot[color=blue,thick,samples=500]{max(-2.0*max(-x+0.50,0)+1.00,0)};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nSo, when we are below the activation of the first neuron, the output will be the bias of the second neuron \\(1.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue,\n        fill=green\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) node [left] {\\large $0.50$} -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (6,0) node [right] {$1.00$};\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe second neuron is activated if it’s input is smaller than \\(0.50\\).\nConsider what happens when the input to the first neuron is \\(0.00, -0.10, \\ldots\\). The output of the first neuron is \\(0.50, 0.60, \\ldots\\) which implies that the second neuron is deactivated, so the output of the second neuron is simply zero.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue,\n        fill=green\n        ] (Input) at (0,0) {};\n\n    \\node[circle, \n        minimum size = 15mm,\n        draw=blue\n        ] (Hidden1) at (3,0) {};\n    \n    \\node[] (w1) at (-3,0) {};\n    \\node[] (b1) at (0,-2) {};\n    \\node[] (b2) at (3,-2) {};\n\n    \\draw[-&gt;, shorten &gt;=1pt] (w1) node [left] {\\large $0.50$} -- (Input) node [midway,above] {\\large $-1.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b1) node [below] {\\large $0.50$} -- (Input);\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1) node [midway,above] {\\large $-2.00$};\n    \\draw[-&gt;, shorten &gt;=1pt] (b2) node [below] {\\large $1.00$} -- (Hidden1);\n    \\draw[-&gt;, shorten &gt;=1pt] (Hidden1) -- (6,0) node [right] {$0.00$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-hidden-layers",
    "href": "posts/coding-a-neural-network-layer/index.html#relu-activation-in-hidden-layers",
    "title": "Coding a neural network layer",
    "section": "ReLU Activation in hidden layers",
    "text": "ReLU Activation in hidden layers\nLet’s now take this concept and use it to fit to a sine wave-like function using two hidden layers of \\(8\\) neurons each and we can hand-tune the values to fit the curve. We’ll do this by working with \\(1\\) pair of neurons at a time, which means \\(1\\) neuron from each layer individually. For simplicity, we are also going to assume that the layers are not densely connected, and each neuron from the first hidden layer connects to only one neuron from the second hidden layer. That’s usually not the case with the real models, but we want this simplification for the purpose of this demo. Additionally, this example model takes a single value as an input, the input to the sine function, and outputs a single value like the sine function. The output layer uses the linear activation function and the hidden layers will use the rectified linear activation function.\nTo start, we’ll set all weights to \\(0\\) and work with the first pair of neurons:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway] {$0.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$0.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nNext, we can set the weight for the hidden layer neurons and the output neuron to \\(1.00\\), and we can see how this impacts the output:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$1.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe output is:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWe can increase the slope of the output by adjusting the weight of the first neuron of the first layer to \\(6.00\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.00$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWe can now see, for example, that the initial slope of this function is what we’d like, but we have a problem.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(6*x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nCurrently, this function never ends because this neuron pair never deactivates. We can visually see where we’d like the deactivation to occur. It’s where the red fitment line diverges from our green sine wave. So now, while we have the correct slope, we need to set this spot as our deactivation point. To do that, we start by increasing the bias for the second neuron of the hidden layer to \\(0.70\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nRecall, that this offsets the overall function vertically:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nNow, we can set the weight for the second neuron to \\(-1\\), causing a deactivation point to occur, atleast horizontally, where we want it.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWe get:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{max(-max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nNow, we’d like to flip this slope back. How might we flip the output of these two neurons?\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,8}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,8}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nIt seems like we can take the weights of the connection to the output neuron, which is currently \\(1.0\\) and just flip it to a \\(-1\\), and that flips the function:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWe’re certainly getting closer to making this first section fit how we want. Now, all we need to do is offset this up a bit. For this hand-optimized example, we’re going to use the first \\(7\\) pairs of neurons in the hidden layers to create the sine wave’s shape.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {2,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {2,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {2,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nIf we set the bias of the second neuron in the bottom pair to \\(1.0\\) and the weight to the output neuron to \\(0.70\\), we can vertically shift the line like so:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nAt this point, we have completed the first section with an “area of effect” being the first upward section of the sine wave. We can start on the next section that we wish to do. We can start on the next section that we wish to do. We can start by setting all weights for this second pair of neurons to \\(1\\) including the output neuron.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$1.00$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$0.00$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.00$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nAt this point, this second pair of neurons activation is beginning too soon, which is impacting the area of effect of the top pair we already aligned.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(max(x,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nTo fix this, we want this second pair to start influencing the output where the first pair deactivates, so we want to adjust the function horizontally. As you can recall from earlier, we adjust the first neuron’s bias in this neuron pair to achieve this. Also, to modify the slope, we’ll set the weight coming into that first neuron for the second pair, setting it to \\(3.50\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.00$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nAfter these adjustments:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(max(3.50*x - 0.42,0),0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWe will now use the same methodology as we did with the first pair of neurons to set the deactivation point. We set the weight for the second neuron in the hidden layer pair to \\(-1.00\\) and the bias to \\(0.27\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThis results in:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70+max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThen, we can flip this section’s function again the same way we did with the first one, by setting the weight to the output neuron from \\(1.0\\) to \\(-1.0\\).\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.70$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$-1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nConsequently, we have:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.70-max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nAnd again, just like the first pair, we use the bottom pair to fix the vertical offset.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Input) at (0,0) {\\large $x_1$};\n\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden1-\\i) at (3.0,-\\i * 2) {\\large $h_\\i^{(1)}$};\n        \n}\n\\foreach \\i in {1,2,...,8}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        fill=blue!50,\n        yshift=90 mm\n        ] (Hidden2-\\i) at (6.0,-\\i * 2) {\\large $h_\\i^{(2)}$};\n        \n}\n\n\\node[circle, \n    minimum size = 15mm,\n    fill=red!30\n    ] (Output) at (9,0) {\\large $\\hat{y}_1$};\n\n% Connect neurons In-Hidden1\n\\foreach \\j in {3,...,7}\n{\n    \\draw[-&gt;, shorten &gt;=1pt] (Input) -- (Hidden1-\\j);   \n}\n% Connect neurons Hidden1-Hidden2\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden1-\\i) -- (Hidden2-\\i);   \n    \n}\n\n\\foreach \\i in {3,...,7}\n{\n    \n        \\draw[-&gt;, shorten &gt;=1pt] (Hidden2-\\i) -- (Output);   \n}\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$6.00$} (Hidden1-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-1) -- node [midway,above] {$-1.00$} (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-1) -- node [midway] {$-1.00$} (Output); \n\\draw[-&gt;, shorten &gt;=1pt] (6,6) node [below] {$0.70$} --  (Hidden2-1); \n\\draw[-&gt;, shorten &gt;=1pt] (3,6) node [below] {$0.00$} --  (Hidden1-1); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$0.00$} (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (3,-9) node [below] {$0.00$} --  (Hidden1-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-8) -- node [midway,above] {$0.00$} (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (6,-9) node [below] {$1.00$} --  (Hidden2-8); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-8) -- node [midway] {$0.97$} (Output); \n\n\\draw[-&gt;, shorten &gt;=1pt] (Input) -- node [midway] {$3.50$} (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (3,4) node [below] {$-0.42$} --  (Hidden1-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden1-2) -- node [midway,above] {$-1.00$} (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (6,4) node [below] {$0.27$} --  (Hidden2-2); \n\\draw[-&gt;, shorten &gt;=1pt] (Hidden2-2) -- node [midway] {$-1.00$} (Output); \n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWe get:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz,pgfplots --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\begin{axis}[grid,ymin=-2,ymax=2,ytick={-2,-1,...,2},xmin=0,xmax=1]\n\\addplot[color=blue,thick,samples=1000]{-max(-max(6*x,0)+0.70,0))+0.97-max(-max(3.50*x - 0.42,0)+0.27,0)};\n\\addplot[color=green,samples=1000,domain=0:1]{sin(deg(2*3.14*x))};\n\\end{axis}\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nWe then just continue this methodology. It should begin to make more sense to you now, how more neurons can enable more unique areas of effect, why we need two or more hidden layers, and why we need nonlinear activation functions to map nonlinear problems.\nWe can write a ReLUActivation class to represent the ReLU activation function:\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from the inputs\n        self.output = np.maximum(0, inputs)\n\nLet’s apply this activation function to the DenseLayer’s outputs in our code:\n\nfrom nnfs.datasets import spiral_data\nimport numpy as np\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create Dense layer with 2 input features and 3 output values\ndense1 = DenseLayer(2, 3)\n\n# Create ReLU activation function (to be used with the DenseLayer)\nactivation1 = ReLUActivation()\n\n# Make a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Forward pass through our activation function\n# Takes in output from the previous layer\nactivation1.forward(dense1.output)\n\n# Let's see output of the first few samples\nprint(activation1.output[:5])\n\n[[0.0000000e+00 0.0000000e+00 0.0000000e+00]\n [1.3520580e-04 1.8173116e-05 0.0000000e+00]\n [2.3245417e-04 0.0000000e+00 0.0000000e+00]\n [3.8226307e-04 0.0000000e+00 0.0000000e+00]\n [5.7436468e-04 0.0000000e+00 0.0000000e+00]]\n\n\nAs we can see, negative values have been clipped (modified to zero). That’s all there is to the rectified linear activation function used in the hidden layer. Let’s talk about the activation function that we are going to use on the output of the last layer."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#the-softmax-activation-function",
    "href": "posts/coding-a-neural-network-layer/index.html#the-softmax-activation-function",
    "title": "Coding a neural network layer",
    "section": "The Softmax Activation function",
    "text": "The Softmax Activation function\nIn our case, we’re looking to get this model to be a classifier, so we want an activation function meant for classification. One of these is the softmax activation function. First, why are we bothering with another activation function? It just depends on what our overall goals are.\nThe rectified linear unit is unbounded, not normalized with other units and exclusive. “Not normalized” implies the values can be anything, an output of [12,99,318] is without context, and exclusive means each output is independent of others. To address this lack of context, the softmax activation function on the output data can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for our classes. In the case of classification, what we want to see is a prediction of which class the network thinks the input represents. This distribution returned by the softmax activation function represents confidence scores in our overarching algorithm/program that uses this network. For example, if our network has a confidence distirbution for two classes \\([0.45,0.55]\\), the prediction is the \\(2\\)nd class, but the confidence in this prediction isn’t very high.\nMaybe our program wouldn’t act in this case, since it’s not very confident.\nThe softmax function takes as input a vector of \\(L\\) real numbers and normalizes it into a probability distribution consisting of \\(L\\) probabilities proportional to the exponentials of the input numbers.\nDefinition. The standard(unit) softmax function \\(\\sigma:\\mathbf{R}^L \\to (0,1)^L\\) takes a vector \\(\\mathbf{z}=(z_1,\\ldots,z_l)\\in\\mathbf{R}^L\\) and computes each component of the vector \\(\\sigma(\\mathbf{z})\\in(0,1)^L\\) with:\n\\[\\begin{align*}\n\\sigma(\\mathbf{z})_i = \\frac{e^{z_{i}}}{\\sum_{l=1}^{L}e^{z_{l}}}\n\\end{align*}\\]\nThat might look daunting, but it’s easy to follow. Suppose the example outputs from a neural network layer are:\n\nlayer_outputs = [4.80, 1.21, 2.385]\n\nThen, the normalized values are:\n\nimport numpy as np\n\nnorm_values = np.exp(layer_outputs)/np.sum(np.exp(layer_outputs))\nprint(norm_values)\n\n[0.89528266 0.02470831 0.08000903]\n\n\nTo train in batches, we need to convert this functionality to accept layer outputs in batches. Do this is easy:\n\nlayer_outputs = np.random.randn(100,3)\nnorm_values = np.exp(layer_outputs)/np.sum(np.exp(layer_outputs),axis=1,keepdims=True)\n\nWe can now write a SoftmaxActivation class as:\n\n# Softmax activation\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\nWe also included a subtraction of the largest of the inputs before we do the exponentiation. This does not affect the output of the softmax function, since:\n\\[\\begin{align*}\n\\frac{e^{z_{i}-||\\mathbf{z}||}}{\\sum_{l=1}^{L}e^{z_{l}-||\\mathbf{z}||}} = \\frac{e^{-||\\mathbf{z}||}\\cdot e^{z_{i}}}{e^{-||\\mathbf{z}||}\\cdot \\sum_{l=1}^{L}e^{z_{l}}} = \\sigma(\\mathbf{z})_i\n\\end{align*}\\]\nThere are two main pervasive challenges with neural networks : dead neurons and very large numbers (referred to as exploding values). Dead neurons and enormous numbers can wreak havoc down the line and render a network useless over time."
  },
  {
    "objectID": "posts/coding-a-neural-network-layer/index.html#the-output-layer",
    "href": "posts/coding-a-neural-network-layer/index.html#the-output-layer",
    "title": "Coding a neural network layer",
    "section": "The output layer",
    "text": "The output layer\nNow, we can add another DenseLayer as the output layer, setting it to contain as many inputs as the previous layer outputs and as many outputs as our data includes classes. Then, we can apply the softmax function to the output of this new layer.\n\nFull code upto this point\n\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nclass DenseLayer:\n\n    def __init__(self, n_inputs, n_neurons):\n        # Initialize all weights and biases\n        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1,n_neurons))\n    \n    def forward(self, inputs):\n        self.output = np.dot(inputs,self.weights) + self.biases\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        self.output = np.maximum(inputs, 0)\n\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self,inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\n# Create a DenseLayer with 2 input features and 3 neurons\ndense1 = DenseLayer(2, 3)\n\n# Create ReLU Activation (to be used with DenseLayer)\nactivation1 = ReLUActivation()\n\n# Create a second DenseLayer with 3 input features and 3 output values\ndense2 = DenseLayer(3, 3)\n\n# Create Softmax activation to be used with the output layer\nactivation2 = SoftmaxActivation()\n\n# Make a forward pass of our training data through this layer\ndense1.forward(X)\n\n# Make a forward pass through the activation function \n# It takes the output of the first dense layer\nactivation1.forward(dense1.output)\n\n# Make a forward pass through the second DenseLayer\n# It takes outputs of the activation function of the first layer\n# as inputs\ndense2.forward(activation1.output)\n\n# Make a forward pass through activation function\n# It takes outputs of the second dense layer\nactivation2.forward(dense2.output)\n\n# Let's see output of the first few examples\nprint(activation2.output[:5])\n\n[[0.33333334 0.33333334 0.33333334]\n [0.33333322 0.3333335  0.33333322]\n [0.3333332  0.3333332  0.3333336 ]\n [0.3333332  0.3333336  0.3333332 ]\n [0.33333287 0.33333436 0.33333275]]\n\n\nWe’ve completed what we need for forward-passing data through the model.\nOur example model is currently random. To remedy this, we need a way to calculate how wrong the neural network is at current predictions and begin adjusting weights and biases to decrease error over time. Thus, our next step is to quantify how wrong the model is through what’s defined as a loss function."
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html",
    "title": "Black Scholes Formula for a European Call",
    "section": "",
    "text": "The mean rate of growth of all assets under the risk-neutral measure \\(\\mathbb{Q}\\) is risk-free rate \\(r\\).\nThe stock price process has the \\(\\mathbb{Q}\\)-dynamics:\n\\[dS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}(t) \\tag{1}\\]\nThe solution to this SDE is:\n\\[S(t) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}}(t)\\right]\\tag{2}\\]\nConsider a call option with maturity time \\(T\\). Then, the stock price at \\(T\\) is:\n\\[S(T) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma W^{\\mathbb{Q}}(T)\\right]\\tag{3}\\]\nDenoting \\(\\tau = T - t\\), we have:\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma (W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t))\\right]\\tag{4}\\]\nSince, \\(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)\\) is a gaussian random variable with mean \\(0\\) and variance \\(\\tau = T-t\\), we can write \\(-(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)) = \\sqrt{\\tau}Z\\), where \\(Z\\) is a standard normal random variable. Thus,\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right]\\tag{5}\\]\nBy the risk-neutral pricing formula, the time-\\(t\\) price of the European call option is:\n\\[\n\\begin{align*}\nV(t) &= \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r(T-t)}\\max(S(T) - K,0)|\\mathcal{F}_t\\right] \\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S(T)&gt;K}|\\mathcal{F}_t\\right]\\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S_t e^{(r-\\sigma^2/2) - \\sigma\\tau Z}&gt;K}\\right]\n\\end{align*}\n\\]\nIn the last-but-one step, everything is \\(\\mathcal{F}_t\\)-measurable.\nThe domain of integration is all \\(z\\) satisfying:\n\\[\n\\begin{align*}\nS(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right] &&gt;  K\\\\\n\\log \\frac{S(t)}{K} + \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau  &&gt; \\sigma \\sqrt{\\tau}Z\n\\end{align*}\n\\]\nDefine \\(d_{-} = \\frac{\\log \\frac{S(t)}{K} +(r-\\sigma^2/2)\\tau}{\\sigma\\sqrt{\\tau}}\\).\nThen, the region \\(D\\) is:\n\\[Z &lt; d_{-}\\]\nSo, we can expand the expectation in (6) as:\n\\[\n\\begin{align*}\nV(t) &=  \\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z \\right\\} - K\\right)d\\mathbb{Q} \\\\\n&=\\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) f_Z^{\\mathbb{Q}}(z) dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}}e^{-r\\tau} \\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) e^{-\\frac{z^2}{2}} dz\n\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz \\\\\n&- Ke^{-r\\tau}\\cdot \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-\\frac{z^2}{2}} dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz - Ke^{-r\\tau}\\Phi(d_{-})\\tag{7}\n\\end{align*}\n\\]\nWe have:\n\\[\n\\begin{align*}\n&\\exp \\left[-\\frac{\\sigma^2}{2}\\tau - \\sigma\\sqrt{\\tau} z - \\frac{z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{\\sigma^2 \\tau + 2\\sigma \\sqrt{\\tau}z + z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] \\tag{8}\n\\end{align*}\n\\]\nSubstituting (8) into (7), we get:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S(t)\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] dz - Ke^{-r\\tau}\\Phi(d_{-}) \\tag{9}\n\\end{align*}\n\\]\nPut \\(u = z + \\sigma \\sqrt{\\tau}\\). Then, \\(dz = du\\). The upper limit of integration is \\(d_{+} = d_{-} + \\sigma \\sqrt{\\tau}\\), which is:\n\\[\n\\begin{align*}\nd_{+} &=\\frac{\\log \\frac{S(t)}{K} + (r-\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}} + \\sigma \\sqrt{\\tau}\\\\\n&= \\frac{\\log \\frac{S(t)}{K} + (r+\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}}\n\\end{align*}\n\\]\nSo, the equation (9) can be written as:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{+}} S(t)e^{-\\frac{u^2}{2}} du - Ke^{-r\\tau}\\Phi(d_{-}) \\\\\n&= S(t)\\Phi(d_{+}) - Ke^{-r\\tau} \\Phi(d_{-})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html#the-black-scholes-formula-for-a-european-call",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html#the-black-scholes-formula-for-a-european-call",
    "title": "Black Scholes Formula for a European Call",
    "section": "",
    "text": "The mean rate of growth of all assets under the risk-neutral measure \\(\\mathbb{Q}\\) is risk-free rate \\(r\\).\nThe stock price process has the \\(\\mathbb{Q}\\)-dynamics:\n\\[dS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}}(t) \\tag{1}\\]\nThe solution to this SDE is:\n\\[S(t) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}}(t)\\right]\\tag{2}\\]\nConsider a call option with maturity time \\(T\\). Then, the stock price at \\(T\\) is:\n\\[S(T) = S(0)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma W^{\\mathbb{Q}}(T)\\right]\\tag{3}\\]\nDenoting \\(\\tau = T - t\\), we have:\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau + \\sigma (W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t))\\right]\\tag{4}\\]\nSince, \\(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)\\) is a gaussian random variable with mean \\(0\\) and variance \\(\\tau = T-t\\), we can write \\(-(W^{\\mathbb{Q}}(T)-W^{\\mathbb{Q}}(t)) = \\sqrt{\\tau}Z\\), where \\(Z\\) is a standard normal random variable. Thus,\n\\[S(T) = S(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right]\\tag{5}\\]\nBy the risk-neutral pricing formula, the time-\\(t\\) price of the European call option is:\n\\[\n\\begin{align*}\nV(t) &= \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r(T-t)}\\max(S(T) - K,0)|\\mathcal{F}_t\\right] \\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S(T)&gt;K}|\\mathcal{F}_t\\right]\\\\\n&= e^{-r(T-t)}\\mathbb{E}^{\\mathbb{Q}}\\left[\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right\\} - K\\right)\\cdot 1_{S_t e^{(r-\\sigma^2/2) - \\sigma\\tau Z}&gt;K}\\right]\n\\end{align*}\n\\]\nIn the last-but-one step, everything is \\(\\mathcal{F}_t\\)-measurable.\nThe domain of integration is all \\(z\\) satisfying:\n\\[\n\\begin{align*}\nS(t)\\exp \\left[ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}Z\\right] &&gt;  K\\\\\n\\log \\frac{S(t)}{K} + \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau  &&gt; \\sigma \\sqrt{\\tau}Z\n\\end{align*}\n\\]\nDefine \\(d_{-} = \\frac{\\log \\frac{S(t)}{K} +(r-\\sigma^2/2)\\tau}{\\sigma\\sqrt{\\tau}}\\).\nThen, the region \\(D\\) is:\n\\[Z &lt; d_{-}\\]\nSo, we can expand the expectation in (6) as:\n\\[\n\\begin{align*}\nV(t) &=  \\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{\\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z \\right\\} - K\\right)d\\mathbb{Q} \\\\\n&=\\int_{-\\infty}^{d_{-}} e^{-r\\tau}\\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) f_Z^{\\mathbb{Q}}(z) dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}}e^{-r\\tau} \\left(S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\} - K\\right) e^{-\\frac{z^2}{2}} dz\n\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz \\\\\n&- Ke^{-r\\tau}\\cdot \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-\\frac{z^2}{2}} dz \\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} e^{-r\\tau}S(t)\\exp \\left\\{ \\left(r - \\frac{\\sigma^2}{2}\\right)\\tau - \\sigma \\sqrt{\\tau}z\\right\\}e^{-\\frac{z^2}{2}} dz - Ke^{-r\\tau}\\Phi(d_{-})\\tag{7}\n\\end{align*}\n\\]\nWe have:\n\\[\n\\begin{align*}\n&\\exp \\left[-\\frac{\\sigma^2}{2}\\tau - \\sigma\\sqrt{\\tau} z - \\frac{z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{\\sigma^2 \\tau + 2\\sigma \\sqrt{\\tau}z + z^2}{2}\\right]\\\\\n=&\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] \\tag{8}\n\\end{align*}\n\\]\nSubstituting (8) into (7), we get:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S(t)\\exp\\left[-\\frac{(z+\\sigma\\sqrt{\\tau})^2}{2}\\right] dz - Ke^{-r\\tau}\\Phi(d_{-}) \\tag{9}\n\\end{align*}\n\\]\nPut \\(u = z + \\sigma \\sqrt{\\tau}\\). Then, \\(dz = du\\). The upper limit of integration is \\(d_{+} = d_{-} + \\sigma \\sqrt{\\tau}\\), which is:\n\\[\n\\begin{align*}\nd_{+} &=\\frac{\\log \\frac{S(t)}{K} + (r-\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}} + \\sigma \\sqrt{\\tau}\\\\\n&= \\frac{\\log \\frac{S(t)}{K} + (r+\\sigma^2/2)\\tau}{\\sigma \\sqrt{\\tau}}\n\\end{align*}\n\\]\nSo, the equation (9) can be written as:\n\\[\n\\begin{align*}\nV(t) &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{+}} S(t)e^{-\\frac{u^2}{2}} du - Ke^{-r\\tau}\\Phi(d_{-}) \\\\\n&= S(t)\\Phi(d_{+}) - Ke^{-r\\tau} \\Phi(d_{-})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/black-scholes-formula-for-a-european-call/index.html#appendix",
    "href": "posts/black-scholes-formula-for-a-european-call/index.html#appendix",
    "title": "Black Scholes Formula for a European Call",
    "section": "Appendix",
    "text": "Appendix\nLemma. The discounted stock-price process \\((D(t)S(t),t\\geq 0)\\) is a \\(\\mathbb{Q}\\)-martingale.\nSuppose we have a risk-free money-market account with the dynamics:\n\\[dM(t) = rM(t)dt\\]\nand the dynamics of the stock-price process is:\n\\[dS(t) = \\mu S(t) dt + \\sigma S(t) dW^\\mathbb{P}(t)\\]\nThus, the discounting process is:\n\\[dD(t) = -rD(t)dt\\]\nwhere the instantaneous interest rate \\(r\\) is a constant.\nBy Ito’s product rule:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= dD(t) S(t) + D(t)dS(t)\\\\\n&= -rD(t)S(t)dt + D(t)(\\mu S(t) dt + \\sigma S(t)dW^\\mathbb{P}(t))\\\\\n&= D(t)S(t)((\\mu - r)dt + \\sigma dW^\\mathbb{P}(t))\\\\\n\\end{align*}\n\\]\nWe are interested to write:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= D(t)S(t)\\sigma dW^\\mathbb{Q}(t)\n\\end{align*}\n\\]\nComparing the right hand sides, we have: \\[\n\\begin{align*}\n\\sigma dW^\\mathbb{Q}(t) &= (\\mu - r)dt + \\sigma dW^\\mathbb{P}(t)\n\\end{align*}\n\\]\nLet’s define:\n\\[dW^\\mathbb{Q}(t) = \\theta dt + dW^\\mathbb{P}(t)\\]\nwhere \\(\\theta = (\\mu - r)/\\sigma\\) and the Radon-Nikodym derivative \\(Z\\) as:\n\\[Z = \\exp\\left[-\\int_0^T \\theta dW^\\mathbb{P}(u) - \\frac{1}{2}\\int_0^T \\theta^2 du \\right]\\]\nBy the Girsanov theorem, \\(W^\\mathbb{Q}(t)\\) is a \\(\\mathbb{Q}\\)-standard brownian motion. Hence, we can write:\n\\[\n\\begin{align*}\nd(D(t)S(t)) &= D(t)S(t)\\sigma dW^\\mathbb{Q}(t)\n\\end{align*}\n\\]\nSince the Ito integral is a martingale, \\(D(t)S(t)\\) is a \\(\\mathbb{Q}\\)-martingale. This closes the proof.\nClaim. The \\(\\mathbb{Q}\\)-dynamics of \\(S_t\\) satisfy :\n\\[dS(t) = rS(t) dt + \\sigma S(t) dW^{\\mathbb{Q}}(t)\\]\nProof.\nWe have:\n\\[\n\\begin{align*}dS(t) &= d(S(t)D(t)M(t))\\\\\n&= d(S(t)D(t))M(t) + S(t)D(t)dM(t)\\\\\n&= D(t)M(t) S(t)\\sigma dW^\\mathbb{Q}(t) + S(t)D(t)r M(t)dt\\\\\n&= S(t)(rdt + \\sigma dW^\\mathbb{Q}(t))\n\\end{align*}\n\\]\nWe can easily solve this linear SDE; its solution is:\n\\[S(t) = S(0)\\exp\\left[\\left(\\mu - \\frac{\\sigma^2}{2}\\right)dt + \\sigma W^\\mathbb{Q}(t)\\right]\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nHi there! Welcome to my blog. I’m Quasar and I’m excited to share my self-learning journey with you. Over the years, I have delved into various topics in C++ programming and mathematical finance.\nThroughout my career, I have transitioned from different roles, eventually becoming an analyst and a now a sell-side quant. Driven by a spirit of enquiry, I am embarking on a self-learning path exploring modern C++ and quantitative finance. Through this blog, my goal is to provide clear and efficient explanations of various topics, offering insights that I wish I had when I started my self-learning journey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFun with numeraires!\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nNov 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Template Argument Deduction(CTAD)\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nNov 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType Traits\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nNov 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTridiagonal Systems\n\n\n\nNumerical Methods\n\n\n\n\n\n\n\nQuasar\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpolation and Approximation\n\n\n\nNumerical Methods\n\n\n\n\n\n\n\nQuasar\n\n\nNov 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical Integration\n\n\n\nNumerical Methods\n\n\n\n\n\n\n\nQuasar\n\n\nNov 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemplate programming\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nNov 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA hitchhiker’s guide to move semantics and perfect forwarding\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nOct 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcurrent programming - A Primer (Part I)\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nSep 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNorms\n\n\n\nNumerical Methods\n\n\n\n\n\n\n\nQuasar\n\n\nJul 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingular Value Decomposition(SVD)\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJul 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Spectral Theorem\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJul 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEigenthingies and Diagonalizability\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJul 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Markov Property\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIto Calculus\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIto Processes and Stochastic Differential Equations\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Ito Calculus\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of the first passage time of Brownian Motion\n\n\n\nStochastic Calculus\n\n\n\n\n\n\n\nQuasar\n\n\nJul 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBorel-Cantelli Lemmas\n\n\n\nProbability Theory\n\n\n\n\n\n\n\nQuasar\n\n\nJun 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositive Definiteness\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\nQuasar\n\n\nJun 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization Algorithms\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackpropogation\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification Algorithms\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding a neural network layer\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the Least Squares Estimate Beta in Linear Regression\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nQuasar\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCox-Ingersoll-Ross (CIR) model\n\n\n\nInterest Rate Modelling\n\n\n\n\n\n\n\nQuasar\n\n\nMay 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlack Scholes Formula for a European Call\n\n\n\nVanilla Options\n\n\nBlack-Scholes\n\n\n\n\n\n\n\nQuasar\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Option Greeks\n\n\n\nVanilla Options\n\n\n\n\n\n\n\nQuasar\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC++ Refresher - Part I\n\n\n\nC++\n\n\n\n\n\n\n\nQuasar\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Vanna Volga\n\n\n\nVolatility modelling\n\n\n\n\n\n\n\nQuasar\n\n\nNov 26, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/backpropogation/index.html",
    "href": "posts/backpropogation/index.html",
    "title": "Backpropogation",
    "section": "",
    "text": "With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate the error in our model. The loss function also referred to as the cost function quantifies the error.\n\n\nLet \\(\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}\\) be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the logit vector in machine learning literature.\n\n\n\nLet \\(X\\) be a random variable with possible outcomes \\(\\mathcal{X}\\). Let \\(P\\) be the true probability distribution of \\(X\\) with probability mass function \\(p(x)\\). Let \\(Q\\) be an approximating distribution with probability mass function \\(q(x)\\).\nDefinition. The entropy of \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\\]\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm \\(\\log p(x)\\), we concentrate on the order of the surprise. Entropy, then, is an expectation over the uncertainties or the expected surprise.\nDefinition. The cross-entropy of \\(Q\\) relative to \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\\]\nDefinition. For discrete distributions \\(P\\) and \\(Q\\) defined on the sample space \\(\\mathcal{X}\\), the Kullback-Leibler(KL) divergence (or relative entropy) from \\(Q\\) to \\(P\\) is defined as:\n\\[\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\\]\nIntuitively, it is the expected excess surprise from using \\(Q\\) as a model instead of \\(P\\), when the actual distribution is \\(P\\). Note that, \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\), so it is not symmetric and hence it is not a norm.\n\n\n\nWe are going to work on a multi-class classification problem.\nFor any input \\(\\mathbf{x}_i\\), the target vector \\(\\mathbf{y}_i\\) could be specified using one-hot encoding or an integer in the range [0,numClasses).\nLet’s say, we have numClasses = 3.\nIn one-hot encoding, the target vector y_true is an array like [1, 0, 0], [0, 1, 0], or [0, 0, 1]. The category/class is determined by the index which is hot. For example, if y_true equals [0, 1, 0], then the sample belongs to class \\(1\\), whilst if y_true equals [0, 0, 1], the sample belongs to class \\(2\\).\nIn integer encoding, the target vector y_true is an integer. For example, if y_true equals \\(1\\), the sample belongs to class \\(1\\), whilst if y_true equals \\(2\\), the sample belongs to class \\(2\\).\nThe categorical_crossentropy is defined as:\n\\[\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\\]\nAssume that we have a softmax output \\(\\hat{\\mathbf{y}}_i\\), [0.7, 0.1, 0.2] and target vector \\(\\mathbf{y}_i\\) [1, 0, 0]. Then, we can compute the categorical cross entropy loss as:\n\\[\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\\]\nLet’s that we have a batch of \\(3\\) samples. Additionally, suppose the target y_true is integer encoded. After running through the softmax activation function, the network’s output layer yields:\n\n%load_ext itikz\n\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n\n0.7\n0.5\n0.08\n\n\nThis can be simplified.\n\nprint(y_pred[[0,1,2],y_true])\n\n[0.7  0.5  0.08]\n\n\nnumpy lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\nprint(y_pred[range(len(y_pred)),y_true])\n\n[0.7  0.5  0.08]\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n\n[0.35667494 0.69314718 2.52572864]\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If y_true.shape has \\(2\\) dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if y_true is a list, that is y_true.shape has \\(1\\) dimension, then it means, we have sparse labels/integer encoding.\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIf the neural network output y_pred for some reason is the vector [1, 0, 0], this would result in numpy.log function returning a negative infinity. To avoid such situations, it’s safer to apply a ceil and floor to y_pred.\n\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)"
  },
  {
    "objectID": "posts/backpropogation/index.html#calculating-the-network-error-with-loss",
    "href": "posts/backpropogation/index.html#calculating-the-network-error-with-loss",
    "title": "Backpropogation",
    "section": "",
    "text": "With a randomly initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate the error in our model. The loss function also referred to as the cost function quantifies the error.\n\n\nLet \\(\\vec{l} = \\mathbf{w}\\cdot \\mathbf{x} + \\mathbf{b}\\) be the result of the last dense layer of a neural network (the inner product between an input feature vector and the weights vector of the layer, added to the bias factor). This is commonly referred to as the logit vector in machine learning literature.\n\n\n\nLet \\(X\\) be a random variable with possible outcomes \\(\\mathcal{X}\\). Let \\(P\\) be the true probability distribution of \\(X\\) with probability mass function \\(p(x)\\). Let \\(Q\\) be an approximating distribution with probability mass function \\(q(x)\\).\nDefinition. The entropy of \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log p(x)\n\\end{align*}\\]\nIn information theory, entropy is the measure of uncertainty, surprise of a system. By taking the logarithm \\(\\log p(x)\\), we concentrate on the order of the surprise. Entropy, then, is an expectation over the uncertainties or the expected surprise.\nDefinition. The cross-entropy of \\(Q\\) relative to \\(P\\) is defined as:\n\\[\\begin{align*}\nH(P,Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log q(x)\n\\end{align*}\\]\nDefinition. For discrete distributions \\(P\\) and \\(Q\\) defined on the sample space \\(\\mathcal{X}\\), the Kullback-Leibler(KL) divergence (or relative entropy) from \\(Q\\) to \\(P\\) is defined as:\n\\[\\begin{align*}\nD_{KL}(P||Q) = -\\sum_{x\\in\\mathcal{X}} p(x) \\cdot \\log \\frac{p(x)}{q(x)}\n\\end{align*}\\]\nIntuitively, it is the expected excess surprise from using \\(Q\\) as a model instead of \\(P\\), when the actual distribution is \\(P\\). Note that, \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\), so it is not symmetric and hence it is not a norm.\n\n\n\nWe are going to work on a multi-class classification problem.\nFor any input \\(\\mathbf{x}_i\\), the target vector \\(\\mathbf{y}_i\\) could be specified using one-hot encoding or an integer in the range [0,numClasses).\nLet’s say, we have numClasses = 3.\nIn one-hot encoding, the target vector y_true is an array like [1, 0, 0], [0, 1, 0], or [0, 0, 1]. The category/class is determined by the index which is hot. For example, if y_true equals [0, 1, 0], then the sample belongs to class \\(1\\), whilst if y_true equals [0, 0, 1], the sample belongs to class \\(2\\).\nIn integer encoding, the target vector y_true is an integer. For example, if y_true equals \\(1\\), the sample belongs to class \\(1\\), whilst if y_true equals \\(2\\), the sample belongs to class \\(2\\).\nThe categorical_crossentropy is defined as:\n\\[\\begin{align*}\nL_i = -\\sum_{j} y_{i,j} \\log(\\hat{y}_{i,j})\n\\end{align*}\\]\nAssume that we have a softmax output \\(\\hat{\\mathbf{y}}_i\\), [0.7, 0.1, 0.2] and target vector \\(\\mathbf{y}_i\\) [1, 0, 0]. Then, we can compute the categorical cross entropy loss as:\n\\[\\begin{align*}\n-\\left(1\\cdot \\log (0.7) + 0 \\cdot \\log (0.1) + 0 \\cdot \\log(0.2)\\right) = 0.35667494\n\\end{align*}\\]\nLet’s that we have a batch of \\(3\\) samples. Additionally, suppose the target y_true is integer encoded. After running through the softmax activation function, the network’s output layer yields:\n\n%load_ext itikz\n\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = [0, 1, 2]\n\nWith a collection of softmax outputs and their intended targets, we can map these indices to retrieve the predicted probabilities of the true class labels:\n\nfor targ_index, distribution in zip(y_true,y_pred):\n    print(distribution[targ_index])\n\n0.7\n0.5\n0.08\n\n\nThis can be simplified.\n\nprint(y_pred[[0,1,2],y_true])\n\n[0.7  0.5  0.08]\n\n\nnumpy lets us index an 2D-array in multiple ways. One of them is to use a list filled with row indices and a list with column indices. We could, thus, write:\n\nprint(y_pred[range(len(y_pred)),y_true])\n\n[0.7  0.5  0.08]\n\n\nThe categorical cross-entropy loss for each of the samples is:\n\nprint(-np.log(y_pred[range(len(y_pred)),y_true]))\n\n[0.35667494 0.69314718 2.52572864]\n\n\nFinally, we want an average loss for the entire batch, to have an idea about how our model is doing during the training phase. Therefore, we have:\n\nneg_log = -np.log(y_pred[range(len(y_pred)),y_true])\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIn the case, that the targets are one-hot encoded, we need to handle this case a bit differently. If y_true.shape has \\(2\\) dimensions, then it implies, we have a set of one-hot encoded vectors. On the other hand, if y_true is a list, that is y_true.shape has \\(1\\) dimension, then it means, we have sparse labels/integer encoding.\n\nimport numpy as np\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\ncorrect_confidences = np.array([])\n\n# If categorical labels\nif(len(y_pred.shape) == 1):\n    correct_confidences = y_pred[range(len(y_pred)), y_true]\nelif(len(y_pred.shape)==2):\n    correct_confidences = np.sum(y_pred * y_true, axis=1)\n\nneg_log = -np.log(correct_confidences)\naverage_loss = np.mean(neg_log)\nprint(average_loss)\n\n1.191850256268978\n\n\nIf the neural network output y_pred for some reason is the vector [1, 0, 0], this would result in numpy.log function returning a negative infinity. To avoid such situations, it’s safer to apply a ceil and floor to y_pred.\n\nepsilon = 1e-7\ny_pred_clipped = np.clip(y_pred, epsilon, 1-epsilon)"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-class",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-class",
    "title": "Backpropogation",
    "section": "Categorical Cross-Entropy Loss Class",
    "text": "Categorical Cross-Entropy Loss Class\nI first create an abstract base class Loss. Every Loss object exposes the calculate method which in turn calls Loss object’s forward method to compute the log-loss for each sample and then takes an average of the sample losses.\nCategoricalCrossEntropyLoss class is a child class of Loss and provides an implementation of the forward method.\n\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\nfrom abc import abstractmethod\n\n\n# Abstract base class for losses\nclass Loss:\n    @abstractmethod\n    def forward(self, y_pred, y_true):\n        pass\n\n    @abstractmethod\n    def backward(self, y_pred, y_true):\n        pass\n\n    # Calculates the data and regularization losses\n    # given model output and ground truth values\n    def calculate(self, output, y):\n\n        # Calculate the sample losses\n        sample_losses = self.forward(output, y)\n\n        # Calculate the mean loss\n        data_loss = np.mean(sample_losses)\n\n        # Return loss\n        return data_loss\n\n\n# Cross-Entropy loss\nclass CategoricalCrossEntropyLoss(Loss):\n\n    # Forward pass\n    def forward(self, y_pred, y_true):\n        num_samples = len(y_pred)\n\n        # Clip data to prevent division by 0\n        # Clip both sides to not drag mean towards any value\n        epsilon = 1e-7\n        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # If categorical labels\n        if len(y_pred.shape) == 1:\n            correct_confidences = y_pred[range(len(y_pred)), y_true]\n        # else if one-hot encoding\n        elif len(y_pred.shape) == 2:\n            correct_confidences = np.sum(y_pred * y_true, axis=1)\n\n        neg_log = -np.log(correct_confidences)\n        return neg_log\n\nUsing the manual created outputs and targets, we have:\n\ny_pred = np.array(\n    [\n        [0.7, 0.1, 0.2],\n        [0.1, 0.5, 0.4],\n        [0.02, 0.9, 0.08]\n    ]\n)\n\ny_true = np.array(\n    [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n)\n\nloss_function = CategoricalCrossEntropyLoss()\nloss = loss_function.calculate(y_pred, y_true)\nprint(loss)\n\n1.191850256268978"
  },
  {
    "objectID": "posts/backpropogation/index.html#backpropogation",
    "href": "posts/backpropogation/index.html#backpropogation",
    "title": "Backpropogation",
    "section": "Backpropogation",
    "text": "Backpropogation\nBackpropogation consists going backwards along the edges and passing along gradients. We are going to chop up a neuron into it’s elementary operations and draw a computational graph. Each node in the graph receives an upstream gradient. The goal is pass on the correct downstream gradient.\nEach node has a local gradient - the gradient of it’s output with respect to it’s input. Consider a node receiving an input \\(z\\) and producing an output \\(h=f(z)\\). Then, we have:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n    \\node [circle,minimum size=40mm,draw] (f) at (0,0) {\\huge $f$};\n    \\node [blue] (localgrad) at (-1,0) {\\huge $\\frac{\\partial h}{\\partial z}$};\n    \\node [blue] (lgrad) at (0.0,1) {\\large Local gradient};\n    \\draw [-&gt;, shorten &gt;=1pt] (1.80,1) -- node [above,midway] {\\huge $h$} (5,1);\n    \\draw [-&gt;, shorten &gt;=1pt] (5,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial h}$} (1.80,-1);\n    \\node [] (upgrad) at (4.0,-3) {\\huge Upstream gradient};\n    \\draw [-&gt;, shorten &gt;=1pt] (-5,1) -- node [above,midway] {\\huge $z$} (-1.80,1);\n    \\draw [-&gt;, shorten &gt;=1pt] (-1.80,-1) -- node [below,midway] {\\huge $\\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z}$} (-5,-1);\n    \\node [] (downgrad) at (-4.0,-3) {\\huge Downstream gradient};\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe downstream gradient \\(\\frac{\\partial s}{\\partial z}\\) equals the upstream graient \\(\\frac{\\partial s}{\\partial h}\\) times the local gradient \\(\\frac{\\partial h}{\\partial z}\\).\nWhat about nodes with multiple inputs? Say that, \\(h=f(x,y)\\). Multiple inputs imply multiple local gradients.\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,scale=1.75]\n%uncomment if require: \\path (0,216); %set diagram left start at 0, and has height of 216\n\n%Shape: Circle [id:dp08328772161506959] \n\\draw   (302.75,83.38) .. controls (302.75,53.62) and (326.87,29.5) .. (356.63,29.5) .. controls (386.38,29.5) and (410.5,53.62) .. (410.5,83.38) .. controls (410.5,113.13) and (386.38,137.25) .. (356.63,137.25) .. controls (326.87,137.25) and (302.75,113.13) .. (302.75,83.38) -- cycle ;\n%Straight Lines [id:da2730189357413113] \n\\draw    (406,59.38) -- (513.5,59.74) ;\n\\draw [shift={(515.5,59.75)}, rotate = 180.2] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da21080101466010737] \n\\draw    (515,110.75) -- (405,110.26) ;\n\\draw [shift={(403,110.25)}, rotate = 0.26] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da05192158713361961] \n\\draw    (209,1.75) -- (309.71,51.37) ;\n\\draw [shift={(311.5,52.25)}, rotate = 206.23] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da3568530309648137] \n\\draw    (305,68.25) -- (204.31,20.61) ;\n\\draw [shift={(202.5,19.75)}, rotate = 25.32] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da4437541566257528] \n\\draw    (205,167.25) -- (311.2,116.12) ;\n\\draw [shift={(313,115.25)}, rotate = 154.29] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n%Straight Lines [id:da2672766038605987] \n\\draw    (304.5,101.75) -- (205.82,146.92) ;\n\\draw [shift={(204,147.75)}, rotate = 335.41] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;\n\n% Text Node\n\\draw (352,76.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $f$};\n% Text Node\n\\draw (318.5,44.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial x}$};\n% Text Node\n\\draw (318.5,88.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 36; blue, 255 }  ,opacity=1 ]  {\\huge $\\frac{\\partial h}{\\partial y}$};\n% Text Node\n\\draw (258.5,7.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $x$};\n% Text Node\n\\draw (264,136.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $y$};\n% Text Node\n\\draw (151.5,96.9) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial y} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial y}$};\n% Text Node\n\\draw (150,33.4) node [anchor=north west][inner sep=0.75pt]  [font=\\small,color={rgb, 255:red, 0; green, 28; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial x} =\\frac{\\partial s}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$};\n% Text Node\n\\draw (322.5,4.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h=f(x,y)$};\n% Text Node\n\\draw (449.5,39.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $h$};\n% Text Node\n\\draw (451.5,112.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $\\frac{\\partial s}{\\partial h}$};\n% Text Node\n\\draw (164.5,172.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\nDownstream\\ \\\\\ngradients\n\\end{array}$};\n% Text Node\n\\draw (430.5,175.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $ \\begin{array}{l}\nUpstream\\ \\\\\ngradients\n\\end{array}$};\n% Text Node\n\\draw (318.5,173.9) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 3; green, 50; blue, 255 }  ,opacity=1 ]  {\\huge $ \\begin{array}{l}\nLocal\\ \\\\\ngradients\n\\end{array}$};\n\n\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nLet’s start with a simple forward pass with \\(1\\) neuron. Let’s say, we have the following input vector, weights and bias:\n\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0] # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x,w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe ReLU function \\(f(x)=\\max(x,0)\\) is differentiable everywhere except at \\(x = 0\\). We define \\(f'(x)\\) as:\n\\[\\begin{align*}\nf'(x) =\n\\begin{cases}\n1 & x &gt; 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align*}\\]\nIn Python, we write:\n\nrelu_dz = (1. if z &gt; 0 else 0.)\n\nThe input to the ReLU function is \\(6.00\\), so the derivative equals \\(1.00\\). We multiply this local gradient by the upstream gradient to calculate the downstream gradient.\n\nimport numpy as np\n\nx = [1.0, -2.0, 3.0]  # input values\nw = [-3.0, -1.0, 2.0]  # weights\nb = 1.0\n\n# Forward pass\nz = np.dot(x, w) + b\n\n# ReLU Activation function\ny = max(z, 0)\n\n# Backward pass\n# Upstream gradient\nds_drelu = 1.0\n\n# Derivative of the ReLU and the chain rule\ndrelu_dz = 1.0 if z &gt; 0 else 0.0\nds_dz = ds_drelu * drelu_dz\nprint(ds_dz)\n\n1.0\n\n\nThe results with the derivative of the ReLU function and chain rule look as follows:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nMoving backward through our neural network, consider the add function \\(f(x,y,z)=x + y + z\\). The partial derivatives \\(\\frac{\\partial f}{\\partial x}\\), \\(\\frac{\\partial f}{\\partial y}\\) and \\(\\frac{\\partial f}{\\partial z}\\) are all equal to \\(1\\). So, the add gate always takes on the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.\n\n# Local gradients for the + function\ndz_dw0x0 = 1\ndz_dw1x1 = 1\ndz_dw2x2 = 1\ndz_db = 1\n\n# Calculate the downstream gradients\nds_dw0x0 = ds_dz * dz_dw0x0\nds_dw1x1 = ds_dz * dz_dw1x1\nds_dw2x2 = ds_dz * dz_dw2x2\nds_db = ds_dz * dz_db\nprint(ds_dw0x0, ds_dw1x1, ds_dw2x2, ds_db)\n\n1.0 1.0 1.0 1.0\n\n\nWe can update the computation graph as:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n\\node [red] (f) at (5,-12.5) {\\large $1.00$};\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nNow, consider the production function \\(f(x,y) = x * y\\). The gradients of \\(f\\) are \\(\\frac{\\partial f}{\\partial x} = y\\), \\(\\frac{\\partial f}{\\partial y} = x\\). The multiply gate is therefore a little less easy to interpret. Its local gradients are the input values, except switched and this is multiplied by the upstream gradient.\n\n# Local gradients for the * function\ndw0x0_dx0 = w[0]\ndw0x0_dw0 = x[0]\ndw1x1_dx1 = w[1]\ndw1x1_dw1 = x[1]\ndw2x2_dx2 = w[2]\ndw2x2_dw2 = x[2]\n\n# Calculate the downstream gradients\nds_dx0 = ds_dw0x0 * dw0x0_dx0\nds_dw0 = ds_dw0x0 * dw0x0_dw0\nds_dx1 = ds_dw1x1 * dw1x1_dx1\nds_dw1 = ds_dw1x1 * dw1x1_dw1\nds_dx2 = ds_dw2x2 * dw2x2_dx2\nds_dw2 = ds_dw2x2 * dw2x2_dw2\n\nprint(ds_dx0, ds_dw0, ds_dx1, ds_dw1, ds_dx2, ds_dw2)\n\n-3.0 1.0 -1.0 -2.0 2.0 3.0\n\n\nWe can update the computation graph as follows:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Input-\\i) at (0,-\\i * 4) {\\large $x[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[] (Weight-\\i) at (0,-\\i * 4-2) {\\large $w[\\i]$};\n}\n\\foreach \\i in {0,...,2}\n{\n    \\node[circle, \n        minimum size = 15mm,\n        draw,\n        ] (Mult-\\i) at (3.0,-\\i * 4 - 1) {\\large $\\times$};\n        \n}\n\n\\node [] (bias) at (0,-12) {\\large $b$};\n\n\\node [circle,minimum size=15mm,draw] (Add) at (6,-5) {\\large +};\n\\node [circle,minimum size=15mm,draw] (ReLU) at (9,-5) {\\large $\\max(x,0)$};\n\\node [] (NextLayer) at (12,-5) {};\n\n\\draw[-&gt;, shorten &gt;=1pt] (Input-0) -- node[midway,above,blue] {\\large $1.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-0) -- node[midway,above,blue] {\\large $-3.0$} (Mult-0);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-1) -- node[midway,above,blue] {\\large $-2.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-1) -- node[midway,above,blue] {\\large $-1.0$}(Mult-1);   \n\\draw[-&gt;, shorten &gt;=1pt] (Input-2) -- node[midway,above,blue] {\\large $3.0$}(Mult-2);   \n\\draw[-&gt;, shorten &gt;=1pt] (Weight-2) -- node[midway,above,blue] {\\large $2.0$}(Mult-2);   \n\n\\draw (bias) -- node[midway,above,blue] {\\large $1.0$}(6,-12);\n\\draw[-&gt;, shorten &gt;=1pt] (6,-12) -- (Add);\n\n\\draw[-&gt;, shorten &gt;=1pt] (Mult-0) -- node[midway,above,blue] {\\large $-3.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-1) -- node[midway,above,blue] {\\large $2.0$}(Add);   \n\\draw[-&gt;, shorten &gt;=1pt] (Mult-2) -- node[midway,above,blue] {\\large $6.0$}(Add);   \n\n\\draw[-&gt;, shorten &gt;=1pt] (Add) -- node[midway,above,blue] {\\large $6.0$}(ReLU);   \n\\draw[-&gt;, shorten &gt;=1pt] (ReLU) -- node[midway,above,blue] {\\large $6.0$}(NextLayer);\n\\node [red] (A) at (11,-5.5) {\\large $1.00$};\n\\node [red] (B) at (7,-5.5) {\\large $1.00$};\n\\node [red] (C) at (5,-3.5) {\\large $1.00$};\n\\node [red] (D) at (5,-5.5) {\\large $1.00$};\n\\node [red] (E) at (5,-7.5) {\\large $1.00$};\n\\node [red] (F) at (5,-12.5) {\\large $1.00$};\n\\node [red] (G) at (1,-0.75) {\\large $-3.0$};\n\\node [red] (H) at (1,-2) {\\large $1.0$};\n\\node [red] (I) at (1,-4.75) {\\large $-1.0$};\n\\node [red] (J) at (1,-6) {\\large $-2.0$};\n\\node [red] (K) at (1,-8.75) {\\large $2.0$};\n\\node [red] (L) at (1,-10) {\\large $3.0$};\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nGradients sum at outward branches. Consider the following computation graph:\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n%uncomment if require: \\path (0,211); %set diagram left start at 0, and has height of 211\n\n%Shape: Ellipse [id:dp4612472925724298] \n\\draw   (444.62,95) .. controls (444.62,81.19) and (455.38,70) .. (468.64,70) .. controls (481.91,70) and (492.66,81.19) .. (492.66,95) .. controls (492.66,108.81) and (481.91,120) .. (468.64,120) .. controls (455.38,120) and (444.62,108.81) .. (444.62,95) -- cycle ;\n%Shape: Ellipse [id:dp4844626229099638] \n\\draw   (299.33,31.5) .. controls (299.33,17.69) and (310.08,6.5) .. (323.35,6.5) .. controls (336.61,6.5) and (347.37,17.69) .. (347.37,31.5) .. controls (347.37,45.31) and (336.61,56.5) .. (323.35,56.5) .. controls (310.08,56.5) and (299.33,45.31) .. (299.33,31.5) -- cycle ;\n%Shape: Ellipse [id:dp2271780920027553] \n\\draw   (303.25,94.7) .. controls (303.25,80.89) and (314,69.7) .. (327.27,69.7) .. controls (340.53,69.7) and (351.29,80.89) .. (351.29,94.7) .. controls (351.29,108.51) and (340.53,119.7) .. (327.27,119.7) .. controls (314,119.7) and (303.25,108.51) .. (303.25,94.7) -- cycle ;\n%Shape: Ellipse [id:dp150108609534231] \n\\draw   (299.25,167.7) .. controls (299.25,153.89) and (310,142.7) .. (323.27,142.7) .. controls (336.53,142.7) and (347.29,153.89) .. (347.29,167.7) .. controls (347.29,181.51) and (336.53,192.7) .. (323.27,192.7) .. controls (310,192.7) and (299.25,181.51) .. (299.25,167.7) -- cycle ;\n%Straight Lines [id:da7844123205705824] \n\\draw    (347.37,31.5) -- (450.04,76.06) ;\n\\draw [shift={(452.79,77.25)}, rotate = 203.46] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da814168086414518] \n\\draw    (351.29,94.7) -- (441.62,94.99) ;\n\\draw [shift={(444.62,95)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da7411937688169676] \n\\draw    (347.29,167.7) -- (446.35,110.75) ;\n\\draw [shift={(448.95,109.25)}, rotate = 150.1] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Shape: Circle [id:dp515320046458885] \n\\draw   (163,96) .. controls (163,82.19) and (174.19,71) .. (188,71) .. controls (201.81,71) and (213,82.19) .. (213,96) .. controls (213,109.81) and (201.81,121) .. (188,121) .. controls (174.19,121) and (163,109.81) .. (163,96) -- cycle ;\n%Straight Lines [id:da6219161786925074] \n\\draw    (492.66,95) -- (567,94.52) ;\n\\draw [shift={(570,94.5)}, rotate = 179.63] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da5694521418691749] \n\\draw    (84.5,95.75) -- (160,95.99) ;\n\\draw [shift={(163,96)}, rotate = 180.18] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.04,-3.86) -- (0,0) -- (8.04,3.86) -- (5.34,0) -- cycle    ;\n%Straight Lines [id:da08990804845355682] \n\\draw    (210.69,85.5) -- (296.86,31.4) ;\n\\draw [shift={(299.4,29.8)}, rotate = 147.88] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da1505672958459916] \n\\draw    (212.61,96) -- (300.4,95.03) ;\n\\draw [shift={(303.4,95)}, rotate = 179.37] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n%Straight Lines [id:da23258128449735227] \n\\draw    (203,116.5) -- (296.36,167.17) ;\n\\draw [shift={(299,168.6)}, rotate = 208.49] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;\n\n% Text Node\n\\draw (464.08,84.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $s$};\n% Text Node\n\\draw (317.25,18.9) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{1}$};\n% Text Node\n\\draw (321.65,82.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{2}$};\n% Text Node\n\\draw (317.65,155.6) node [anchor=north west][inner sep=0.75pt]    {\\huge $z^{3}$};\n% Text Node\n\\draw (365.04,44.2) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{1}}$};\n% Text Node\n\\draw (365.52,94.3) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{2}}$};\n% Text Node\n\\draw (366.72,154) node [anchor=north west][inner sep=0.75pt]  [color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\huge $\\frac{\\partial s}{\\partial z^{3}}$};\n% Text Node\n\\draw (183.5,85.4) node [anchor=north west][inner sep=0.75pt]    {\\huge $a$};\n% Text Node\n\\draw (304.78,21.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{1}}{\\partial a}$};\n% Text Node\n\\draw (305.82,84.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{2}}{\\partial a}$};\n% Text Node\n\\draw (303.26,156.6) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial z^{3}}{\\partial a}$};\n% Text Node\n\\draw (251.38,53.4) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{1}} \\cdot \\frac{\\partial z^{1}}{\\partial a}$};\n% Text Node\n\\draw (249.38,99.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{2}} \\cdot \\frac{\\partial z^{2}}{\\partial a}$};\n% Text Node\n\\draw (245.78,165.8) node [anchor=north west][inner sep=0.75pt]  [font=\\tiny,color={rgb, 255:red, 0; green, 13; blue, 247 }  ,opacity=1 ]  {\\normalsize $\\frac{\\partial s}{\\partial z^{3}} \\cdot \\frac{\\partial z^{3}}{\\partial a}$};\n\n\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\n\nThe upstream gradient for the node \\(a\\) is \\(\\frac{ds}{da}\\). By the law of total derivatives:\n\\[\\begin{align*}\n\\frac{ds}{da} = \\frac{\\partial s}{\\partial z^1} \\cdot \\frac{\\partial z^1}{\\partial a} + \\frac{\\partial s}{\\partial z^2} \\cdot \\frac{\\partial z^2}{\\partial a} + \\frac{\\partial s}{\\partial z^3} \\cdot \\frac{\\partial z^3}{\\partial a}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-for-a-single-neuron---a-python-implementation",
    "href": "posts/backpropogation/index.html#backprop-for-a-single-neuron---a-python-implementation",
    "title": "Backpropogation",
    "section": "Backprop for a single neuron - a python implementation",
    "text": "Backprop for a single neuron - a python implementation\nWe can write a naive implementation for the backprop algorithm for a single neuron.\n\nimport numpy as np\n\nweights = np.array([-3.0, -1.0, 2.0])\nbias = 1.0\ninputs = np.array([1.0, -2.0, 3.0])\ntarget_output = 0.0\nlearning_rate = 0.001\n\n\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1.0, 0.0)\n\n\nfor iter in range(200):\n    # Forward pass\n    z = np.dot(weights, inputs) + bias\n    a = relu(z)\n    loss = (a - target_output) ** 2\n\n    # Backward pass\n    dloss_da = 2 * (a - target_output)\n    dloss_dz = dloss_da * relu_derivative(z)\n    dz_dx = weights\n    dz_dw = inputs\n    dz_db = 1.0\n    dloss_dx = dloss_dz * dz_dx\n    dloss_dw = dloss_dz * dz_dw\n    dloss_db = dloss_dz * dz_db\n\n    # Update the weights and bias\n    weights -= learning_rate * dloss_dw\n    bias -= learning_rate * dloss_db\n\n    # print the loss for this iteration\n    if (iter + 1) % 10 == 0:\n        print(f\"Iteration {iter + 1}, loss: {loss}\")\n\nprint(\"Final weights : \", weights)\nprint(\"Final bias : \", bias)\n\nIteration 10, loss: 20.80624545154949\nIteration 20, loss: 11.314318574097976\nIteration 30, loss: 6.152662434665503\nIteration 40, loss: 3.345783025909011\nIteration 50, loss: 1.8194178821496518\nIteration 60, loss: 0.9893891517327431\nIteration 70, loss: 0.5380242236653578\nIteration 80, loss: 0.29257452918677535\nIteration 90, loss: 0.1591003738562249\nIteration 100, loss: 0.08651788326054576\nIteration 110, loss: 0.04704793547908108\nIteration 120, loss: 0.025584401159906914\nIteration 130, loss: 0.013912652617925996\nIteration 140, loss: 0.007565621788733219\nIteration 150, loss: 0.004114142329436494\nIteration 160, loss: 0.00223724732474303\nIteration 170, loss: 0.0012166024389232565\nIteration 180, loss: 0.0006615815238773228\nIteration 190, loss: 0.0003597642900693548\nIteration 200, loss: 0.00019563778572677352\nFinal weights :  [-3.3990955  -0.20180899  0.80271349]\nFinal bias :  0.6009044964039992"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-for-a-layer-of-neurons",
    "href": "posts/backpropogation/index.html#backprop-for-a-layer-of-neurons",
    "title": "Backpropogation",
    "section": "Backprop for a layer of neurons",
    "text": "Backprop for a layer of neurons\nWe are now in a position to write a naive implementation of the backprop algorithm for a layer of neurons.\nA neural network with a single hidden layer is shown below.\n\n\n\nbackprop\n\n\nLet \\(\\mathcal{L}\\) be a loss function of a neural network to minimize. Let \\(x \\in \\mathbf{R}^{d_0}\\) be a single sample(input). Let \\(d_{l}\\) be number of neurons(inputs) in layer \\(l\\). In our example, \\(x \\in \\mathbf{R}^4\\).\nLet’s derive expressions for all the derivatives we want to compute.\n\nGradient of the loss with respect to \\(\\hat{y}\\)\nThe gradient of the loss function \\(\\mathcal{L}\\) with respect to \\(\\hat{y}\\) is:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} &= 2*(\\hat{y} - y)\n\\end{align*}\\]\n\n\nGradient of the loss with respect to \\(a\\)\nThe gradient of \\(\\hat{y}\\) with respect to \\(a_1, a_2, a_3\\) is:\n\\[\\begin{align*}\n\\frac{\\partial \\hat{y}}{\\partial a} &= \\left[\\frac{\\partial \\hat{y}}{\\partial a_1}, \\frac{\\partial \\hat{y}}{\\partial a_2}, \\frac{\\partial \\hat{y}}{\\partial a_3}\\right] = [1, 1, 1]\n\\end{align*}\\]\nSo, by chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial a} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial a_1}, \\frac{\\partial \\mathcal{L}}{\\partial a_2}, \\frac{\\partial \\mathcal{L}}{\\partial a_3}\\right] \\\\\n&=\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_1}, \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_2}, \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_3}\\right] \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a}\n\\end{align*}\\]\nThis vector has the shape [1,layer_width]. In this example, it’s dimensions are (1,3).\n\n\nGradient of the loss with respect to \\(z\\)\nIn our example, \\(a_1 = max(z_1,0)\\), \\(a_2 = max(z_2,0)\\) and \\(a_3 = max(z_3,0)\\). Consequently, the derivative:\n\\[\\begin{align*}\n\\frac{\\partial a}{\\partial z} &= \\left[\\frac{\\partial a_1}{\\partial z_1}, \\frac{\\partial a_2}{\\partial z_2}, \\frac{\\partial a_3}{\\partial z_3}\\right]\\\\\n&= \\left[1_{(z_1 &gt; 0)}, 1_{(z_2 &gt; 0)}, 1_{(z_3 &gt; 0)}\\right]\n\\end{align*}\\]\nAgain this vector has shape [1,layer_width], which in our example equals (1,3).\nBy the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial z} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}, \\frac{\\partial \\mathcal{L}}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}, \\frac{\\partial \\mathcal{L}}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial z_3}\\right]\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial a} \\odot \\frac{\\partial \\mathcal{a}}{\\partial z}\n\\end{align*}\\]\nwhere \\(\\odot\\) denotes the element wise product of the two vectors. The gradient of the loss with respect to \\(z\\), is also a vector of shape [1,layer_width].\n\n\nGradient of the loss with respect to weights \\(W\\)\nSince\n\\[\\begin{align*}\nz_1 &= w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + w_{14}x_4 + b_1 \\\\\nz_2 &= w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + w_{24}x_4 + b_2 \\\\\nz_3 &= w_{31}x_1 + w_{32}x_2 + w_{23}x_3 + w_{24}x_4 + b_3\n\\end{align*}\\]\nit follows that: \\[\\begin{align*}\n\\frac{\\partial z_i}{\\partial w_{ij}} = x_j\n\\end{align*}\\]\nNow,\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{ij}} &= \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w_{ij}} \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\cdot x_j\n\\end{align*}\\]\nIn other words:\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{13}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{14}}\n\\end{bmatrix}\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{12}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{13}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{14}}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_1\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_2\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_3\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_4\n\\end{bmatrix}\n\\end{align*}\\]\nPutting this together, we define the jacobian matrix \\(\\frac{\\partial \\mathcal{L}}{\\partial W}\\) as:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W}&=\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{31}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{41}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{12}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{22}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{32}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{42}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{13}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{23}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{33}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{43}} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_{14}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{24}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{34}} & \\frac{\\partial \\mathcal{L}}{\\partial w_{44}} \\\\\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{31}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{12}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{22}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{32}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{13}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{23}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{33}}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{14}} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{24}} & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_{34}}\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_1 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_1 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_1 \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_2 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_2 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_2\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_3 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_3 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_3\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot x_4 & \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot x_4 & \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\cdot x_4\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{bmatrix} \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} & \\frac{\\partial \\mathcal{L}}{\\partial z_2} & \\frac{\\partial \\mathcal{L}}{\\partial z_3}\n\\end{bmatrix} \\\\\n&= X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\nThe dimensions of \\(X^T\\) and \\(\\frac{\\partial \\mathcal{L}}{\\partial z}\\) are [input_size,1] and [1,layer_width] respectively. Therefore, \\(\\frac{\\partial \\mathcal{L}}{\\partial W}\\) will be of dimensions [input_size,layer_width]. In our example this equals (4,3).\nThe first column of \\(X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\\) gives the derivative with respect to the first neuron’s weights, the second column gives the derivative with respect to the second neuron’s weights and so forth.\n\n\nGradient of the loss with respect to the biases \\(b\\)\nSince\n\\[\\begin{align*}\n\\frac{\\partial z}{\\partial b} &= \\left[\\frac{\\partial z_1}{\\partial b_1}, \\frac{\\partial z_2}{\\partial b_2}, \\frac{\\partial z_3}{\\partial b_3}\\right]\\\\\n&= [1,1,1]\n\\end{align*}\\]\nIt follows that:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b} &= \\left[\\frac{\\partial \\mathcal{L}}{\\partial b_1}, \\frac{\\partial \\mathcal{L}}{\\partial b_2}, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\right]\\\\\n&= \\left[\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1}, \\frac{\\partial \\mathcal{L}}{\\partial b_2} \\cdot \\frac{\\partial z_2}{\\partial b_21}, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\cdot \\cdot \\frac{\\partial z_3}{\\partial b_3}\\right]\\\\\n&=\\left[\\frac{\\partial \\mathcal{L}}{\\partial z_1} \\cdot 1, \\frac{\\partial \\mathcal{L}}{\\partial b_2} \\cdot 1, \\frac{\\partial \\mathcal{L}}{\\partial b_3}\\cdot \\cdot 1\\right]\\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\n\n\nNaive Python implementation\n\nimport numpy as np\n\ninputs = np.array([1, 2, 3, 4])\nweights = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]])\n\nbiases = np.array([0.1, 0.2, 0.3])\n\n# Learning rate\nlearning_rate = 0.001\n\n\n# ReLU Activation function and its derivative\ndef relu(x):\n    return np.maximum(x, 0)\n\n\ndef relu_derivative(z):\n    return np.where(z &gt; 0.0, 1.0, 0.0)\n\n\nfor iter in range(200):\n    # Forward pass\n    z = np.dot(weights, inputs) + biases\n    a = relu(z)\n    y_pred = np.sum(a)\n    y_true = 0.0\n    loss = (y_pred - y_true) ** 2\n\n    # Backward pass\n    # Gradient of loss with respect to y_pred\n    dloss_dy = 2 * (y_pred - y_true)\n\n    # Gradient of y_pred with respect to a\n    dy_da = np.ones_like(a)\n\n    # Gradient of the activation function with respect to z\n    da_dz = relu_derivative(z)\n\n    # Gradient of z with respect to the weights\n    dz_dw = inputs\n\n    # Gradient of z with respect to inputs\n    dz_dx = weights\n\n    # Gradient of loss with respect to a\n    dloss_da = dloss_dy * dy_da\n\n    # Gradient of loss with respect to z\n    dloss_dz = dloss_da * da_dz\n\n    # Gradient of loss with respect to the weights\n    dloss_dw = np.outer(dloss_dz, dz_dw)\n\n    # Gradient of loss with respect to biases\n    dloss_db = dloss_dz\n\n    weights -= learning_rate * dloss_dw\n    biases -= learning_rate * dloss_db\n\n    if (iter + 1) % 20 == 0:\n        print(f\"Iteration {iter+1}, loss = {loss}\")\n\nprint(\"Final weights : \", weights)\nprint(\"Final bias : \", biases)\n\nIteration 20, loss = 6.057433318678514\nIteration 40, loss = 0.4681684867419663\nIteration 60, loss = 0.03618392815029436\nIteration 80, loss = 0.0027965928794077364\nIteration 100, loss = 0.00021614380010564146\nIteration 120, loss = 1.670537841532316e-05\nIteration 140, loss = 1.2911296454618448e-06\nIteration 160, loss = 9.978916489916474e-08\nIteration 180, loss = 7.712531012091791e-09\nIteration 200, loss = 5.96088109107831e-10\nFinal weights :  [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\nFinal bias :  [-0.00698895 -0.04024714 -0.06451539]"
  },
  {
    "objectID": "posts/backpropogation/index.html#backprop-with-a-batch-of-inputs",
    "href": "posts/backpropogation/index.html#backprop-with-a-batch-of-inputs",
    "title": "Backpropogation",
    "section": "Backprop with a batch of inputs",
    "text": "Backprop with a batch of inputs\nLet \\(x\\) be a batch of inputs of dimensions [batch_size,input_size]. Consider\n\nx = np.array(\n    [\n        [1, 2, 3, 2.5],\n        [2, 5, -1, 2],\n        [-1.5, 2.7, 3.3, -0.8]\n    ]\n)\n\nof shape (3,4). Each sample will give one loss. Hence, the total loss \\(\\mathcal{L} = L_1 + L_2 + L_3\\).\n\nGradient of the loss with respect to weights \\(w\\)\nI am going to denote use the following convention for the \\(z\\)’s:\n\\[\\begin{align*}\n\\begin{array}[c|ccc]\n\\text{} & \\text{Neuron}-1 & \\text{Neuron}-2 & \\text{Neuron}-3\\\\\n\\hline\n\\text{Sample}-1 & z_{11} & z_{12} & z_{13} \\\\\n\\text{Sample}-2 & z_{21} & z_{22} & z_{23} \\\\\n\\text{Sample}-3 & z_{31} & z_{32} & z_{33} \\\\\n\\text{Sample}-4 & z_{41} & z_{42} & z_{43}\n\\end{array}\n\\end{align*}\\]\nIn this case \\(\\frac{d\\mathcal{L}}{dz}\\) will be a matrix of partial derivatives of shape [batch_size,layer_width].\nI can write:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial w_{11}} &= \\frac{\\partial L_1}{\\partial w_{11}} + \\frac{\\partial L_2}{\\partial w_{11}} + \\frac{\\partial L_3}{\\partial w_{11}} \\\\\n&= \\frac{\\partial L_1}{\\partial z_{11}}\\cdot \\frac{\\partial z_{11}}{\\partial w_{11}} + \\frac{\\partial L_2}{\\partial z_{21}}\\cdot\\frac{\\partial z_{21}}{\\partial w_{11}} + \\frac{\\partial L_3}{\\partial z_{31}} \\cdot \\frac{\\partial z_{31}}{\\partial w_{11}}\\\\\n&=\\frac{\\partial L_1}{\\partial z_{11}}\\cdot x_{11} + \\frac{\\partial L_2}{\\partial z_{21}}\\cdot x_{21} + \\frac{\\partial L_3}{\\partial z_{31}} \\cdot x_{31}\n\\end{align*}\\]\nIf you work out the derivatives of the loss function with respect to each of the weights, you would find:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial W} &= X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z}\n\\end{align*}\\]\nX.T has shape [input_size,batch_size] and dloss_dz has shape [batch_size,layer_width], so the matrix product will have dimensions [input_size,layer_width].\n\n\nGradient of the loss with respect to the biases \\(b\\)\nConsider again:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial b_1} &= \\frac{\\partial L}{\\partial z_{11}} \\cdot \\frac{\\partial z_{11}}{\\partial b_1} + \\frac{\\partial L}{\\partial z_{21}} \\cdot \\frac{\\partial z_{21}}{\\partial b_1} + \\frac{\\partial L}{\\partial z_{31}} \\cdot \\frac{\\partial z_{31}}{\\partial b_1} \\\\\n&= \\frac{\\partial L}{\\partial z_{11}} \\cdot 1 + \\frac{\\partial L}{\\partial z_{21}} \\cdot 1 + \\frac{\\partial L}{\\partial z_{31}} \\cdot 1\n\\end{align*}\\]\nSo, to find the partial derivative of the loss with respect to \\(b_1\\), we will just look at the partial derivatives of the loss with respect to the first neuron and then add them up.\nIn python, we would write this as\ndloss_dbiases = np.sum(dloss_dz, axis=0, keepdims=True)\n\n\nGradient of the loss with respect to the inputs\nThe gradients of the loss with respect to the weights in the layer \\(l\\), require the gradients of the loss with respect to the inputs in layer \\(l+1\\). It’s easy to see that:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w_{11}^{(l)}} &= \\frac{\\partial L}{\\partial x_1^{(l+1)}}\\cdot \\frac{\\partial x_1^{(l+1)}}{\\partial z_{1}^{l}} \\cdot \\frac{\\partial z_1^{(l)}}{\\partial w_{11}^{(l)}}\n\\end{align*}\\]\nWhat is \\(\\frac{\\partial \\mathcal{L}}{\\partial x_1}\\), \\(\\frac{\\partial \\mathcal{L}}{\\partial x_2}\\), \\(\\frac{\\partial \\mathcal{L}}{\\partial x_3}\\) and \\(\\frac{\\partial \\mathcal{L}}{\\partial x_4}\\)?\nBy the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial x_1} &= \\frac{\\partial L}{\\partial z_1}\\cdot \\frac{\\partial z_1}{\\partial x_1} +  \\frac{\\partial L}{\\partial z_2}\\cdot \\frac{\\partial z_2}{\\partial x_1} +  \\frac{\\partial L}{\\partial z_3}\\cdot \\frac{\\partial z_3}{\\partial x_1} \\\\\n&= \\frac{\\partial L}{\\partial z_1}\\cdot w_{11} +  \\frac{\\partial L}{\\partial z_2}\\cdot w_{21} +  \\frac{\\partial L}{\\partial z_3}\\cdot w_{31}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial x_1} & \\frac{\\partial \\mathcal{L}}{\\partial x_2} & \\frac{\\partial \\mathcal{L}}{\\partial x_3} & \\frac{\\partial \\mathcal{L}}{\\partial x_4}\n\\end{bmatrix} &=\n\\begin{bmatrix}\n\\frac{\\partial L}{\\partial z_1} & \\frac{\\partial L}{\\partial z_2} & \\frac{\\partial L}{\\partial z_3}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_{11} & w_{12} & w_{13} & w_{14}\\\\\nw_{21} & w_{22} & w_{23} & w_{24}\\\\\nw_{31} & w_{32} & w_{33} & w_{34}\n\\end{bmatrix}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial x} &= \\frac{\\partial L}{\\partial z} \\cdot W\n\\end{align*}\\]\nWhat if we have a batch of input data of 3 examples? In such case, \\(\\frac{\\partial \\mathcal{L}}{\\partial z}\\) will have shape (3,3) and \\(W\\) will have shape (3,4). So, we can multiply them and the result would be (3,4)."
  },
  {
    "objectID": "posts/backpropogation/index.html#adding-backward-to-denselayer",
    "href": "posts/backpropogation/index.html#adding-backward-to-denselayer",
    "title": "Backpropogation",
    "section": "Adding backward() to DenseLayer",
    "text": "Adding backward() to DenseLayer\nWe will now add backward pass code to the DenseLayer and ReLUActivation classes.\n\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\n\nclass DenseLayer:\n    def __init__(self, n_inputs, n_neurons):\n        self.width = n_neurons\n        # Weight vectors per neuron\n        self.weights = np.array(\n            [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]]\n        )\n        self.biases = np.array([0.1, 0.2, 0.3])\n\n    def forward(self, inputs):\n        self.inputs = inputs\n        self.output = np.dot(inputs, self.weights.T) + self.biases\n\n    def backward(self, dloss_dz):\n        self.dloss_dz = dloss_dz\n        self.dz_dweights = self.inputs\n        self.dz_dbiases = np.ones_like(self.inputs)\n        self.dz_dinputs = self.weights\n        self.dloss_dweights = np.dot(self.inputs.T, self.dloss_dz).T\n        self.dloss_dbiases = np.sum(self.dloss_dz, axis=0, keepdims=True)\n        self.dloss_dinputs = np.dot(self.dloss_dz, self.dz_dinputs)\n\n\nclass ReLUActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        # Calculate output values from the inputs\n        self.inputs = inputs\n        self.output = np.maximum(0, inputs)\n\n    # Backward pass\n    def backward(self, dloss_da):\n        self.dloss_da = dloss_da\n        self.da_dz = np.where(self.inputs &gt; 0.0, 1.0, 0.0)\n        self.dloss_dz = self.dloss_da * self.da_dz\n\n\n# Create dataset\nX = np.array([[1, 2, 3, 2.5], [2, 5, -1, 2], [-1.5, 2.7, 3.3, -0.8]])\n\n# Create a dense layer with 4 input features and 3 output values\ndense1 = DenseLayer(4, 3)\nrelu = ReLUActivation()\n\n# Perform a forward pass of our training data through this layer\ndense1.forward(X)\nrelu.forward(dense1.output)\n\n# Calculate loss\ny_pred = np.sum(relu.output)\ny_true = 0.0\nloss = (y_pred - y_true) ** 2\n\n# Gradient of the loss with respect to y\ndloss_dy = 2 * (y_pred - y_true)\ndy_da = np.ones_like(relu.output)\ndloss_da = dloss_dy * dy_da\n\nrelu.backward(dloss_da)\ndense1.backward(relu.dloss_dz)\nprint(f\"dloss_dweights = {dense1.dloss_dweights}\")\nprint(f\"dloss_dbiases = {dense1.dloss_dbiases}\")\nprint(f\"dloss_dinputs = {dense1.dloss_dinputs}\")\n\ndloss_dweights = [[124.560005 805.48804  440.112    307.24802 ]\n [124.560005 805.48804  440.112    307.24802 ]\n [124.560005 805.48804  440.112    307.24802 ]]\ndloss_dbiases = [[249.12000303 249.12000303 249.12000303]]\ndloss_dinputs = [[124.560005 149.472    174.384    199.296   ]\n [124.560005 149.472    174.384    199.296   ]\n [124.560005 149.472    174.384    199.296   ]]"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-derivative",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-derivative",
    "title": "Backpropogation",
    "section": "Categorical cross-entropy loss derivative",
    "text": "Categorical cross-entropy loss derivative\nThe cross-entropy loss of the \\(i\\)-th sample is given by:\n\\[\\begin{align*}\nL_i = -\\sum_k y_{ik}log(\\hat{y}_ik)\n\\end{align*}\\]\nDifferentiating with respect to \\(\\hat{y}_{ij}\\), we have:\n\\[\\begin{align*}\n\\frac{\\partial L_i}{\\partial \\hat{y}_{ij}} &= -\\frac{\\partial}{\\partial \\hat{y}_{ik}} \\left[\\sum_k y_{ik}\\log (\\hat{y}_{ik})\\right] \\\\\n&= -y_{ij} \\cdot \\frac{\\partial }{\\partial \\hat{y}_{ij}} \\log (\\hat{y}_{ij})\\\\\n&= -\\frac{y_{ij}}{\\hat{y}_{ij}}\n\\end{align*}\\]\n\nAdding backward() to CategoricalCrossEntropyLoss\n\n# Cross-Entropy loss\nclass CategoricalCrossEntropyLoss(Loss):\n\n    # Forward pass\n    def forward(self, y_pred, y_true):\n        num_samples = len(y_pred)\n\n        # Clip data to prevent division by 0\n        # Clip both sides to not drag mean towards any value\n        epsilon = 1e-7\n        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n        # If categorical labels\n        if len(y_true.shape) == 1:\n            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n        # else if one-hot encoding\n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n\n        neg_log = -np.log(correct_confidences)\n        return neg_log\n\n    # Backward pass\n    def backward(self, y_pred, y_true):\n\n        # number of samples\n        batch_size = len(y_pred)\n\n        # number of labels\n        num_labels = len(y_pred[0])\n\n        # If labels are sparse, turn them into a one-hot vector\n        if len(y_true.shape) == 1:\n            y_true = np.eye(num_labels)[y_true]\n\n        # Calculate gradient\n        self.dloss_da = -y_true / y_pred\n\n        # Normalize the gradient\n        self.dloss_da = self.dloss_da / batch_size"
  },
  {
    "objectID": "posts/backpropogation/index.html#softmax-activation-function-derivative",
    "href": "posts/backpropogation/index.html#softmax-activation-function-derivative",
    "title": "Backpropogation",
    "section": "Softmax Activation function derivative",
    "text": "Softmax Activation function derivative\nWe are interested to calculate the derivative of the softmax function. The softmax activation function is defined as:\n\\[\\begin{align*}\nS_{i,j} &= \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}}\n\\end{align*}\\]\nwhere \\(S_{i,j}\\) denotes the output of the \\(j\\)-th neuron for the \\(i\\)-th sample. Thus, \\(S_{i,j} = f(z_{i,1},\\ldots,z_{i,d_l})\\). Let’s calculate the partial derivative of \\(S_{i,j}\\) with respect to \\(z_{i,k}\\).\nBy the \\(u/v\\) rule:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= \\frac{\\sum_{l=1}^{d_l} e^{z_{i,l}} \\cdot \\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}}-e^{z_{i,j}} \\cdot \\frac{\\partial}{\\partial z_{i,k}} \\sum_{l=1}^{d_l} e^{z_{i,l}}}{\\left(\\sum_{l=1}^{d_l} e^{z_{i,l}}\\right)^2}\n\\end{align*}\\]\nWe have two cases. If \\(j=k\\), then \\(\\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}} = e^{z_{i,k}}\\) and we get:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= \\frac{e^{z_{i,k}} \\cdot \\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}} \\cdot e^{z_{i,k}}}{\\left(\\sum_{l=1}^{d_l} e^{z_{i,l}}\\right)^2}\\\\\n&=\\frac{e^{z_{i,k}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}} \\cdot \\frac{\\sum_{l=1}^{d_l} e^{z_{i,l}} -e^{z_{i,k}}}{\\sum_{l=1}^{d_l} e^{z_{i,l}}}\\\\\n&=S_{i,k}(1-S_{i,k})\n\\end{align*}\\]\nIn the case where \\(j \\neq k\\), \\(\\frac{\\partial e^{z_{i,j}}}{\\partial z_{i,k}} = 0\\) and we have:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= -\\frac{e^{z_{i,j}}}{\\sum_{l=1}^{d_l}e^{z_{i,l}}}\\cdot \\frac{e^{z_{i,k}}}{\\sum_{l=1}^{d_l}e^{z_{i,l}}}\\\\\n&=-S_{i,j} S_{i,k}\n\\end{align*}\\]\nSo, the derivative of the softmax activation function can be expressed in terms of Kronecker’s delta as:\n\\[\\begin{align*}\n\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} &= S_{i,j}(\\delta_{j,k} -  S_{i,k})\\\\\n&= S_{i,j} \\delta_{j,k} - S_{i,j}S_{i,k}\n\\end{align*}\\]\nNow, like before, let’s say we have neural network with a single hidden layer with \\(d_1 = 3\\) neurons. We apply the softmax activation function to the output of this layer. The jacobian matrix \\(\\frac{\\partial S_i}{\\partial z_i}\\) for the \\(i\\)-th sample can be expressed as:\n\\[\\begin{align*}\n\\frac{\\partial S_i}{\\partial z_i} &=\n\\begin{bmatrix}\n\\frac{\\partial S_{i1}}{\\partial z_{i1}} & \\frac{\\partial S_{i1}}{\\partial z_{i2}} & \\frac{\\partial S_{i1}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i2}}{\\partial z_{i1}} & \\frac{\\partial S_{i2}}{\\partial z_{i2}} & \\frac{\\partial S_{i2}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i3}}{\\partial z_{i1}} & \\frac{\\partial S_{i3}}{\\partial z_{i2}} & \\frac{\\partial S_{i3}}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}(\\delta_{11} - S_{i1}) & S_{i1}(\\delta_{12} - S_{i2}) & S_{i1}(\\delta_{13} - S_{i3}) \\\\\nS_{i2}(\\delta_{21} - S_{i1}) & S_{i2}(\\delta_{22} - S_{i2}) & S_{i2}(\\delta_{23} - S_{i3}) \\\\\nS_{i3}(\\delta_{31} - S_{i1}) & S_{i3}(\\delta_{32} - S_{i2}) & S_{i3}(\\delta_{33} - S_{i3})\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}(1 - S_{i1}) & S_{i1}(0 - S_{i2}) & S_{i1}(0 - S_{i3}) \\\\\nS_{i2}(0 - S_{i1}) & S_{i2}(1 - S_{i2}) & S_{i2}(0 - S_{i3}) \\\\\nS_{i3}(0 - S_{i1}) & S_{i3}(0 - S_{i2}) & S_{i3}(1 - S_{i3})\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\nS_{i1}\\\\\nS_{i2}\\\\\nS_{i3}\n\\end{bmatrix}\\odot\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} -\n\\begin{bmatrix}\nS_{i1}\\\\\nS_{i2}\\\\\nS_{i3}\n\\end{bmatrix}\\begin{bmatrix}\nS_{i1} & S_{i2} & S_{i3}\n\\end{bmatrix}\n\\end{align*}\\]\nSay the softmax_output=[0.70, 0.10, 0.20]. Then, in python, we can find the Jacobian matrix as:\n\nimport numpy as np\n\nsoftmax_output = np.array([0.70, 0.10, 0.20])\n\n# Reshape as a column vector\nsoftmax_output = softmax_output.reshape(-1, 1)\n\nda_dz = np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T)\n\nprint(f\"softmax_output = {softmax_output}\")\nprint(f\"da_dz = {da_dz}\")\n\nsoftmax_output = [[0.7]\n [0.1]\n [0.2]]\nda_dz = [[ 0.20999999 -0.07       -0.14      ]\n [-0.07        0.09       -0.02      ]\n [-0.14       -0.02        0.16      ]]\n\n\nWhat happens when we have a batch of inputs? By the chain rule:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_{11}} &= \\frac{\\partial L}{\\partial S_{11}} \\cdot \\frac{\\partial S_{11}}{\\partial z_{11}} + \\frac{\\partial L}{\\partial S_{12}} \\cdot \\frac{\\partial S_{12}}{\\partial z_{11}} + \\frac{\\partial L}{\\partial S_{13}}\\cdot \\frac{\\partial S_{13}}{\\partial z_{11}}\n\\end{align*}\\]\nIn general,\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_{ij}} &= \\frac{\\partial L}{\\partial S_{i1}} \\cdot \\frac{\\partial S_{i1}}{\\partial z_{ij}} + \\frac{\\partial L}{\\partial S_{i2}} \\cdot \\frac{\\partial S_{i2}}{\\partial z_{ij}} + \\frac{\\partial L}{\\partial S_{i3}}\\cdot \\frac{\\partial S_{i3}}{\\partial z_{ij}}\\\\\n&=\\sum_{k=1}^{3} \\frac{\\partial L}{\\partial S_{ik}} \\cdot \\frac{\\partial S_{ik}}{\\partial z_{ij}}\n\\end{align*}\\]\nIt follows that:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial z_i} &= \\begin{bmatrix}\n\\frac{\\partial L}{\\partial z_{i1}} & \\frac{\\partial L}{\\partial z_{i2}} & \\frac{\\partial L}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\n\\frac{\\partial L}{\\partial S_{i1}} & \\frac{\\partial L}{\\partial S_{i2}} & \\frac{\\partial L}{\\partial S_{i3}}\n\\end{bmatrix} \\begin{bmatrix}\n\\frac{\\partial S_{i1}}{\\partial z_{i1}} & \\frac{\\partial S_{i1}}{\\partial z_{i2}} & \\frac{\\partial S_{i1}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i2}}{\\partial z_{i1}} & \\frac{\\partial S_{i2}}{\\partial z_{i2}} & \\frac{\\partial S_{i2}}{\\partial z_{i3}} \\\\\n\\frac{\\partial S_{i3}}{\\partial z_{i1}} & \\frac{\\partial S_{i3}}{\\partial z_{i2}} & \\frac{\\partial S_{i3}}{\\partial z_{i3}}\n\\end{bmatrix}\\\\\n&=\\frac{\\partial L}{\\partial S_i} \\cdot \\frac{\\partial S_i}{\\partial z_i}\n\\end{align*}\\]\nNow, \\(\\partial L/\\partial S_i\\) has shape [1,3] and \\(\\partial S_i/\\partial z_i\\) is a matrix of size [3,3]. So, \\(\\partial L/\\partial z_i\\) will have dimensions [1,3]."
  },
  {
    "objectID": "posts/backpropogation/index.html#softmax-backward-implementation",
    "href": "posts/backpropogation/index.html#softmax-backward-implementation",
    "title": "Backpropogation",
    "section": "Softmax backward() implementation",
    "text": "Softmax backward() implementation\nWe are now in a position to add backward() pass to the SoftmaxActivation layer.\n\nclass SoftmaxActivation:\n\n    # Forward pass\n    def forward(self, inputs):\n        self.inputs = inputs\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\n    # Backward pass\n    def backward(self, dloss_da):\n        dloss_dz = []\n        n = len(self.output)\n        for i in range(n):\n            softmax_output = self.output[i]\n\n            # Reshape as a column vector\n            softmax_output = softmax_output.reshape(-1, 1)\n\n            dsoftmax_dz = np.diagflat(softmax_output) - np.dot(\n                softmax_output, softmax_output.T\n            )\n            dloss_dz.append(np.dot(dloss_da[i], dsoftmax_dz))\n\n        self.dloss_dz = np.array(dloss_dz)"
  },
  {
    "objectID": "posts/backpropogation/index.html#categorical-cross-entropy-loss-and-softmax-activation-function-derivative",
    "href": "posts/backpropogation/index.html#categorical-cross-entropy-loss-and-softmax-activation-function-derivative",
    "title": "Backpropogation",
    "section": "Categorical cross-entropy loss and softmax activation function derivative",
    "text": "Categorical cross-entropy loss and softmax activation function derivative\nThe derivative of the categorical cross entropy loss and softmax activation function can be combined and results in a faster and simple implementation. The current implementation of the backward function in SoftMaxActivation is not vectorized and has a loop.\nLet’s focus again on \\(\\frac{\\partial L_{i}}{\\partial z_{ij}}\\). We have:\n\\[\\begin{align*}\n\\frac{\\partial L_i}{\\partial z_{ij}} &= \\sum_{k} \\frac{\\partial L_i}{\\partial S_{ik}} \\frac{\\partial S_{ik}}{\\partial z_{ij}} \\\\\n&= \\frac{\\partial L_i}{S_{ij}} \\cdot \\frac{\\partial S_{ij}}{\\partial z_{ij}} + \\sum_{k\\neq j}\\frac{\\partial L_i}{\\partial S_{ik}} \\frac{\\partial S_{ik}}{\\partial z_{ij}} \\\\\n&= -\\frac{y_{ij}}{\\hat{y}_{ij}}\\hat{y}_{ij}(1-\\hat{y}_{ij}) + \\sum_{k \\neq j}-\\frac{y_{ik}}{\\hat{y}_{ik}}\\cdot \\hat{y}_{ik}(0 - \\hat{y}_{ij})\\\\\n&= -\\frac{y_{ij}}{\\cancel{\\hat{y}_{ij}}}\\cancel{\\hat{y}_{ij}}(1-\\hat{y}_{ij}) + \\sum_{k \\neq j}-\\frac{y_{ik}}{\\cancel{\\hat{y}_{ik}}}\\cdot \\cancel{\\hat{y}_{ik}}(0 - \\hat{y}_{ij})\\\\\n&= -y_{ij} + y_{ij}\\hat{y}_{ij} + \\sum_{k\\neq j}y_{ik} \\hat{y}_{ij}\\\\\n&= -y_{ij} + \\hat{y}_{ij}(\\sum_{k}y_{ik})\\\\\n&= \\hat{y}_{ij} - y_{ij}\n\\end{align*}\\]\n\nclass CategoricalCrossEntropySoftmax:\n\n    # create activation and loss function objects\n    def __init__(self):\n        self.activation = SoftmaxActivation()\n        self.loss = CategoricalCrossEntropyLoss()\n\n    # forward pass\n    def forward(self, inputs, y_true):\n\n        self.inputs = inputs\n        self.activation.forward(inputs)\n\n        self.output = self.activation.output\n\n        return self.loss.calculate(self.output, y_true)\n\n    # Backward pass\n    def backward(self, y_pred, y_true):\n        # number of samples\n        batch_size = len(y_pred)\n\n        # number of labels\n        num_labels = len(y_pred[0])\n\n        # If labels are sparse, turn them into a one-hot vector\n        if len(y_true.shape) == 1:\n            y_true = np.eye(num_labels)[y_true]\n\n        # Calculate the gradient\n        self.dloss_dz = y_pred - y_true\n\n        # Normalize the gradient\n        self.dloss_dz = self.dloss_dz / batch_size\n\nWe can now test if the combined backward step returns the same values compared to when we backpropogate gradients through both of the functions separately.\n\nimport numpy as np\nimport nnfs\n\nnnfs.init()\n\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n\nclass_targets = np.array([0, 1, 1])\n\n\nactivation = SoftmaxActivation()\nactivation.output = softmax_outputs\n\nloss = CategoricalCrossEntropyLoss()\nloss.backward(softmax_outputs, class_targets)\nprint(\"Gradients : separate loss and activation\")\nprint(f\"dloss_da = {loss.dloss_da}\")\n\nactivation.backward(loss.dloss_da)\nprint(f\"dloss_dz = {activation.dloss_dz}\")\n\nsoftmax_cce = CategoricalCrossEntropySoftmax()\nsoftmax_cce.backward(softmax_outputs, class_targets)\nprint(\"Gradients : combined loss and activation\")\nprint(f\"dloss_dz = {softmax_cce.dloss_dz}\")\n\nGradients : separate loss and activation\ndloss_da = [[-0.47619048 -0.         -0.        ]\n [-0.         -0.66666667 -0.        ]\n [-0.         -0.37037037 -0.        ]]\ndloss_dz = [[-0.09999999  0.03333334  0.06666667]\n [ 0.03333334 -0.16666667  0.13333334]\n [ 0.00666667 -0.03333333  0.02666667]]\nGradients : combined loss and activation\ndloss_dz = [[-0.1         0.03333333  0.06666667]\n [ 0.03333333 -0.16666667  0.13333333]\n [ 0.00666667 -0.03333333  0.02666667]]"
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html",
    "href": "posts/borel_cantelli_lemmas/index.html",
    "title": "Borel-Cantelli Lemmas",
    "section": "",
    "text": "Borel-Cantelli Lemmas.\n\n(First Borel-Cantelli Lemma) Let \\((A_n)\\) be a sequence of events such that the series \\(\\sum_n \\mathbb{P}(A_n)\\) converges to a finite value \\(L\\). Then, almost surely finitely many \\(A_n\\)’s will occur.\n(Second Borel-Cantelli Lemma) Let \\((A_n)\\) be a sequence of independent events, such that the infinite series \\(\\sum_n \\mathbb{P}(A_n)\\) diverges to \\(\\infty\\). Then, almost surely, infinitely many \\(A_n\\)’s will occur.\n\nFix a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). Let \\(A_1, A_2, A_3, \\ldots\\) be an infinite sequence of events belonging to \\(\\mathcal{F}\\). We shall often be interested in finding out how many of the \\(A_n\\)’s occur.\nThe event \\(A_n\\) occurs infinitely often (\\(A_n\\hspace{2mm}i.o\\)) is the set of all \\(\\omega\\) that belong to infinitely many \\(A_n\\)’s.\nImagine that an infinite number of \\(A_n\\)’s occur. That is, \\((\\forall n)(\\exists m \\geq n)(\\text{s.t. }A_m \\text{ occurs})\\). In other words:\n\\[\\begin{align*}\n\\{A_n \\text{ infinitely often}\\} = \\bigcap_{n=1}^{\\infty} \\underbrace{\\bigcup_{m\\geq n} A_m}_{B_n} \\tag{1}\n\\end{align*}\\]\nHere, \\(B_n\\) is the event that atleast one of \\(A_n,A_{n+1},\\ldots\\) occur. For that reason, \\(B_n\\) is referred to as the \\(n\\)-th tail event. \\(\\{A_n \\hspace{2mm} i.o.\\}\\) is the intersection of all \\(B_n\\)’s, so it is the event that all the \\(B_n\\)’s occur. Therefore, no matter how far I go, no matter how big \\(n_0\\) is, atleast one of \\(A_{n_0}, A_{n_0}+1,\\ldots\\) occurs.\nTaking the complement of both sides in (1), we get the expression for the event that \\(A_n\\) occurs finitely many times.\n\\[\\begin{align*}\n\\{A_n \\text{ occurs finitely often}\\} = \\bigcup_{n_0=1}^{\\infty} \\bigcap_{n \\geq n_0} A_n^C\n\\end{align*}\\]\nIt means that, there exists an \\(n_0\\), such that each of the further \\(A_i\\)’s fail to occur.\nIn order to prove the Borel-Cantelli lemmas, we require the following lemma."
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html#borel-cantelli-lemmas",
    "href": "posts/borel_cantelli_lemmas/index.html#borel-cantelli-lemmas",
    "title": "Borel-Cantelli Lemmas",
    "section": "",
    "text": "Borel-Cantelli Lemmas.\n\n(First Borel-Cantelli Lemma) Let \\((A_n)\\) be a sequence of events such that the series \\(\\sum_n \\mathbb{P}(A_n)\\) converges to a finite value \\(L\\). Then, almost surely finitely many \\(A_n\\)’s will occur.\n(Second Borel-Cantelli Lemma) Let \\((A_n)\\) be a sequence of independent events, such that the infinite series \\(\\sum_n \\mathbb{P}(A_n)\\) diverges to \\(\\infty\\). Then, almost surely, infinitely many \\(A_n\\)’s will occur.\n\nFix a probability space \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). Let \\(A_1, A_2, A_3, \\ldots\\) be an infinite sequence of events belonging to \\(\\mathcal{F}\\). We shall often be interested in finding out how many of the \\(A_n\\)’s occur.\nThe event \\(A_n\\) occurs infinitely often (\\(A_n\\hspace{2mm}i.o\\)) is the set of all \\(\\omega\\) that belong to infinitely many \\(A_n\\)’s.\nImagine that an infinite number of \\(A_n\\)’s occur. That is, \\((\\forall n)(\\exists m \\geq n)(\\text{s.t. }A_m \\text{ occurs})\\). In other words:\n\\[\\begin{align*}\n\\{A_n \\text{ infinitely often}\\} = \\bigcap_{n=1}^{\\infty} \\underbrace{\\bigcup_{m\\geq n} A_m}_{B_n} \\tag{1}\n\\end{align*}\\]\nHere, \\(B_n\\) is the event that atleast one of \\(A_n,A_{n+1},\\ldots\\) occur. For that reason, \\(B_n\\) is referred to as the \\(n\\)-th tail event. \\(\\{A_n \\hspace{2mm} i.o.\\}\\) is the intersection of all \\(B_n\\)’s, so it is the event that all the \\(B_n\\)’s occur. Therefore, no matter how far I go, no matter how big \\(n_0\\) is, atleast one of \\(A_{n_0}, A_{n_0}+1,\\ldots\\) occurs.\nTaking the complement of both sides in (1), we get the expression for the event that \\(A_n\\) occurs finitely many times.\n\\[\\begin{align*}\n\\{A_n \\text{ occurs finitely often}\\} = \\bigcup_{n_0=1}^{\\infty} \\bigcap_{n \\geq n_0} A_n^C\n\\end{align*}\\]\nIt means that, there exists an \\(n_0\\), such that each of the further \\(A_i\\)’s fail to occur.\nIn order to prove the Borel-Cantelli lemmas, we require the following lemma."
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html#limit-of-product-series",
    "href": "posts/borel_cantelli_lemmas/index.html#limit-of-product-series",
    "title": "Borel-Cantelli Lemmas",
    "section": "Limit of product series",
    "text": "Limit of product series\nLemma. If \\(\\sum_{i=1}^\\infty p_i = \\infty\\), then \\(\\lim \\prod_{i=1}^{n}(1-p_i) = 0\\).\nProof.\nWe know that:\nUsing Taylor’s series expansion of \\(\\ln(1+x)\\) about \\(a=0\\), we have:\n\\[\\begin{align*}\n\\ln(1+x) &= x - \\frac{f''(c)}{2!}x^2\\\\\n&= x - \\frac{1}{(1+c)^2} \\cdot \\frac{x^2}{2}\\\\\n&\\leq x\\\\\n&\\quad \\{\\text{since } \\left(\\frac{x}{1+c}\\right)^2 \\geq 0\\}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\n\\ln(1 - p_i) &\\leq -p_i\\\\\n\\sum_{i=1}^{n} \\ln(1 - p_i) &\\leq \\sum_{i=1}^{n} (-p_i)\\\\\n\\ln\\left(\\prod_{i=1}^{n}(1-p_i)\\right) &\\leq \\sum_{i=1}^{n} (-p_i)\\\\\n\\prod_{i=1}^{n}(1-p_i) &\\leq e^{-\\sum_{i=1}^{n} p_i}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\n0 \\leq \\prod_{i=1}^{n}(1-p_i) \\leq e^{-\\lim \\sum_{i=1}^{n} p_i}\n\\end{align*}\\]\nNow, \\(e^{-\\lim \\sum_{i=1}^{n} p_i} = 0\\), so by the squeeze theorem, \\(\\lim \\prod_{i=1}^{n}(1-p_i) = 0\\)."
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html#proof-of-the-first-borel-cantelli-lemma",
    "href": "posts/borel_cantelli_lemmas/index.html#proof-of-the-first-borel-cantelli-lemma",
    "title": "Borel-Cantelli Lemmas",
    "section": "Proof of the First Borel-Cantelli Lemma",
    "text": "Proof of the First Borel-Cantelli Lemma\nOur claim is that \\(\\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty} B_n\\right) = 0\\). Observe that, \\(B_1 \\supseteq B_2 \\supseteq B_3 \\supseteq \\ldots\\). So, \\((B_n)\\) is a decreasing sequence of events.\n\\[\\begin{align*}\n\\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty} B_n\\right) &= \\lim_{n \\to \\infty }\\mathbb{P}(B_n) \\\\\n& \\quad \\{ \\text{Continuity of probability measure} \\}\\\\\n&= \\lim_{n\\to\\infty} \\mathbb{P}\\left(\\bigcup_{n=1}^{\\infty}A_n\\right)\\\\\n&\\leq \\lim_{n\\to\\infty} \\sum_{n=1}^{\\infty} \\mathbb{P}\\left(A_n\\right)\\\\\n& \\quad \\{ \\text{Union bound} \\}\n\\end{align*}\\]\nThe infinite series \\(\\sum_{n=1}^{\\infty} \\mathbb{P}\\left(A_n\\right)\\) is convergent. The tail sum \\(\\lim_{k \\to \\infty} \\sum a_k\\) of a convergent series \\(\\sum a_k\\) is zero. Hence,\n\\[\\begin{align*}\n0 \\leq \\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty} B_n\\right) \\leq 0\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\n\\mathbb{P}\\left(\\bigcap_{n=1}^{\\infty} B_n\\right) = 0\n\\end{align*}\\]\nHence, \\(A_n\\) occurs only finitely many times."
  },
  {
    "objectID": "posts/borel_cantelli_lemmas/index.html#proof-of-the-second-borel-cantelli-lemma",
    "href": "posts/borel_cantelli_lemmas/index.html#proof-of-the-second-borel-cantelli-lemma",
    "title": "Borel-Cantelli Lemmas",
    "section": "Proof of the second Borel-Cantelli Lemma",
    "text": "Proof of the second Borel-Cantelli Lemma\nOur claim is that \\(\\mathbb{P}\\left(A_n \\hspace{2mm} i.o.\\right) = 1\\). We must therefore prove that:\n\\[\\begin{align*}\n\\mathbb{P} \\left(\\bigcap_{m=1}^{\\infty} \\bigcup_{n=m}^{\\infty}A_n \\right) = \\mathbb{P} \\left(\\bigcap_{m=1}^{\\infty} B_m \\right) = 1\n\\end{align*}\\]\nOr:\n\\[\\begin{align*}\n\\mathbb{P} \\left(\\bigcup_{n=1}^{\\infty} B_n^C \\right) = 0\n\\end{align*}\\]\nSince \\((B_n^C)\\) is an increasing sequence of events, we have:\n\\[\\begin{align*}\n\\mathbb{P} \\left(\\bigcup_{n=1}^{\\infty} B_n^C\\right) &= \\lim \\mathbb{P}(B_n^C)\\\\\n&= \\lim \\mathbb{P}\\left\\{ \\left(\\bigcup_{m \\geq n} A_m\\right)^C \\right\\} \\\\\n&= \\lim \\mathbb{P} \\left\\{\\bigcap_{m \\geq n} A_m^C \\right\\}\\\\\n&= \\lim \\prod_{m=n}^{\\infty} \\mathbb{P} (A_m^C)\\\\\n&= \\lim \\prod_{m=n}^{\\infty} (1-P(A_m))\n\\end{align*}\\]\nSince \\(\\sum_i P(A_i)\\) diverges to \\(\\infty\\), \\(\\prod_i (1-P(A_i))\\) converges to zero. Consequently,\n\\[\\begin{align*}\n\\mathbb{P} \\left(\\bigcup_{n=1}^{\\infty} B_n^C \\right) = 0\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/concurrent-programming-a-quick-primer-part-I/index.html",
    "href": "posts/concurrent-programming-a-quick-primer-part-I/index.html",
    "title": "Concurrent programming - A Primer (Part I)",
    "section": "",
    "text": "A thread std::thread represents an executable unit. This executable unit, which the thread immediately starts, gets its work-package as a callable unit. A thread is not copy-constructible or copy-assignable, but move-constructible and move-assignable. A callable unit is any entity that behaves like a function, so this could be a function, a function pointer, a lambda function or std::function (function) object.\nFor example:\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\nvoid do_some_work()\n{\n    std::cout &lt;&lt; \"\\nPerforming some work in a separate thread...\";\n}\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nmain() thread\";\n    std::thread t(do_some_work);      // Spawn a new thread\n    t.join();\n    std::cout &lt;&lt; \"\\nExiting main() thread\";\n    return 0;\n}\nstdout:\nmain() thread\nPerforming some work in a separate thread...\nExiting main() thread\nOnce you’ve started your thread, you need to explicitly decide whether to wait for it to finish(by joining with it) or leave it to run on its own (by detaching it). If you don’t decide before the std::thread object goes out of scope and is destroyed, then your program is terminated (the std::thread destructor calls std::terminate()). So, ensure that the thread is correctly joined or detached. You only have to make this decision before the std::thread object is destroyed - the thread itself may well have finished before you join with it or detach it, and if you detach it, then if the thread is still running, it will continue to do so, and may continue running long after the std::thread object is destroyed; it will only stop running when it finally returns from the thread function.\nPassing arguments is easy. If the function signature is void func(int arg1, std::vector&lt;double&gt; arg2, char arg3), you just specify the comma separated list in the std::thread() constructor, as std::thread my_thread(func, arg1, arg2, arg3). This copies the value of the parameters arg1, arg2 and arg3 onto the thread stack. It’s a pass-by-value. To pass by reference, you std::ref() the arguments, so for example, if the function signature is void func(int arg1, std::vector&lt;double&gt;& arg2, char arg3), while spawning a new thread, you’d write std::thread my_thread(func, arg1, std::ref(arg2), arg3).\nIf you don’t wait for the thread to finish in the main() thread, you need to ensure that the data accessed by the thread is valid until the thread is finished with it. This isn’t a new problem - even in single threaded code it’s undefined behavior to access an object after it’s been destroyed - but the use of threads provides another opportunity to encounter such lifetime issues.\nOne situation in which you can encounter such problems is when the thread function holds pointers or references to local variables and the thread hasn’t finished when the function exits.\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\nvoid do_some_work(int& i)\n{\n    std::cout &lt;&lt; \"\\nThe value of i is = \" &lt;&lt; i; //Potential access to dangling reference\n    std::cout &lt;&lt; \"\\nIncrementing the value of i\";\n    ++i;\n    std::cout &lt;&lt; \"\\nThe new value of i = \" &lt;&lt; i;\n}\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nmain() thread\";\n    int i{1};\n    std::thread my_thread(do_some_work, std::ref(i)); // Spawn a new thread\n    my_thread.detach(); // Don't wait for the thread to finish\n    std::cout &lt;&lt; \"\\nExiting main() thread\"; //New thread might still be running\n    return 0;\n}\nstdout:\nmain() thread\nExiting main() thread\nIn this case, the new thread associated with my_thread is still running when main() exits, because you explicitly decided not to wait for it by calling detach(). If the thread is still running, the statements in do_some_work(int& ) will access an already destroyed variable i. This is like normal single-threaded code - allowing a pointer or reference to a local variable to persist beyond the function exit, which is never a good idea. But, it’s easier to make this mistake with multi-threaded code, because it isn’t immediately apparent that this has happened.\n\n\nIf you need to wait for a thread to complete, you do this by calling join() on the associated std::thread instance. In the previous listing, replacing the call to my_thread.detach() by my_thread.join() would therefore be sufficient to ensure that the thead was finished before the function was exited and thus before the local variables were destroyed. In real code, the original thread would either have work to do or would have launched several threads to do useful work before waiting for them to complete.\njoin() is a simple and brute-force technique - either you wait for a thread to finish or you don’t. If you need more fine-grained control over waiting for a thread, such as to check whether a thread is finished, or wait only a certain period of time, then you have to use alternative mechanisms such as condition variables and futures.\n\n\n\nYou need to ensure that you’ve called either join() or detach() before a std::thread object is destroyed. If you’re detaching a thread, you can usually call detach() immediately after the thread has been started, so this isn’t a problem. But, if you’re intending to wait for a thread, you need to carefully pick the place in the code where you call join(). This means that the call to join() is liable to be skipped if an exception is thrown after the thread has been started but before the call to join().\nTo avoid your application being terminated when an exception is thrown, you need to make a decision about what to do in this case. In general, if you were intending to call join() in a non-exceptional case, you also need to call join() in the presence of an exception to avoid accidental lifetime problems.\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n#include &lt;stdexcept&gt;\n#include &lt;string&gt;\n\nint main()\n{\n    int i {1};\n\n    auto my_func = [&i](){\n        for(int j{0};j&lt;10000000;++j)\n        {\n            ++i;\n        }\n        std::cout &lt;&lt; \"\\nt's work package is complete.\" &lt;&lt; std::flush;\n\n    };\n\n    std::cout &lt;&lt; \"main thread\" &lt;&lt; std::flush;\n    std::thread t(my_func);\n    try\n    {\n        std::string(\"abc\").substr(10); // throws std::out_of_range\n    }\n    catch (const std::exception& e)\n    {\n        std::cout &lt;&lt; \"\\n\" &lt;&lt; e.what();\n        t.join();\n        throw e; // copy-initializes a new exception object of type std::exception\n    }\n\n    t.join();\n    return 0;\n}\nstdout:\nmain thread\ninvalid string position\nt's work package is complete.\nThe code in the listing above uses try/catch block to ensure that a thread with access to the local state is finished before the main thread exits normally, or by an exception.\nA more structured way of doing is to use the standard Resource Aquisition is Initialization(RAII) idiom and provide a wrapper class that does the join() in its destructor, as in the following listing:\n//thread_guard.cpp\n\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n#include &lt;stdexcept&gt;\n#include &lt;string&gt;\n\nclass thread_guard\n{\n    private:\n    std::thread& m_t;\n\n    public:\n    explicit thread_guard(std::thread& t) : m_t{t} {}\n    ~thread_guard()\n    {\n        if(m_t.joinable()){\n            m_t.join();\n        }\n    }\n\n    thread_guard(const thread_guard& _tg) = delete;\n    thread_guard& operator=(thread_guard& _tg) = delete;\n};\n\nint main()\n{\n    int i {1};\n\n    auto my_func = [&i](){\n        for(int j{0};j&lt;10000000;++j)\n        {\n            ++i;\n        }\n        std::cout &lt;&lt; \"\\nt's work package is complete.\" &lt;&lt; std::flush;\n\n    };\n\n    std::cout &lt;&lt; \"main thread\" &lt;&lt; std::flush;\n    std::thread t(my_func);\n    thread_guard g(t);\n    try\n    {\n        std::string(\"abc\").substr(10); // throws std::out_of_range\n    }\n    catch (const std::exception& e)\n    {\n        std::cout &lt;&lt; \"\\n\" &lt;&lt; e.what();\n        throw e; // copy-initializes a new exception object of type std::exception\n    }\n\n    return 0;\n}\nWhen the execution of the main thread reaches the end, the local objects are destroyed in the reverse order of their construction. Consequently, the thread_guard object id object g is destroyed first, and the thread is joined with, which is a blocking call in the destructor. This happens even if there is an exception in the main thread.\nThe copy constructor and copy-assignment operators are marked =delete to ensure that they’re not automatically provided by the compiler. Copying or assigning such an object would be dangerous, because it might outlive the scope of the thread it was joining. By declaring them as as deleted, any attempt to copy a thread_guard object will generate a compilation error.\nIf you don’t need to wait for a thread to finish, you can avoid this exception-safety by detaching from it. This breaks the association of the thread with the std::thread object and ensures that std::terminate() won’t be called when the std::thread object is destroyed, even though the thread is still running in the background. Note that, the std::thread destructor checks if the thread is joinable, and if this condition is true, it invokes std::terminate.\n\n\n\nCalling detach() on a std::thread object leaves the thread to run in the background with no direct means of communicating with it. It’s no longer possible to wait for that thread to complete; if a thread becomes detached, it isn’t possible to obtain a std::thread object that references it, so it can no longer be joined. Detached threads truly run in the background; ownership and control are passed over to the C++ runtime library, which ensures that the resources associated with the thread are correctly reclaimed when the thread exits.\nDetached threads are often called daemon threads after the UNIX concept of a daemon process that runs in the background without any explicit user interface. Such threads are typically long-running; they run for almost the entire lifetime of the application, performing a background task such as monitoring the filesystem, clearing unused entries out of object caches, or optimizing data-structures. At the other extreme, it may make sense to use a detached thread where there’s another mechanism for identifying when the thread has completed or where the thread is used for a fire-and-forget task.\nAs we’ve seen earlier, we detach a thread by calling detach() member function of the std::thread object. After the call completes, the std::thread object is no longer associated with the actual thread of execution and is therefore no longer joinable.\nIn order to detach the thread from a std::thread object, there must be a thread to detach: you can’t call detach() on a std::thread object with no associated thread of execution. This is exactly the same requirement for join(), and you can check it in the same way - you can call t.detach() for a std::thread object t when t.joinable() returns true.\nConsider an application such as a word-processor that can edit multiple documents at once. There are many ways to handle this, both at the UI level and internally. One way that’s increasingly common at the moment is to have multiple, independent, top-level windows, one for each document being edited. Although these windows appear to be completely independent, each with its own menus, they’re running within the same instance of the application. One way to handle this internally is to run each document-editing window in its own thread; each thread runs the same code but with different data relating to the document being edited. Opening a new document therefore requires starting a new thread. The thread handling the request isn’t going to care about waiting for that other thread to finish, because it’s working on an unrelated document, so this makes it a prime candidate for running a detached thread.\n\n\n\nPassing arguments to the callable object or function is fundamentally as simple as passing additional arguments to the std::thread constructor. But, its important to bear in mind that the arguments are copied into internal storage, where they can be accessed by the newly created thread of execution and then passed to the callable function or object as rvalues as if they were temporaries.\nTherefore, if you want to pass an object’s member function as a work-package to a thread, you pass the member-function pointer as the function, and an object pointer as the first argument:\n#include &lt;thread&gt;\n\nclass X{\n    public do_lengthy_work();\n};\n\nint main()\n{\n    X x;\n    std::thread t(&X::do_length_work, &x);\n    t.join();\n    return 0;\n}\nYou can also supply arguments to such a member-function call: the third argument to the std::thread constructor will be the first argument to the function, and so forth. std::thread is a variadic template.\nAnother interesting scenario for supplying arguments is where the arguments cannot be copied but can only be moved: the data held within one object is transferred over to another, leaving the original object empty. An example of such a type is std::unique_ptr&lt;T&gt;, which provides automatic memory management for dynamically allocated objects. Only one std::unique_ptr&lt;T&gt; instance can point to a given object at a time, and when that instance is destroyed, the pointed-to-object is also deleted. The move constructor and the move assignment operator allow the ownership of an object to be transferred around between std::unique_ptr&lt;T&gt; instances. Such a transfer leaves the source object with a nullptr. This moving of values allows objects of this type to be accepted as function parameters or returned from functions. When the source object is temporary, the move is automatic, but where the source is a named value, the transfer must be requested directly by invoking std::move().\nSeveral of the classes in the C++ standard library exhibit the same ownership semantics as std::unique_ptr&lt;T&gt;, and std::thread is one of them. Though std::thread instances don’t own a dynamic object in the same way as the std::unique_ptr&lt;T&gt; does, they do own a resource: each instance is responsible for managing a thread of execution. This ownership can be transferred between instances, because instances of std::thread are moveable, even though they aren’t copyable. This ensures, that only one object is associated with a particular thread of execution at any time while allowing programmers the option of transferring ownership between objects."
  },
  {
    "objectID": "posts/concurrent-programming-a-quick-primer-part-I/index.html#basic-thread-management",
    "href": "posts/concurrent-programming-a-quick-primer-part-I/index.html#basic-thread-management",
    "title": "Concurrent programming - A Primer (Part I)",
    "section": "",
    "text": "A thread std::thread represents an executable unit. This executable unit, which the thread immediately starts, gets its work-package as a callable unit. A thread is not copy-constructible or copy-assignable, but move-constructible and move-assignable. A callable unit is any entity that behaves like a function, so this could be a function, a function pointer, a lambda function or std::function (function) object.\nFor example:\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\nvoid do_some_work()\n{\n    std::cout &lt;&lt; \"\\nPerforming some work in a separate thread...\";\n}\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nmain() thread\";\n    std::thread t(do_some_work);      // Spawn a new thread\n    t.join();\n    std::cout &lt;&lt; \"\\nExiting main() thread\";\n    return 0;\n}\nstdout:\nmain() thread\nPerforming some work in a separate thread...\nExiting main() thread\nOnce you’ve started your thread, you need to explicitly decide whether to wait for it to finish(by joining with it) or leave it to run on its own (by detaching it). If you don’t decide before the std::thread object goes out of scope and is destroyed, then your program is terminated (the std::thread destructor calls std::terminate()). So, ensure that the thread is correctly joined or detached. You only have to make this decision before the std::thread object is destroyed - the thread itself may well have finished before you join with it or detach it, and if you detach it, then if the thread is still running, it will continue to do so, and may continue running long after the std::thread object is destroyed; it will only stop running when it finally returns from the thread function.\nPassing arguments is easy. If the function signature is void func(int arg1, std::vector&lt;double&gt; arg2, char arg3), you just specify the comma separated list in the std::thread() constructor, as std::thread my_thread(func, arg1, arg2, arg3). This copies the value of the parameters arg1, arg2 and arg3 onto the thread stack. It’s a pass-by-value. To pass by reference, you std::ref() the arguments, so for example, if the function signature is void func(int arg1, std::vector&lt;double&gt;& arg2, char arg3), while spawning a new thread, you’d write std::thread my_thread(func, arg1, std::ref(arg2), arg3).\nIf you don’t wait for the thread to finish in the main() thread, you need to ensure that the data accessed by the thread is valid until the thread is finished with it. This isn’t a new problem - even in single threaded code it’s undefined behavior to access an object after it’s been destroyed - but the use of threads provides another opportunity to encounter such lifetime issues.\nOne situation in which you can encounter such problems is when the thread function holds pointers or references to local variables and the thread hasn’t finished when the function exits.\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\nvoid do_some_work(int& i)\n{\n    std::cout &lt;&lt; \"\\nThe value of i is = \" &lt;&lt; i; //Potential access to dangling reference\n    std::cout &lt;&lt; \"\\nIncrementing the value of i\";\n    ++i;\n    std::cout &lt;&lt; \"\\nThe new value of i = \" &lt;&lt; i;\n}\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nmain() thread\";\n    int i{1};\n    std::thread my_thread(do_some_work, std::ref(i)); // Spawn a new thread\n    my_thread.detach(); // Don't wait for the thread to finish\n    std::cout &lt;&lt; \"\\nExiting main() thread\"; //New thread might still be running\n    return 0;\n}\nstdout:\nmain() thread\nExiting main() thread\nIn this case, the new thread associated with my_thread is still running when main() exits, because you explicitly decided not to wait for it by calling detach(). If the thread is still running, the statements in do_some_work(int& ) will access an already destroyed variable i. This is like normal single-threaded code - allowing a pointer or reference to a local variable to persist beyond the function exit, which is never a good idea. But, it’s easier to make this mistake with multi-threaded code, because it isn’t immediately apparent that this has happened.\n\n\nIf you need to wait for a thread to complete, you do this by calling join() on the associated std::thread instance. In the previous listing, replacing the call to my_thread.detach() by my_thread.join() would therefore be sufficient to ensure that the thead was finished before the function was exited and thus before the local variables were destroyed. In real code, the original thread would either have work to do or would have launched several threads to do useful work before waiting for them to complete.\njoin() is a simple and brute-force technique - either you wait for a thread to finish or you don’t. If you need more fine-grained control over waiting for a thread, such as to check whether a thread is finished, or wait only a certain period of time, then you have to use alternative mechanisms such as condition variables and futures.\n\n\n\nYou need to ensure that you’ve called either join() or detach() before a std::thread object is destroyed. If you’re detaching a thread, you can usually call detach() immediately after the thread has been started, so this isn’t a problem. But, if you’re intending to wait for a thread, you need to carefully pick the place in the code where you call join(). This means that the call to join() is liable to be skipped if an exception is thrown after the thread has been started but before the call to join().\nTo avoid your application being terminated when an exception is thrown, you need to make a decision about what to do in this case. In general, if you were intending to call join() in a non-exceptional case, you also need to call join() in the presence of an exception to avoid accidental lifetime problems.\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n#include &lt;stdexcept&gt;\n#include &lt;string&gt;\n\nint main()\n{\n    int i {1};\n\n    auto my_func = [&i](){\n        for(int j{0};j&lt;10000000;++j)\n        {\n            ++i;\n        }\n        std::cout &lt;&lt; \"\\nt's work package is complete.\" &lt;&lt; std::flush;\n\n    };\n\n    std::cout &lt;&lt; \"main thread\" &lt;&lt; std::flush;\n    std::thread t(my_func);\n    try\n    {\n        std::string(\"abc\").substr(10); // throws std::out_of_range\n    }\n    catch (const std::exception& e)\n    {\n        std::cout &lt;&lt; \"\\n\" &lt;&lt; e.what();\n        t.join();\n        throw e; // copy-initializes a new exception object of type std::exception\n    }\n\n    t.join();\n    return 0;\n}\nstdout:\nmain thread\ninvalid string position\nt's work package is complete.\nThe code in the listing above uses try/catch block to ensure that a thread with access to the local state is finished before the main thread exits normally, or by an exception.\nA more structured way of doing is to use the standard Resource Aquisition is Initialization(RAII) idiom and provide a wrapper class that does the join() in its destructor, as in the following listing:\n//thread_guard.cpp\n\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n#include &lt;stdexcept&gt;\n#include &lt;string&gt;\n\nclass thread_guard\n{\n    private:\n    std::thread& m_t;\n\n    public:\n    explicit thread_guard(std::thread& t) : m_t{t} {}\n    ~thread_guard()\n    {\n        if(m_t.joinable()){\n            m_t.join();\n        }\n    }\n\n    thread_guard(const thread_guard& _tg) = delete;\n    thread_guard& operator=(thread_guard& _tg) = delete;\n};\n\nint main()\n{\n    int i {1};\n\n    auto my_func = [&i](){\n        for(int j{0};j&lt;10000000;++j)\n        {\n            ++i;\n        }\n        std::cout &lt;&lt; \"\\nt's work package is complete.\" &lt;&lt; std::flush;\n\n    };\n\n    std::cout &lt;&lt; \"main thread\" &lt;&lt; std::flush;\n    std::thread t(my_func);\n    thread_guard g(t);\n    try\n    {\n        std::string(\"abc\").substr(10); // throws std::out_of_range\n    }\n    catch (const std::exception& e)\n    {\n        std::cout &lt;&lt; \"\\n\" &lt;&lt; e.what();\n        throw e; // copy-initializes a new exception object of type std::exception\n    }\n\n    return 0;\n}\nWhen the execution of the main thread reaches the end, the local objects are destroyed in the reverse order of their construction. Consequently, the thread_guard object id object g is destroyed first, and the thread is joined with, which is a blocking call in the destructor. This happens even if there is an exception in the main thread.\nThe copy constructor and copy-assignment operators are marked =delete to ensure that they’re not automatically provided by the compiler. Copying or assigning such an object would be dangerous, because it might outlive the scope of the thread it was joining. By declaring them as as deleted, any attempt to copy a thread_guard object will generate a compilation error.\nIf you don’t need to wait for a thread to finish, you can avoid this exception-safety by detaching from it. This breaks the association of the thread with the std::thread object and ensures that std::terminate() won’t be called when the std::thread object is destroyed, even though the thread is still running in the background. Note that, the std::thread destructor checks if the thread is joinable, and if this condition is true, it invokes std::terminate.\n\n\n\nCalling detach() on a std::thread object leaves the thread to run in the background with no direct means of communicating with it. It’s no longer possible to wait for that thread to complete; if a thread becomes detached, it isn’t possible to obtain a std::thread object that references it, so it can no longer be joined. Detached threads truly run in the background; ownership and control are passed over to the C++ runtime library, which ensures that the resources associated with the thread are correctly reclaimed when the thread exits.\nDetached threads are often called daemon threads after the UNIX concept of a daemon process that runs in the background without any explicit user interface. Such threads are typically long-running; they run for almost the entire lifetime of the application, performing a background task such as monitoring the filesystem, clearing unused entries out of object caches, or optimizing data-structures. At the other extreme, it may make sense to use a detached thread where there’s another mechanism for identifying when the thread has completed or where the thread is used for a fire-and-forget task.\nAs we’ve seen earlier, we detach a thread by calling detach() member function of the std::thread object. After the call completes, the std::thread object is no longer associated with the actual thread of execution and is therefore no longer joinable.\nIn order to detach the thread from a std::thread object, there must be a thread to detach: you can’t call detach() on a std::thread object with no associated thread of execution. This is exactly the same requirement for join(), and you can check it in the same way - you can call t.detach() for a std::thread object t when t.joinable() returns true.\nConsider an application such as a word-processor that can edit multiple documents at once. There are many ways to handle this, both at the UI level and internally. One way that’s increasingly common at the moment is to have multiple, independent, top-level windows, one for each document being edited. Although these windows appear to be completely independent, each with its own menus, they’re running within the same instance of the application. One way to handle this internally is to run each document-editing window in its own thread; each thread runs the same code but with different data relating to the document being edited. Opening a new document therefore requires starting a new thread. The thread handling the request isn’t going to care about waiting for that other thread to finish, because it’s working on an unrelated document, so this makes it a prime candidate for running a detached thread.\n\n\n\nPassing arguments to the callable object or function is fundamentally as simple as passing additional arguments to the std::thread constructor. But, its important to bear in mind that the arguments are copied into internal storage, where they can be accessed by the newly created thread of execution and then passed to the callable function or object as rvalues as if they were temporaries.\nTherefore, if you want to pass an object’s member function as a work-package to a thread, you pass the member-function pointer as the function, and an object pointer as the first argument:\n#include &lt;thread&gt;\n\nclass X{\n    public do_lengthy_work();\n};\n\nint main()\n{\n    X x;\n    std::thread t(&X::do_length_work, &x);\n    t.join();\n    return 0;\n}\nYou can also supply arguments to such a member-function call: the third argument to the std::thread constructor will be the first argument to the function, and so forth. std::thread is a variadic template.\nAnother interesting scenario for supplying arguments is where the arguments cannot be copied but can only be moved: the data held within one object is transferred over to another, leaving the original object empty. An example of such a type is std::unique_ptr&lt;T&gt;, which provides automatic memory management for dynamically allocated objects. Only one std::unique_ptr&lt;T&gt; instance can point to a given object at a time, and when that instance is destroyed, the pointed-to-object is also deleted. The move constructor and the move assignment operator allow the ownership of an object to be transferred around between std::unique_ptr&lt;T&gt; instances. Such a transfer leaves the source object with a nullptr. This moving of values allows objects of this type to be accepted as function parameters or returned from functions. When the source object is temporary, the move is automatic, but where the source is a named value, the transfer must be requested directly by invoking std::move().\nSeveral of the classes in the C++ standard library exhibit the same ownership semantics as std::unique_ptr&lt;T&gt;, and std::thread is one of them. Though std::thread instances don’t own a dynamic object in the same way as the std::unique_ptr&lt;T&gt; does, they do own a resource: each instance is responsible for managing a thread of execution. This ownership can be transferred between instances, because instances of std::thread are moveable, even though they aren’t copyable. This ensures, that only one object is associated with a particular thread of execution at any time while allowing programmers the option of transferring ownership between objects."
  },
  {
    "objectID": "posts/concurrent-programming-a-quick-primer-part-I/index.html#transferring-the-ownership-of-a-thread",
    "href": "posts/concurrent-programming-a-quick-primer-part-I/index.html#transferring-the-ownership-of-a-thread",
    "title": "Concurrent programming - A Primer (Part I)",
    "section": "Transferring the ownership of a thread",
    "text": "Transferring the ownership of a thread\nSuppose you want to write a function that creates a thread to run in the background, but passes ownership of the new thread back to the calling function rather than waiting for it to complete; or maybe you want to do the reverse: create a thread and pass the ownership in to some function that should wait for it to complete. In either case, you need to transfer the ownership from one place to another.\nThis is where the move support of std::thread comes in. Many resource-owing types in the C++ standard library, such as std::ifstream and std::unique_ptr&lt;T&gt; are movable but not copyable, and std::thread is one of them. This means that the ownership of a particular thread of execution can be moved between std::thread instances, as in the following example. The example shows the creation of two threads of execution and the transfer of ownership of those threads among three std::thread instances t1, t2 and t3:\n\n#include &lt;thread&gt;\n#include &lt;iostream&gt;\n\nvoid some_function(){\n    std::cout&lt;&lt; \"\\nSome function\";\n}\n\nvoid some_other_function()\n{\n    std::cout&lt;&lt;\"\\nSome other function\";\n}\n\nint main()\n{\n    std::thread t1(some_function);\n    std::thread t2 = std::move(t1);\n    t1 = std::thread(some_other_function);\n    std::thread t3 = std::move(t2);\n    //t1 = std::move(t3)    //this assignment will terminate the program\n    t1.join();\n    t3.join();\n    return 0;\n}\nCompiler Explorer\nstdout:\nSome other function\nSome function\nFirst, a new thread is started and associated with t1. Ownership is then transferred over to t2 when t2 is constructed by invoking std::move() to explicitly move ownership. At this point, t1 no longer has an associated thread of execution; the thread running some_function is now associated with t2.\nThen, a new thread is started with a temporary std::thread object. The subsequent transfer of ownership into t1 doesn’t require a call to std::move() to explicitly move ownership, because the owner is a temporary object - moving from temporaries is automatic and implicit.\nAfter all these moves, t1 is associated with the thread running some_other_function and t3 is associated with the thread running some_function.\nThe commented line transfers the ownership of the thread running some_function back to t1, where it started. But, in this case t1 already had an associated thread (which was running some_other_function) that is joinable, so std::terminate() will be called in this move assignment. This is done for consistency with the std::thread destructor.\nThe move support in std::thread means that ownership can be readily transferred out of a function.\n\nLifetime issues of threads\nLet’s look at an example of the erroneous moving of threads.\n//threadMoved.cpp\n\n#include&lt;iostream&gt;\n#include&lt;thread&gt;\n#include&lt;utility&gt;\n\nint main()\n{\n    std::thread t1([](){std::cout &lt;&lt; \"\\n\"&lt;&lt;std::this_thread::get_id();});\n    std::thread t2([](){std::cout &lt;&lt; \"\\n\"&lt;&lt;std::this_thread::get_id();});\n    t1 = std::move(t2);\n    t1.join();\n    t2.join();\n    return 0;\n}\nCompiler Explorer\nBoth threads t1 and t2 are meant to do their simple job : printing their IDs. In addition to that, thread t2 is moved to t1. In the main thread takes care of its children and waits for them. But, the result is very different from my expectations:\nstdout:\nterminate called without an active exception\nProgram terminated with signal: SIGSEGV\nWhat is going wrong? We have two issues:\n\nBy moving the thread t2, t1 gets a new work package and its move assignment operator calls the destructor to free up any resources it holds. As a result, t1’s destructor calls std::terminate, because it is still joinable.\nThread t2 has no associated callable unit. The invocation of join on a thread without a callable unit leads to the exception std::system_error.\n\nKnowing this fixing errors is straightforward:\n//threadMoveFixed.cpp\n\n#include&lt;iostream&gt;\n#include&lt;thread&gt;\n#include&lt;utility&gt;\n\nint main()\n{\n    std::thread t1([](){std::cout &lt;&lt; \"\\nThread id=\"&lt;&lt;std::this_thread::get_id();});\n    std::thread t2([](){std::cout &lt;&lt; \"\\nThread id=\"&lt;&lt;std::this_thread::get_id();});\n    t1.join();\n    t1 = std::move(t2);\n    t1.join();\n\n    std::cout &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; std::boolalpha &lt;&lt; \"t2.joinable() = \" &lt;&lt; t2.joinable();\n    return 0;\n}\nCompiler Explorer\n\n\nscoped_thread by Anthony Williams\nOne benefit of the move support of std::thread is that you can build on the thread_guard class and have it take ownership of the thread. Anthony Williams in his excellent book Concurrency in action has written a scoped_thread class. This avoids any unpleasant consequences should the thread_guard object outlive the thread it was referencing, and it also means that no one else can join or detach the thread once ownership has been transferred into the object. Because, this would primarily be aimed at ensuring that threads are completed before a scope is exited, it’s called scoped_thread. Here is the implementation:\n//scoped_thread.cpp\n\n#include &lt;thread&gt;\n#include &lt;utility&gt;\n#include &lt;stdexcept&gt;\n\nclass scoped_thread\n{\n    private:\n    std::thread t;\n    \n    public:\n    explicit scoped_thread(std::thread t_) : t(std::move(t_)) {\n        if(!t.joinable()) throw std::logic_error(\"No thread\");\n    }\n\n    ~scoped_thread(){\n        t.join();\n    }\n\n    scoped_thread(scoped_thread&) = delete;\n    scoped_thread& operator=(const scoped_thread&) = delete;\n};\n\nint main()\n{\n    {\n        scoped_thread t(std::thread([]{for(int i{0};i&lt;1000000;++i);}));\n    }\n    return 0;\n}\nCompiler Explorer\nThis example is similar to thread_guard.cpp, but the new thread is passed into the scoped_thread rather than having to create a separate named variable for it. When the main thread reaches its end, the scoped_thread object is destroyed and then joins the thread supplied to the constructor. Whereas with the thread_guard class, the destructor had to check that the thread was still joinable, you can do that in the constructor and throw an exception if it’s not."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html",
    "href": "posts/cpp-refresher-part-1/index.html",
    "title": "C++ Refresher - Part I",
    "section": "",
    "text": "A dangling pointer is a pointer variable that still contains the address to the free store memory that has already been deallocated using delete or delete[]. Dereferencing a dangling pointer makes you read from, or even worse write to memory that might already be allocated to and used by other parts of the program, resulting in all kinds of unpredictable results.\nMultiple deallocations which occur when you deallocate an already deallocated memory (and hence dangling) pointer for a second time is a recipe for disaster.\nOne basic strategy to guard yourself against dangling pointers is to always reset a pointer to nullptr, after the memory it points to is released. However, in more complex programs, different parts of the code often collaborate by accessing the same memory - an object or an array of objects - all through distinct copies of the same pointer. In such cases, our simple strategy falls short. Which part of the code is going to call delete/delete[]? And when? How do you ensure that no other part of the code is still using the same dynamically allocated memory.\n\n\n\nA dynamically allocated array, allocated using new[], is captured in a regular pointer cariable. But, so is a single allocated value that is allocated using new.\ndouble* single_df {new double {0.95}};\ndouble* array_of_dfs {new double[3] {1.00, 0.95, 0.90}};\nAfter this the compiler has no way to distinguish between the two, especially once such a pointer gets passed around different parts of the program. This means that the following two statements will compile without error.\ndelete[] single_df;\ndelete array_of_dfs;\nEvery new must be paired with a single delete; every new[] must be paired with a single delete[].\n\n\n\nA memory leak occurs when you allocate memory using new or new[] and fail to release it. If you lose the address of free store memory you have allocated by overwriting the address in the pointer you were using to access it, for instance, you have a memory leak.\nWhen it comes to scope, pointers are just like any other variable. The lifetime of a pointer extends from the point at which you define it in a block to the closing brace of the block. After that it no longer exists, the free store goes out of scope and it’s no longer possible to delete the memory.\nIt’s still relatively easy to see, where you’ve simply forgotten to use delete to free memory when use of the memory ceases at a point close to where you allocated it, but you’d be surprised how often programmers make mistakes like this, especially if, for instance, return statements creep in between allocation and deallocation of your variable. And naturally, memory leaks are even more difficult to spot in complex programs, where memory may be allocated in part of the the program and should be released in a completely separate part.\nOne basic strategy for avoiding memory leaks is to immediately add delete operation at an appropriate place each time you use the new operator. But this strategy by no means is fail-safe. Even C++ programmers are fallible creatures.\n\n\n\nMemory fragmentation can arise in programs that frequently dynamically allocate and release memory blocks. Each time, the new operator is used, it allocates a contiguous block of bytes. If you create and destroy many memory blocks of different sizes, it’s possible to arrive at a situation in which the allocated memory is interspersed with small blocks of free memory, none of which is large enough to accomodate a new memory allocation request by your program. The aggregate of the free memory can be quite large, but if all the individual blocks are small (smaller than a current allocation request), the allocation request will fail.\n\n\n\nNever use the operators new, new[], delete and delete[] directly in day-to-day coding. These operators have no place in modern C++ code. Always use either the std::vector&lt;T&gt; container to replace dynamic arrays or a smart pointer to dynamically allocate individual objects and manage their lifetimes."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#hazards-of-dynamic-memory-allocation.",
    "href": "posts/cpp-refresher-part-1/index.html#hazards-of-dynamic-memory-allocation.",
    "title": "C++ Refresher - Part I",
    "section": "",
    "text": "A dangling pointer is a pointer variable that still contains the address to the free store memory that has already been deallocated using delete or delete[]. Dereferencing a dangling pointer makes you read from, or even worse write to memory that might already be allocated to and used by other parts of the program, resulting in all kinds of unpredictable results.\nMultiple deallocations which occur when you deallocate an already deallocated memory (and hence dangling) pointer for a second time is a recipe for disaster.\nOne basic strategy to guard yourself against dangling pointers is to always reset a pointer to nullptr, after the memory it points to is released. However, in more complex programs, different parts of the code often collaborate by accessing the same memory - an object or an array of objects - all through distinct copies of the same pointer. In such cases, our simple strategy falls short. Which part of the code is going to call delete/delete[]? And when? How do you ensure that no other part of the code is still using the same dynamically allocated memory.\n\n\n\nA dynamically allocated array, allocated using new[], is captured in a regular pointer cariable. But, so is a single allocated value that is allocated using new.\ndouble* single_df {new double {0.95}};\ndouble* array_of_dfs {new double[3] {1.00, 0.95, 0.90}};\nAfter this the compiler has no way to distinguish between the two, especially once such a pointer gets passed around different parts of the program. This means that the following two statements will compile without error.\ndelete[] single_df;\ndelete array_of_dfs;\nEvery new must be paired with a single delete; every new[] must be paired with a single delete[].\n\n\n\nA memory leak occurs when you allocate memory using new or new[] and fail to release it. If you lose the address of free store memory you have allocated by overwriting the address in the pointer you were using to access it, for instance, you have a memory leak.\nWhen it comes to scope, pointers are just like any other variable. The lifetime of a pointer extends from the point at which you define it in a block to the closing brace of the block. After that it no longer exists, the free store goes out of scope and it’s no longer possible to delete the memory.\nIt’s still relatively easy to see, where you’ve simply forgotten to use delete to free memory when use of the memory ceases at a point close to where you allocated it, but you’d be surprised how often programmers make mistakes like this, especially if, for instance, return statements creep in between allocation and deallocation of your variable. And naturally, memory leaks are even more difficult to spot in complex programs, where memory may be allocated in part of the the program and should be released in a completely separate part.\nOne basic strategy for avoiding memory leaks is to immediately add delete operation at an appropriate place each time you use the new operator. But this strategy by no means is fail-safe. Even C++ programmers are fallible creatures.\n\n\n\nMemory fragmentation can arise in programs that frequently dynamically allocate and release memory blocks. Each time, the new operator is used, it allocates a contiguous block of bytes. If you create and destroy many memory blocks of different sizes, it’s possible to arrive at a situation in which the allocated memory is interspersed with small blocks of free memory, none of which is large enough to accomodate a new memory allocation request by your program. The aggregate of the free memory can be quite large, but if all the individual blocks are small (smaller than a current allocation request), the allocation request will fail.\n\n\n\nNever use the operators new, new[], delete and delete[] directly in day-to-day coding. These operators have no place in modern C++ code. Always use either the std::vector&lt;T&gt; container to replace dynamic arrays or a smart pointer to dynamically allocate individual objects and manage their lifetimes."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#raw-pointers-and-smart-pointers.",
    "href": "posts/cpp-refresher-part-1/index.html#raw-pointers-and-smart-pointers.",
    "title": "C++ Refresher - Part I",
    "section": "Raw pointers and Smart Pointers.",
    "text": "Raw pointers and Smart Pointers.\nPointer types int*, double* are referred to as raw pointers because variables of these types contain nothing more than an address. A raw pointer can store the address of an automatic variable or a memory-block allocated in the free-store.\nA smart pointer is an object that mimics a raw pointer in that, it contains an address, and you can use it in the same way in many respects. Smart pointers are normally used only to store the address of memory allocated in the free store. A smart pointer does much more than a raw pointer, though. The most notable feature of a smart pointer, is that we don’t have to worry about using the delete or delete[] operator to free memory. It will be released automatically, when it is no longer needed. This means that dangling pointers and multiple deallocations, allocation/deallocation mismatches and memory leaks will no longer be possible.\n\nA std::unique_ptr&lt;T&gt; object behaves as a pointer to type T and is unique in the sense that there can be only one single unique_ptr&lt;&gt; object containing the same address. In other words, there can never be two or more unique_ptr&lt;T&gt; objects pointing to the same memory address at the same time. A unique_ptr&lt;&gt; object is said to own the object it points to exclusively. The uniqueness is enforced by the fact, that a compiler will never allow you to copy a unique_ptr&lt;&gt;.\nA std::shared_ptr&lt;T&gt; object also behaves as a pointer to type T, but in contrast with unique_ptr&lt;T&gt; there can be any number of shared_ptr&lt;&gt; objects that allow shared ownership of an object in the free-store. At any given moment, the number of shared_ptr&lt;&gt; objects that contain a given address in time is known by the runtime. This is called reference counting. The reference count for a shared_ptr&lt;&gt; containing a given free store address is incremented each time a new shared_ptr object is creating containing that address, and its decremented when a shared_ptr containing the address is destroyed or assigned to point to a different address. When there are no shared_ptr objects containing a given address, the reference count will have dropped to zero, and the memory for the object at that address is released automatically. All shared_ptr&lt;&gt; objects that point to the same address have access to the the count of how many there are.\nA weak_ptr&lt;T&gt; is linked to a shared_ptr&lt;T&gt; and contains the same address. Creating a weak_ptr&lt;&gt; does not increment the reference count associated with the linked shared_ptr&lt;&gt; object, though, so a weak_ptr&lt;&gt; does not prevent the object pointed to from being destroyed. Its memory will still be released when the last shared_ptr&lt;&gt; referencing it is destroyed or reassigned to point to a different address, even when associated weak_ptr&lt;&gt; objects still exist. If this happens, the weak_ptr&lt;&gt; will nevertheless not contain a dangling pointer, atleast not one that you could inadvertently access. The reason is that you cannot access the address encapsulated by a weak_ptr&lt;T&gt; directly. The compiler forces you to first create a shared_ptr&lt;T&gt; object out of it that refers to the same address. If the memory address for the weak_ptr&lt;&gt; is still valid, forcing you to create a shared_ptr&lt;&gt; first ensures that the reference count is again incremented and that the pointer can be used safely again. If the memory is released already, however, this operation will result in a shared_ptr&lt;T&gt; containing a nullptr.\n\nOne use for having weak_ptr&lt;&gt; objects is to avoid so called reference cycles with shared_ptr&lt;&gt; objects. Conceptually, a reference cycle is where a shared_ptr&lt;Y&gt; inside the object x points to some other object y that contains a shared_ptr&lt;X&gt;, which points back to x. With this situation, neither x nor y can be destroyed. In practice, this may occur in many ways. weak_ptr allows you to break such cycles. Another use of weak pointers is in the implementation of object caches.\nIn the below code snippet, the destructors ~A() and ~B() are not invoked even when the objects shrd_a and shrd_b go out of scope.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nusing namespace std;\n\nclass A;\nclass B;\n\nclass A{\n    public:\n    shared_ptr&lt;B&gt; m_b;\n    A() {cout &lt;&lt; \"\\nA()\";}\n    ~A() {cout &lt;&lt; \"\\n~A()\";}\n};\n\nclass B{\n    public:\n    shared_ptr&lt;A&gt; m_a;\n    B () {cout &lt;&lt; \"\\nB()\";}\n    ~B() {cout &lt;&lt; \"\\n~B()\";}\n};\n\nint main()\n{\n    {\n        shared_ptr&lt;A&gt; shrd_a {make_shared&lt;A&gt;()}; //A's ref count = 1\n        shared_ptr&lt;B&gt; shrd_b {make_shared&lt;B&gt;()}; //B's ref count = 1\n    \n        shrd_a-&gt;m_b = shrd_b; //B's ref count = 2\n        shrd_b-&gt;m_a = shrd_a; //A's ref count = 2\n    }\n    //shrd_a and shrd_b go out of scope and are destroyed\n    // A's ref count = 1\n    // B's ref count = 1\n    // ((Memory of A, B is deallocated only when ref count drops to 0))\n    return 0;\n}\nA()\nB()\nTo solve it, the programmer needs to be aware of the ownership relationship among the objects, or needs to invent an ownership relationship, if no such ownership exists. The above C++ code can be changed so that A owns B:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nusing namespace std;\n\nclass A;\nclass B;\n\nclass A{\n    public:\n    shared_ptr&lt;B&gt; m_b;\n    A() {cout &lt;&lt; \"\\nA()\";}\n    ~A() {cout &lt;&lt; \"\\n~A()\";}\n};\n\nclass B{\n    public:\n    weak_ptr&lt;A&gt; m_a;\n    B () {cout &lt;&lt; \"\\nB()\";}\n    ~B() {cout &lt;&lt; \"\\n~B()\";}\n};\n\nint main()\n{\n    {\n        shared_ptr&lt;A&gt; shrd_a {make_shared&lt;A&gt;()}; //A's ref count = 1\n        shared_ptr&lt;B&gt; shrd_b {make_shared&lt;B&gt;()}; //B's ref count = 1\n    \n        shrd_a-&gt;m_b = shrd_b; //B's ref count = 2\n        shrd_b-&gt;m_a = shrd_a; //A's ref count = 1\n    }\n    //shrd_a and shrd_b go out of scope and are destroyed\n    // A's ref count = 0\n    // B's ref count = 1\n    // A is destroyed\n    // B's ref count = 0\n    // B is destroyed\n    //\n    return 0;\n}\nA()\nB()\n~A()\n~B()\n\nUsing unique_ptr&lt;T&gt; and shared_ptr&lt;T&gt; pointers.\nA unique_ptr&lt;T&gt; object stores an address uniquely, so the value to which it points is owned exlusively by the unique_ptr&lt;T&gt; smart pointer. When the unique_ptr&lt;T&gt; is destroyed, so is the value to which it points. Like all smart pointers, a unique_ptr&lt;&gt; is most useful when working with dynamically allocated objects. Objects then should not be shared by multiple parts of the program, or where the lifetime of the dynamic pobject is naturally tied to a single other object in your program.\nOne common use for a unique_ptr&lt;&gt; is to hold something called a polymorphic pointer, which in essence is a pointer to a dynamically allocated object that can be of any number of related class types.\nTo create and initialize a double variable on the free-store, we write:\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main()\n{\n    std::unique_ptr&lt;double&gt; pDiscountFactor {std::make_unique&lt;double&gt;(0.95)};\n    \n    std::cout &lt;&lt; \"Discount Factor = \" &lt;&lt; *pDiscountFactor;\n    \n    return 0;\n}\nDiscount Factor = 0.95\nThe memory allocated on the free store holding 0.95 is released once pDiscountFactor goes out of scope and is destroyed after the return statement.\nThe below code snippet shows how smart pointers work.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nclass X{\n    public:\n        X()\n        {\n          std::cout &lt;&lt; \"\\nX created\";\n        }\n        \n        ~X()\n        {\n          std::cout &lt;&lt; \"\\nX destroyed\";\n        }\n};\n\nclass Y{\n    \n    public:\n        Y()\n        {\n          std::cout &lt;&lt; \"\\nY created\";\n        }\n        \n        ~Y()\n        {\n          std::cout &lt;&lt; \"\\nY destroyed\";\n        } \n};\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nInside main\";\n    std::shared_ptr&lt;Y&gt; sPtrY1 {std::make_shared&lt;Y&gt;()};\n    \n    \n    {\n        //inner scope\n        std::cout &lt;&lt; \"\\nInside inner\";\n        \n        std::unique_ptr&lt;X&gt; uPtrX1 {std::make_unique&lt;X&gt;()};\n        std::shared_ptr&lt;Y&gt; sPtrY2 {sPtrY1};\n        \n        // copy assignment and copy construction is not allowed on unique_ptr objects\n        //std::unique_ptr&lt;X&gt; uPtrX2 = uPtrX1;\n        \n        std::cout &lt;&lt; \"\\nExiting inner\";\n    }\n    \n    std::cout &lt;&lt; \"\\nExiting main\";\n    return 0;\n}\nInside main\nY created\nInside inner\nX created\nExiting inner\nX destroyed\nExiting main\nY destroyed"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#references.",
    "href": "posts/cpp-refresher-part-1/index.html#references.",
    "title": "C++ Refresher - Part I",
    "section": "References.",
    "text": "References.\nA reference is a name that you can use as an alias for another variable. Unlike a pointer, you cannot declare a reference and not initialize it. Because a reference is an alias, the variable which it is an alias must be provided when the reference is initialized. Also, a reference cannot be modified to be an alias for something else.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\n\nvoid swap(int& a, int& b)\n{\n    int temp {a};\n    a = b;\n    b = temp;\n}\n\nint main()\n{\n    int x {10}; \n    int y {15};\n    \n    std::cout &lt;&lt; \"\\n Before swap:\";\n    std::cout &lt;&lt; \"\\n x = \" &lt;&lt; x ;\n    std::cout &lt;&lt; \"\\n y = \" &lt;&lt; y;\n    \n    swap(x,y);\n    \n    std::cout &lt;&lt; \"\\n After swap:\";\n    std::cout &lt;&lt; \"\\n x = \" &lt;&lt; x ;\n    std::cout &lt;&lt; \"\\n y = \" &lt;&lt; y;\n    return 0;\n}\n Before swap:\n x = 10\n y = 15\n After swap:\n x = 15\n y = 10\nNever return a pointer or reference to an automatic stack-allocated local variable from within a function. Automatic variables are destroyed and the stack is popped, once the control goes outside the scope in which they are declared."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#function-templates.",
    "href": "posts/cpp-refresher-part-1/index.html#function-templates.",
    "title": "C++ Refresher - Part I",
    "section": "Function Templates.",
    "text": "Function Templates.\nA function template itself is not a definition of a function; it is a blueprint or a recipe for definining an entire family of functions. A function template is a parametric function definition, where a particular function instance is created by one or more parameter values. The compiler uses a function template to generate a function definition when necessary. If it is never necessary, no code results from the template. A function definition that is generated from a template is an instance or instantiation of the template.\nThe parameters of a function template are usually data-types, where an instance can be generated for a parameter value of type int, for example, and another with parameter valuer of type string. But parameters are not necessarily types. They can be other things such as a dimension, for example.\ntemplate &lt;class T&gt;\nT larger(T a, T b)\n{\n    return a &gt; b ? a : b;\n}\nThe compiler creates instances of the template from any statement that uses the larger() function. Here’s an example:\nint main()\n{\n    std::cout &lt;&lt; \"\\nLarger of 1.50 and 2.50 is : \"  &lt;&lt; larger(1.5,2.5);\n    return 0;\n}\nLarger of 1.50 and 2.50 is : 2.5\nYou just use the function in the normal way. You don’t need to specify a value for the template parameter T. The compiler deduces the type that is to replace T from the arguments in the larger function call. This mechanism is referred to as template argument deduction. The arguments to larger() are literals of type double, so this call causes the compiler to search for an existing definition of larger() with double parameters. If it doesn’t find one, the compiler creates this version of larger() from the template by susbstituting double for T in the template definition.\nThe resulting function accepts arguments of type double and returs a double value.\nThe compiler makes sure to generate each template instance only once. If a subsequent function call requires the same instance, then it calls the instance that exists.\n\nTemplate type parameters.\nThe name of the template type parameter can be used anywhere in the template’s function signature, return type and body. It is a placeholder for a type and can thus be put in any context you would normally put a concrete type.\ntemplate &lt;class T&gt;\nconst T& larger(const T& a,const T& b)\n{\n    return a &gt; b ? a : b;\n}\n\n\nFunction Template overloading.\nTemplated functions can be overloaded.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\ntemplate &lt;typename T&gt;\nconst T& largest(const T& a,const T& b)\n{\n    return a &gt; b ? a : b;\n}\n\ntemplate &lt;typename T&gt;\nconst T largest(const std::vector&lt;T&gt;& data)\n{\n    T max {};\n    for(auto v:data)\n    {\n        if (v &gt;= max)\n            max = v;\n    }\n    return max;\n}\n\nint main()\n{\n    std::cout &lt;&lt; \"\\nLarger of 1.50 and 2.50 is : \"  &lt;&lt; largest(1.5,2.5);\n    std::vector&lt;int&gt; data {\n        2, 5, 8, 4, 7, 3\n    };\n    std::cout &lt;&lt; \"\\nLargest of [2,5,8,4,7,3] is : \" &lt;&lt; largest(data);\n    return 0;\n}\nLarger of 1.50 and 2.50 is : 2.5\nLargest of [2,5,8,4,7,3] is : 8"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#classes-and-object-oriented-programming.",
    "href": "posts/cpp-refresher-part-1/index.html#classes-and-object-oriented-programming.",
    "title": "C++ Refresher - Part I",
    "section": "Classes and Object Oriented Programming.",
    "text": "Classes and Object Oriented Programming.\nAn interesting exercise to write a Matrix&lt;T&gt; class.\n// Matrix.h\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;initializer_list&gt;\n#include &lt;stdexcept&gt;\n\ntemplate &lt;typename T = double&gt;\nclass Matrix {\npublic:\n    //Default constructor\n    Matrix() : Matrix(3, 3) {}\n\n    //Parameterized constructor with number of rows, cols as \n    // as arguments.\n    Matrix(std::size_t m, std::size_t n) : m_rows(m), m_cols(n)\n    {\n        m_data.resize(m_rows * m_cols, 0);\n    }\n\n    //Parameterized constructor with matrix elements provided \n    // in brace initializer lists.\n    Matrix(std::initializer_list&lt;std::initializer_list&lt;T&gt;&gt; m) {\n        int i{}, j{};\n        for (auto row : m)\n        {\n            for (auto el : row)\n            {\n                m_data.push_back(el);\n                if (i == 0)\n                    ++j;\n            }\n            ++i;\n        }\n\n        m_rows = i;\n        m_cols = j;\n    }\n\n\n    //Copy constructor\n    Matrix(const Matrix& A) : m_rows{ A.m_rows }, m_cols{ A.m_cols }, m_data{ A.m_data } {}\n\n    std::size_t rows() const\n    {\n        return m_rows;\n    }\n\n    std::size_t cols() const\n    {\n        return m_cols;\n    }\n\n    T& at(int i, int j)\n    {\n        return m_data[i * m_cols + j];\n    }\n\n    const T& at(int i, int j) const\n    {\n        return m_data[i * m_cols + j];\n    }\n\n    T& operator()(int i, int j)\n    {\n        if (i &lt; 0)\n            throw std::invalid_argument(\"The row index must be non-negative!\");\n\n        if (j &lt; 0)\n            throw std::invalid_argument(\"The column index must be non-negative!\");\n\n        if (i &gt;= m_rows)\n            throw std::invalid_argument(\"The row index must be less than \" + m_rows);\n\n        if (j &gt;= m_cols)\n            throw std::invalid_argument(\"The col index must be less than \" + m_cols);\n\n        return at(i, j);\n    }\n\n    const T operator()(int i, int j) const\n    {\n        if (i &lt; 0)\n            throw std::invalid_argument(\"The row index must be non-negative!\");\n\n        if (j &lt; 0)\n            throw std::invalid_argument(\"The column index must be non-negative!\");\n\n        if (i &gt;= m_rows)\n            throw std::invalid_argument(\"The row index must be less than \" + m_rows);\n\n        if (j &gt;= m_cols)\n            throw std::invalid_argument(\"The col index must be less than \" + m_cols);\n\n        return at(i, j);\n    }\n\n    const Matrix operator+(const Matrix& mat)\n    {\n        if (mat.rows() != rows())\n            throw std::runtime_error(\"In A + B, matrices A, B should have the same number of rows!\");\n\n        if (mat.cols() != cols())\n            throw std::runtime_error(\"In A + B, matrices A, B should have the same number of cols!\");\n\n        Matrix result(rows(), cols());\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int j{}; j &lt; cols(); ++j)\n            {\n                result(i, j) = at(i, j) + mat(i, j);\n            }\n        }\n        return result;\n    }\n\n    const Matrix operator-(const Matrix& mat)\n    {\n        if (mat.rows() != rows())\n            throw std::runtime_error(\"In A - B, matrices A, B should have the same number of rows!\");\n\n        if (mat.cols() != cols())\n            throw std::runtime_error(\"In A - B, matrices A, B should have the same number of cols!\");\n\n        Matrix result(rows(), cols());\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int j{}; j &lt; cols(); ++j)\n            {\n                result(i, j) = at(i, j) - mat(i, j);\n            }\n        }\n        return result;\n    }\n\n    Matrix& operator=(const Matrix& mat)\n    {\n        m_data = mat.m_data;\n        m_rows = mat.rows();\n        m_cols = mat.cols();\n\n        return *this;\n    }\n\n    const Matrix operator*(const Matrix& mat)\n    {\n        if (cols() != mat.rows())\n            throw std::runtime_error(\"In A * B, cols of A must equal rows of B!\");\n\n        Matrix result{ rows(), mat.cols() };\n\n        for (int i{}; i &lt; rows(); ++i)\n        {\n            for (int k{}; k &lt; cols(); ++k)\n            {\n                for (int j{}; j &lt; mat.cols(); ++j)\n                {\n                    result(i, j) += at(i, k) * mat(k, j);\n                }\n            }\n        }\n\n        return result;\n    }\n\n\nprivate:\n    std::vector&lt;T&gt; m_data{};\n    int m_rows;\n    int m_cols;\n};\n//Matrix.cpp\n\n#include &lt;iostream&gt;\n#include \"Matrix.h\"\n\nint main()\n{\n    Matrix&lt;double&gt; A{\n        {1, 0},\n        {0, 1}\n    };\n\n    Matrix&lt;double&gt; B{\n        {1, 0},\n        {0, 1}\n    };\n\n    Matrix&lt;double&gt; result = A + B;\n\n    std::cout &lt;&lt; result(0, 0) &lt;&lt; \"\\t\" &lt;&lt; result(0, 1) &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; result(1, 0) &lt;&lt; \"\\t\" &lt;&lt; result(1, 1);\n\n    return 0;\n}\n\nAccess specifiers and class hierarchies.\n\nThe private members of the base class are inaccessible to the derived class.\nWhen the base class specifier is public, the access status of the inherited members remains unchanged. Thus, inherited public members are public, and inherited protected members are protected in a derived class.\nWhen the base class specifier is protected, both public and protected members of the base class are inherited as protected members in the child class.\nWhen the base class specifier is private, inherited public and protected members become private to the derived class, so that they’re accessible by member functions of the the derived class, but they cannot be accessed if they’re inherited in another derived class.\n\n\n\nConstructors and Destructors in derived classes.\nEvery constructor of the derived class always starts by invoking a constructor of the base class. And that base class constructor then invokes the constructor of its base class, and so on.\nRemark. You cannot initialize the member variables of a base class in the initialization list for the derived class constructor. Not even if those members are public or protected.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;string&gt;\n\nclass A{\n    public:\n    A(){\n        std::cout &lt;&lt; \"\\nInside A's constructor\";\n    }\n    ~A()\n    {\n        std::cout &lt;&lt; \"\\nInside A's destructor\";\n    }\n};\n\nclass B : public A\n{\n    public:\n    B()\n    {\n        std::cout &lt;&lt; \"\\nInside B's constructor\";\n    }\n    \n    ~B()\n    {\n        std::cout &lt;&lt; \"\\nInside B's destructor\";\n    }\n};\n\nint main()\n{\n    B b;\n    \n    return 0;\n}\nInside A's constructor\nInside B's constructor\nInside B's destructor\nInside A's destructor\nSuppose you have a base class Parent, two child classes Child_1 and Child_2 that inherit from Parent and a Grandchild class that inherits from Child_1 and Child_2. This is the diamond problem, named after the shape of such inheritance diagrams. The Grandchild inherits two copies of Parent : one through Child_1 and another through Child_2.\nTo prevent the duplication of the base class, we identify to the compiler that the base class should appear only once within the derived class. We do this by specifying the class as a virtual base class using the virtual keword. The Child_1 and Child_2 classes would be defined like this:\nclass Child_1 : public virtual Parent\n{\n    //...\n};\n\nclass Child_2 : public virtual Parent\n{\n    //...\n};\n\n\nPolymorphism.\nEvery derived class object is a base class object. So, you can use a base class pointer/reference to store the address of a derived class object. It is easy to implement dynamic dispatch through virtual methods.\nThe below code snippet is instructive in understanding run-time polymorphism.\n#include &lt;memory&gt;\n#include &lt;iostream&gt;\n\nusing namespace std;\n\nclass A {\npublic:\n    void foo() {\n        std::cout &lt;&lt; \"\\nGreetings from a!\";\n    }\n};\n\nclass B :public A {\npublic:\n    virtual void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from b!\";\n    }\n};\n\n\nclass C : public B {\nprivate:\n    virtual void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from c!\";\n    }\n};\n\nclass D : public C {\npublic:\n    void foo()\n    {\n        std::cout &lt;&lt; \"\\nGreetings from d!\";\n    }\n};\n\nint main()\n{\n    std::shared_ptr&lt;A&gt; a_ptr = std::make_shared&lt;D&gt;();\n    a_ptr -&gt;foo();\n    \n    std::shared_ptr&lt;B&gt; b_ptr = std::make_shared&lt;D&gt;();\n    b_ptr -&gt;foo();\n    \n    std::shared_ptr&lt;C&gt; c_ptr = std::make_shared&lt;D&gt;();\n    //c_ptr -&gt;foo();  //will not compile, foo() is a private member is not inherited by D\n    \n    std::shared_ptr&lt;D&gt; d_ptr = std::make_shared&lt;D&gt;();\n    d_ptr -&gt;foo();\n}\nGreetings from a!\nGreetings from d!\nGreetings from d!\nWhen you specify a function as virtual in a base class, you indicate to the compiler that you want dynamic binding for function calls in any class that’s derived from this base class. A function that you specify as virtual in the base class will be virtual in all classes that directly or indirectly derive from the base class. This is the case, whether or not you specify the function as virtual in the derived class.\nThe call to a virtual function using an object is always resolved statically. You only get dybamic resolution of calls to virtual functions through a pointer or a reference. Consider the below code snippet:\n    D d{};\n    \n    A& aRef = d;\n    B& bRef = d;\n    A a; B b;\n    \n    aRef.foo();\n    bRef.foo();\n    \n    a.foo();\n    b.foo();\nGreetings from a!\nGreetings from d!\nGreetings from a!\nGreetings from b!\n\nRequirements for a virtual function.\nFor a function to be virtual, its definition in a derived class must have the same signature as it has in the base class. If the base class function is const, for instance, then the derive class function must also be const. Generally, the return type of a virtual function in a derived class must be the same as in the base class as well, but there’s an exception when the return type in the base class is a pointer or a reference to a class type. In this case, the derived class version of a virtual function may return a pointer or a reference to a more specialized type than that of the base. This is called covariance.\nAnother restriction is that a virtual function can’t be a template function.\nIn standard object-oriented programming terms, a function in a derived class that redefines a function of the base class is said to override this function. A function with the same name as a virtual function in a base class only overrides that function if the remainder of their signatures match exactly as well; if they do not, the function in the derived class is a new function that hides the one in the base class. This means that if you try to use different parameters for a virtual function in a derived class or use different const specifiers, then the virtual function mechanism won’t work. The function in the derived class then defines, a new different function - and this new function will therefore operate with static binding that is established and fixed at compile time.\n\n\noverride specifier.\nThe override specification guarantees that you don’t make mistakes in function overrides and these exactly match the virtual function signatures in base class.\n\n\nfinal qualifier.\nSometimes, we may want to prevent a member function from being overriden in a derived class. We can do this by specifying that a function is final.\n\n\n\nVirtual destructors.\nAlong with the other function, the destructor methods of classes should also be resolved dynamically. That is, if a Base* pointer points to Derived object, the Derived class destructor method should be called first. (Object creation is top-down, destruction is bottom-up in an inheritance hierarchy). So, it’s always prudent to declare destructor methods as virtual.\n\n\nCalling the base class version of a virtual function.\nIt’s easy to call the derived class version of a virtual function through a pointer or reference to a derived class object - the call is made dynamically. However, what do you do when you actually want to call the base class function for a derived class object?\nConsider the Box and ToughPack classes.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;string&gt;\n\nclass Box{\n    public:\n    \n    Box() : Box(1.0) {}\n    Box(double side) : Box(side, side, side) {}\n    Box(double length, double width, double height) : m_length(length), m_width(width), m_height(height) {}\n  \n    double virtual volume()\n    {\n        return m_length * m_width * m_height;\n    }\n    \n    ~Box()\n    {\n        std::cout &lt;&lt; \"\\nBox dtor\";\n    }\n    protected:\n    double m_length;\n    double m_width;\n    double m_height;\n};\n\nclass ToughPack : public Box\n{\n    public:\n    ToughPack() : Box() {}\n    ToughPack(double side) : Box(side) {}\n    ToughPack(double x, double y, double z) : Box(x,y,z) {}\n    \n    //Function to calculate volume allowing for 15% of packing\n    double volume() override\n    {\n        return 0.85 * m_length * m_width * m_height ;\n    }\n    \n    ~ToughPack()\n    {\n        std::cout &lt;&lt; \"\\nToughPack dtor\";\n    }\n};\nIn ToughPack’s volume() method, the m_length*m_width*m_height part of the return statement is exactly the formula used to compute the volume() inside the base class Box. In this case, the amount of code we had to retype was limited, but this won’t always be the case. It would therefore be much better if you could simply call the base class version of this function isntead.\nA plausible first attempt to do so would be:\ndouble volume() const override\n{\n    return 0.85 * volume(); // Infinite recursion!\n}\nHowever, this would call volume() override itself, which would then be calling itself again, which would then be calling itself again! This leads to infinite recursion and a crash.\nCalling the base class version from within a function override like this is common. The solution is to explicitly ask the compiler to call the base class version of the function.\ndouble volume() const override\n{\n    return 0.85 * Box::volume(); \n}\n\n\nWhen my base class’s constructor calls a virtual function on its this object, why doesn’t my derived class’s override of that virtual function get invoked?\nWhat happens when we call virtual functions from inside constructors and destructors? Calling a polymorphic function from inside a constructor/desctructor is a recipe for disaster in most cases. It should be avoided whenver possible.\nIn a constructor, the virtual call mechanism is disabled, because overriding from derived classes hasn’t happened yet. Objects are constructed from Base up, “Base before derived”.\nSince Base object must be constructed before Derived, the call to f() always resolves statically to Base::f() from inside the constructor.\n#include&lt;string&gt;\n#include&lt;iostream&gt;\nusing namespace std;\nclass B {\npublic:\n    B(const string& ss) { cout &lt;&lt; \"B constructor\\n\"; f(ss); }\n    virtual void f(const string&) { cout &lt;&lt; \"B::f\\n\";}\n};\nclass D : public B {\npublic:\n    D(const string & ss) :B(ss) { cout &lt;&lt; \"D constructor\\n\";}\n    void f(const string& ss) { cout &lt;&lt; \"D::f\\n\"; s = ss; }\nprivate:\n    string s;\n};\nint main()\n{\n    D d(\"Hello\");\n}\nB constructor\nB::f\nD constructor\n\n\nHow can I set up my class so it won’t be inherited from?\nJust declare the class as final."
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#pure-virtual-functions.",
    "href": "posts/cpp-refresher-part-1/index.html#pure-virtual-functions.",
    "title": "C++ Refresher - Part I",
    "section": "Pure virtual functions.",
    "text": "Pure virtual functions.\nThere are situations where we require a base class with a virtual function that’s redefined in each of the derived classes, but hwere there’s no meaningful definition for the function in the base class. For example, you might define a base class Shape, from which you derive classes definining specific shapes, such as Circle, Ellipse, Rectangle, Hexagon and so on. The Shape class could include a virtual function area(), that you’d call for the derived class object to compute the area of a particular shape. The Shape class itself, though, cannot possibly provide a meaningful implementation of the area() function, one that caters, for instance, to both Circles and Rectangles. This is a job for a pure virtual function.\nThe purpose of a pure virtual function is to enable the derived class versions of the function to be called polymorphically. To declare a pure virtual function rather than an ordinary virtual function that has a definition, you use the same syntax but add =0 to it’s declaration within the class.\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;vector&gt;\n\nclass Shape {\npublic:\n    Shape() = default;\n    virtual double area() = 0; //pure virtual function\n\n};\n\nclass Rectangle : public Shape {\npublic:\n    Rectangle(double l, double w) : m_length(l), m_width(w) {}\n\n    double area() override {\n        return m_length * m_width;\n    }\nprivate:\n    double m_length;\n    double m_width;\n};\n\nclass Circle : public Shape {\n\npublic:\n    Circle(double r) : m_radius(r) {}\n\n    double area() override {\n        return 3.14159 * m_radius * m_radius;\n    }\n\nprivate:\n    double m_radius;\n};\n\nint main()\n{\n    //Let's create a container to hold different kinds of shapes\n    std::vector&lt;std::unique_ptr&lt;Shape&gt;&gt; shapes{};\n\n    shapes.push_back(std::make_unique&lt;Rectangle&gt;(5.0, 5.0));\n    shapes.push_back(std::make_unique&lt;Circle&gt;(3.0));\n    shapes.push_back(std::make_unique&lt;Rectangle&gt;(10.0, 12.0));\n    shapes.push_back(std::make_unique&lt;Circle&gt;(5.0));\n\n    for (int i{}; i &lt; shapes.size(); ++i)\n    {\n        std::cout &lt;&lt; \"\\nArea = \" &lt;&lt; shapes[i]-&gt;area();\n    }\n\n    return 0;\n}\nArea = 25\nArea = 28.2743\nArea = 120\nArea = 78.5397"
  },
  {
    "objectID": "posts/cpp-refresher-part-1/index.html#abstract-classes.",
    "href": "posts/cpp-refresher-part-1/index.html#abstract-classes.",
    "title": "C++ Refresher - Part I",
    "section": "Abstract Classes.",
    "text": "Abstract Classes.\nAn abstract class purely exists for the purpose of deriving classes from it and cannot be instantiated.\nAny class that contains atleast one pure virtual function is an abstract class. Because an abstract class cannot be instantiated, you cannot pass it by value to a function, a parameter of type Shape will not compile. Similarly, you cannot return a Shape object from a functiojn. However, pointers or references to an abstract class can be used as parameter or return types, so types such as Shape* std::shared_ptr&lt;Shape&gt; and Shape& are fine in these settings.\nAny class that inherits from Shape is obligated to provide an implementation of the area() method. If it doesn’t, it too is an abstract class. More specifically, if any pure virtual function of an abstract base class isn’t in a derived class, then the pure virtual function will be inherited as such, and the derived class becomes an abstract class.\nThus, abstract base classes (ABCs) are often used as interfaces."
  },
  {
    "objectID": "posts/diagonalization/index.html",
    "href": "posts/diagonalization/index.html",
    "title": "Eigenthingies and Diagonalizability",
    "section": "",
    "text": "Each square matrix possesses a collection of one or more complex scalars, called eigenvalues and associated vectors called eigenvectors. A matrix is a concrete realization of a linear transformation on a vector space. The eigenvectors indicate the directions of pure stretch and the eigenvalues the extent of stretching."
  },
  {
    "objectID": "posts/diagonalization/index.html#eigenvalues-and-eigenvectors",
    "href": "posts/diagonalization/index.html#eigenvalues-and-eigenvectors",
    "title": "Eigenthingies and Diagonalizability",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\nDefinition 1 (Eigenvalue and Eigenvector) Let \\(A\\) be an \\(n \\times n\\) matrix. A scalar \\(\\lambda\\) is called an eigenvalue of \\(A\\) if there exists a non-zero vector \\(\\mathbf{v} \\neq \\mathbf{0}\\) such that\n\\[\nA\\mathbf{v} = \\lambda \\mathbf{v}\n\\tag{1}\\]\n\nIn geometric terms, the matrix \\(A\\) has the effect of stretching the eigenvector \\(\\mathbf{v}\\) by an amount specified by the eigenvalue \\(\\lambda\\).\nThe eigenvalue equation (Equation 1) is a system of linear equations is a system of linear equations for the entries of the eigenvector \\(\\mathbf{v}\\), provided that the eigenvaluen \\(\\lambda\\) is specified in advance. But, Gaussian elimination per se cannot solve the problem of determining two unknowns \\(\\lambda\\) and \\(\\mathbf{v}\\). We can rewrite the equation in the form:\n\\[\n(A- \\lambda I)\\mathbf{v} = \\mathbf{0}\n\\tag{2}\\]\nThis is a homogenous system of linear equations. It has the trivial solution \\(\\mathbf{v}=0\\). But, we are specifically seeking a non-zero solution. The homogenous system \\(R\\mathbf{x}=\\mathbf{0}\\) has a non-trivial solution, if and only if, \\(R\\) is singular, \\(rank(R) &lt; n\\) or equivalently \\(det(R) = 0\\). Consequently, we desire\n\\[\ndet(A-\\lambda I) = 0\n\\tag{3}\\]\nThis is called the characteristic equation and \\(p(\\lambda) = det(A-\\lambda I)\\) is called the characteristic polynomial.\nIn practice, one first solves the characteristic equation (Equation 3) to obtain a set of eigenvalues. Then, for each eigenvalue, we use standard linear algebra methods e.g. Gaussian elimination to solve the correponding linear system Equation 2 for the associated eigenvector \\(\\mathbf{v}\\)."
  },
  {
    "objectID": "posts/diagonalization/index.html#emhe",
    "href": "posts/diagonalization/index.html#emhe",
    "title": "Eigenthingies and Diagonalizability",
    "section": "EMHE",
    "text": "EMHE\n\nTheorem 1 Every matrix has atleast one eigenvalue, and a corresponding eigenvector.\n\nProof.\nThis is just the FTA(Fundamental Theorem of Algebra), but it’s still worth enumerating as a theorem.\nLet \\(A \\in \\mathbb{C}^{n \\times n}\\) and the scalar field \\(\\mathbb{F}= \\mathbb{R}\\).\nLet \\(\\mathbf{v}\\) be any non-zero vector in \\(\\mathbb{C}^n\\). Consider the list \\(\\{\\mathbf{v},A\\mathbf{v},\\ldots,A^n \\mathbf{v}\\}\\). These are \\(n+1\\) vectors and this must be a linearly dependent set. There exists \\(a_0, \\ldots, a_n\\) not all zero, such that:\n\\[\na_n A^n \\mathbf{v} + a_{n-1}A^{n-1}\\mathbf{v} + \\ldots + a_1 A \\mathbf{v} + a_0 I \\mathbf{v} = \\mathbf{0}\n\\]\nSince this holds for all \\(\\mathbf{v}\\neq \\mathbf{0}\\), the linear operator \\(a_n A^n + \\ldots + a_1 A + a_0 I\\) must be the zero transformation.\nBy FTA, the polynomial equation with complex coefficients of degree \\(n\\):\n\\[\np(x) = a_0 + a_1 x + a_2 x^2 + \\ldots + a_{n}x^n\n\\]\ncan be factorized as :\n\\[\np(x) = (x - \\lambda_1)(x - \\lambda_2)\\cdots(x - \\lambda_n)\n\\]\nPutting it all together,\n\\[\n\\begin{align*}\np(A)\\mathbf{v} &= (A - \\lambda_1 I)(A - \\lambda_2 I)\\cdots (A - \\lambda_n I)\\mathbf{v} = \\mathbf{0}\n\\end{align*}\n\\]\n\\(\\forall \\mathbf{v} \\neq \\mathbf{0}\\).\nSo, the composition of the factors \\((A-\\lambda_1 I)\\cdots (A - \\lambda_n I)\\) has a non-trivial null space.\n\\[\nker((A-\\lambda_1 I)(A-\\lambda_2 I)\\cdots (A - \\lambda_n I)) \\neq \\{\\mathbf{0}\\}\n\\]\nSo, atleast one of the factors must fail to be injective. There exists \\(\\lambda_i\\), such that \\((A-\\lambda_i I)\\mathbf{v}=\\mathbf{0}\\) such that \\(\\mathbf{v}\\neq \\mathbf{0}\\). Thus, \\(A\\) has atleast one eigenvalue and one eigenvector. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/diagonalization/index.html#eigenvectors-as-the-basis-of-a-vector-space",
    "href": "posts/diagonalization/index.html#eigenvectors-as-the-basis-of-a-vector-space",
    "title": "Eigenthingies and Diagonalizability",
    "section": "Eigenvectors as the basis of a vector space",
    "text": "Eigenvectors as the basis of a vector space\n\nLemma 1 If \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n\\) are \\(n\\) distinct eigenvalues of a matrix \\(A\\), \\(\\lambda_i \\neq \\lambda_j\\), \\(\\forall i \\neq j\\), then the corresponding eigenvectors \\(\\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\) are linearly independent.\n\nProof.\nWe use induction on the number of eigenvalues. The case \\(k=1\\) is immediate, since an eigenvector cannot be zero. Assume that we know that the result is valid for \\((k-1)\\) eigenvalues. Our claim is that \\(\\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_{k-1},\\mathbf{v}_k\\}\\) are linearly independent.\nSuppose we have a vanishing linear combination:\n\\[\nc_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\ldots + c_{k} \\mathbf{v}_k = \\mathbf{0}\n\\tag{4}\\]\nLet us multiply this equation by the matrix \\(A\\):\n\\[\n\\begin{align*}\nc_1 A\\mathbf{v}_1 + c_2 A\\mathbf{v}_2 + \\ldots + c_{k} A\\mathbf{v}_k &= \\mathbf{0}\\\\\n\\Longrightarrow c_1 \\lambda_1 \\mathbf{v}_1 + c_2 \\lambda_2 \\mathbf{v}_2 + \\ldots + c_k \\lambda_k \\mathbf{v}_k &= \\mathbf{0}\n\\end{align*}\n\\]\nOn the other hand if we multiply the original Equation 4 by \\(\\lambda_k\\), we have:\n\\[\nc_1 \\lambda_k \\mathbf{v}_1 + c_2 \\lambda_k \\mathbf{v}_2 + \\ldots + c_{k} \\lambda_k \\mathbf{v}_k = \\mathbf{0}\n\\]\nUpon subtracting this from the previous equation, we obtain:\n\\[\nc_1 (\\lambda_1 - \\lambda_k) \\mathbf{v}_1 + c_2 (\\lambda_2 - \\lambda_k)\\mathbf{v}_2 + \\ldots + c_{k-1} (\\lambda_{k-1} - \\lambda_k)\\mathbf{v}_{k-1} = \\mathbf{0}\n\\]\nThis is a vanishing linear combination of the first \\((k-1)\\) eigenvectors, and so, by our induction hypothesis, it can only happen if all the coefficients are zero:\n\\[\nc_1(\\lambda_1 - \\lambda_k) = c_2(\\lambda_2 - \\lambda_k) = \\ldots = c_{k-1}(\\lambda_{k-1} - \\lambda_k) = 0\n\\]\nThe eigenvalues were assumed to be distinct, and consequently \\(c_1 = c_2 = \\ldots = c_{k-1} = 0\\). Substituting these values back into Equation 4, we find that \\(c_k \\mathbf{v}_k = 0\\), and so \\(c_k = 0\\) also, since \\(\\mathbf{v}_k \\neq \\mathbf{0}\\). Thus, we have proved that, if Equation 4 holds, then \\(c_1 = \\ldots = c_k = 0\\). Thus, \\(\\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\) is a linearly independent set. \\(\\blacksquare\\)\n\nTheorem 2 If the \\(n \\times n\\) real matrix \\(A\\) has \\(n\\) distinct real eigenvalues \\(\\lambda_1,\\lambda_2,\\ldots,\\lambda_n\\), then the corresponding real eigenvectors \\(\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\) form a basis of \\(\\mathbb{R}^n\\). If \\(A\\) (which may be either real or complex-valued matrix) has \\(n\\) distinct complex eigenvalues, then the corresponding eigenvectors \\(\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\) form a basis of \\(\\mathbb{C}^n\\)."
  },
  {
    "objectID": "posts/diagonalization/index.html#diagonalization",
    "href": "posts/diagonalization/index.html#diagonalization",
    "title": "Eigenthingies and Diagonalizability",
    "section": "Diagonalization",
    "text": "Diagonalization\nConsider a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) with \\(n\\) distinct eigenvalues. We can then write:\n\\[\nA\\begin{bmatrix}\\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\vdots \\\\ \\mathbf{v}_n\\end{bmatrix} =\n\\begin{bmatrix}\n\\lambda_1 \\\\\n& \\lambda_2 \\\\\n& & \\ddots \\\\\n& & & \\lambda_n\n\\end{bmatrix}\\begin{bmatrix}\\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\vdots \\\\ \\mathbf{v}_n\\end{bmatrix}\n\\]\nDefine \\(P\\) as \\((\\mathbf{v}_1,\\mathbf{v}_2,\\ldots,\\mathbf{v}_n)^T\\). So, we can write:\n\\[\n\\begin{align*}\nAP &= \\Lambda P\\\\\nA & = P^{-1}\\Lambda P\n\\end{align*}\n\\]\nor equivalently \\(A=P\\Lambda P^{-1}\\), where \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\) is a diagonal matrix. Consequently, if the matrix \\(A\\) has \\(n\\) distinct eigenvalues, then \\(A\\) is said to be diagonalizable.\n\nDefinition 2 A square matrix \\(A\\) is said to be diagonalizable, if and only if, there exists a non-singular matrix \\(P\\), such that \\(A\\) has a matrix factorization:\n\\[\nA = P\\Lambda P^{-1}\n\\]\nwhere \\(\\Lambda=diag(\\lambda_1,\\ldots,\\lambda_n)\\) ."
  },
  {
    "objectID": "posts/diagonalization/index.html#gershgorin-circle-theorem",
    "href": "posts/diagonalization/index.html#gershgorin-circle-theorem",
    "title": "Eigenthingies and Diagonalizability",
    "section": "Gershgorin-Circle Theorem",
    "text": "Gershgorin-Circle Theorem\nIn pratice, precisely computing the eigenvalues of a matrix is done using a numerical algorithm. In certain theoretical applications, we may not require numerical values, but only their approximate locations. The Gershgorin circle theorem, due to early 20th century Russian mathematician Semyon Gershgorin, serves to restrict the eigenvalues to a certain well-defined region in the complex plane.\n\nDefinition 3 Let \\(A \\in \\mathbb{C}^{n \\times n}\\) be a square matrix. For each \\(1 \\leq i \\leq n\\) , define the \\(i\\) th Gershgorin disk\n\\[\nD_i = \\{|z - a_{ii}|&lt;r_i:z\\in\\mathbb{C}\\}, \\quad r_i = \\sum_{j,j\\neq i} |a_{ij}|\n\\tag{5}\\]\nThe Gershgorin domain \\(D_A = \\bigcup_{i=1}^n D_i \\subset \\mathbb{C}\\) is the union of the Gershgorin disks.\n\nThus, the \\(i\\)th Gershgorin disk \\(D_i\\) is centered at the \\(i\\)-th diagonal entry of \\(A\\) and is an open ball of radius \\(r_i\\) equal to the sum of the absolute values of the off-diagonal entries that are in it’s \\(i\\)-th row.\nProof\nLet \\(\\mathbf{v}\\) be an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\). Let \\(\\mathbf{u}=\\mathbf{v}/||v||_{\\infty}\\) be the corresponding unit eigenvector with respect to the \\(\\infty\\)-norm, so that:\n\\[\n||u||_{\\infty} = \\max\\{|u|_1,|u|_2,\\ldots,|u|_n\\} = 1\n\\]\nLet \\(u_i\\) be an entry of \\(\\mathbf{u}\\) that achieves the maximum: \\(|u_i|=1\\). Writing out the \\(i\\)-th component of the eigenvalue equation \\(A\\mathbf{u}=\\lambda \\mathbf{u}\\), we obtain:\n\\[\n\\begin{align*}\n\\sum_{j=1}^{n} a_{ij}u_j &= \\lambda u_i \\\\\n\\sum_{j \\neq i} a_{ij}u_j &= (\\lambda - a_{ii}) u_i\n\\end{align*}\n\\]\nTherefore, since all \\(|u_j| \\leq 1\\), while \\(|u_i|=1\\), the distance between \\(\\lambda\\) and \\(a_{ii}\\) can be bounded from above as:\n\\[\n\\begin{align*}\n|\\lambda - a_{ii}| &= \\Bigg|\\sum_{j \\neq i} a_{ij}u_j \\Bigg|\\\\\n&\\leq \\sum_{j \\neq i} |a_{ij}||u_j| & \\{\\text{ Triangle Inequality }\\}\\\\\n&\\leq \\sum_{j \\neq i} |a_{ij}| & \\{ |u_j| \\leq 1 \\}\\\\\n&= r_i\n\\end{align*}\n\\]\nThis immediately implies that \\(\\lambda \\in D_i \\subset D_A\\) belongs to the \\(i\\)th Gershgorin disk."
  },
  {
    "objectID": "posts/first_passage_time_of_BM/index.html",
    "href": "posts/first_passage_time_of_BM/index.html",
    "title": "The distribution of the first passage time of Brownian Motion",
    "section": "",
    "text": "The distribution of Brownian motion enjoys many interesting symmetries. The reflection of a Brownian motion about any time \\(s\\) is also a Brownian motion.\nLemma 1. (Reflection at time \\(s\\)) Let \\(B_t\\) be a standard Brownian motion. Then, the process \\((-B_t,t \\geq 0)\\) is a Brownian motion. More generally, for any \\(s \\geq 0\\), the process \\((\\tilde{B_t},t\\geq 0)\\) defined by:\n\\[\\begin{align*}\n\\tilde{B}_t = \\begin{cases}\nB_t & \\text{ if } t\\leq s\\\\\nB_s - (B_t - B_s) & \\text{ if }t &gt; s\n\\end{cases}\n\\end{align*}\\]\nis a Brownian motion.\nClaim. \\((-B_t,t\\geq 0)\\) is a Brownian motion.\nProof.\nWe have, \\(-B(0) = 0\\).\nFor any increment \\(s &lt; t\\), the increment \\((-B_t) - (-B_s) = B_s - B_t\\) is a Gaussian random variable with mean \\(0\\) and variance \\(t - s\\).\nFor any choice of \\(n\\) times, \\(0 \\leq t_1 \\leq t_2 \\leq \\ldots \\leq t_n\\), the increments:\n\\[\\begin{align*}\n(B_{0} - B_{t_1}), (B_{t_1} - B_{t_2}), \\ldots, (B_{t_n} - B_{t_{n-1}})\n\\end{align*}\\]\nare independent\nThe paths \\(-B_t(\\omega)\\) are continuous.\nThus, \\((-B_t,t\\geq 0)\\) is a standard Brownian motion.\nClaim. \\((\\tilde{B_t},t\\geq 0)\\) is a Brownian motion.\nProof.\nLet \\(s \\geq 0\\) be any arbitrary time.\nWe have, \\(\\tilde{B}(0) = 0\\).\nConsider any increment \\(\\tilde{B}(t_2) - \\tilde{B}(t_1)\\), \\(t_2 &lt; t_1\\).\nCase I. \\(s \\leq t_1 &lt; t_2\\)\nHere\n\\[\\begin{align*}\n\\tilde{B}(t_2) - \\tilde{B}(t_1) &= B(s) - (B(t_2) - B(s)) - (B(s) - (B(t_1) - B(s))) \\\\\n&= -(B(t_2) - B(t_1))\n\\end{align*}\\]\nHence, \\(\\tilde{B}(t_2) - \\tilde{B}(t_1) \\sim \\mathcal{N}(0,t_2 - t_1)\\).\nCase II. \\(t_1 &lt; s &lt; t_2\\)\nHere\n\\[\\begin{align*}\n\\tilde{B}(t_2) - \\tilde{B}(t_1) &= B(s) - (B(t_2) - B(s)) - B(t_1)\\\\\n&= (B(s) - B(t_1)) - (B(t_2) - B(s))\n\\end{align*}\\]\n\\(B(s) - B(t_1)\\) and \\(B(t_2) - B(s)\\) are independent random variables. Moreover, \\(B(s) - B(t_1) \\sim \\mathcal{N}(0,s - t_1)\\) and \\(B(t_2) - B(s) \\sim \\mathcal{N}(0,t_2 - s)\\). Consequently, \\(\\tilde{B}(t_2) - \\tilde{B}(t_1) \\sim \\mathcal{N}(0,t_2 - t_1)\\).\nCase III. \\(t_1 &lt; t_2 \\leq s\\)\nHere\n\\[\\begin{align*}\n\\tilde{B}(t_2) - \\tilde{B}(t_1) &= B(t_2) - B(t_1)\n\\end{align*}\\]\nso \\(\\tilde{B}(t_2) - \\tilde{B}(t_1) \\sim \\mathcal{N}(0,t_2 - t_1)\\).\nFinally, the paths \\(\\tilde{B}(t,\\omega)\\) are continuous. Hence, \\((\\tilde{B}(t),t\\geq 0)\\) is a standard brownian motion."
  },
  {
    "objectID": "posts/first_passage_time_of_BM/index.html#reflection-principle",
    "href": "posts/first_passage_time_of_BM/index.html#reflection-principle",
    "title": "The distribution of the first passage time of Brownian Motion",
    "section": "Reflection Principle",
    "text": "Reflection Principle\nIt turns out that the above reflection property holds even if \\(s\\) is replaced by a stopping time. I prove this here.\nLemma 2. (Reflection Principle) Let \\((B_t,t \\geq 0)\\) be a standard Brownian motion and \\(\\tau\\) be a stopping time for its filtration. Then, the process \\((\\tilde{B}(t),t\\geq 0)\\) defined by the reflection at time \\(\\tau\\):\n\\[\\begin{align*}\n\\tilde{B}(t) &= \\begin{cases}\nB_t & \\text{ if } t\\leq \\tau\\\\\nB_\\tau - (B_t - B_\\tau) & \\text{ if }t &gt; \\tau\n\\end{cases}\n\\end{align*}\\]\nis also a standard Brownian motion."
  },
  {
    "objectID": "posts/first_passage_time_of_BM/index.html#bacheliers-formula",
    "href": "posts/first_passage_time_of_BM/index.html#bacheliers-formula",
    "title": "The distribution of the first passage time of Brownian Motion",
    "section": "Bachelier’s formula",
    "text": "Bachelier’s formula\nIt is an amazing fact, that some simple manipulations using stopping time yield the complete distribution of the first passage time \\(\\tau_a\\) of a Brownian motion as well as the distribution of the running maximum of the Brownian path on an interval of time \\([0,T]\\). This is surprising since the maximum of the Brownian path on \\([0,T]\\), denoted by \\(\\sup_{0\\leq t \\leq T} B_t\\) is a random variable that depends on the whole path on \\([0,T]\\). This beautiful result is due to Bachelier.\nProposition 3. (Bachelier’s formula) Let \\((B_t,t\\leq T)\\) be a standard Brownian motion on \\([0,T]\\). Then, the CDF of the random variable \\(\\sup_{0 \\leq t\\leq T} B_t\\) is:\n\\[\\begin{align*}\n\\mathbb{P}\\left(\\sup_{0\\leq t \\leq T} B_t \\leq a\\right) = \\mathbb{P}(|B_T| \\leq a) \\quad \\text{ for any }a\\geq 0\n\\end{align*}\\]\nIn particular, its PDF is:\n\\[\\begin{align*}\nf_{max}(a) = \\frac{2}{\\sqrt{2\\pi T}} e^{-a^2/2T}\n\\end{align*}\\]\nIn other words, the random variable \\(\\sup_{0 \\leq t \\leq T} B_t\\) (the maximum of the brownian motion at any time \\(t\\)) has the same distribution as \\(|B_T|\\) (the terminal distribution of the absolute value of the brownian motion).\nThis equality holds in distribution for a fixed \\(t\\). As a bonus corollary, we get the distribution of the first passage time at \\(a\\).\nProof. Consider \\(\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a)\\). By splitting this probability over the event of the endpoint, we have:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a) &= \\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a, B_T &gt; a) + \\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a, B_T \\leq a)\n\\end{align*}\\]\nNote also that \\(\\mathbb{P}(B_T = a) = 0\\). Hence, the first probability equals \\(\\mathbb{P}(B_T \\geq a)\\). As for the second, consider the time \\(\\tau_a\\). On the event considered, we have \\(\\tau_a \\leq T\\) and using the reflection principle (lemma 2), we get:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a, B_T \\leq a) &= \\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a, \\tilde{B}_T \\geq a)\n\\end{align*}\\]\nObserve that, since \\(\\tau_a \\leq T\\), the event \\(\\{\\sup_{t \\leq T} B_t \\geq a\\}\\) is the same as \\(\\{\\sup_{t\\leq T} \\tilde{B}(t) \\geq a\\}\\). Therefore the above probability is:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a) &= \\mathbb{P}(B_T &gt; a) + \\mathbb{P}(\\sup_{t \\leq T} \\tilde{B}_t \\geq a, \\tilde{B}_T \\geq a)\n\\end{align*}\\]\nBut, \\(\\tilde{B}_t\\) is also a standard brownian motion and has the same distribution as \\(B_t\\). \\(\\mathbb{P}(B_t \\in S) = \\mathbb{P}(\\tilde{B}_t \\in S)\\) by the reflection principle. So, we can simply drop the tilde signs and write:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a) &= \\mathbb{P}(B_T &gt; a) + \\mathbb{P}(\\sup_{t \\leq T} {B}_t \\geq a, {B}_T \\geq a)\\\\\n&=\\mathbb{P}(B_T &gt; a) + \\mathbb{P}({B}_T \\geq a)\\\\\n&= \\mathbb{P}(B_T \\leq -a) + \\mathbb{P}(B_T \\geq a) \\\\\n& \\quad \\{\\text{ By symmetry of the Gaussian distribution }\\}\\\\\n&= \\mathbb{P}(B_T \\leq -a \\cup B_T \\geq a) \\\\\n&= \\mathbb{P}(|B_T| \\geq a)\n\\end{align*}\\]\nWe conclude that:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup B_t \\leq a) = \\mathbb{P}(|B_T| \\leq a)\n\\end{align*}\\]\nas claimed.\nTo derive the PDF, we can always write:\n\\[\\begin{align*}\n\\mathbb{P}(\\sup_{t \\leq T} B_t \\geq a) &= \\mathbb{P}(B_T &gt; a) + \\mathbb{P}({B}_T \\geq a)\\\\\n&= 2\\mathbb{P}(B_T \\geq a)\\\\\n&= 2(1 - F_{B_T}(a))\n\\end{align*}\\]\nSo:\n\\[\\begin{align*}\nF_{\\sup B_t}(a) &= 1 - 2(1 - F_{B_T}(a))\\\\\n\\frac{d}{da}(F_{\\sup B_t}(a)) &= 2 \\frac{d}{da}(F_{B_T}(a))\\\\\nf_{\\sup B_t}(a) &= 2 f_{B_T}(a)\\\\\n&= \\frac{2}{\\sqrt{2\\pi T}}\\exp\\left[-\\frac{a^2}{2T}\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/first_passage_time_of_BM/index.html#distribution-of-the-first-passage-time-tau_a",
    "href": "posts/first_passage_time_of_BM/index.html#distribution-of-the-first-passage-time-tau_a",
    "title": "The distribution of the first passage time of Brownian Motion",
    "section": "Distribution of the first passage time \\(\\tau_a\\)",
    "text": "Distribution of the first passage time \\(\\tau_a\\)\nCorollary 4. Let \\(a \\geq 0\\) and let \\(\\tau_a = \\min \\{t \\geq 0: B_t \\geq a\\}\\). Then:\n\\[\\begin{align*}\n\\mathbb{P}(\\tau_a \\leq T) = \\mathbb{P}\\left(\\sup_{0 \\leq t \\leq T} B_t \\geq a\\right) = \\int_{a}^{\\infty} \\frac{2}{\\sqrt{2\\pi T}}e^{-x^2/2T} dx\n\\end{align*}\\]\nIn particular, the random variable \\(\\tau_a\\) has PDF:\n\\[\\begin{align*}\nf_{\\tau_a}(t) = \\frac{a}{\\sqrt{2\\pi}} \\frac{e^{-a^2/2t}}{t^{3/2}}\n\\end{align*}\\]\nThis implies that it is heavy-tailed and \\(\\mathbb{E}[\\tau_a] = \\infty\\).\nProof.\nThe maximum on \\([0,T]\\) is larger than or equal to \\(a\\), if and only if, \\(\\tau_a \\leq T\\). Therefore, the events \\(\\{\\sup_{0 \\leq t \\leq T} B_t \\geq a\\}\\) and \\(\\{\\tau_a \\leq T\\}\\) are the same. So, the CDF \\(\\mathbb{P}(\\tau_a \\leq t)\\) of \\(\\tau_a\\) is\n\\[\\begin{align*}\nF_{\\tau_a}(t) = \\int_{a}^{\\infty} \\frac{2}{\\sqrt{2\\pi t}} e^{-\\frac{x^2}{2t}} dx\n\\end{align*}\\]\nTo get the PDF, it remains to differentiate the integral with respect to \\(t\\). This is easy to do, once we realise a change of variable \\(u = x/\\sqrt{t}\\), \\(du = dx/\\sqrt{t}\\) that:\n\\[\\begin{align*}\nF_{\\tau_a}(t) &= \\int_{a/\\sqrt{t}}^{\\infty} \\frac{2}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}}du\\\\\n&= 2(1 - \\Phi\\left(\\frac{a}{\\sqrt{t}}\\right))\n\\end{align*}\\]\nDifferentiating on both sides with respect to \\(t\\), we get:\n\\[\\begin{align*}\nf_{\\tau_a}(t) &= - 2\\phi\\left(\\frac{a}{\\sqrt{t}}\\right) \\left(-\\frac{1}{2}\\right) \\frac{a}{t^{3/2}}\\\\\n&= \\frac{a}{t^{3/2}} \\frac{e^{-a^2/2t}}{\\sqrt{2\\pi}}\n\\end{align*}\\]\nThis closes the proof."
  },
  {
    "objectID": "posts/interpolation-and-approximation/index.html",
    "href": "posts/interpolation-and-approximation/index.html",
    "title": "Interpolation and Approximation",
    "section": "",
    "text": "In this blog post, I would like to implement some interpolation algorithms using modern C++. It’s important we understand how and why these algorithms work. Because there are nuances, exceptions and corner-cases, we need to understand. It’s a great learning experience! You never fully understand something like the \\(QR\\)-algorithm (which is actually incredibly complicated to get the details right), unless you spend some time implementing it. A lot of the skills you pick up will translate when you write numerical codes.\nEven if you have excellent implementations of any algorithm, you still need to understand some of the internals in order to make good use of your library. For example, any linear algebra library can compute an inverse matrix really fast and as accurately as mathematics allows. But, you have to know a little theory in order to know that you should still avoid using that if at all possible. (Matrix decomposition and solving without explicitly computing the inverse is better, generally speaking)."
  },
  {
    "objectID": "posts/interpolation-and-approximation/index.html#introduction",
    "href": "posts/interpolation-and-approximation/index.html#introduction",
    "title": "Interpolation and Approximation",
    "section": "",
    "text": "In this blog post, I would like to implement some interpolation algorithms using modern C++. It’s important we understand how and why these algorithms work. Because there are nuances, exceptions and corner-cases, we need to understand. It’s a great learning experience! You never fully understand something like the \\(QR\\)-algorithm (which is actually incredibly complicated to get the details right), unless you spend some time implementing it. A lot of the skills you pick up will translate when you write numerical codes.\nEven if you have excellent implementations of any algorithm, you still need to understand some of the internals in order to make good use of your library. For example, any linear algebra library can compute an inverse matrix really fast and as accurately as mathematics allows. But, you have to know a little theory in order to know that you should still avoid using that if at all possible. (Matrix decomposition and solving without explicitly computing the inverse is better, generally speaking)."
  },
  {
    "objectID": "posts/interpolation-and-approximation/index.html#the-interpolation-problem",
    "href": "posts/interpolation-and-approximation/index.html#the-interpolation-problem",
    "title": "Interpolation and Approximation",
    "section": "The Interpolation Problem",
    "text": "The Interpolation Problem\nPolynomials are used as the basic means of approximation and are ubiquitous in nearly all areas of computational science.\nLet \\(a=x_1 &lt; x_2 &lt; \\ldots &lt; x_n = b\\) be a grid of distinct points. Let \\(\\mathcal{P}_n\\) be the vector space of of polynomials in one variable with degree \\(\\leq n - 1\\). We are interested to find a polynomial \\(p \\in \\mathcal{P}_n\\) such that:\n\\[\n\\begin{align*}\np(x_i) = f(x_i), \\quad i = 1 : n\n\\end{align*}\n\\tag{1}\\]\n\nTheorem 1 (Uniqueness of the interpolating polynomial) If \\(x_1,\\ldots,x_n\\) are distinct real numbers, then for arbitrary values \\(y_1,\\ldots,y_n\\), there is a unique polynomial \\(p\\in \\mathcal{P}_{n}\\) of degree at most \\(n-1\\) such that:\n\\[p(x_i) = y_i,  \\quad i = 1 : n\\]\n\nProof.\nSuppose that there were two such polynomials \\(p\\) and \\(q\\). Then, the polynomial \\(p-q\\) would have the property \\((p-q)(x_i)=0\\) for \\(1 \\leq i \\leq n\\). Since, the degree of \\((p-q)\\) can be at most \\(n-1\\), this polynomial can have atmost \\((n-1)\\) zeroes, if is not the \\(0\\) polynomial. Since the \\(x_i\\)’s are distinct, it follows that:\n\\[\n(p-q)(x) = (x - x_1)(x - x_2)\\ldots(x-x_n) = \\prod_{i=1}^n (x-x_i)\n\\]\nand it has atleast \\(n\\) zeroes. Hence, \\((p-q)(x)\\equiv 0\\) - it must be identically equal to zero. So, \\(p(x) = q(x)\\) for all \\(x\\). This closes the proof. \\(\\blacksquare\\)\n\nBases for polynomial interpolation\nA set of polynomials \\(\\{p_1(x), p_2(x), \\ldots, p_n(x)\\}\\) such that the polynomial \\(p \\in \\mathcal{P}_n\\) can be expressed as a linear combination :\n\\[\np(x) = \\sum_{j=1} c_j p_j(x)\n\\]\nis called a basis for \\(\\mathcal{P}_n\\). The column vector \\(c=(c_1,c_2,\\ldots,c_n)^T\\) can be viewed as the coordinate vector of \\(p\\) in the polynomial space \\(\\mathcal{P}_n\\). The inerpolation problem leads to a system of equations:\n\\[\n\\begin{align*}\nc_1 p_1(x_i) + c_2 p_2(x_i) + \\ldots + c_n p_n(x_i) = f(x_i), \\quad i=1:n\n\\end{align*}\n\\tag{2}\\]\nIf we introduce the matrix :\n\\[\n\\begin{align*}\nP_n = [p_j(x_i)]_{i,j=1}^n\n\\end{align*}\n\\tag{3}\\]\nand the column vector \\(f=(f(x_1),\\ldots,f(x_n))^T\\), then the linear system becomes:\n\\[\n\\begin{align*}\nP_n c = f\n\\end{align*}\n\\tag{4}\\]\nMathematically, the choice of a basis (for a finite-dimensional space) makes no difference. Computationally, when working with rounded values of coefficients, the choice of basis can make a great difference. If the purpose is to compute derivatives or integrals of the interpolation polynomial, the power basis or the shifted power basis, where \\(p_j(x) = (x - c)^{j-1}\\) that is:\n\\[\np(x)= \\sum_{j=1}^n c_j(x)(x - c)^{j-1}\n\\]\nis convenient although not always the best. If a shifted power basis is to be used for polynomial approximation on an interval \\([a,b]\\), it is often the best to choose \\(c = (a + b)/2\\), equal to the midpoint of the interval.\nFor the power basis \\(p_j(x) = x^{j-1}\\), the coefficients of the interpolation polynomial are given by the solution of the linear system \\(V_n^T c = f\\), where \\(V_n\\) is the Vandermonde matrix\n\\[\nV_n = [x_j^{i-1}]_{i,j=1}^n =\n\\begin{bmatrix}\n1 & 1 & \\ldots & 1\\\\\nx_1 & x_2 & \\ldots & x_n \\\\\n\\vdots & \\vdots & \\ldots & \\vdots\\\\\nx_1^{n-1} & x_2^{n-1} & \\ldots & x_n^{n-1}\n\\end{bmatrix}\n\\tag{5}\\]"
  },
  {
    "objectID": "posts/interpolation-and-approximation/index.html#piecewise-polynomial-interpolation",
    "href": "posts/interpolation-and-approximation/index.html#piecewise-polynomial-interpolation",
    "title": "Interpolation and Approximation",
    "section": "Piecewise Polynomial Interpolation",
    "text": "Piecewise Polynomial Interpolation\nInterpolating a given function by a single polynomial over its entire range can be an ill-conditioned problem, as illustrated by Runge’s phenomenon. On the other hand, polynomials of low degree can give good approximations locally in a small interval. I would like to discuss approximation schemes for piecewise polynomial interpolation with different degrees of global continuity.\nWith the use of piecewise polynomials, there is no reason to fear equidistant data, as opposed to the situation with higher-degree polynomials. In computer graphics and computer-aided design(CAD), curves and surfaces have to be represented mathematically, so that they can be manipulated and visualized easily. In 1962, Bezier and de Casteljau, working for French car companies Renault and Citroen, independently developed Bezier curves for fitting curves and surfaces. Similar work, using bicubic splines, was done in USA at general motors by Garret Birkhoff and Henry Garabedian.\nToday, Bezier curves and spline functions are used extensively in all aircraft and automotive industries. Spline functions can also be used in the numerical treatment of boundary value problems for differential equations. Bezier curves have found use in computer graphics and typography. Trutype font glyphs are made of quadratic bezier curves.\n\nBernstein Polynomials and Bezier Curves\nParametric curves are often used find the functional form of a curve given geometrically by a set of points \\(p_i \\in \\mathbf{R}^d\\), \\(i=0:n\\).\nLet \\(c(t) \\in \\mathbf{R}^d\\), \\(t\\in[0,1]\\), be a parameteric curve. In the simplest case, \\(n=1\\), we take \\(c(t)\\) to be linear:\n\\[\nc(t) = (1-t)p_0 + tp_1\n\\]\nand connecting the two points \\(p_0\\) and \\(p_1\\), so that \\(p(0)=c_0\\) and \\(p_1 = c(1)\\). It is the parametric equation for a straight-line.\nFor \\(n &gt; 1\\), this will not give a smooth curve and is therefore of limited interest.\nWe can generalize this approach and take \\(c(t)\\) to be a polynomial of degree \\(n\\):\n\\[\nc(t) =\\sum_{i=0}^{n-1} p_i B_i(t),\\quad t\\in[0,1]\n\\]\nwhere \\(B_i(t)\\), \\(i=0 : n\\) are the Bernstein polynomials defined by :\n\\[\nB_i^{n}(t) = {n \\choose i} t^{i} (1-t)^{n-i}, \\quad i=0:n\n\\]\nUsing the binomial theorem, we have:\n\\[\n1 = ((1-t) + t)^n = \\sum_{i=0}^n {n \\choose i}t^i (1-t)^{n-i} = \\sum_{i=0}^n B_i^{n}(t)\n\\]\nThus, the Bernstein polynomials of degree \\(n\\) are non-negative on \\([0,1]\\) and give a partition of unity.\nFor \\(n=3\\), the four cubic Bernstein polynmials are:\n\\[\n\\begin{align*}\nB_0^3 &= (1-t)^3\\\\\nB_1^3 &= 3(1-t)^2 t\\\\\nB_2^3 &= 3(1-t)t^2\\\\\nB_3^3 &= t^3\n\\end{align*}\n\\]\n\nusing Plots\nusing LaTeXStrings\n\nB₀(t) = (1-t)^3\nB₁(t) = 3*(1-t)^2*t\nB₂(t) = 3*(1-t)*t^2\nB₃(t) = t^3\n\nplot([B₀, B₁, B₂, B₃], 0.0, 1.0, label=[L\"B_0\" L\"B_1\" L\"B_2\" L\"B_3\"])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome important properties of the Bernstein polynomials are given in the following theorem.\n\nTheorem 2 (Berstein polynomial properties) The Bernstein polynomials \\(B_i^{n}(t)\\) have the following properties:\n\nNon-negativity: \\(B_i^{n}(t) &gt; 0\\), \\(t\\in(0,1)\\)\nSymmetry: \\(B_i^{n}(t)=B_{n-i}^{n}(1-t)\\)\n\\(B_i^{n}(t)\\) has a root \\(t=0\\) of multiplicity \\(i\\) and a root \\(t=1\\) of multiplicity \\(n-i\\)\nThe Bernstein polynomials \\(B_i^{n}(t)\\) have a unique maximum value at \\(t=i/n\\) on \\([0,1]\\)\nThe Bernstein polynomials satisfy the following recursion formula: \\[\n\\begin{align*}\nB_i^n(t) = (1-t)B_i^{n-1}(t) + tB_{i-1}^{n-1}(t),\\quad i=0:n\n\\end{align*}\n\\tag{6}\\]\n\n\n\nThe Bernstein polynomials of degree \\(n\\) form a basis for the space of polynomials of degree \\(\\leq n\\).\n\nProof.\nNon-negativity: For \\(t\\in[0,1]\\), \\(0&lt;1-t&lt;1\\), so \\(B_i^{n}(t) \\geq 0\\).\nSymmetry: Since \\({n \\choose k} = {n \\choose n-k}\\), we have:\n\\[\nB_k^{n}(t) = {n \\choose k}t^k (1-t)^{n-k} = {n \\choose (n-k)} (1-t)^{n-k} t^k = B_{n-k}^n(1-t)\n\\]\nRoots. By definition, \\(B_k^{n}(t) = {n \\choose k}t^k (1-t)^{n-k}\\) so it has a root \\(t=0\\) with multiplicity \\(k\\) and a root \\(t=1\\) with multiplicity \\((n-k)\\).\nMoreover, differentiating \\(B_k^{n}(t)\\) with respect to \\(t\\), setting the first derivative equal to \\(0\\), we have:\n\\[\n\\begin{align*}\n\\frac{d}{dt}(B_k^{n}(t)) &= {n \\choose k} kt^{k-1} (1-t)^{n-k} - (n-k)t^k(1-t)^{n-k-1} = 0 \\\\\n0 &= k(1-t) - (n-k)t \\\\\n0 &= k - kt - nt + kt \\\\\nnt &= k \\\\\nt &= \\frac{k}{n}\n\\end{align*}\n\\]\nConsider the combinatorial identity:\n\\[\n{n \\choose k} = {n-1 \\choose k} + {n - 1\\choose k - 1}\n\\]\nAssume that we would like to assemble team of size \\(k\\) from a population of size \\(n\\). There are \\({n \\choose k}\\) distinguishable teams. Another way to count is as follows. Label one member of the population as president. Then, there are \\({n - 1 \\choose k}\\) distinguishable teams that always include the president and \\({n - 1 \\choose k - 1}\\) distinct teams excluding the president. The sum must equal \\({n \\choose k}\\).\nWe can use this to prove the recursion formula:\n\\[\n\\begin{align*}\n{n \\choose k}t^k(1-t)^{n-k} &= {n-1 \\choose k}t^k(1-t)^{n-k} + {n - 1\\choose k - 1}t^k(1-t)^{n-k}\\\\\n&= (1-t) {n-1 \\choose k}t^k(1-t)^{n-1-k} + t{n - 1\\choose k - 1}t^{k-1}(1-t)^{(n-1)-(k-1)}\\\\\n&= (1-t)B_{k}^{n-1}(t) + tB_{k-1}^{n-1}(t)\n\\end{align*}\n\\]\nTo show the linear independence, we observe that if:\n\\[\n\\begin{align*}\n\\sum_{i=0}^n a_i B_i^{n}(t) &\\equiv 0\n\\end{align*}\n\\tag{7}\\]\nfor all \\(t \\in [0,1]\\). Then, expanding and substituting \\(t=1\\) in the above expression, we have:\n\\[\n\\begin{align*}\n\\sum_{i=0}^n a_i B_i^{n}(i) &= 0\\\\\na_0 (1-t)^n + a_1 {n \\choose 1} (1-t)^{n-1}t + \\ldots + a_n t^n &= 0\\\\\na_n &= 0\n\\end{align*}\n\\]\nSubstituting \\(a_n = 0\\) in Equation 7, we get:\n\\[\n\\begin{align*}\na_0 (1-t)^n + a_1 {n \\choose 1} (1-t)^{n-1} t + \\ldots + a_{n-1}(1-t)t^{n-1} &= 0\\\\\na_0 (1-t)^{n-1} + a_1 {n \\choose 1} (1-t)^{n-2} t + \\ldots + a_{n-1}t^{n-1} &= 0\n\\end{align*}\n\\]\nAgain, subbing \\(t=1\\), we find that \\(a_{n-1}=0\\). By repeatedly dividing by \\((1-t)\\) and using the same argument, we find that:\n\\[\na_0 = a_1 = \\ldots = a_n = 0\n\\]\nThis closes the proof. \\(\\blacksquare\\)\nThe unique parametric Bezier curve corresponding to a given set of \\(n+1\\) control points \\(p_i\\), \\(i=0:n\\), equals:\n\\[\n\\begin{align*}\nc(t) = \\sum_{i=0}^n p_i B_i^{n}(t), \\quad t\\in[0,1]\n\\end{align*}\n\\tag{8}\\]\nwhere \\(B_i^{n}(t)\\) are Bernstein polynomials of degree \\(n\\). By property 3, in Theorem 2, the Bezier curve interpolates the first and last control points \\(p_0\\) and \\(p_n\\). Often, a curve is constructed by smoothly patching together several Bezier curves of low order.\n\n\nIntuition\n\nThe case \\(n=1\\)\nImagine a particle travelling in a straight line joining the points \\(p_0=(x_0,y_0)\\) and \\(p_1=(x_1,y_1)\\), where the time \\(t\\in[0,1]\\). Let’s compute the position \\(c(t)=(x(t),y(t))\\) of the particle at time \\(t\\). The point \\(c(t)\\) divides the straight-line \\(p_0p_1\\) in the proportion \\(t:(1-t)\\). By the section formula, the position vector of \\(c(t)\\) is:\n\\[\n\\begin{align*}\n(x(t),y(t)) &= ((1-t)x_0 + tx_1, (1-t)y_0 + ty_1)\\\\\n&= (1-t)(x_0,y_0) + t(x_1,y_1)\\\\\n&= (1-t)p_0 + tp_1\n\\end{align*}\n\\]\nThis is the parametric equation of motion of the particle. It is the Bezier curve for \\(n=1\\). The points \\(p_0\\) and \\(p_1\\) control what the straight-line path looks like, so they are called control-points.\n\nusing Plots\nusing LaTeXStrings\n\nfunction bezier_1(t, a, b)\n    @. (1-t)*a + t*b\nend\n\ntvec = range(0.0, 1.0, 101)\npts = [\n    0.0 1.0;\n    1.0 0.0;\n]\n\nx = bezier_1(tvec, pts[1,1], pts[2,1])\ny = bezier_1(tvec, pts[1,2], pts[2,2])\n\n@gif for (xVal, yVal) in zip(x,y)\n    plot(x, y, line=(:path,:dash,:gray))\n    scatter!([xVal], [yVal], marker=(:circle,3,:green,:green))\nend\n\n[ Info: Saved animation to C:\\Data\\dev\\repo\\quantinsights.github.io\\posts\\interpolation-and-approximation\\tmp.gif\n\n\n\n\n\n\n\n\nThe case \\(n=2\\)\nAssume that, we have 3 points \\(p_0=(0.0, 1.0)\\), \\(p_1=(1.3, 1.3)\\) and \\(p_2=(1.0, 0.0)\\). Imagine a red particle \\(p_R\\) moving from \\(p_0\\) towards \\(p_1\\) on a straight-line, a blue particle \\(p_B\\) moving from \\(p_1\\) towards \\(p_2\\). Suppose, a third green particle \\(p_G\\) moves along the straight-line joining the blue and green particles instantaneously:\n\n\nShow the code\nfunction bezier_2(t, a, b, c)\n    @. (1-t)^2 *a + 2*(1-t)*t*b + t^2 * c\nend\n\npts = [\n    0.0 1.0;\n    1.3 1.3;\n    1.0 0.0;\n]\n\nxRed = bezier_1(tvec, pts[1,1], pts[2,1])\nyRed = bezier_1(tvec, pts[1,2], pts[2,2])\n\nxBlue = bezier_1(tvec, pts[2,1], pts[3,1])\nyBlue = bezier_1(tvec, pts[2,2], pts[3,2])\n\nxGreen = bezier_2(tvec, pts[1,1], pts[2,1], pts[3,1])\nyGreen = bezier_2(tvec, pts[1,2], pts[2,2], pts[3,2])\n\n@gif for ((xr, yr),(xb,yb),(xg,yg)) in zip(zip(xRed,yRed),zip(xBlue,yBlue),zip(xGreen,yGreen))\n        plot(xRed, yRed, line=(:path,:dash,:gray))\n        scatter!([xr], [yr], marker=(:circle,3,:red,:red))\n        plot!(xBlue, yBlue, line=(:path,:dash,:gray))\n        scatter!([xb], [yb], marker=(:circle,3,:blue,:blue))\n        x = bezier_1(tvec, xr, xb)\n        y = bezier_1(tvec, yr, yb)\n        plot!(x, y, line=(:path,:dash,:gray))\n        plot!(xGreen, yGreen, line=(:path,:solid,:gray))\n        scatter!([xg], [yg], marker=(:circle,3,:green,:green))\nend\n\n\n[ Info: Saved animation to C:\\Data\\dev\\repo\\quantinsights.github.io\\posts\\interpolation-and-approximation\\tmp.gif\n\n\n\n\n\nHow might we compute the trajectory of the green particle? By a double-application of the section formula, we have:\n\\[\n\\begin{align*}\np_G(t) &= (1-t)\\cdot p_R(t) + t\\cdot p_B(t)\\\\\n&= (1-t)((1-t)p_0 + tp_1) + t((1-t)p_1 + t p_2)\\\\\n&= (1-t)^2 p_0 +2t(1-t)p_1 + t^2 p_2\n\\end{align*}\n\\]\nThus, the trajectory of the green particle is a quadratic Bezier curve with \\(n+1=3\\) control points. The quadratic Bezier curve interpolates between the points \\(p_0\\) and \\(p_2\\), whereas \\(p_1\\) is an off-curve point.\nComputer graphics(CG) aficionados reserve the term control point for an off-curve point such as \\(p_1\\) and refer to the on-curve points, as anchor points. In CG editors such as Adobe Illustrator, not all control points are known in advance. The shape of the quadratic curve is controlled by moving around the control points using the Pen tool (Bezier tool) until the curve has the desired shape. Thus, using the Bernstein basis to represent degree 2 polynomials is advantageous. Moving \\(p_1\\) has a direct and intuitive effect on the curve.\nThe Bezier polygon is the closed piecewise linear curve connecting the control points \\(p_i\\) and \\(p_{i+1}\\), \\(i=0:n-1\\) and finally \\(p_n\\) and back to \\(p_0\\). This polygon provides a rough idea of the shape of the curve. From the definition(Equation 8) of the Bezier curve, it follows that for all \\(t\\in[0,1]\\), the curve \\(c(t)\\) is a convex combination of the control points. Therefore, \\(c(t)\\) lies within the convex hull of the control points.\n\nTheorem 3 The Bezier curve \\(c(t)\\) is tangent to \\(p_1- p_0\\) and \\(p_n - p_{n-1}\\) for \\(t=0\\) and \\(t=1\\) respectively.\n\nProof."
  },
  {
    "objectID": "posts/interpolation-and-approximation/index.html#spline-functions",
    "href": "posts/interpolation-and-approximation/index.html#spline-functions",
    "title": "Interpolation and Approximation",
    "section": "Spline Functions",
    "text": "Spline Functions\nThe mathematical concept of spline functions was introduced in 1946 by Schoenberg. The importance of the B-spline basis for approximation was also first appreciated by Schoenberg. Today, B-Splines enable the mathematical representation of surfaces far beyond hand-techniques. In aircraft design computations, they may involve more than \\(50,000\\) data points.\n\nLinear and Cubic Splines\nWe start by formally defining a spline function of order \\(k \\geq 1\\).\n\nDefinition 1 A spline function \\(S(x)\\) of order \\(k \\geq 1\\) (degree \\(k-1 \\geq 0\\)), on a grid\n\\[\n\\Delta = \\{a=x_0 &lt; x_1 &lt; \\ldots &lt; x_n = b\\}\n\\]\nof distinct knots is a real function \\(s\\) with the following properties:\n\nFor \\(x \\in [x_i, x_{i+1}]\\), \\(i=0:m-1\\), \\(S(x)\\) is a polynomial of degree \\(&lt;k\\)."
  },
  {
    "objectID": "posts/ito_calculus/index.html",
    "href": "posts/ito_calculus/index.html",
    "title": "Ito Calculus",
    "section": "",
    "text": "Exercise 1 (A strange martingale) Let \\((B_t,t\\geq 0)\\) be a standard Brownian motion. Consider the process:\n\\[\nM_t = \\frac{1}{\\sqrt{1-t}}\\exp\\left(\\frac{-B_t^2}{2(1-t)}\\right), \\quad \\text{ for }0 \\leq t &lt; 1\n\\]\n\nShow that \\(M_t\\) can be represented by:\n\n\\[\nM_t = 1 + \\int_0^t \\frac{-B_s M_s}{1-s}dB_s, \\quad \\text{ for } 0 \\leq t \\leq 1\n\\]\n\nDeduce from the previous question that \\((M_s,s \\leq t)\\) is a martingale for \\(t &lt; 1\\) and for the Brownian filtration.\nShow that \\(\\mathbb{E}[M_t] = 1\\) for all \\(t &lt; 1\\).\nProve that \\(\\lim_{t \\to 1^-} M_t = 0\\) almost surely.\n\n\nSolution.\nLet \\(f(t,x) = \\frac{1}{\\sqrt{1-t}}e^{-\\frac{x^2}{2(1-t)}}\\). We have:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{\\sqrt{1-t}\\cdot e^{-\\frac{x^2}{2(1-t)}}\\cdot \\frac{x^2}{2(1-t)^2}\\cdot (-1) - e^{-\\frac{x^2}{2(1-t)}}\\cdot \\frac{1}{2\\sqrt{1-t}}\\cdot(-1)}{(1-t)}\\\\\n&=\\frac{e^{-\\frac{x^2}{2(1-t)}}}{(1-t)}\\left( -\\frac{x^2}{2(1-t)^{3/2}}+\\frac{1-t}{2(1-t)^{3/2}}\\right)\\\\\n&= \\frac{e^{-\\frac{x^2}{2(1-t)}}((1-t) - x^2)}{2(1-t)^{5/2}}\n\\end{align*}\n\\]\nAlso, the first and second derivatives with respect to the space variable \\(x\\) are:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial x} &= \\frac{1}{\\sqrt{1-t}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\\left(-\\frac{x}{(1-t)}\\right)\\\\\n&= -\\frac{x}{(1-t)^{3/2}} \\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x^2} &= - \\frac{1}{(1-t)^{3/2}}\\left\\{\\exp\\left(-\\frac{x^2}{2(1-t)}\\right) + x\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\\left(-\\frac{x}{(1-t)}\\right)\\right\\}\\\\\n&= - \\frac{1}{(1-t)^{3/2}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right) \\left\\{1 - \\frac{x^2}{(1-t)}\\right\\}\\\\\n&= \\frac{(x^2 - (1-t))}{(1-t)^{5/2}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\n\\end{align*}\n\\]\nBy Ito’s formula:\n\\[\n\\begin{align*}\ndf(t,B_t) &= (f_t + \\frac{1}{2}f_{xx})dt + f_x dB_t\\\\\ndM_t &= -\\frac{B_t}{(1-t)} \\frac{e^{-\\frac{B_t^2}{2(1-t)}}}{\\sqrt{1-t}}\\\\\n&= -\\frac{B_t M_t}{(1-t)}\n\\end{align*}\n\\]\nwhere in the second step, we used the fact that \\(f_t + \\frac{1}{2}f_{xx} = 0\\), so the \\(dt\\) term is zero.\nIn the integral form:\n\\[\n\\begin{align*}\nM_t - M_0 &= \\int \\frac{- B_s M_s}{1 - s}dB_s\\\\\nM_t &= 1 + \\int \\frac{- B_s M_s}{1 - s}dB_s\n\\end{align*}\n\\]\n\nThe Ito integral \\(\\int_0^t \\frac{-B_s M_s}{1-s}dB_s\\) is a continuous martingale with respect to the brownian filtration. Hence \\((M_t,t \\geq 0)\\) is a martingale.\nThe expectation of an Ito integral is zero. Hence, \\(\\mathbb{E}[M_t] = 1\\) for all \\(t &lt; 1\\).\nUsing Markov’s inequality followed by (concave) Jensen’s inequality, because we know that \\(\\mathbb{E}[B_t^2]=t\\), \\(\\mathbb{E}[B_t^4]=3t^2\\) and \\(Var(B_t^2) = 2t^2\\) for any \\(\\epsilon &gt; 0\\):"
  },
  {
    "objectID": "posts/ito_calculus/index.html#exercises",
    "href": "posts/ito_calculus/index.html#exercises",
    "title": "Ito Calculus",
    "section": "",
    "text": "Exercise 1 (A strange martingale) Let \\((B_t,t\\geq 0)\\) be a standard Brownian motion. Consider the process:\n\\[\nM_t = \\frac{1}{\\sqrt{1-t}}\\exp\\left(\\frac{-B_t^2}{2(1-t)}\\right), \\quad \\text{ for }0 \\leq t &lt; 1\n\\]\n\nShow that \\(M_t\\) can be represented by:\n\n\\[\nM_t = 1 + \\int_0^t \\frac{-B_s M_s}{1-s}dB_s, \\quad \\text{ for } 0 \\leq t \\leq 1\n\\]\n\nDeduce from the previous question that \\((M_s,s \\leq t)\\) is a martingale for \\(t &lt; 1\\) and for the Brownian filtration.\nShow that \\(\\mathbb{E}[M_t] = 1\\) for all \\(t &lt; 1\\).\nProve that \\(\\lim_{t \\to 1^-} M_t = 0\\) almost surely.\n\n\nSolution.\nLet \\(f(t,x) = \\frac{1}{\\sqrt{1-t}}e^{-\\frac{x^2}{2(1-t)}}\\). We have:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{\\sqrt{1-t}\\cdot e^{-\\frac{x^2}{2(1-t)}}\\cdot \\frac{x^2}{2(1-t)^2}\\cdot (-1) - e^{-\\frac{x^2}{2(1-t)}}\\cdot \\frac{1}{2\\sqrt{1-t}}\\cdot(-1)}{(1-t)}\\\\\n&=\\frac{e^{-\\frac{x^2}{2(1-t)}}}{(1-t)}\\left( -\\frac{x^2}{2(1-t)^{3/2}}+\\frac{1-t}{2(1-t)^{3/2}}\\right)\\\\\n&= \\frac{e^{-\\frac{x^2}{2(1-t)}}((1-t) - x^2)}{2(1-t)^{5/2}}\n\\end{align*}\n\\]\nAlso, the first and second derivatives with respect to the space variable \\(x\\) are:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial x} &= \\frac{1}{\\sqrt{1-t}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\\left(-\\frac{x}{(1-t)}\\right)\\\\\n&= -\\frac{x}{(1-t)^{3/2}} \\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x^2} &= - \\frac{1}{(1-t)^{3/2}}\\left\\{\\exp\\left(-\\frac{x^2}{2(1-t)}\\right) + x\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\\left(-\\frac{x}{(1-t)}\\right)\\right\\}\\\\\n&= - \\frac{1}{(1-t)^{3/2}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right) \\left\\{1 - \\frac{x^2}{(1-t)}\\right\\}\\\\\n&= \\frac{(x^2 - (1-t))}{(1-t)^{5/2}}\\exp\\left(-\\frac{x^2}{2(1-t)}\\right)\n\\end{align*}\n\\]\nBy Ito’s formula:\n\\[\n\\begin{align*}\ndf(t,B_t) &= (f_t + \\frac{1}{2}f_{xx})dt + f_x dB_t\\\\\ndM_t &= -\\frac{B_t}{(1-t)} \\frac{e^{-\\frac{B_t^2}{2(1-t)}}}{\\sqrt{1-t}}\\\\\n&= -\\frac{B_t M_t}{(1-t)}\n\\end{align*}\n\\]\nwhere in the second step, we used the fact that \\(f_t + \\frac{1}{2}f_{xx} = 0\\), so the \\(dt\\) term is zero.\nIn the integral form:\n\\[\n\\begin{align*}\nM_t - M_0 &= \\int \\frac{- B_s M_s}{1 - s}dB_s\\\\\nM_t &= 1 + \\int \\frac{- B_s M_s}{1 - s}dB_s\n\\end{align*}\n\\]\n\nThe Ito integral \\(\\int_0^t \\frac{-B_s M_s}{1-s}dB_s\\) is a continuous martingale with respect to the brownian filtration. Hence \\((M_t,t \\geq 0)\\) is a martingale.\nThe expectation of an Ito integral is zero. Hence, \\(\\mathbb{E}[M_t] = 1\\) for all \\(t &lt; 1\\).\nUsing Markov’s inequality followed by (concave) Jensen’s inequality, because we know that \\(\\mathbb{E}[B_t^2]=t\\), \\(\\mathbb{E}[B_t^4]=3t^2\\) and \\(Var(B_t^2) = 2t^2\\) for any \\(\\epsilon &gt; 0\\):"
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html",
    "href": "posts/multivariate_ito_calculus/index.html",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "We can generalize the theory to functions of several brownian motions. This unleashes the full power of Ito calculus.\n\n\n\nDefinition 1 (Brownian motion in \\(\\mathbf{R}^{d}\\).) Take \\(d\\in\\mathbf{N}\\). Let \\(B^{(1)},\\ldots,B^{(d)}\\) be independent standard Brownian motions in \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). The process \\((B_{t}:t\\geq0)\\) taking values in \\(\\mathbf{R}^{d}\\) defined by :\n\\[\\begin{aligned}\nB_{t} & =(B_{t}^{(1)},\\ldots,B_{t}^{(d)}),\\quad t\\geq0\n\\end{aligned}\\]\nis called a \\(d-\\)dimensional Brownian motion or a Brownian motion in \\(\\mathbf{R}^{d}\\).\n\nThe Brownian filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) is now composed of the information of all Brownian motions. In other words, it is given by the sigma-fields:\n\\[\\begin{aligned}\n\\mathcal{F}_{t} & =\\sigma(B_{s}^{(i)},1\\leq i\\leq d,s\\leq t)\n\\end{aligned}\\]\nFor every outcome \\(\\omega\\), the path of trajectory of a \\(d-\\)dimensional Brownian motion is a curve in space parametrized by the time \\(t\\):\n\\[\\begin{aligned}\nt\\mapsto B_{t}(\\omega) & =(B_{t}^{(1)}(\\omega),B_{t}^{(2)}(\\omega),\\ldots,B_{t}^{(d)}(\\omega))\n\\end{aligned}\\]\nOf course, this curve is continuous, since each coordinate is. The below numerical project gives an example of one path of a two-dimensional brownian motion. This is a very rugged and intertwined curve! We might wonder, what it does as \\(t\\to\\infty\\). Does it wander around \\((0,0)\\) ad infinitum or does it eventually escape to infinity? We will answer this question in a later section. For doing so, we shall need a version of Ito’s formula for multi-dimensional Brownian motion. We finish this section by noticing that it is also easy to construct Brownian motions in higher dimensions for which the coordinates are correlated.\n\n\nConsider a two-dimensional Brownian motion \\((B_{t}^{(1)},B_{2}^{(2)})\\) starting at \\((0,0)\\).\nLet’s plot one path of this Brownian motion on the plane \\(\\mathbf{R}^{2}\\) on the plane in \\(\\mathbf{R}^{2}\\) on the time interval \\([0,5]\\) using a discretization of \\(0.001\\).\nWe define BrownianMotion2D class.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nclass BrownianMotion2D:\n    \"\"\"\n    Generate sample paths for a 2d-brownian motion\n    \"\"\"\n    def __init__(\n        self,\n        endpoint : float = 1.0,\n        discretization : float = 0.01\n    ):\n        self.endpoint = endpoint\n        self.discretization = discretization\n\n    def generate_paths(self, num_paths : int):\n        steps = int(self.endpoint/self.discretization)\n        delta_z = np.sqrt(self.discretization) * np.random.randn(num_paths,2,steps)\n        \n        # paths has shape [num_paths,2,num_steps]\n        paths = np.cumsum(delta_z,axis=2)\n\n        # Brownian motion has position (0,0) at time t=0\n        # Append initial position \n        init_position = np.zeros((num_paths,2,1))\n        paths = np.append(init_position,paths,axis=2)\n\n        return paths\n\nWe can then configure and use BrownianMotion2D objects as we please.\n\n\nShow the code\nbm = BrownianMotion2D(endpoint=5.0,discretization=0.001)\npaths = bm.generate_paths(num_paths=1)\npath = paths[0]\n\n\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.title(r'2D-Brownian motion')\nplt.grid(True)\nplt.plot(path[0],path[1],linewidth=1,color='blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1 (Brownian motion with correlated coordinates) Let \\((B_{t}:t\\geq0)\\) be a two dimensional brownian motion. Let \\(-1&lt;\\rho&lt;1\\). We construct the two dimensional process as follows: \\(W_{t}=(W_{t}^{(1)},W_{t}^{(2)})\\) where:\n\\[\\begin{aligned}\nW_{t}^{(1)} & =B_{t}^{(1)}\\\\\nW_{t}^{(2)} & =\\rho B_{t}^{(1)}+\\sqrt{1-\\rho^{2}}B_{t}^{(2)}\n\\end{aligned}\\]\n\\(W_{t}^{(1)}=B_{t}^{(1)}\\) is Gaussian with mean \\(0\\) and variance \\(t\\). Since, \\(B_{t}^{(1)}\\) and \\(B_{t}^{(2)}\\)are independent gaussian random variables and the sum of IID Gaussians is Gaussian, \\(W_{t}^{(2)}\\) is Gaussian with mean \\(0\\) and variance \\(t\\). The covariance between \\(W_{t}^{(1)}\\) and \\(W_{t}^{(2)}\\) is:\n\\[\\begin{aligned}\n\\mathbf{E}[W_{t}^{(1)}W_{t}^{(2)}] & =\\mathbf{E}[B_{t}^{(1)}(\\rho B_{t}^{(1)}+\\sqrt{1-\\rho^{2}}B_{t}^{(2)})]\\\\\n& =\\mathbf{E}[\\rho(B_{t}^{(1)})^{2}+\\sqrt{1-\\rho^{2}}B_{t}^{(1)}B_{t}^{(2)}]\\\\\n& =\\rho t\n\\end{aligned}\\]\nHence, the coordinates at time \\(t\\) are not independent.\n\nConsider now the process \\((W_{t}:t\\geq0)\\) for \\(\\rho=1/2\\) as in example 2. Let’s plot one path of this process on the plane \\(\\mathbf{R}^{2}\\) on the time-interval \\([0,5]\\) using a discretization of \\(0.001\\).\n\nclass BrownianMotion2DWithCorrelatedCoords(BrownianMotion2D):\n    \"\"\"\n    Generate sample paths for a 2d-brownian motion\n    have correlated coordiantes\n    \"\"\"\n\n    def __init__(\n        self, \n        endpoint: float = 1.0, \n        discretization: float = 0.01, \n        rho: float = 0.0\n    ):\n        self.endpoint = endpoint\n        self.discretization = discretization\n        self.rho = rho\n\n    def generate_paths(self, num_paths: int):\n\n        # Call the base class generate_paths() method to generate a pair of\n        # independent brownian motion paths. It returns a tensor of\n        # shape (num_paths, 2, num_steps)\n        paths = super().generate_paths(num_paths=num_paths)\n\n        # Extract the brownian motions B1(t) and B2(t)\n        # B1(t) and B2(t) have dimensions [num_paths,num_steps]\n        x_coord = paths[:, 0, :]\n        y_coord = paths[:, 1, :]\n\n        # Apply the transformation\n        # W1(t) = B1(t)\n        # W2(t) = rho * B1(t) + sqrt(1 - rho**2) B2(t)\n        x_coord = x_coord\n        y_coord = self.rho * x_coord + np.sqrt(1 - self.rho**2) * y_coord\n\n        # Assemble back the x- and y- coordinates\n        x_coord = np.expand_dims(x_coord, axis=1)\n        y_coord = np.expand_dims(y_coord, axis=1)\n\n        # Return a tensor of shape [num_paths,d=2,num_steps]\n        return np.concatenate([x_coord, y_coord], axis=1)\n\nLet’s configure the BrownianMotion2DWithCorrelatedCoords engine with the settings endpoint=5.0, discretization=0.001 and rho=0.5 and generate a sample path.\n\n\nShow the code\nbm = BrownianMotion2DWithCorrelatedCoords(\n    endpoint=5.0,\n    discretization=0.001,\n    rho=0.50\n)\npaths = bm.generate_paths(num_paths=1)\npath = paths[0]\n\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.title(r'Correlated Brownian motion $W(t)=(W^{(1)}(t),W^{(2)}(t))$')\nplt.grid(True)\nplt.plot(path[0],path[1],linewidth=1,color='blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplt.xlabel(r'$t$')\nplt.ylabel(r'$W_t$')\nplt.title(r'Correlated Brownian motion paths')\nplt.grid(True)\nt = np.linspace(0.0,stop=5.0,num=5001)\nplt.plot(t,path[0],linewidth=0.75)\nplt.plot(t,path[1],linewidth=0.75)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1 (Ito’s Formula.) Let \\((B_{t}:t\\geq0)\\) be a \\(d-\\)dimensional brownian motion. Consider \\(f\\in\\mathcal{C}^{2}(\\mathbf{R}^{d})\\). Then, we have with probability one that for all \\(t\\geq0\\):\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{i=1}^{d}\\int_{0}^{t}\\partial_{x_{i}}f(B_{s})dB_{s}^{(i)}+\\frac{1}{2}\\int_{0}^{t}\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{s})ds\\label{eq:multidimensional-ito-formula}\n\\end{aligned}\\]\n\n\nRemark 1. I stress that, as in the one-dimensional case, Ito’s formula is an equality of processes (and not an equality in distribution). Thus, the processes on both sides must agree for each path.\n\nInterestingly, the mixed partials \\(\\partial_{x_{i}x_{j}}f(B_{s})\\), \\(i\\neq j\\) do not appear in the formula! We see from Ito’s formula that the process \\(f(B_{t})\\) can be represented as a sum of \\(d+1\\) processes: \\(d\\) Ito integrals and one Riemann integral (which is a process of finite variation). In vector notation, the formula takes the form:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\int_{0}^{t}\\nabla f(B_{s})^{T}dB_{s}+\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}f(B_{s})ds\n\\end{aligned}\\]\nwhere it is understood that the first term is the sum of the \\(d\\) Ito integrals in the equation. The symbol \\(\\nabla^{2}\\) is the Laplacian of \\(f\\):\n\\(\\sum_{i=1}^{d}\\frac{\\partial^{2}}{\\partial x_{i}^{2}}f(B_{s}^{(1)},\\ldots,B_{s}^{(d)})ds\\)\nIn differential form, Ito’s formula becomes very neat:\n\\[\\begin{aligned}\ndf(B_{t}) & =\\sum_{i=1}^{d}\\partial_{x_{i}}f(B_{s})dB_{t}^{(i)}+\\frac{1}{2}\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{s})dt=\\nabla f(B_{t})^{T}dB_{s}+\\frac{1}{2}\\nabla^{2}f(B_{t})dt\n\\end{aligned}\\]\n\nExample 2 Consider the functions (1) \\(f(x_{1},x_{2})=x_{1}^{2}+x_{2}^{2}\\) (2) \\(f(x_{1},x_{2})=e^{x_{1}}\\cos x_{2}\\) and the processes \\((X_{t}:t\\geq0)\\) and \\((Y_{t}:t\\geq0)\\). If we apply Ito’s formula to the first process, we have:\n\\[\\begin{aligned}\nX_{t} & =\\int_{0}^{t}2B_{s}^{(1)}dB_{s}^{(1)}+\\int_{0}^{t}2B_{s}^{(2)}dB_{s}^{(2)}+\\frac{1}{2}\\int_{0}^{t}(4dt)\\\\\n& =\\int_{0}^{t}2B_{s}^{(1)}dB_{s}^{(1)}+\\int_{0}^{t}2B_{s}^{(2)}dB_{s}^{(2)}+2t\n\\end{aligned}\\]\nThe second process gives:\n\\[\\begin{aligned}\nY_{t} & =\\cos B_{s}^{(2)}\\int_{0}^{t}e^{B_{s}^{(1)}}dB_{s}^{(1)}-e^{B_{s}^{(1)}}\\int\\sin B_{s}^{(2)}dB_{s}^{(2)}+\\frac{1}{2}\\int_{0}^{t}\\left(e^{B_{s}^{(1)}}\\cos B_{s}^{(2)}-e^{B_{s}^{(1)}}\\cos B_{s}^{(2)}\\right)dt\\\\\n& =1+\\cos B_{s}^{(2)}\\int_{0}^{t}e^{B_{s}^{(1)}}dB_{s}^{(1)}-e^{B_{s}^{(1)}}\\int\\sin B_{s}^{(2)}dB_{s}^{(2)}\n\\end{aligned}\\]\n\n\nExample 3 (Cross-Variation of \\(B_{t}^{(1)}\\) and \\(B_{t}^{(2)}\\)). Let \\((t_{j}:j\\leq n)\\) be a sequence of partitions of \\([0,t]\\) such that \\(\\max_{j}|t_{j+1}-t_{j}|\\to0\\) as \\(n\\to\\infty\\). Prove that:\n\\[\\begin{aligned}\n\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)}) & =0\\quad\\text{in } L^{2}\n\\end{aligned}\\]\nThis justifies the rule \\(dB_{t}^{(1)}\\cdot dB_{t}^{(2)}=0\\).\nHint: Just compute the second moment of the sum.\n\n\nSolution 1. We have:\n\\[\\begin{aligned}\n& \\mathbf{E}\\left[\\left(\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})\\right)^{2}\\right]\\\\\n= & \\sum_{j=0}^{n-1}\\mathbf{E}[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})^{2}(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})^{2}]\\\\\n+ & 2\\sum_{j&lt;k}\\mathbf{E}\\left[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{k+1}}^{(1)}-B_{t_{k}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})(B_{t_{k+1}}^{(2)}-B_{t_{k}}^{(2)})\\right]\n\\end{aligned}\\]\nBoth these expectations are zero, since the brownian motions are independent and non-overlapping increments are independent.\nConsequently, \\(\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})\\to0\\) in the \\(L^{2}\\) sense.\n\n\nProof. The proof of the formula follows the usual recipe: Taylor’s theorem together with the quadratic variation and the cross-variation. In this case, we do get a cross-variation between the different Brownian motions. More precisely, consider a partition \\((t_{j}:j\\leq n)\\) of \\([0,t]\\). Then we can write:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{j=0}^{n-1}(f(B_{t_{j+1}})-f(B_{t_{j}}))\n\\end{aligned}\\]\nWe can apply the Taylor’s series expansion for each \\(j\\) to get:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{j=0}^{n-1}\\nabla f(B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})\\\\\n& +\\frac{1}{2}\\sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})^{T}Hf(B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})+\\mathcal{E}\n\\end{aligned}\\]\nwhere \\(Hf\\) is the Hessian matrix of \\(f\\). We wrote the expansion using the vector notation to be economical. Let’s keep in mind that each term is a sum over the derivatives. The first term will converge to \\(d\\) Ito integrals as in the one-dimensional case. Now, the summand in the second term is:\n\\[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)},\\ldots,B_{t_{j+1}}^{(d)}-B_{t_{j}}^{(d)})\\left[\\begin{array}{ccc}\n\\partial_{x_{1}}^{2}f(B_{t_{j}}) & \\ldots & \\partial_{x_{1}x_{d}}^{2}f(B_{t_{j}})\\\\\n\\vdots & \\ddots\\\\\n\\partial_{x_{d}x_{1}}^{2}f(B_{t_{j}}) &  & \\partial_{x_{d}}^{2}f(B_{t_{j}})\n\\end{array}\\right]\\left[\\begin{array}{c}\nB_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)}\\\\\n\\vdots\\\\\nB_{t_{j+1}}^{(d)}-B_{t_{j}}^{(d)}\n\\end{array}\\right]\\]\nSo, \\((B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})\\) is pre-multiplied with the term \\(\\partial_{x_{i}x_{k}}^{2}f(B_{t_{j}})\\) and it is post-multiplied \\((B_{t_{j+1}}^{(k)}-B_{t_{j}}^{(k)})\\). Consequently, the second term in the Taylor’s series expansion can be re-written as:\n\\[\\sum_{j=0}^{n-1}\\left(\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{t_{j}})(B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})^{2}+\\sum_{1\\leq i&lt;k\\leq d}\\partial_{x_{i}x_{k}}^{2}f(B_{t_{j}})(B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})(B_{t_{j+1}}^{(k)}-B_{t_{j}}^{(k)})\\right)\\]\nThe second term on the right converges to \\(0\\) in the \\(L^{2}\\) sense when \\(i\\neq k\\), from exercise. This explains why the mixed derivatives disappear in the multi-dimensional Ito’s formula. As for the case \\(i=k\\), it reduces to the quadratic variation as in the one-dimensional case. This is where the Riemann integral arises, after suitable conditioning on \\(\\mathcal{F}_{t_{j}}\\), the sigma-field generated by \\(B_{s}\\), \\(s\\leq t_{j}\\).\n\nAs in the one-dimensional case,it is not necessary to learn Ito’s formula by heart. It suffices to write the differential of the function \\(f\\) to second order. We can then apply the rules of multivariate Ito calculus:\n\n\n\n\\(\\cdot\\)\n\\(dt\\)\n\\(dB_{t}^{(1)}\\)\n\\(dB_{t}^{(2)}\\)\n\\(\\ldots\\)\n\n\n\n\n\\(dt\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(dB_{t}^{(1)}\\)\n\\(0\\)\n\\(dt\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(dB_{t}^{(2)}\\)\n\\(0\\)\n\\(0\\)\n\\(dt\\)\n\\(0\\)\n\n\n\\(\\ldots\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(dt\\)\n\n\n\nNote that the rule \\(dB_{t}^{(i)}dB_{t}^{(j)}=0\\) for \\(i\\neq j\\) is being motivated by the cross-variation result.\nHow can we construct martingales using the Ito’s formula? Recall that an Ito integral \\((\\int_{0}^{t}X_{s}dB_{s},t\\leq T)\\) is a martingale whenever the integrand is in \\(\\mathcal{L}_{c}^{2}(T)\\), the space of adapted processes with continuous paths and for which:\n\\[\\begin{aligned}\n\\int_{0}^{T}\\mathbf{E}[X_{s}^{2}]ds & &lt;\\infty\n\\end{aligned}\\]\nThe only difference here is that the integrand is a function of many Brownian motions. However, the integrands involved in the Ito integrals of the multidimensional Ito’s formula are clearly adapted to the filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) of \\((B_{t}:t\\geq0)\\) as they are functions of the Brownian motion at the time. The arguments of Ito integral in and apply verbatim, if we take the definition of \\(\\mathcal{L}_{c}^{2}(t)\\) with the filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) of \\((B_{t}:t\\geq0)\\). With this in mind, we have the following corollary.\n\n\n\nCorollary 1 (Brownian Martingales.) Let \\((B_{t}:t\\geq0)\\) be a Brownian motion in \\(\\mathbf{R}^{d}\\). Consider \\(f\\in\\mathcal{C}^{2}(\\mathbf{R}^{d})\\) such that processes \\((\\partial_{x {i}}f(B_{t}),t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\) for every \\(i\\leq d\\). Then, the process :\n\\[f(B_{t})-\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}f(B_{s})ds,\\quad t\\leq T\\]\nwhere \\(\\nabla^{2}=\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}\\) is the Laplacian, is a martingale for the Brownian filtration.\n\nFor example, consider the processes \\(X_{t}=(B_{t}^{(1)})^{2}+(B_{t}^{(2)})^{2}\\) and \\(Y_{t}=\\exp(B_{t}^{(1)})\\cos(B_{t}^{(2)})\\). Then, we have :\n\\[\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}X_{s}ds=\\frac{1}{2}\\int_{0}^{t}4ds=2t\\]\nand\n\\[\\begin{aligned}\n\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}Y_{s}ds & =\\frac{1}{2}\\int_{0}^{t}0\\cdot ds=0\n\\end{aligned}\\]\nThus, the processes \\(X_{t}-2t\\) and \\(Y_{t}\\) are martingales for the Brownian filtration. In one dimension, there are no interesting martingales constructed with functions of space only. Indeed, \\((f(B_{t}):t\\geq0)\\) is a martingale if and only if \\(f''(x)=0\\) for all \\(x\\). But, such functions are of the form \\(f(x)=ax+b\\), \\(a,b\\in\\mathbf{R}\\). In other words, in one dimension, Brownian martingales of the form \\(f(B_{t})\\) are simply \\(aB_{t}+b\\). Not very surprising! The situation is very different in higher dimensions. Indeed, corollary implies that \\(f(B_{t})\\) is a martingale whenever \\(f\\) is a harmonic function:\n\nDefinition 2 (Harmonic function.) A function \\(f:\\mathbf{R}^{d}\\to\\mathbf{R}\\) is harmonic in \\(\\mathbf{R}^{d}\\) if and only if \\(\\nabla^{2}f(x)\\equiv0\\) for all \\(x\\in\\mathbf{R}^{d}\\). More generally, a function \\(f:\\mathbf{R}^{d}\\to\\mathbf{R}\\) is harmonic in the region \\(\\mathcal{O}\\subset\\mathbf{R}^{d}\\) if and only if \\(\\nabla^{2}f(x)\\equiv0\\) for all \\(x\\in\\mathcal{O}\\).\n\nNote that the function \\(f(x)=e^{x_{1}}\\cos x_{2}\\) is harmonic in \\(\\mathbf{R}^{d}\\). This is why the process \\(Y_{t}=\\exp(B_{t}^{(1)})\\cos(B_{t}^{(2)})\\) is a martingale. The distinction to a subset of \\(\\mathbf{R}^{d}\\) in the above definition is important since it may happen that the function is harmonic only in a subset of the space; see for example equation. It is possible to define a Brownian martingale in such cases by considering the process until it exits the region. This will turn out to be important as we move ahead.\nThe multidimensional Ito’s formula generalizes to functions of time and space as in proposition:\n\nDefinition 3 A function \\(f:[0,\\infty)\\times\\mathbf{R}^{d}\\to\\mathbf{R}\\) is in \\(\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R}^{d})\\) if the partial derivative in time :\n\\[\\frac{\\partial}{\\partial t}f(t,\\mathbf{x})\\]\nexists and is continuous and the second order partial derivatives in space:\n\\[\\frac{\\partial^{2}}{\\partial x_{i}^{2}}f(t,x_{1},x_{2},\\ldots,x_{i},\\ldots,x_{d}),\\quad1\\leq i\\leq d\\]\nexist and are continuous.\n\n\n\n\n\n\nTheorem 2 (Multidimensional Ito’s formula for functions of space and time) Let \\((B_{t}:t\\leq T)\\) be a \\(d\\)-dimensional Brownian motion. Consider a function \\(f\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R}^{d})\\). Then, we have with probability one for all \\(t\\leq T\\):\n\\[\\begin{aligned}\nf(t,B_{t})-f(0,B_{0}) & =\\sum_{i=1}^{d}\\int_{0}^{t}\\partial_{x_{i}}f(s,B_{s})dB_{s}^{(i)}+\\int_{0}^{t}\\left(\\partial_{t}f(s,B_{s})+\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(s,B_{s})\\right)ds\n\\end{aligned}\\]\n\nThe martingale condition is then similar to the ones in corollary ([cor:brownian-martingales-in-Rd]): if the processes \\((\\partial_{x_{i}}f(s,B_{s}),t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\) for every \\(1\\leq i\\leq d\\), then the process\n\\[f(t,B_{t})-\\int_{0}^{t}\\left\\{ \\partial_{t}f(s,B_{s})++\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(s,B_{s})\\right\\} ds,\\quad t\\leq T \\tag{1}\\]\nis a martingale for the Brownian filtration. In particular, if \\(f\\) satisfies the partial differential equation:\n\\[\n\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\nabla^{2}f & =0\n\\end{aligned}\n\\tag{2}\\]\nthen the process \\((f(t,B_{t}):t\\leq T)\\) itself is a martingale.\n\n\n\nConsider a simple random walk on the integer lattice \\(\\mathbb{Z}^d\\). At each time step, a random walker makes a random move of length one in one of the lattice directions.\nWe say that, if the random walk returns to the origin with probability one infinitely many times, it is recurrent, if it does not we will call it transient.\nIt’s worth to spend a moment visiting Polya’s Random Walk Theorem, a fascinating, but not so intuitive result.\nAssuming all random walks start at the origin, we define \\(u\\) to be the probability that a random walker returns to the origin. The probability that the random walker returns to the origin exactly \\(m\\) times is:\n\\[\\begin{align*}\n{m \\choose m-1} u^{m-1} (1 - u)^{m - (m - 1)} = u^{m-1}(1-u)\n\\end{align*}\\]\nThe expected number of times the particle returns to the origin for an infinite random walk is:\n\\[\\begin{align*}\nE &= \\sum_{m = 1}^{\\infty} m (u^{m-1}(1-u))\\\\\n&= (1-u)\\sum_{m=1}^{\\infty}mu^{m-1}\\\\\n&= (1-u)\\sum_{m=1}^{\\infty}\\frac{d}{du}u^{m}\\\\\n\\end{align*}\\]\nWe note that limit exchanging is acceptable since the power series \\(\\sum_{n=1}^{\\infty}x^n\\) converges uniformly for \\(u &lt; 1\\) and we can apply the differentiable limit theorem.\nSo we obtain:\n\\[\\begin{align*}\nE &= (1-u)\\frac{d}{du}\\sum_{m = 1}^{\\infty} u^m \\\\\n&= (1-u)\\cdot \\frac{1}{(1-u)^2}\\\\\n&= \\frac{1}{1-u}\n\\end{align*}\\]\nSo, we see that if \\(E\\) is finite, \\(u &lt; 1\\), then the walk is transient and if \\(E = +\\infty\\), \\(u=1\\), then the walk is recurrent.\nWe then define \\(u_n\\) to be the probability that a given walk is at the origin on the \\(n\\)th step, defining the value \\(u_0=1\\) for the trivial loop. We also introduce an indicator random variable \\(x_n\\), which takes the value \\(1\\), if the particle is at the origin on the \\(n\\)th step and zero otherwise.\nThen\n\\[\\begin{align*}\nT = \\sum_{n=1}^{\\infty}x_n\n\\end{align*}\\]\nis the total number of times, the particle is at the origin, so \\(E\\) is equal to the expectation of \\(T\\), \\(\\mathbb{E}(T)\\), which is equal to:\n\\[\\begin{align*}\n\\mathbb{E}[T] &= \\sum_{n=1}^{\\infty}\\mathbb{E}[x_n]\\\\\n&= \\sum_{n=1}^{\\infty} u_n\n\\end{align*}\\]\nBut, we showed previously that if \\(E\\) is finite, then the walk is transient and if \\(E = \\infty\\), then the walk is recurrent, so we have established that if:\n\\[\\begin{align*}\n\\sum_{n=1}^{\\infty}u_n\n\\end{align*}\\]\nconverges then the walk is transient and if the sum diverges, then the walk is recurrent.\n\n\nNow that we have built up the necessary tools, we will consider the cases. We start by considering a simple random walk on \\(\\mathbb{Z}^1\\). Since, as mentioned previously, a walk must have an even number of steps to be a loop, we only look at \\(u_{2n},n \\in \\mathbb{Z}^+\\). A path of length \\(2n\\) returning to the origin must have \\(n\\) up-moves and \\(n\\) down-moves. The number of such paths are \\({2n \\choose n}\\). Each such path has a probability of occurrence \\(\\frac{1}{2^{2n}}\\). Thus,\n\\[\\begin{align*}\nu_{2n} = \\frac{1}{2^{2n}}{2n \\choose n}\n\\end{align*}\\]\nUsing Stirling’s approximation \\(n! \\sim \\sqrt{2\\pi n} e^{-n} n^{n}\\), we obtain:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{2^{2n}} \\frac{(2n)!}{n! n!}\\\\\n&\\approx \\frac{1}{2^{2n}} \\cdot \\frac{\\sqrt{2\\pi(2n)} e^{-2n}(2n)^{2n}}{\\left(\\sqrt{2\\pi n} e^{-n} n^n\\right)^2}\\\\\n&=\\frac{1}{\\sqrt{\\pi n}}\n\\end{align*}\\]\nSo, we see that:\n\\[\\begin{align*}\n\\sum_{n=0}^{\\infty} u_{2n} \\approx \\sum_{n=0}^{\\infty} \\frac{1}{\\sqrt{n\\pi}}\n\\end{align*}\\]\nAnd the series on the right certainly diverges so, we see that a simple random walk in \\(\\mathbb{Z}^1\\) is recurrent since \\(E\\) diverges.\n\n\n\nA particle has an equal probability \\(1/4\\) of moving left, right, up or down randomly in the \\(d=2\\) dimensional lattice. Each path of \\(2n\\) steps has a probability \\(\\frac{1}{4^{2n}}\\) of occurring. We then consider the number of paths with equal steps left and right (say \\(L\\) steps in each horizontal direction) and equal steps up and down (then \\(n-L\\)) is:\n\\[\\begin{align*}\n{2n \\choose L}{2n - L \\choose L}{2n - 2L \\choose n - L}{n-L \\choose n - L} = \\frac{(2n)!}{L! L! (n-L)! (n-L)!}\n\\end{align*}\\]\nSo we get:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{4^{2n}} \\sum_{L=0}^n \\frac{(2n)!}{L! L! (n-L)! (n-L)!} \\\\\n&= \\frac{1}{4^{2n}} \\sum_{L=0}^n \\frac{(2n)!}{n! n!} \\cdot \\left(\\frac{n!}{L! (n-L)!}\\right)^2 \\\\\n&=\\frac{1}{4^{2n}} {2n \\choose n}\\sum_{L=0}^n {n \\choose L}^2\n\\end{align*}\\]\nSuppose, we are to form a team of \\(n\\) players from a population of \\(2n\\) participants. There \\({2n \\choose n}\\) distinguishable teams of size \\(n\\). Alternatively, we could divide the population into two halves of \\(n\\) participants each, choose \\(L\\) participants from the first sub-population, and the remaining \\(n-L\\) participants from the second sub-population. So, the number of distinguishable teams is:\n\\[\\begin{align*}\n\\sum_{L=0}^{n}{n \\choose L}{n \\choose n - L} = \\sum_{L=0}^{n} {n \\choose L}^2\n\\end{align*}\\]\nSo, we have the combinatorial identity:\n\\[\\begin{align*}\n\\sum_{L=0}^{n} {n \\choose L}^2 = {2n \\choose n}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\nu_{2n} &=\\frac{1}{4^{2n}} {2n \\choose n}\\sum_{L=0}^n {n \\choose L}^2\\\\\n&= \\left(\\frac{1}{2^{2n}} {2n \\choose n}\\right)^2\n\\end{align*}\\]\nSo, we have that, it is just the square of the result from \\(\\mathbb{Z}^1\\), so we see in this case that:\n\\[\\begin{align*}\nu_{2n} &\\approx \\sum_{n=0}^{\\infty} \\frac{1}{\\pi n}\n\\end{align*}\\]\nAnd the series on the right certainly diverges, so we see that a simple random walk in \\(\\mathbb{Z}^2\\) is recurrent as \\(E\\) diverges.\n\n\n\nAs before, in order to have a walk return to the origin we must have equal steps in the positive and negative directions for each direction and each path of \\(2n\\) has a probability of occurring of \\(\\frac{1}{6^{2n}}\\). Then, extending the idea from the previous subsection, the number of paths (of total length \\(2n\\)) with \\(L\\) steps left and right, \\(U\\) steps up and down and \\(n - L - U\\) steps forward and backward is:\n\\[\\begin{align*}\n\\frac{(2n)!}{L! L! U! U! (n - L - U)! (n - L - U)!}\n\\end{align*}\\]\nSo, we get:\n\\[\n\\begin{align*}\nu_{2n} &= \\frac{1}{6^{2n}} \\sum_{L,U,L+U \\leq n} \\frac{(2n)!}{L! L! U! U! (n - L - U)! (n - L - U)!} \\\\\n&= \\frac{1}{2^{2n}} {2n \\choose n} \\sum_{L,U,L+U \\leq n} \\left(\\frac{1}{3^n}\\frac{n!}{L! U! (n - U - L)!}\\right)^2\n\\end{align*}\n\\tag{3}\\]\nConsider a \\(3\\)-sided fair coin, with \\(\\mathbb{P}\\{\\text{Heads}\\} = \\mathbb{P}\\{\\text{Tails}\\} = \\mathbb{P}\\{\\text{Edge}\\} = \\frac{1}{3}\\). Suppose that the coin is tossed \\(n=3^2 = 9\\) times. The probability of landing \\(L\\) heads, \\(U\\) tails and \\(n - L - U\\) edges is \\(\\frac{n!}{L!U!(n-L-U)!} \\frac{1}{3^n}\\). This is precisely the term seen in the above expression. The number of heads, tails and edges in \\(n\\) coin tosses follows a multinomial distribution. It can be easily demonstrated, that in, \\(n/3 = 3\\) heads, \\(n/3\\) tails and \\(n/3\\) edges are most likely to occur in \\(n\\) coin tosses. So, we can find an upper bound on the summand in Equation 3 as:\n\\[\\begin{align*}\n\\frac{n!}{3^n L! U! (n- L - U)!} \\leq \\frac{n!}{3^n \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor!}\n\\end{align*}\\]\nAlso, from the multinomial expansion theorem:\n\\[\\begin{align*}\n(1 + a + b)^n = \\sum_{L,U,L+U\\leq n} \\frac{n!}{L! U! (n-L-U)!} a^L b^U\n\\end{align*}\\]\nSetting \\(a = b = 1\\), we get:\n\\[\\begin{align*}\n\\sum_{L,U,L+U\\leq n} \\frac{n!}{3^n L! U! (n-L-U)!} a^L b^U = 1\n\\end{align*}\\]\nWe then see that:\n\\[\\begin{align*}\nu_{2n} \\leq \\frac{1}{2^{2n}} {2n \\choose n} \\frac{n!}{3^n \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor!}\n\\end{align*}\\]\nWe can simplify this as:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{2^{2n}}{2n \\choose n} \\frac{n!}{3^n \\left(\\lfloor\\frac{n}{3}\\rfloor!\\right)^3} \\\\\n&\\approx \\frac{1}{\\sqrt{\\pi n}} \\times \\frac{\\sqrt{2\\pi n} e^{-n} n^n}{3^n \\left\\{\\sqrt{2\\pi \\frac{n}{3}}e^{-n/3} \\left(\\frac{n}{3}\\right)^{n/3}\\right\\}^3}\\\\\n&= \\frac{1}{\\sqrt{\\pi n}} \\frac{\\sqrt{2\\pi n} e^{-n} n^n}{3^n \\frac{(2\\pi n)^{3/2}}{3^{3/2}} e^{-n} \\frac{n^n}{3^n}}\\\\\n&= \\frac{1}{\\sqrt{\\pi n}} \\frac{\\sqrt{2\\pi n} \\cancel{e^{-n}} \\cancel{n^n}}{\\cancel{3^n} \\frac{(2\\pi n)^{3/2}}{3^{3/2}} \\cancel{e^{-n}} \\frac{ \\cancel{n^n}}{\\cancel{3^n}}}\\\\\n&\\approx \\frac{M}{(\\pi n)^{3/2}}\n\\end{align*}\\]\nwhere \\(M\\) is a positive constant. We then see:\n\\[\\begin{align*}\n\\sum_n u_{2n} &= M \\sum_n \\frac{1}{n^{\\frac{3}{2}}}\n\\end{align*}\\]\nThe sum on the right hand side converges, so we have that a simple random walk in three dimensions is transient. The proof of Polya’s Random Walk Theorem is complete.\n\n\n\n\nIn one dimension, we established that every path of a Brownian motion reaches any level \\(a \\in \\mathbf{R}\\). More precisely, for \\(a &gt; 0\\), if we define the stopping time :\n\\[\\begin{align*}\n\\tau = \\min \\{ t \\geq 0 : B_t \\geq a\\}\n\\end{align*}\\]\nwe have \\(\\mathbb{P}\\{\\tau_a &lt; \\infty \\} = 1\\). This implies in particular that every path will come back to \\(0\\) and will do infinitely many times. This is because \\(B(t+\\tau) - B(t)\\) is a standard brownian motion. This property of Brownian motion is called recurrence. This is to be compared with Brownian motion with a drift. In our analysis, we found that, if the drift is negative, then there are paths that will not reach a given level \\(a &gt; 0\\) with positive probability. Such paths go to infinity without ever going back to \\(0\\). This property of the process is called transience.\nWe can derive similar properties for the multidimensional Brownian motion. We will rely heavily on Corollary 1 and on some knowledge of harmonic functions. Harmonic functions play a very important role in mathematics, physics and in nature in general. As we mentioned earlier, if \\(d = 1\\), the only harmonic functions are the linear functions, since the equation \\(f''(x) = 0\\) has the solutions \\(f(x)=ax+b\\). However, in higher dimensions, the collection of harmonic functions is very rich. This gives access to a plethora of Brownian martingales. For example, the following functions are harmonic in the whole space minus the origin, \\(\\mathbf{R}^d \\setminus \\{0\\}\\).\n\\[\nh(x) = \\begin{cases}\n\\log ||x||, & d = 2 \\\\\n||x||^{2 - d}, & d \\geq 3\n\\end{cases}\n\\tag{4}\\]\nwhere \\(x\\in\\mathbf{R}^d\\).\n\nProof. Consider\n\\[\\begin{align*}\nf(x_1,x_2) = \\log \\sqrt {x_1^2 + x_2^2}\n\\end{align*}\\]\nWe have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x_1} &= \\frac{1}{\\sqrt{x_1^2 + x_2^2}} \\cdot \\frac{\\partial }{\\partial x_1} (\\sqrt{x_1^2 + x_2^2})\\\\\n&=\\frac{1}{\\sqrt{x_1^2 + x_2^2}} \\cdot \\frac{x_1}{\\sqrt{x_1^2 + x_2^2}}\\\\\n&= \\frac{x_1}{x_1^2 + x_2^2}\n\\end{align*}\\]\nFurther,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= \\frac{(x_1^2 + x_2^2)(1) - x_1(2x_1)}{(x_1^2 + x_2^2)^2} \\\\\n&= \\frac{x_1^2 - x_2^2}{(x_1^2 + x_2^2)}\n\\end{align*}\\]\nBy symmetry,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_2^2}\n&= \\frac{x_2^2 - x_1^2}{(x_1^2 + x_2^2)}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= - \\frac{\\partial^2 f}{\\partial x_2^2}\\\\\n\\frac{\\partial^2 f}{\\partial x_1^2} + \\frac{\\partial^2 f}{\\partial x_2^2}&=0\n\\end{align*}\\]\nWe conclude that \\(f\\) is harmonic.\nNext, consider\n\\[\\begin{align*}\ng(x_1,x_2,\\ldots,x_d) = \\frac{1}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-2}}\n\\end{align*}\\]\nNow,\n\\[\\begin{align*}\n\\frac{\\partial g}{\\partial x_i} &= \\frac{(2-d)}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}} \\cdot \\frac{\\partial }{\\partial x_i}\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)\\\\\n&= \\frac{(2-d)}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}}\\cdot \\frac{2x_i}{2\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)}\\\\\n&= \\frac{(2-d)x_i}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^d}\n\\end{align*}\\]\nFurther, we have:\n\\[\\begin{align*}\n\\frac{\\partial^2 g}{\\partial x_i^2} &= (2-d)\\frac{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^d (1) - (x_i)\\cdot d \\cdot\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}\\cdot \\frac{x_i}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)}}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{2d}}\\\\\n&=(2-d)\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-2} \\frac{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^2  - d x_i^2}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{2d}}\\\\\n&=(2-d) \\frac{\\left(\\sum_{i=1}^d x_i^2\\right) - dx_i^2 }{||x||^{d+2}}\n\\end{align*}\\]\nWe conclude that:\n\\[\\begin{align*}\n\\sum_{i=1}^{d} \\frac{\\partial^2 g}{\\partial x_i^2} = 0\n\\end{align*}\\]\nand hence \\(g\\) is harmonic.\n\nInterestingly enough, the answer to the recurrence versus transience puzzle depends on the dimension. We will show that Brownian motion is recurrent in dimension \\(d=2\\) in the sense that every Brownian motion path starting from \\(x\\) will eventually enter a disc around the origin, no matter how small the disc is. The Brownian path will then enter this disc infinitely many times, as \\(t \\to \\infty\\). Note that, we did not say that the path actually hits \\(0\\), but that it enters a disc around the \\(0\\). This nuance is important, as we will in fact show that a Brownian path actually never hits a given point. In dimension \\(d=3\\) or higher, it is proved that there are some paths starting from a given \\(x\\) that will never enter a given ball around the origin with positive probability. This is the transience property.\nThe strategy of the proof is similar to the approach for Brownian motion with a drift. We will find a good function \\(h : \\mathbf{R}^d \\to \\mathbf{R}\\) for which \\(h(B_t)\\) is a martingale. In light of Corollary 1, this function needs to be harmonic in a suitable region. The desired probability is then obtained by considering the right boundary values.\n\nTheorem 3 (Recurrence and transience of a brownian motion in \\(\\mathbf{R}^d\\)) Let \\((B_t,t\\geq 0)\\) be a Brownian motion in \\(\\mathbf{R}^d\\) starting at \\(B_0 = x\\). Consider for \\(r &lt; ||x||\\), the stopping time\n\\[\\begin{align*}\n\\tau_r = \\min \\{t \\geq 0 : ||B_t|| &lt; r\\}\n\\end{align*}\\]\nthe first hitting time of the paths in a ball of radius \\(r\\) around the origin. We have:\n\\[\\begin{align*}\n\\mathbb{P}(\\tau_r &lt; \\infty) =\n\\begin{cases}\n1 & \\text{ if }d \\leq 2 \\\\\n\\left(\\frac{r}{||x||}\\right)^{d-2} & \\text{ if }d \\geq 3\n\\end{cases}\n\\end{align*}\\]\nIn particular, for \\(d \\leq 2\\), the paths are recurrent; that is, they come back infinitely many times in a neighbourhood of the origin. For \\(d \\geq 3\\), each path will eventually never come back to a neighbourhood of the origin.\n\nNote that we made sure that the starting point of the Brownian motion \\(x\\) is outside the ball of radius \\(r\\).\n\nProof. Consider another hitting time of a ball with a radius larger than \\(||x||\\):\n\\[\\begin{align*}\n\\tau_R' = \\min \\{t \\geq 0 : ||B_t|| \\geq R\\}, \\quad r &lt; ||x|| &lt; R\n\\end{align*}\\]\nNote that \\(\\tau_R'\\) must increase with \\(R\\) and that it must go to \\(+\\infty\\) as \\(R \\to \\infty\\). Moreover, the sequence of events \\(\\{\\tau_r &lt; \\tau_R'\\}_{R &gt; ||x||}\\) is increasing. In particular, by continuity of probability measure, we have:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R') &= \\mathbb{P}(\\lim_{R \\to \\infty} \\tau_r &lt; \\tau_R')\\\\\n&= \\mathbb{P}(\\tau_r &lt; \\infty)\n\\end{align*}\\]\nIf we set \\(\\tau = \\tau_r \\land \\tau_R'\\), then the event \\(\\{\\tau_r &lt; \\tau_R'\\}\\) is the same as the event \\(\\{||B_\\tau||=r\\}\\). So, \\(\\lim_{R \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R') = \\lim_{R \\to \\infty} \\mathbb{P}(||B_\\tau||=r)\\).\nNow, consider the event \\(\\{\\tau &lt; \\infty\\}\\). Let \\(E_n\\) be the event that the \\(n\\)th increment \\(||B_n - B_{n-1}||\\) exceeds \\(R\\). If \\(E_n\\) occurs, then we must have the Brownian motion exits the spherical shell of thickness \\(R-r\\). Moreover, we have \\(\\mathbb{P}(E_1) = \\ldots = \\mathbb{P}(E_n)\\), for all \\(n\\). Call this probability \\(p\\). Clearly, \\(0 &lt; p &lt; 1\\). Since the events \\(E_n\\) are independent, we have:\n\\[\\begin{align*}\n\\mathbb{P}(E_1^C \\cap \\ldots \\cap E_n^C) = (1 - p)^n\n\\end{align*}\\]\nAs \\(n \\to \\infty\\), we have \\(\\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}E_k^C) = 0\\). Now, let \\(F_k\\) be the event that there are no excursions upto \\(k\\). That is, \\(F_k = \\bigcap_{i=1}^{k}E_i^C\\). Now, the event \\(F_{k+1}\\) implies \\(F_k\\), or equivalently, \\(F_{k+1} \\subseteq F_k\\). So, \\(\\{F_k\\}_{k=1}^{\\infty}\\) is a decreasing sequence of events. Moreover, \\(\\bigcap_{k=1}^n F_k = \\bigcap_{k=1}^n E_k^C\\). So, by continuity of probability measure,\n\\[\\begin{align*}\n\\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}E_k^C)\n&= \\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}F_k) \\\\\n&= \\mathbb{P}(\\lim_{n \\to \\infty} \\bigcap_{k=1}^{n}F_k) \\\\\n&= \\mathbb{P}(\\bigcap_{k=1}^{\\infty}F_k) \\\\\n&= \\mathbb{P}(\\bigcap_{k=1}^{\\infty}E_k^C)\n\\end{align*}\\]\nSo, \\(\\mathbb{P}(\\bigcup_{k=1}^{\\infty}E_k) = 1\\). There exists an \\(n_0 \\in \\mathbf{N}\\), such that the event \\(E_{n_0}\\), an excursion at \\(n_0\\) occurs with probability \\(1\\). So, the event \\(\\{\\tau &lt; \\infty\\}\\) occurs almost surely.\nTo compute \\(\\mathbb{P}(||B_\\tau||=r)\\), the idea is to find a good function \\(h\\) with the following properties:\n\nMartingale: \\((h(B_t),t\\leq \\tau)\\) is a bounded martingale, so that the optional stopping theorem implies:\n\n\\[\\begin{align*}\n\\mathbb{E}[h(B_\\tau)] = h(B_0) = h(x)\n\\end{align*}\\]\n\nBoundary values: Since \\(B_\\tau\\) is a point at \\(||x||=r\\) or at \\(||x||=R\\), we pick boundary conditions for \\(h\\), so that \\(\\mathbb{E}[h(B_{\\tau})]=\\mathbb{P}(||B_\\tau||=r)\\).\n\nThe second point is easy; it suffices to take the boundary conditions \\(h(x)=0\\) at \\(||x||=R\\) and \\(h(x)=1\\) at \\(||x||=r\\). For the first point, by Ito’s formula, we pick \\(h\\) to be harmonic in the annulus \\(\\{x \\in \\mathbf{R}^d : r &lt; ||x||&lt;R \\}\\). Thus, we can use the function in Equation 4. Of course, the functions \\(ah(x) + b\\) remain harmonic. To satisfy the boundary conditions, we pick \\(a\\) and \\(b\\) suitably:\n\n\nLet \\(h(x) = ax + b\\). Then, at \\(x=r\\), we have:\n\\[\n\\begin{align*}\nar + b = 1\n\\end{align*}\n\\tag{5}\\]\nand at \\(x = R\\), we have:\n\\[\n\\begin{align*}\naR + b = 0\n\\end{align*}\n\\tag{6}\\]\nSo, \\(b = -aR\\). Substituting in Equation 5, we get:\n\\[\\begin{align*}\na &= \\frac{1}{r - R}\nb &= -\\frac{R}{r - R}\n\\end{align*}\\]\nConsequently,\n\\[\n\\begin{align*}\nh(x) &= \\frac{R - x}{R - r}\n\\end{align*}\n\\tag{7}\\]\n\n\n\nLet \\(h(x) = \\log ||x||\\). Then, at \\(||x||=r\\), we have:\n\\[\n\\begin{align*}\na \\log ||x|| + b &= 1\na \\log r + b &= 1\n\\end{align*}\n\\tag{8}\\]\nAt \\(||x||=R\\), we have:\n\\[\n\\begin{align*}\na \\log ||x|| + b &= 0\na \\log R + b &= 0\n\\end{align*}\n\\tag{9}\\]\nThus, from Equation 9, \\(b = -a \\log R\\). Substituting the value for \\(b\\) in Equation 8, we have:\n\\[\n\\begin{align*}\na \\log r - a \\log R &= 1\\\\\na &= \\frac{1}{\\log r - \\log R}\n\\end{align*}\n\\]\nConsequently,\n\\[\n\\begin{align*}\nh(x) &= \\frac{\\log ||x||}{\\log r - \\log R} -  \\frac{1}{\\log r - \\log R} \\log R\\\\\n&= \\frac{\\log R - \\log ||x||}{\\log R - \\log r}\n\\end{align*}\n\\tag{10}\\]\n\n\n\nLet \\(h(x)=||x||^{2-d}\\). At \\(||x||=r\\), \\(a h(x) + b = 1\\). So, we have:\n\\[\n\\begin{align*}\na r^{2-d} + b = 1\n\\end{align*}\n\\tag{11}\\]\nAt \\(||x||=R\\), \\(ah(x) + b = 0\\). So, we have:\n\\[\n\\begin{align*}\na ||R||^{2-d} + b &= 0\n\\end{align*}\n\\tag{12}\\]\nFrom Equation 12, we get \\(b = -a R^{2-d}\\). Substituting this value in Equation 11, we get:\n\\[\\begin{align*}\na r^{2-d} - aR^{2-d}  &= 1\\\\\na &= \\frac{1}{r^{2-d} - R^{2-d}}\n\\end{align*}\\]\nAnd\n\\[\n\\begin{align*}\nb = - \\frac{R^{2-d}}{r^{2-d} - R^{2-d}}\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\nh(x) &= \\frac{||x||^{2-d}}{r^{2-d} - R^{2-d}} - \\frac{R^{2-d}}{r^{2-d} - R^{2-d}}\\\\\n&= \\frac{R^{2-d} - ||x||}{R^{2-d} - r^{2-d}}\n\\end{align*}\n\\tag{13}\\]\nCollecting all the results together, we get:\n\\[\nh(x) = \\begin{cases}\n\\frac{R - x}{R - r}, & d=1 \\\\\n\\frac{\\log R - \\log ||x||}{\\log R - \\log r}, & d=2 \\\\\n\\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}, & d \\geq 3\n\\end{cases}\n\\tag{14}\\]\nPassing to the limit as \\(R \\to \\infty\\), for \\(d=1\\), we have:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} h(x) &= \\lim_{R \\to \\infty} \\frac{R - x}{R - r} \\\\\n&= \\lim_{R \\to \\infty} \\frac{1 - \\frac{x}{R}}{1 - \\frac{r}{R}}\\\\\n&= 1\n\\end{align*}\\]\nFor \\(d = 2\\), we get:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} h(x) &= \\lim_{R \\to \\infty} \\frac{1 - \\frac{\\log x}{\\log R}}{1 - \\frac{\\log r}{\\log R}} \\\\\n&= 1\n\\end{align*}\\]\nFor \\(d = 3\\), we get:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty}  h(x) &= \\lim_{R \\to \\infty} \\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}\\\\\n&= \\left(\\frac{||x||}{r}\\right)^{2-d}\n\\end{align*}\\]\nThis concludes the proof. \\(\\blacksquare\\)\n\n\n\nExample 4 We get more from the above proof. Indeed, in dimension \\(d \\geq 2\\), it implies that the probability that a Brownian motion starting at \\(B_0 = x\\) eventually hits any other point \\(y\\) is \\(0\\). Indeed, if we take \\(y = 0\\) and \\(x \\neq 0\\), we have:\n\\[\n\\begin{align*}\n\\mathbb{P}(\\exists t &gt; 0: B_t = 0) &= \\lim_{R \\to \\infty} \\lim_{r \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R')\\\\\n&= \\lim_{R \\to \\infty} \\lim_{r \\to \\infty} \\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}\\\\\n&=\\lim_{R \\to \\infty}  \\lim_{r \\to \\infty} \\frac{(R/r)^{2-d} - (||x||/r)^{2-d}}{(R/r)^{2-d} - 1}\\\\\n&= \\lim_{R \\to \\infty} 0\\\\\n&= 0\n\\end{align*}\n\\]\nHere, we shrink the inner radius to \\(0\\) first and then we let the outer radius go to infinity after. It remains to take the limits of the expression obtained in equation Equation 14. The first limit \\(r \\to 0\\) gives \\(0\\) right away.\n\n\n\n\nIn the last section, we translated the problem of recurrence and transience of Brownian motion in terms of a boundary value problem: we needed to find a particular function \\(h\\) that solves a PDE (that is, \\(\\nabla^2 h=0\\)) in a certain region of space(the annulus) and that satisfies given boundary conditions (the function is \\(1\\) on the inner circle and \\(0\\) on the outer circle). This is a particular case of the Dirichlet problem in PDE.\nConsider a region \\(\\mathcal{O} \\subset \\mathbb{R}^d\\). \\(\\mathcal{O}\\) is an open ball. That is, for all \\(x \\in \\mathcal{O}\\), there exists \\(\\epsilon&gt;0\\), such that \\(V_\\epsilon(x) = \\{y : ||y - x|| &lt; \\epsilon\\}\\) is contained in \\(\\mathcal{O}\\). \\(\\mathcal{O}\\) doesn’t contain boundary points \\(\\partial \\mathcal{O}\\). In the proof of Theorem 3, the region was the annulus \\(\\{x: r &lt; ||x|| &lt; R\\}\\) and the boundary was given by the spheres \\(\\{x :||x|| = r\\}\\) and \\(\\{x: ||x||=R\\}\\). The Dirichlet problem in \\(\\mathcal{O}\\) can be stated as follows:\nLet \\(f:\\partial\\mathcal{O} \\to \\mathbb{R}\\) be a function on the boundary of \\(\\mathcal{O}\\). Can we extend the function \\(f\\) to \\(O\\) in such a way that the extension is harmonic on \\(\\mathcal{O}\\) and coincides with \\(f\\) on the boundary?\nIn the instance of the proof of the Theorem 3, we knew the solution of the problem, thanks to Equation 4. The only job left to do was to adjust some parameters to fit the boundary conditions. It turns out that the formalism of stochastic calculus allows us to express the solution of the Dirichlet problem as an average over Brownian paths for general regions of space and boundary conditions. To do so, we look at a twist on the Ito’s formula called the Dynkin’s formula.\nLet \\(x\\) be a point in \\(\\mathcal{O}\\). Consider a Brownian motion \\((B_t,t\\geq 0)\\) starting at \\(x\\), \\(B_0 = x\\). To emphasize the dependence on \\(x\\), we write \\(\\mathbb{P}_x\\) and \\(\\mathbb{E}_x\\) for the probability and the expectation for the Brownian motion. Dykin’s formula is obtained by merging Corollary 1 for the Brownian martingales together with the optional stopping theorem. More precisely, consider \\(f \\in C^2(\\mathbb{R}^d)\\) that satisfies the assumption of the Corollary 1. We then have that the process:\n\\[\\begin{align*}\nM_t &= f(B_t) - \\frac{1}{2}\\int_{0}^t \\nabla^2 f(B_s) ds, \\quad t \\geq 0\n\\end{align*}\\]\nis a Brownian martingale.\nConsider also the stopping time \\(\\tau = \\min \\{t \\geq 0 : B_t \\in \\mathcal{O}^C\\}\\). This is the first exit time of \\(\\mathcal{O}\\) of the Brownian motion path. Then, if the assumptions of the optional stopping theorem are fulfilled, we get Dynkin’s formula:\n\\[\n\\begin{align*}\n\\mathbb{E}[M_\\tau] = \\mathbb{E}[f(B_\\tau)] - \\frac{1}{2} \\mathbb{E}\\left[\\int_0^\\tau \\nabla^2 f(B_s)ds\\right] = f(B_0) = M_0\n\\end{align*}\n\\tag{15}\\]\nAs an application of Dynkin’s formula, let’s express the solution of the Dirichlet problem as an average over Brownian motion. If \\(f\\) is harmonic in \\(\\mathcal{O}\\), then \\(\\nabla^2 f = 0\\) in \\(\\mathcal{O}\\), so the process \\((f(B_t),t\\leq \\tau)\\) is itself a martingale. Note that since \\(\\mathcal{O}\\) is bounded, this is a bounded martingale! Moreover, the integrands \\(\\partial_i f(B_s)\\), \\(1 \\leq i \\leq d\\), are bounded. We conclude that the solution to the Dirichlet problem can be represented as follows:\n\\[\n\\begin{align*}\nf(x) = \\mathbb{E}_x[f(B_\\tau)], \\quad x \\in \\mathcal{O}\n\\end{align*}\n\\tag{16}\\]\nIn other words, the value of the harmonic function of \\(f\\) at \\(x\\) is given by the average over the Brownian paths of the value of the function at the exit point of the path on the boundary of \\(\\mathcal{O}\\). Equation Equation 16 does not prove the existence of the solution to the Dirichlet problem, as we assume that such a function exists. Existence of the solution to the Dirichlet problem can be shown this way, assuming the region \\(\\mathcal{O}\\) and the function on the boundary are nice enough. However, note that the averaging on the right-hand side of the formula makes sense even in the case where \\(f\\) is simply defined on the boundary \\(\\partial \\mathcal{O}\\). We then define a posteriori the harmonic extension of \\(f\\) inside \\(\\mathcal{O}\\) by defining its value at \\(x\\) as \\(\\mathbb{E}_x[f(B_\\tau)]\\).\n\n\n\n\n\n\n\nExercise 1 Gaussian Integration by parts.\n\nLet \\(Z\\) be a standard Gaussian random variable. Show using integration by parts that for a differentiable function \\(g\\):\n\n\\[\n\\begin{align*}\n\\mathbb{E}[Z g(Z)] = \\mathbb{E}[g'(Z)]\n\\end{align*}\n\\tag{17}\\]\nwhere we assume that both expectations are well-defined.\n\nUse this to recover the expression for the moments for \\(\\mathbb{E}[Z^{2j}],j \\in \\mathbb{N}\\).\n\n\nSolution.\nWe have:\n\\[\n\\begin{align*}\n\\mathbb{E}[g'(Z)] &= \\int_{-\\infty}^{\\infty}\\phi(z) \\cdot g'(z) dz \\\\\n&= \\left[\\frac{e^{-z^2/2}}{\\sqrt{2\\pi}}\\cdot g(z)\\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} g(z) \\frac{-ze^{-z^2/2}}{\\sqrt{2\\pi}}dz \\\\\n&= 0 + \\int_{-\\infty}^{\\infty} z g(z) \\phi(z)dz\\\\\n&= \\mathbb{E}[Zg(Z)]\n\\end{align*}\n\\]\nMoreover, we can write:\n\\[\n\\begin{align*}\n\\mathbb{E}[Z^{2n}] &= \\mathbb{E}[Z \\cdot Z^{2n-1}] \\\\\n&= \\mathbb{E}[Z \\cdot g(Z)], \\quad \\{ \\text{ where }g(x)=x^{2n - 1}\\} \\\\\n&= \\mathbb{E}[g'(Z)] \\\\\n&= (2n - 1) \\mathbb{E}[Z^{(2n - 2)}] \\\\\n& \\vdots \\\\\n&= (2n - 1)\\cdots 5 \\cdot 3 \\cdot \\mathbb{E}[Z^2] \\\\\n&= (2n - 1)\\cdots 5 \\cdot 3 \\cdot 1\n\\end{align*}\n\\]\n\nExercise 2 The Heat Equation.\nUse the Gaussian integration by parts identity to check that the solution \\(f(t,x)\\) to the heat equation is:"
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#multidimensional-brownian-motion.",
    "href": "posts/multivariate_ito_calculus/index.html#multidimensional-brownian-motion.",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Definition 1 (Brownian motion in \\(\\mathbf{R}^{d}\\).) Take \\(d\\in\\mathbf{N}\\). Let \\(B^{(1)},\\ldots,B^{(d)}\\) be independent standard Brownian motions in \\((\\Omega,\\mathcal{F},\\mathbb{P})\\). The process \\((B_{t}:t\\geq0)\\) taking values in \\(\\mathbf{R}^{d}\\) defined by :\n\\[\\begin{aligned}\nB_{t} & =(B_{t}^{(1)},\\ldots,B_{t}^{(d)}),\\quad t\\geq0\n\\end{aligned}\\]\nis called a \\(d-\\)dimensional Brownian motion or a Brownian motion in \\(\\mathbf{R}^{d}\\).\n\nThe Brownian filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) is now composed of the information of all Brownian motions. In other words, it is given by the sigma-fields:\n\\[\\begin{aligned}\n\\mathcal{F}_{t} & =\\sigma(B_{s}^{(i)},1\\leq i\\leq d,s\\leq t)\n\\end{aligned}\\]\nFor every outcome \\(\\omega\\), the path of trajectory of a \\(d-\\)dimensional Brownian motion is a curve in space parametrized by the time \\(t\\):\n\\[\\begin{aligned}\nt\\mapsto B_{t}(\\omega) & =(B_{t}^{(1)}(\\omega),B_{t}^{(2)}(\\omega),\\ldots,B_{t}^{(d)}(\\omega))\n\\end{aligned}\\]\nOf course, this curve is continuous, since each coordinate is. The below numerical project gives an example of one path of a two-dimensional brownian motion. This is a very rugged and intertwined curve! We might wonder, what it does as \\(t\\to\\infty\\). Does it wander around \\((0,0)\\) ad infinitum or does it eventually escape to infinity? We will answer this question in a later section. For doing so, we shall need a version of Ito’s formula for multi-dimensional Brownian motion. We finish this section by noticing that it is also easy to construct Brownian motions in higher dimensions for which the coordinates are correlated.\n\n\nConsider a two-dimensional Brownian motion \\((B_{t}^{(1)},B_{2}^{(2)})\\) starting at \\((0,0)\\).\nLet’s plot one path of this Brownian motion on the plane \\(\\mathbf{R}^{2}\\) on the plane in \\(\\mathbf{R}^{2}\\) on the time interval \\([0,5]\\) using a discretization of \\(0.001\\).\nWe define BrownianMotion2D class.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nclass BrownianMotion2D:\n    \"\"\"\n    Generate sample paths for a 2d-brownian motion\n    \"\"\"\n    def __init__(\n        self,\n        endpoint : float = 1.0,\n        discretization : float = 0.01\n    ):\n        self.endpoint = endpoint\n        self.discretization = discretization\n\n    def generate_paths(self, num_paths : int):\n        steps = int(self.endpoint/self.discretization)\n        delta_z = np.sqrt(self.discretization) * np.random.randn(num_paths,2,steps)\n        \n        # paths has shape [num_paths,2,num_steps]\n        paths = np.cumsum(delta_z,axis=2)\n\n        # Brownian motion has position (0,0) at time t=0\n        # Append initial position \n        init_position = np.zeros((num_paths,2,1))\n        paths = np.append(init_position,paths,axis=2)\n\n        return paths\n\nWe can then configure and use BrownianMotion2D objects as we please.\n\n\nShow the code\nbm = BrownianMotion2D(endpoint=5.0,discretization=0.001)\npaths = bm.generate_paths(num_paths=1)\npath = paths[0]\n\n\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.title(r'2D-Brownian motion')\nplt.grid(True)\nplt.plot(path[0],path[1],linewidth=1,color='blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1 (Brownian motion with correlated coordinates) Let \\((B_{t}:t\\geq0)\\) be a two dimensional brownian motion. Let \\(-1&lt;\\rho&lt;1\\). We construct the two dimensional process as follows: \\(W_{t}=(W_{t}^{(1)},W_{t}^{(2)})\\) where:\n\\[\\begin{aligned}\nW_{t}^{(1)} & =B_{t}^{(1)}\\\\\nW_{t}^{(2)} & =\\rho B_{t}^{(1)}+\\sqrt{1-\\rho^{2}}B_{t}^{(2)}\n\\end{aligned}\\]\n\\(W_{t}^{(1)}=B_{t}^{(1)}\\) is Gaussian with mean \\(0\\) and variance \\(t\\). Since, \\(B_{t}^{(1)}\\) and \\(B_{t}^{(2)}\\)are independent gaussian random variables and the sum of IID Gaussians is Gaussian, \\(W_{t}^{(2)}\\) is Gaussian with mean \\(0\\) and variance \\(t\\). The covariance between \\(W_{t}^{(1)}\\) and \\(W_{t}^{(2)}\\) is:\n\\[\\begin{aligned}\n\\mathbf{E}[W_{t}^{(1)}W_{t}^{(2)}] & =\\mathbf{E}[B_{t}^{(1)}(\\rho B_{t}^{(1)}+\\sqrt{1-\\rho^{2}}B_{t}^{(2)})]\\\\\n& =\\mathbf{E}[\\rho(B_{t}^{(1)})^{2}+\\sqrt{1-\\rho^{2}}B_{t}^{(1)}B_{t}^{(2)}]\\\\\n& =\\rho t\n\\end{aligned}\\]\nHence, the coordinates at time \\(t\\) are not independent.\n\nConsider now the process \\((W_{t}:t\\geq0)\\) for \\(\\rho=1/2\\) as in example 2. Let’s plot one path of this process on the plane \\(\\mathbf{R}^{2}\\) on the time-interval \\([0,5]\\) using a discretization of \\(0.001\\).\n\nclass BrownianMotion2DWithCorrelatedCoords(BrownianMotion2D):\n    \"\"\"\n    Generate sample paths for a 2d-brownian motion\n    have correlated coordiantes\n    \"\"\"\n\n    def __init__(\n        self, \n        endpoint: float = 1.0, \n        discretization: float = 0.01, \n        rho: float = 0.0\n    ):\n        self.endpoint = endpoint\n        self.discretization = discretization\n        self.rho = rho\n\n    def generate_paths(self, num_paths: int):\n\n        # Call the base class generate_paths() method to generate a pair of\n        # independent brownian motion paths. It returns a tensor of\n        # shape (num_paths, 2, num_steps)\n        paths = super().generate_paths(num_paths=num_paths)\n\n        # Extract the brownian motions B1(t) and B2(t)\n        # B1(t) and B2(t) have dimensions [num_paths,num_steps]\n        x_coord = paths[:, 0, :]\n        y_coord = paths[:, 1, :]\n\n        # Apply the transformation\n        # W1(t) = B1(t)\n        # W2(t) = rho * B1(t) + sqrt(1 - rho**2) B2(t)\n        x_coord = x_coord\n        y_coord = self.rho * x_coord + np.sqrt(1 - self.rho**2) * y_coord\n\n        # Assemble back the x- and y- coordinates\n        x_coord = np.expand_dims(x_coord, axis=1)\n        y_coord = np.expand_dims(y_coord, axis=1)\n\n        # Return a tensor of shape [num_paths,d=2,num_steps]\n        return np.concatenate([x_coord, y_coord], axis=1)\n\nLet’s configure the BrownianMotion2DWithCorrelatedCoords engine with the settings endpoint=5.0, discretization=0.001 and rho=0.5 and generate a sample path.\n\n\nShow the code\nbm = BrownianMotion2DWithCorrelatedCoords(\n    endpoint=5.0,\n    discretization=0.001,\n    rho=0.50\n)\npaths = bm.generate_paths(num_paths=1)\npath = paths[0]\n\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.title(r'Correlated Brownian motion $W(t)=(W^{(1)}(t),W^{(2)}(t))$')\nplt.grid(True)\nplt.plot(path[0],path[1],linewidth=1,color='blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplt.xlabel(r'$t$')\nplt.ylabel(r'$W_t$')\nplt.title(r'Correlated Brownian motion paths')\nplt.grid(True)\nt = np.linspace(0.0,stop=5.0,num=5001)\nplt.plot(t,path[0],linewidth=0.75)\nplt.plot(t,path[1],linewidth=0.75)\nplt.show()"
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#itos-formula.",
    "href": "posts/multivariate_ito_calculus/index.html#itos-formula.",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Theorem 1 (Ito’s Formula.) Let \\((B_{t}:t\\geq0)\\) be a \\(d-\\)dimensional brownian motion. Consider \\(f\\in\\mathcal{C}^{2}(\\mathbf{R}^{d})\\). Then, we have with probability one that for all \\(t\\geq0\\):\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{i=1}^{d}\\int_{0}^{t}\\partial_{x_{i}}f(B_{s})dB_{s}^{(i)}+\\frac{1}{2}\\int_{0}^{t}\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{s})ds\\label{eq:multidimensional-ito-formula}\n\\end{aligned}\\]\n\n\nRemark 1. I stress that, as in the one-dimensional case, Ito’s formula is an equality of processes (and not an equality in distribution). Thus, the processes on both sides must agree for each path.\n\nInterestingly, the mixed partials \\(\\partial_{x_{i}x_{j}}f(B_{s})\\), \\(i\\neq j\\) do not appear in the formula! We see from Ito’s formula that the process \\(f(B_{t})\\) can be represented as a sum of \\(d+1\\) processes: \\(d\\) Ito integrals and one Riemann integral (which is a process of finite variation). In vector notation, the formula takes the form:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\int_{0}^{t}\\nabla f(B_{s})^{T}dB_{s}+\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}f(B_{s})ds\n\\end{aligned}\\]\nwhere it is understood that the first term is the sum of the \\(d\\) Ito integrals in the equation. The symbol \\(\\nabla^{2}\\) is the Laplacian of \\(f\\):\n\\(\\sum_{i=1}^{d}\\frac{\\partial^{2}}{\\partial x_{i}^{2}}f(B_{s}^{(1)},\\ldots,B_{s}^{(d)})ds\\)\nIn differential form, Ito’s formula becomes very neat:\n\\[\\begin{aligned}\ndf(B_{t}) & =\\sum_{i=1}^{d}\\partial_{x_{i}}f(B_{s})dB_{t}^{(i)}+\\frac{1}{2}\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{s})dt=\\nabla f(B_{t})^{T}dB_{s}+\\frac{1}{2}\\nabla^{2}f(B_{t})dt\n\\end{aligned}\\]\n\nExample 2 Consider the functions (1) \\(f(x_{1},x_{2})=x_{1}^{2}+x_{2}^{2}\\) (2) \\(f(x_{1},x_{2})=e^{x_{1}}\\cos x_{2}\\) and the processes \\((X_{t}:t\\geq0)\\) and \\((Y_{t}:t\\geq0)\\). If we apply Ito’s formula to the first process, we have:\n\\[\\begin{aligned}\nX_{t} & =\\int_{0}^{t}2B_{s}^{(1)}dB_{s}^{(1)}+\\int_{0}^{t}2B_{s}^{(2)}dB_{s}^{(2)}+\\frac{1}{2}\\int_{0}^{t}(4dt)\\\\\n& =\\int_{0}^{t}2B_{s}^{(1)}dB_{s}^{(1)}+\\int_{0}^{t}2B_{s}^{(2)}dB_{s}^{(2)}+2t\n\\end{aligned}\\]\nThe second process gives:\n\\[\\begin{aligned}\nY_{t} & =\\cos B_{s}^{(2)}\\int_{0}^{t}e^{B_{s}^{(1)}}dB_{s}^{(1)}-e^{B_{s}^{(1)}}\\int\\sin B_{s}^{(2)}dB_{s}^{(2)}+\\frac{1}{2}\\int_{0}^{t}\\left(e^{B_{s}^{(1)}}\\cos B_{s}^{(2)}-e^{B_{s}^{(1)}}\\cos B_{s}^{(2)}\\right)dt\\\\\n& =1+\\cos B_{s}^{(2)}\\int_{0}^{t}e^{B_{s}^{(1)}}dB_{s}^{(1)}-e^{B_{s}^{(1)}}\\int\\sin B_{s}^{(2)}dB_{s}^{(2)}\n\\end{aligned}\\]\n\n\nExample 3 (Cross-Variation of \\(B_{t}^{(1)}\\) and \\(B_{t}^{(2)}\\)). Let \\((t_{j}:j\\leq n)\\) be a sequence of partitions of \\([0,t]\\) such that \\(\\max_{j}|t_{j+1}-t_{j}|\\to0\\) as \\(n\\to\\infty\\). Prove that:\n\\[\\begin{aligned}\n\\lim_{n\\to\\infty}\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)}) & =0\\quad\\text{in } L^{2}\n\\end{aligned}\\]\nThis justifies the rule \\(dB_{t}^{(1)}\\cdot dB_{t}^{(2)}=0\\).\nHint: Just compute the second moment of the sum.\n\n\nSolution 1. We have:\n\\[\\begin{aligned}\n& \\mathbf{E}\\left[\\left(\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})\\right)^{2}\\right]\\\\\n= & \\sum_{j=0}^{n-1}\\mathbf{E}[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})^{2}(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})^{2}]\\\\\n+ & 2\\sum_{j&lt;k}\\mathbf{E}\\left[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{k+1}}^{(1)}-B_{t_{k}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})(B_{t_{k+1}}^{(2)}-B_{t_{k}}^{(2)})\\right]\n\\end{aligned}\\]\nBoth these expectations are zero, since the brownian motions are independent and non-overlapping increments are independent.\nConsequently, \\(\\sum_{j=0}^{n-1}(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)})(B_{t_{j+1}}^{(2)}-B_{t_{j}}^{(2)})\\to0\\) in the \\(L^{2}\\) sense.\n\n\nProof. The proof of the formula follows the usual recipe: Taylor’s theorem together with the quadratic variation and the cross-variation. In this case, we do get a cross-variation between the different Brownian motions. More precisely, consider a partition \\((t_{j}:j\\leq n)\\) of \\([0,t]\\). Then we can write:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{j=0}^{n-1}(f(B_{t_{j+1}})-f(B_{t_{j}}))\n\\end{aligned}\\]\nWe can apply the Taylor’s series expansion for each \\(j\\) to get:\n\\[\\begin{aligned}\nf(B_{t})-f(B_{0}) & =\\sum_{j=0}^{n-1}\\nabla f(B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})\\\\\n& +\\frac{1}{2}\\sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})^{T}Hf(B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})+\\mathcal{E}\n\\end{aligned}\\]\nwhere \\(Hf\\) is the Hessian matrix of \\(f\\). We wrote the expansion using the vector notation to be economical. Let’s keep in mind that each term is a sum over the derivatives. The first term will converge to \\(d\\) Ito integrals as in the one-dimensional case. Now, the summand in the second term is:\n\\[(B_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)},\\ldots,B_{t_{j+1}}^{(d)}-B_{t_{j}}^{(d)})\\left[\\begin{array}{ccc}\n\\partial_{x_{1}}^{2}f(B_{t_{j}}) & \\ldots & \\partial_{x_{1}x_{d}}^{2}f(B_{t_{j}})\\\\\n\\vdots & \\ddots\\\\\n\\partial_{x_{d}x_{1}}^{2}f(B_{t_{j}}) &  & \\partial_{x_{d}}^{2}f(B_{t_{j}})\n\\end{array}\\right]\\left[\\begin{array}{c}\nB_{t_{j+1}}^{(1)}-B_{t_{j}}^{(1)}\\\\\n\\vdots\\\\\nB_{t_{j+1}}^{(d)}-B_{t_{j}}^{(d)}\n\\end{array}\\right]\\]\nSo, \\((B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})\\) is pre-multiplied with the term \\(\\partial_{x_{i}x_{k}}^{2}f(B_{t_{j}})\\) and it is post-multiplied \\((B_{t_{j+1}}^{(k)}-B_{t_{j}}^{(k)})\\). Consequently, the second term in the Taylor’s series expansion can be re-written as:\n\\[\\sum_{j=0}^{n-1}\\left(\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(B_{t_{j}})(B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})^{2}+\\sum_{1\\leq i&lt;k\\leq d}\\partial_{x_{i}x_{k}}^{2}f(B_{t_{j}})(B_{t_{j+1}}^{(i)}-B_{t_{j}}^{(i)})(B_{t_{j+1}}^{(k)}-B_{t_{j}}^{(k)})\\right)\\]\nThe second term on the right converges to \\(0\\) in the \\(L^{2}\\) sense when \\(i\\neq k\\), from exercise. This explains why the mixed derivatives disappear in the multi-dimensional Ito’s formula. As for the case \\(i=k\\), it reduces to the quadratic variation as in the one-dimensional case. This is where the Riemann integral arises, after suitable conditioning on \\(\\mathcal{F}_{t_{j}}\\), the sigma-field generated by \\(B_{s}\\), \\(s\\leq t_{j}\\).\n\nAs in the one-dimensional case,it is not necessary to learn Ito’s formula by heart. It suffices to write the differential of the function \\(f\\) to second order. We can then apply the rules of multivariate Ito calculus:\n\n\n\n\\(\\cdot\\)\n\\(dt\\)\n\\(dB_{t}^{(1)}\\)\n\\(dB_{t}^{(2)}\\)\n\\(\\ldots\\)\n\n\n\n\n\\(dt\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(dB_{t}^{(1)}\\)\n\\(0\\)\n\\(dt\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(dB_{t}^{(2)}\\)\n\\(0\\)\n\\(0\\)\n\\(dt\\)\n\\(0\\)\n\n\n\\(\\ldots\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(dt\\)\n\n\n\nNote that the rule \\(dB_{t}^{(i)}dB_{t}^{(j)}=0\\) for \\(i\\neq j\\) is being motivated by the cross-variation result.\nHow can we construct martingales using the Ito’s formula? Recall that an Ito integral \\((\\int_{0}^{t}X_{s}dB_{s},t\\leq T)\\) is a martingale whenever the integrand is in \\(\\mathcal{L}_{c}^{2}(T)\\), the space of adapted processes with continuous paths and for which:\n\\[\\begin{aligned}\n\\int_{0}^{T}\\mathbf{E}[X_{s}^{2}]ds & &lt;\\infty\n\\end{aligned}\\]\nThe only difference here is that the integrand is a function of many Brownian motions. However, the integrands involved in the Ito integrals of the multidimensional Ito’s formula are clearly adapted to the filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) of \\((B_{t}:t\\geq0)\\) as they are functions of the Brownian motion at the time. The arguments of Ito integral in and apply verbatim, if we take the definition of \\(\\mathcal{L}_{c}^{2}(t)\\) with the filtration \\((\\mathcal{F}_{t}:t\\geq0)\\) of \\((B_{t}:t\\geq0)\\). With this in mind, we have the following corollary.\n\n\n\nCorollary 1 (Brownian Martingales.) Let \\((B_{t}:t\\geq0)\\) be a Brownian motion in \\(\\mathbf{R}^{d}\\). Consider \\(f\\in\\mathcal{C}^{2}(\\mathbf{R}^{d})\\) such that processes \\((\\partial_{x {i}}f(B_{t}),t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\) for every \\(i\\leq d\\). Then, the process :\n\\[f(B_{t})-\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}f(B_{s})ds,\\quad t\\leq T\\]\nwhere \\(\\nabla^{2}=\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}\\) is the Laplacian, is a martingale for the Brownian filtration.\n\nFor example, consider the processes \\(X_{t}=(B_{t}^{(1)})^{2}+(B_{t}^{(2)})^{2}\\) and \\(Y_{t}=\\exp(B_{t}^{(1)})\\cos(B_{t}^{(2)})\\). Then, we have :\n\\[\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}X_{s}ds=\\frac{1}{2}\\int_{0}^{t}4ds=2t\\]\nand\n\\[\\begin{aligned}\n\\frac{1}{2}\\int_{0}^{t}\\nabla^{2}Y_{s}ds & =\\frac{1}{2}\\int_{0}^{t}0\\cdot ds=0\n\\end{aligned}\\]\nThus, the processes \\(X_{t}-2t\\) and \\(Y_{t}\\) are martingales for the Brownian filtration. In one dimension, there are no interesting martingales constructed with functions of space only. Indeed, \\((f(B_{t}):t\\geq0)\\) is a martingale if and only if \\(f''(x)=0\\) for all \\(x\\). But, such functions are of the form \\(f(x)=ax+b\\), \\(a,b\\in\\mathbf{R}\\). In other words, in one dimension, Brownian martingales of the form \\(f(B_{t})\\) are simply \\(aB_{t}+b\\). Not very surprising! The situation is very different in higher dimensions. Indeed, corollary implies that \\(f(B_{t})\\) is a martingale whenever \\(f\\) is a harmonic function:\n\nDefinition 2 (Harmonic function.) A function \\(f:\\mathbf{R}^{d}\\to\\mathbf{R}\\) is harmonic in \\(\\mathbf{R}^{d}\\) if and only if \\(\\nabla^{2}f(x)\\equiv0\\) for all \\(x\\in\\mathbf{R}^{d}\\). More generally, a function \\(f:\\mathbf{R}^{d}\\to\\mathbf{R}\\) is harmonic in the region \\(\\mathcal{O}\\subset\\mathbf{R}^{d}\\) if and only if \\(\\nabla^{2}f(x)\\equiv0\\) for all \\(x\\in\\mathcal{O}\\).\n\nNote that the function \\(f(x)=e^{x_{1}}\\cos x_{2}\\) is harmonic in \\(\\mathbf{R}^{d}\\). This is why the process \\(Y_{t}=\\exp(B_{t}^{(1)})\\cos(B_{t}^{(2)})\\) is a martingale. The distinction to a subset of \\(\\mathbf{R}^{d}\\) in the above definition is important since it may happen that the function is harmonic only in a subset of the space; see for example equation. It is possible to define a Brownian martingale in such cases by considering the process until it exits the region. This will turn out to be important as we move ahead.\nThe multidimensional Ito’s formula generalizes to functions of time and space as in proposition:\n\nDefinition 3 A function \\(f:[0,\\infty)\\times\\mathbf{R}^{d}\\to\\mathbf{R}\\) is in \\(\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R}^{d})\\) if the partial derivative in time :\n\\[\\frac{\\partial}{\\partial t}f(t,\\mathbf{x})\\]\nexists and is continuous and the second order partial derivatives in space:\n\\[\\frac{\\partial^{2}}{\\partial x_{i}^{2}}f(t,x_{1},x_{2},\\ldots,x_{i},\\ldots,x_{d}),\\quad1\\leq i\\leq d\\]\nexist and are continuous."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#multidimensional-ito-formula---functions-of-space-and-time",
    "href": "posts/multivariate_ito_calculus/index.html#multidimensional-ito-formula---functions-of-space-and-time",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Theorem 2 (Multidimensional Ito’s formula for functions of space and time) Let \\((B_{t}:t\\leq T)\\) be a \\(d\\)-dimensional Brownian motion. Consider a function \\(f\\in\\mathcal{C}^{1,2}([0,T]\\times\\mathbf{R}^{d})\\). Then, we have with probability one for all \\(t\\leq T\\):\n\\[\\begin{aligned}\nf(t,B_{t})-f(0,B_{0}) & =\\sum_{i=1}^{d}\\int_{0}^{t}\\partial_{x_{i}}f(s,B_{s})dB_{s}^{(i)}+\\int_{0}^{t}\\left(\\partial_{t}f(s,B_{s})+\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(s,B_{s})\\right)ds\n\\end{aligned}\\]\n\nThe martingale condition is then similar to the ones in corollary ([cor:brownian-martingales-in-Rd]): if the processes \\((\\partial_{x_{i}}f(s,B_{s}),t\\leq T)\\in\\mathcal{L}_{c}^{2}(T)\\) for every \\(1\\leq i\\leq d\\), then the process\n\\[f(t,B_{t})-\\int_{0}^{t}\\left\\{ \\partial_{t}f(s,B_{s})++\\sum_{i=1}^{d}\\partial_{x_{i}}^{2}f(s,B_{s})\\right\\} ds,\\quad t\\leq T \\tag{1}\\]\nis a martingale for the Brownian filtration. In particular, if \\(f\\) satisfies the partial differential equation:\n\\[\n\\begin{aligned}\n\\frac{\\partial f}{\\partial t}+\\frac{1}{2}\\nabla^{2}f & =0\n\\end{aligned}\n\\tag{2}\\]\nthen the process \\((f(t,B_{t}):t\\leq T)\\) itself is a martingale."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#polyas-random-walk-theorem",
    "href": "posts/multivariate_ito_calculus/index.html#polyas-random-walk-theorem",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Consider a simple random walk on the integer lattice \\(\\mathbb{Z}^d\\). At each time step, a random walker makes a random move of length one in one of the lattice directions.\nWe say that, if the random walk returns to the origin with probability one infinitely many times, it is recurrent, if it does not we will call it transient.\nIt’s worth to spend a moment visiting Polya’s Random Walk Theorem, a fascinating, but not so intuitive result.\nAssuming all random walks start at the origin, we define \\(u\\) to be the probability that a random walker returns to the origin. The probability that the random walker returns to the origin exactly \\(m\\) times is:\n\\[\\begin{align*}\n{m \\choose m-1} u^{m-1} (1 - u)^{m - (m - 1)} = u^{m-1}(1-u)\n\\end{align*}\\]\nThe expected number of times the particle returns to the origin for an infinite random walk is:\n\\[\\begin{align*}\nE &= \\sum_{m = 1}^{\\infty} m (u^{m-1}(1-u))\\\\\n&= (1-u)\\sum_{m=1}^{\\infty}mu^{m-1}\\\\\n&= (1-u)\\sum_{m=1}^{\\infty}\\frac{d}{du}u^{m}\\\\\n\\end{align*}\\]\nWe note that limit exchanging is acceptable since the power series \\(\\sum_{n=1}^{\\infty}x^n\\) converges uniformly for \\(u &lt; 1\\) and we can apply the differentiable limit theorem.\nSo we obtain:\n\\[\\begin{align*}\nE &= (1-u)\\frac{d}{du}\\sum_{m = 1}^{\\infty} u^m \\\\\n&= (1-u)\\cdot \\frac{1}{(1-u)^2}\\\\\n&= \\frac{1}{1-u}\n\\end{align*}\\]\nSo, we see that if \\(E\\) is finite, \\(u &lt; 1\\), then the walk is transient and if \\(E = +\\infty\\), \\(u=1\\), then the walk is recurrent.\nWe then define \\(u_n\\) to be the probability that a given walk is at the origin on the \\(n\\)th step, defining the value \\(u_0=1\\) for the trivial loop. We also introduce an indicator random variable \\(x_n\\), which takes the value \\(1\\), if the particle is at the origin on the \\(n\\)th step and zero otherwise.\nThen\n\\[\\begin{align*}\nT = \\sum_{n=1}^{\\infty}x_n\n\\end{align*}\\]\nis the total number of times, the particle is at the origin, so \\(E\\) is equal to the expectation of \\(T\\), \\(\\mathbb{E}(T)\\), which is equal to:\n\\[\\begin{align*}\n\\mathbb{E}[T] &= \\sum_{n=1}^{\\infty}\\mathbb{E}[x_n]\\\\\n&= \\sum_{n=1}^{\\infty} u_n\n\\end{align*}\\]\nBut, we showed previously that if \\(E\\) is finite, then the walk is transient and if \\(E = \\infty\\), then the walk is recurrent, so we have established that if:\n\\[\\begin{align*}\n\\sum_{n=1}^{\\infty}u_n\n\\end{align*}\\]\nconverges then the walk is transient and if the sum diverges, then the walk is recurrent.\n\n\nNow that we have built up the necessary tools, we will consider the cases. We start by considering a simple random walk on \\(\\mathbb{Z}^1\\). Since, as mentioned previously, a walk must have an even number of steps to be a loop, we only look at \\(u_{2n},n \\in \\mathbb{Z}^+\\). A path of length \\(2n\\) returning to the origin must have \\(n\\) up-moves and \\(n\\) down-moves. The number of such paths are \\({2n \\choose n}\\). Each such path has a probability of occurrence \\(\\frac{1}{2^{2n}}\\). Thus,\n\\[\\begin{align*}\nu_{2n} = \\frac{1}{2^{2n}}{2n \\choose n}\n\\end{align*}\\]\nUsing Stirling’s approximation \\(n! \\sim \\sqrt{2\\pi n} e^{-n} n^{n}\\), we obtain:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{2^{2n}} \\frac{(2n)!}{n! n!}\\\\\n&\\approx \\frac{1}{2^{2n}} \\cdot \\frac{\\sqrt{2\\pi(2n)} e^{-2n}(2n)^{2n}}{\\left(\\sqrt{2\\pi n} e^{-n} n^n\\right)^2}\\\\\n&=\\frac{1}{\\sqrt{\\pi n}}\n\\end{align*}\\]\nSo, we see that:\n\\[\\begin{align*}\n\\sum_{n=0}^{\\infty} u_{2n} \\approx \\sum_{n=0}^{\\infty} \\frac{1}{\\sqrt{n\\pi}}\n\\end{align*}\\]\nAnd the series on the right certainly diverges so, we see that a simple random walk in \\(\\mathbb{Z}^1\\) is recurrent since \\(E\\) diverges.\n\n\n\nA particle has an equal probability \\(1/4\\) of moving left, right, up or down randomly in the \\(d=2\\) dimensional lattice. Each path of \\(2n\\) steps has a probability \\(\\frac{1}{4^{2n}}\\) of occurring. We then consider the number of paths with equal steps left and right (say \\(L\\) steps in each horizontal direction) and equal steps up and down (then \\(n-L\\)) is:\n\\[\\begin{align*}\n{2n \\choose L}{2n - L \\choose L}{2n - 2L \\choose n - L}{n-L \\choose n - L} = \\frac{(2n)!}{L! L! (n-L)! (n-L)!}\n\\end{align*}\\]\nSo we get:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{4^{2n}} \\sum_{L=0}^n \\frac{(2n)!}{L! L! (n-L)! (n-L)!} \\\\\n&= \\frac{1}{4^{2n}} \\sum_{L=0}^n \\frac{(2n)!}{n! n!} \\cdot \\left(\\frac{n!}{L! (n-L)!}\\right)^2 \\\\\n&=\\frac{1}{4^{2n}} {2n \\choose n}\\sum_{L=0}^n {n \\choose L}^2\n\\end{align*}\\]\nSuppose, we are to form a team of \\(n\\) players from a population of \\(2n\\) participants. There \\({2n \\choose n}\\) distinguishable teams of size \\(n\\). Alternatively, we could divide the population into two halves of \\(n\\) participants each, choose \\(L\\) participants from the first sub-population, and the remaining \\(n-L\\) participants from the second sub-population. So, the number of distinguishable teams is:\n\\[\\begin{align*}\n\\sum_{L=0}^{n}{n \\choose L}{n \\choose n - L} = \\sum_{L=0}^{n} {n \\choose L}^2\n\\end{align*}\\]\nSo, we have the combinatorial identity:\n\\[\\begin{align*}\n\\sum_{L=0}^{n} {n \\choose L}^2 = {2n \\choose n}\n\\end{align*}\\]\nThus,\n\\[\\begin{align*}\nu_{2n} &=\\frac{1}{4^{2n}} {2n \\choose n}\\sum_{L=0}^n {n \\choose L}^2\\\\\n&= \\left(\\frac{1}{2^{2n}} {2n \\choose n}\\right)^2\n\\end{align*}\\]\nSo, we have that, it is just the square of the result from \\(\\mathbb{Z}^1\\), so we see in this case that:\n\\[\\begin{align*}\nu_{2n} &\\approx \\sum_{n=0}^{\\infty} \\frac{1}{\\pi n}\n\\end{align*}\\]\nAnd the series on the right certainly diverges, so we see that a simple random walk in \\(\\mathbb{Z}^2\\) is recurrent as \\(E\\) diverges.\n\n\n\nAs before, in order to have a walk return to the origin we must have equal steps in the positive and negative directions for each direction and each path of \\(2n\\) has a probability of occurring of \\(\\frac{1}{6^{2n}}\\). Then, extending the idea from the previous subsection, the number of paths (of total length \\(2n\\)) with \\(L\\) steps left and right, \\(U\\) steps up and down and \\(n - L - U\\) steps forward and backward is:\n\\[\\begin{align*}\n\\frac{(2n)!}{L! L! U! U! (n - L - U)! (n - L - U)!}\n\\end{align*}\\]\nSo, we get:\n\\[\n\\begin{align*}\nu_{2n} &= \\frac{1}{6^{2n}} \\sum_{L,U,L+U \\leq n} \\frac{(2n)!}{L! L! U! U! (n - L - U)! (n - L - U)!} \\\\\n&= \\frac{1}{2^{2n}} {2n \\choose n} \\sum_{L,U,L+U \\leq n} \\left(\\frac{1}{3^n}\\frac{n!}{L! U! (n - U - L)!}\\right)^2\n\\end{align*}\n\\tag{3}\\]\nConsider a \\(3\\)-sided fair coin, with \\(\\mathbb{P}\\{\\text{Heads}\\} = \\mathbb{P}\\{\\text{Tails}\\} = \\mathbb{P}\\{\\text{Edge}\\} = \\frac{1}{3}\\). Suppose that the coin is tossed \\(n=3^2 = 9\\) times. The probability of landing \\(L\\) heads, \\(U\\) tails and \\(n - L - U\\) edges is \\(\\frac{n!}{L!U!(n-L-U)!} \\frac{1}{3^n}\\). This is precisely the term seen in the above expression. The number of heads, tails and edges in \\(n\\) coin tosses follows a multinomial distribution. It can be easily demonstrated, that in, \\(n/3 = 3\\) heads, \\(n/3\\) tails and \\(n/3\\) edges are most likely to occur in \\(n\\) coin tosses. So, we can find an upper bound on the summand in Equation 3 as:\n\\[\\begin{align*}\n\\frac{n!}{3^n L! U! (n- L - U)!} \\leq \\frac{n!}{3^n \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor!}\n\\end{align*}\\]\nAlso, from the multinomial expansion theorem:\n\\[\\begin{align*}\n(1 + a + b)^n = \\sum_{L,U,L+U\\leq n} \\frac{n!}{L! U! (n-L-U)!} a^L b^U\n\\end{align*}\\]\nSetting \\(a = b = 1\\), we get:\n\\[\\begin{align*}\n\\sum_{L,U,L+U\\leq n} \\frac{n!}{3^n L! U! (n-L-U)!} a^L b^U = 1\n\\end{align*}\\]\nWe then see that:\n\\[\\begin{align*}\nu_{2n} \\leq \\frac{1}{2^{2n}} {2n \\choose n} \\frac{n!}{3^n \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor! \\lfloor\\frac{n}{3}\\rfloor!}\n\\end{align*}\\]\nWe can simplify this as:\n\\[\\begin{align*}\nu_{2n} &= \\frac{1}{2^{2n}}{2n \\choose n} \\frac{n!}{3^n \\left(\\lfloor\\frac{n}{3}\\rfloor!\\right)^3} \\\\\n&\\approx \\frac{1}{\\sqrt{\\pi n}} \\times \\frac{\\sqrt{2\\pi n} e^{-n} n^n}{3^n \\left\\{\\sqrt{2\\pi \\frac{n}{3}}e^{-n/3} \\left(\\frac{n}{3}\\right)^{n/3}\\right\\}^3}\\\\\n&= \\frac{1}{\\sqrt{\\pi n}} \\frac{\\sqrt{2\\pi n} e^{-n} n^n}{3^n \\frac{(2\\pi n)^{3/2}}{3^{3/2}} e^{-n} \\frac{n^n}{3^n}}\\\\\n&= \\frac{1}{\\sqrt{\\pi n}} \\frac{\\sqrt{2\\pi n} \\cancel{e^{-n}} \\cancel{n^n}}{\\cancel{3^n} \\frac{(2\\pi n)^{3/2}}{3^{3/2}} \\cancel{e^{-n}} \\frac{ \\cancel{n^n}}{\\cancel{3^n}}}\\\\\n&\\approx \\frac{M}{(\\pi n)^{3/2}}\n\\end{align*}\\]\nwhere \\(M\\) is a positive constant. We then see:\n\\[\\begin{align*}\n\\sum_n u_{2n} &= M \\sum_n \\frac{1}{n^{\\frac{3}{2}}}\n\\end{align*}\\]\nThe sum on the right hand side converges, so we have that a simple random walk in three dimensions is transient. The proof of Polya’s Random Walk Theorem is complete."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#recurrence-and-transience-of-brownian-motion",
    "href": "posts/multivariate_ito_calculus/index.html#recurrence-and-transience-of-brownian-motion",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "In one dimension, we established that every path of a Brownian motion reaches any level \\(a \\in \\mathbf{R}\\). More precisely, for \\(a &gt; 0\\), if we define the stopping time :\n\\[\\begin{align*}\n\\tau = \\min \\{ t \\geq 0 : B_t \\geq a\\}\n\\end{align*}\\]\nwe have \\(\\mathbb{P}\\{\\tau_a &lt; \\infty \\} = 1\\). This implies in particular that every path will come back to \\(0\\) and will do infinitely many times. This is because \\(B(t+\\tau) - B(t)\\) is a standard brownian motion. This property of Brownian motion is called recurrence. This is to be compared with Brownian motion with a drift. In our analysis, we found that, if the drift is negative, then there are paths that will not reach a given level \\(a &gt; 0\\) with positive probability. Such paths go to infinity without ever going back to \\(0\\). This property of the process is called transience.\nWe can derive similar properties for the multidimensional Brownian motion. We will rely heavily on Corollary 1 and on some knowledge of harmonic functions. Harmonic functions play a very important role in mathematics, physics and in nature in general. As we mentioned earlier, if \\(d = 1\\), the only harmonic functions are the linear functions, since the equation \\(f''(x) = 0\\) has the solutions \\(f(x)=ax+b\\). However, in higher dimensions, the collection of harmonic functions is very rich. This gives access to a plethora of Brownian martingales. For example, the following functions are harmonic in the whole space minus the origin, \\(\\mathbf{R}^d \\setminus \\{0\\}\\).\n\\[\nh(x) = \\begin{cases}\n\\log ||x||, & d = 2 \\\\\n||x||^{2 - d}, & d \\geq 3\n\\end{cases}\n\\tag{4}\\]\nwhere \\(x\\in\\mathbf{R}^d\\).\n\nProof. Consider\n\\[\\begin{align*}\nf(x_1,x_2) = \\log \\sqrt {x_1^2 + x_2^2}\n\\end{align*}\\]\nWe have:\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x_1} &= \\frac{1}{\\sqrt{x_1^2 + x_2^2}} \\cdot \\frac{\\partial }{\\partial x_1} (\\sqrt{x_1^2 + x_2^2})\\\\\n&=\\frac{1}{\\sqrt{x_1^2 + x_2^2}} \\cdot \\frac{x_1}{\\sqrt{x_1^2 + x_2^2}}\\\\\n&= \\frac{x_1}{x_1^2 + x_2^2}\n\\end{align*}\\]\nFurther,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= \\frac{(x_1^2 + x_2^2)(1) - x_1(2x_1)}{(x_1^2 + x_2^2)^2} \\\\\n&= \\frac{x_1^2 - x_2^2}{(x_1^2 + x_2^2)}\n\\end{align*}\\]\nBy symmetry,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_2^2}\n&= \\frac{x_2^2 - x_1^2}{(x_1^2 + x_2^2)}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\n\\frac{\\partial^2 f}{\\partial x_1^2} &= - \\frac{\\partial^2 f}{\\partial x_2^2}\\\\\n\\frac{\\partial^2 f}{\\partial x_1^2} + \\frac{\\partial^2 f}{\\partial x_2^2}&=0\n\\end{align*}\\]\nWe conclude that \\(f\\) is harmonic.\nNext, consider\n\\[\\begin{align*}\ng(x_1,x_2,\\ldots,x_d) = \\frac{1}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-2}}\n\\end{align*}\\]\nNow,\n\\[\\begin{align*}\n\\frac{\\partial g}{\\partial x_i} &= \\frac{(2-d)}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}} \\cdot \\frac{\\partial }{\\partial x_i}\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)\\\\\n&= \\frac{(2-d)}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}}\\cdot \\frac{2x_i}{2\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)}\\\\\n&= \\frac{(2-d)x_i}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^d}\n\\end{align*}\\]\nFurther, we have:\n\\[\\begin{align*}\n\\frac{\\partial^2 g}{\\partial x_i^2} &= (2-d)\\frac{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^d (1) - (x_i)\\cdot d \\cdot\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-1}\\cdot \\frac{x_i}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)}}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{2d}}\\\\\n&=(2-d)\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{d-2} \\frac{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^2  - d x_i^2}{\\left(\\sqrt{x_1^2 + x_2^2 + \\ldots + x_d^2}\\right)^{2d}}\\\\\n&=(2-d) \\frac{\\left(\\sum_{i=1}^d x_i^2\\right) - dx_i^2 }{||x||^{d+2}}\n\\end{align*}\\]\nWe conclude that:\n\\[\\begin{align*}\n\\sum_{i=1}^{d} \\frac{\\partial^2 g}{\\partial x_i^2} = 0\n\\end{align*}\\]\nand hence \\(g\\) is harmonic.\n\nInterestingly enough, the answer to the recurrence versus transience puzzle depends on the dimension. We will show that Brownian motion is recurrent in dimension \\(d=2\\) in the sense that every Brownian motion path starting from \\(x\\) will eventually enter a disc around the origin, no matter how small the disc is. The Brownian path will then enter this disc infinitely many times, as \\(t \\to \\infty\\). Note that, we did not say that the path actually hits \\(0\\), but that it enters a disc around the \\(0\\). This nuance is important, as we will in fact show that a Brownian path actually never hits a given point. In dimension \\(d=3\\) or higher, it is proved that there are some paths starting from a given \\(x\\) that will never enter a given ball around the origin with positive probability. This is the transience property.\nThe strategy of the proof is similar to the approach for Brownian motion with a drift. We will find a good function \\(h : \\mathbf{R}^d \\to \\mathbf{R}\\) for which \\(h(B_t)\\) is a martingale. In light of Corollary 1, this function needs to be harmonic in a suitable region. The desired probability is then obtained by considering the right boundary values.\n\nTheorem 3 (Recurrence and transience of a brownian motion in \\(\\mathbf{R}^d\\)) Let \\((B_t,t\\geq 0)\\) be a Brownian motion in \\(\\mathbf{R}^d\\) starting at \\(B_0 = x\\). Consider for \\(r &lt; ||x||\\), the stopping time\n\\[\\begin{align*}\n\\tau_r = \\min \\{t \\geq 0 : ||B_t|| &lt; r\\}\n\\end{align*}\\]\nthe first hitting time of the paths in a ball of radius \\(r\\) around the origin. We have:\n\\[\\begin{align*}\n\\mathbb{P}(\\tau_r &lt; \\infty) =\n\\begin{cases}\n1 & \\text{ if }d \\leq 2 \\\\\n\\left(\\frac{r}{||x||}\\right)^{d-2} & \\text{ if }d \\geq 3\n\\end{cases}\n\\end{align*}\\]\nIn particular, for \\(d \\leq 2\\), the paths are recurrent; that is, they come back infinitely many times in a neighbourhood of the origin. For \\(d \\geq 3\\), each path will eventually never come back to a neighbourhood of the origin.\n\nNote that we made sure that the starting point of the Brownian motion \\(x\\) is outside the ball of radius \\(r\\).\n\nProof. Consider another hitting time of a ball with a radius larger than \\(||x||\\):\n\\[\\begin{align*}\n\\tau_R' = \\min \\{t \\geq 0 : ||B_t|| \\geq R\\}, \\quad r &lt; ||x|| &lt; R\n\\end{align*}\\]\nNote that \\(\\tau_R'\\) must increase with \\(R\\) and that it must go to \\(+\\infty\\) as \\(R \\to \\infty\\). Moreover, the sequence of events \\(\\{\\tau_r &lt; \\tau_R'\\}_{R &gt; ||x||}\\) is increasing. In particular, by continuity of probability measure, we have:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R') &= \\mathbb{P}(\\lim_{R \\to \\infty} \\tau_r &lt; \\tau_R')\\\\\n&= \\mathbb{P}(\\tau_r &lt; \\infty)\n\\end{align*}\\]\nIf we set \\(\\tau = \\tau_r \\land \\tau_R'\\), then the event \\(\\{\\tau_r &lt; \\tau_R'\\}\\) is the same as the event \\(\\{||B_\\tau||=r\\}\\). So, \\(\\lim_{R \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R') = \\lim_{R \\to \\infty} \\mathbb{P}(||B_\\tau||=r)\\).\nNow, consider the event \\(\\{\\tau &lt; \\infty\\}\\). Let \\(E_n\\) be the event that the \\(n\\)th increment \\(||B_n - B_{n-1}||\\) exceeds \\(R\\). If \\(E_n\\) occurs, then we must have the Brownian motion exits the spherical shell of thickness \\(R-r\\). Moreover, we have \\(\\mathbb{P}(E_1) = \\ldots = \\mathbb{P}(E_n)\\), for all \\(n\\). Call this probability \\(p\\). Clearly, \\(0 &lt; p &lt; 1\\). Since the events \\(E_n\\) are independent, we have:\n\\[\\begin{align*}\n\\mathbb{P}(E_1^C \\cap \\ldots \\cap E_n^C) = (1 - p)^n\n\\end{align*}\\]\nAs \\(n \\to \\infty\\), we have \\(\\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}E_k^C) = 0\\). Now, let \\(F_k\\) be the event that there are no excursions upto \\(k\\). That is, \\(F_k = \\bigcap_{i=1}^{k}E_i^C\\). Now, the event \\(F_{k+1}\\) implies \\(F_k\\), or equivalently, \\(F_{k+1} \\subseteq F_k\\). So, \\(\\{F_k\\}_{k=1}^{\\infty}\\) is a decreasing sequence of events. Moreover, \\(\\bigcap_{k=1}^n F_k = \\bigcap_{k=1}^n E_k^C\\). So, by continuity of probability measure,\n\\[\\begin{align*}\n\\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}E_k^C)\n&= \\lim_{n \\to \\infty} \\mathbb{P}(\\bigcap_{k=1}^{n}F_k) \\\\\n&= \\mathbb{P}(\\lim_{n \\to \\infty} \\bigcap_{k=1}^{n}F_k) \\\\\n&= \\mathbb{P}(\\bigcap_{k=1}^{\\infty}F_k) \\\\\n&= \\mathbb{P}(\\bigcap_{k=1}^{\\infty}E_k^C)\n\\end{align*}\\]\nSo, \\(\\mathbb{P}(\\bigcup_{k=1}^{\\infty}E_k) = 1\\). There exists an \\(n_0 \\in \\mathbf{N}\\), such that the event \\(E_{n_0}\\), an excursion at \\(n_0\\) occurs with probability \\(1\\). So, the event \\(\\{\\tau &lt; \\infty\\}\\) occurs almost surely.\nTo compute \\(\\mathbb{P}(||B_\\tau||=r)\\), the idea is to find a good function \\(h\\) with the following properties:\n\nMartingale: \\((h(B_t),t\\leq \\tau)\\) is a bounded martingale, so that the optional stopping theorem implies:\n\n\\[\\begin{align*}\n\\mathbb{E}[h(B_\\tau)] = h(B_0) = h(x)\n\\end{align*}\\]\n\nBoundary values: Since \\(B_\\tau\\) is a point at \\(||x||=r\\) or at \\(||x||=R\\), we pick boundary conditions for \\(h\\), so that \\(\\mathbb{E}[h(B_{\\tau})]=\\mathbb{P}(||B_\\tau||=r)\\).\n\nThe second point is easy; it suffices to take the boundary conditions \\(h(x)=0\\) at \\(||x||=R\\) and \\(h(x)=1\\) at \\(||x||=r\\). For the first point, by Ito’s formula, we pick \\(h\\) to be harmonic in the annulus \\(\\{x \\in \\mathbf{R}^d : r &lt; ||x||&lt;R \\}\\). Thus, we can use the function in Equation 4. Of course, the functions \\(ah(x) + b\\) remain harmonic. To satisfy the boundary conditions, we pick \\(a\\) and \\(b\\) suitably:\n\n\nLet \\(h(x) = ax + b\\). Then, at \\(x=r\\), we have:\n\\[\n\\begin{align*}\nar + b = 1\n\\end{align*}\n\\tag{5}\\]\nand at \\(x = R\\), we have:\n\\[\n\\begin{align*}\naR + b = 0\n\\end{align*}\n\\tag{6}\\]\nSo, \\(b = -aR\\). Substituting in Equation 5, we get:\n\\[\\begin{align*}\na &= \\frac{1}{r - R}\nb &= -\\frac{R}{r - R}\n\\end{align*}\\]\nConsequently,\n\\[\n\\begin{align*}\nh(x) &= \\frac{R - x}{R - r}\n\\end{align*}\n\\tag{7}\\]\n\n\n\nLet \\(h(x) = \\log ||x||\\). Then, at \\(||x||=r\\), we have:\n\\[\n\\begin{align*}\na \\log ||x|| + b &= 1\na \\log r + b &= 1\n\\end{align*}\n\\tag{8}\\]\nAt \\(||x||=R\\), we have:\n\\[\n\\begin{align*}\na \\log ||x|| + b &= 0\na \\log R + b &= 0\n\\end{align*}\n\\tag{9}\\]\nThus, from Equation 9, \\(b = -a \\log R\\). Substituting the value for \\(b\\) in Equation 8, we have:\n\\[\n\\begin{align*}\na \\log r - a \\log R &= 1\\\\\na &= \\frac{1}{\\log r - \\log R}\n\\end{align*}\n\\]\nConsequently,\n\\[\n\\begin{align*}\nh(x) &= \\frac{\\log ||x||}{\\log r - \\log R} -  \\frac{1}{\\log r - \\log R} \\log R\\\\\n&= \\frac{\\log R - \\log ||x||}{\\log R - \\log r}\n\\end{align*}\n\\tag{10}\\]\n\n\n\nLet \\(h(x)=||x||^{2-d}\\). At \\(||x||=r\\), \\(a h(x) + b = 1\\). So, we have:\n\\[\n\\begin{align*}\na r^{2-d} + b = 1\n\\end{align*}\n\\tag{11}\\]\nAt \\(||x||=R\\), \\(ah(x) + b = 0\\). So, we have:\n\\[\n\\begin{align*}\na ||R||^{2-d} + b &= 0\n\\end{align*}\n\\tag{12}\\]\nFrom Equation 12, we get \\(b = -a R^{2-d}\\). Substituting this value in Equation 11, we get:\n\\[\\begin{align*}\na r^{2-d} - aR^{2-d}  &= 1\\\\\na &= \\frac{1}{r^{2-d} - R^{2-d}}\n\\end{align*}\\]\nAnd\n\\[\n\\begin{align*}\nb = - \\frac{R^{2-d}}{r^{2-d} - R^{2-d}}\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\nh(x) &= \\frac{||x||^{2-d}}{r^{2-d} - R^{2-d}} - \\frac{R^{2-d}}{r^{2-d} - R^{2-d}}\\\\\n&= \\frac{R^{2-d} - ||x||}{R^{2-d} - r^{2-d}}\n\\end{align*}\n\\tag{13}\\]\nCollecting all the results together, we get:\n\\[\nh(x) = \\begin{cases}\n\\frac{R - x}{R - r}, & d=1 \\\\\n\\frac{\\log R - \\log ||x||}{\\log R - \\log r}, & d=2 \\\\\n\\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}, & d \\geq 3\n\\end{cases}\n\\tag{14}\\]\nPassing to the limit as \\(R \\to \\infty\\), for \\(d=1\\), we have:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} h(x) &= \\lim_{R \\to \\infty} \\frac{R - x}{R - r} \\\\\n&= \\lim_{R \\to \\infty} \\frac{1 - \\frac{x}{R}}{1 - \\frac{r}{R}}\\\\\n&= 1\n\\end{align*}\\]\nFor \\(d = 2\\), we get:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty} h(x) &= \\lim_{R \\to \\infty} \\frac{1 - \\frac{\\log x}{\\log R}}{1 - \\frac{\\log r}{\\log R}} \\\\\n&= 1\n\\end{align*}\\]\nFor \\(d = 3\\), we get:\n\\[\\begin{align*}\n\\lim_{R \\to \\infty}  h(x) &= \\lim_{R \\to \\infty} \\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}\\\\\n&= \\left(\\frac{||x||}{r}\\right)^{2-d}\n\\end{align*}\\]\nThis concludes the proof. \\(\\blacksquare\\)\n\n\n\nExample 4 We get more from the above proof. Indeed, in dimension \\(d \\geq 2\\), it implies that the probability that a Brownian motion starting at \\(B_0 = x\\) eventually hits any other point \\(y\\) is \\(0\\). Indeed, if we take \\(y = 0\\) and \\(x \\neq 0\\), we have:\n\\[\n\\begin{align*}\n\\mathbb{P}(\\exists t &gt; 0: B_t = 0) &= \\lim_{R \\to \\infty} \\lim_{r \\to \\infty} \\mathbb{P}(\\tau_r &lt; \\tau_R')\\\\\n&= \\lim_{R \\to \\infty} \\lim_{r \\to \\infty} \\frac{R^{2-d} - ||x||^{2-d}}{R^{2-d} - r^{2-d}}\\\\\n&=\\lim_{R \\to \\infty}  \\lim_{r \\to \\infty} \\frac{(R/r)^{2-d} - (||x||/r)^{2-d}}{(R/r)^{2-d} - 1}\\\\\n&= \\lim_{R \\to \\infty} 0\\\\\n&= 0\n\\end{align*}\n\\]\nHere, we shrink the inner radius to \\(0\\) first and then we let the outer radius go to infinity after. It remains to take the limits of the expression obtained in equation Equation 14. The first limit \\(r \\to 0\\) gives \\(0\\) right away."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#dynkins-formula-and-the-dirichlet-problem",
    "href": "posts/multivariate_ito_calculus/index.html#dynkins-formula-and-the-dirichlet-problem",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "In the last section, we translated the problem of recurrence and transience of Brownian motion in terms of a boundary value problem: we needed to find a particular function \\(h\\) that solves a PDE (that is, \\(\\nabla^2 h=0\\)) in a certain region of space(the annulus) and that satisfies given boundary conditions (the function is \\(1\\) on the inner circle and \\(0\\) on the outer circle). This is a particular case of the Dirichlet problem in PDE.\nConsider a region \\(\\mathcal{O} \\subset \\mathbb{R}^d\\). \\(\\mathcal{O}\\) is an open ball. That is, for all \\(x \\in \\mathcal{O}\\), there exists \\(\\epsilon&gt;0\\), such that \\(V_\\epsilon(x) = \\{y : ||y - x|| &lt; \\epsilon\\}\\) is contained in \\(\\mathcal{O}\\). \\(\\mathcal{O}\\) doesn’t contain boundary points \\(\\partial \\mathcal{O}\\). In the proof of Theorem 3, the region was the annulus \\(\\{x: r &lt; ||x|| &lt; R\\}\\) and the boundary was given by the spheres \\(\\{x :||x|| = r\\}\\) and \\(\\{x: ||x||=R\\}\\). The Dirichlet problem in \\(\\mathcal{O}\\) can be stated as follows:\nLet \\(f:\\partial\\mathcal{O} \\to \\mathbb{R}\\) be a function on the boundary of \\(\\mathcal{O}\\). Can we extend the function \\(f\\) to \\(O\\) in such a way that the extension is harmonic on \\(\\mathcal{O}\\) and coincides with \\(f\\) on the boundary?\nIn the instance of the proof of the Theorem 3, we knew the solution of the problem, thanks to Equation 4. The only job left to do was to adjust some parameters to fit the boundary conditions. It turns out that the formalism of stochastic calculus allows us to express the solution of the Dirichlet problem as an average over Brownian paths for general regions of space and boundary conditions. To do so, we look at a twist on the Ito’s formula called the Dynkin’s formula.\nLet \\(x\\) be a point in \\(\\mathcal{O}\\). Consider a Brownian motion \\((B_t,t\\geq 0)\\) starting at \\(x\\), \\(B_0 = x\\). To emphasize the dependence on \\(x\\), we write \\(\\mathbb{P}_x\\) and \\(\\mathbb{E}_x\\) for the probability and the expectation for the Brownian motion. Dykin’s formula is obtained by merging Corollary 1 for the Brownian martingales together with the optional stopping theorem. More precisely, consider \\(f \\in C^2(\\mathbb{R}^d)\\) that satisfies the assumption of the Corollary 1. We then have that the process:\n\\[\\begin{align*}\nM_t &= f(B_t) - \\frac{1}{2}\\int_{0}^t \\nabla^2 f(B_s) ds, \\quad t \\geq 0\n\\end{align*}\\]\nis a Brownian martingale.\nConsider also the stopping time \\(\\tau = \\min \\{t \\geq 0 : B_t \\in \\mathcal{O}^C\\}\\). This is the first exit time of \\(\\mathcal{O}\\) of the Brownian motion path. Then, if the assumptions of the optional stopping theorem are fulfilled, we get Dynkin’s formula:\n\\[\n\\begin{align*}\n\\mathbb{E}[M_\\tau] = \\mathbb{E}[f(B_\\tau)] - \\frac{1}{2} \\mathbb{E}\\left[\\int_0^\\tau \\nabla^2 f(B_s)ds\\right] = f(B_0) = M_0\n\\end{align*}\n\\tag{15}\\]\nAs an application of Dynkin’s formula, let’s express the solution of the Dirichlet problem as an average over Brownian motion. If \\(f\\) is harmonic in \\(\\mathcal{O}\\), then \\(\\nabla^2 f = 0\\) in \\(\\mathcal{O}\\), so the process \\((f(B_t),t\\leq \\tau)\\) is itself a martingale. Note that since \\(\\mathcal{O}\\) is bounded, this is a bounded martingale! Moreover, the integrands \\(\\partial_i f(B_s)\\), \\(1 \\leq i \\leq d\\), are bounded. We conclude that the solution to the Dirichlet problem can be represented as follows:\n\\[\n\\begin{align*}\nf(x) = \\mathbb{E}_x[f(B_\\tau)], \\quad x \\in \\mathcal{O}\n\\end{align*}\n\\tag{16}\\]\nIn other words, the value of the harmonic function of \\(f\\) at \\(x\\) is given by the average over the Brownian paths of the value of the function at the exit point of the path on the boundary of \\(\\mathcal{O}\\). Equation Equation 16 does not prove the existence of the solution to the Dirichlet problem, as we assume that such a function exists. Existence of the solution to the Dirichlet problem can be shown this way, assuming the region \\(\\mathcal{O}\\) and the function on the boundary are nice enough. However, note that the averaging on the right-hand side of the formula makes sense even in the case where \\(f\\) is simply defined on the boundary \\(\\partial \\mathcal{O}\\). We then define a posteriori the harmonic extension of \\(f\\) inside \\(\\mathcal{O}\\) by defining its value at \\(x\\) as \\(\\mathbb{E}_x[f(B_\\tau)]\\)."
  },
  {
    "objectID": "posts/multivariate_ito_calculus/index.html#exercises",
    "href": "posts/multivariate_ito_calculus/index.html#exercises",
    "title": "Multivariate Ito Calculus",
    "section": "",
    "text": "Exercise 1 Gaussian Integration by parts.\n\nLet \\(Z\\) be a standard Gaussian random variable. Show using integration by parts that for a differentiable function \\(g\\):\n\n\\[\n\\begin{align*}\n\\mathbb{E}[Z g(Z)] = \\mathbb{E}[g'(Z)]\n\\end{align*}\n\\tag{17}\\]\nwhere we assume that both expectations are well-defined.\n\nUse this to recover the expression for the moments for \\(\\mathbb{E}[Z^{2j}],j \\in \\mathbb{N}\\).\n\n\nSolution.\nWe have:\n\\[\n\\begin{align*}\n\\mathbb{E}[g'(Z)] &= \\int_{-\\infty}^{\\infty}\\phi(z) \\cdot g'(z) dz \\\\\n&= \\left[\\frac{e^{-z^2/2}}{\\sqrt{2\\pi}}\\cdot g(z)\\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} g(z) \\frac{-ze^{-z^2/2}}{\\sqrt{2\\pi}}dz \\\\\n&= 0 + \\int_{-\\infty}^{\\infty} z g(z) \\phi(z)dz\\\\\n&= \\mathbb{E}[Zg(Z)]\n\\end{align*}\n\\]\nMoreover, we can write:\n\\[\n\\begin{align*}\n\\mathbb{E}[Z^{2n}] &= \\mathbb{E}[Z \\cdot Z^{2n-1}] \\\\\n&= \\mathbb{E}[Z \\cdot g(Z)], \\quad \\{ \\text{ where }g(x)=x^{2n - 1}\\} \\\\\n&= \\mathbb{E}[g'(Z)] \\\\\n&= (2n - 1) \\mathbb{E}[Z^{(2n - 2)}] \\\\\n& \\vdots \\\\\n&= (2n - 1)\\cdots 5 \\cdot 3 \\cdot \\mathbb{E}[Z^2] \\\\\n&= (2n - 1)\\cdots 5 \\cdot 3 \\cdot 1\n\\end{align*}\n\\]\n\nExercise 2 The Heat Equation.\nUse the Gaussian integration by parts identity to check that the solution \\(f(t,x)\\) to the heat equation is:"
  },
  {
    "objectID": "posts/numerical-integration/index.html",
    "href": "posts/numerical-integration/index.html",
    "title": "Numerical Integration",
    "section": "",
    "text": "We are interested in an approximate calculation of the definite integral\n\\[\n\\begin{align*}\nI[f] = \\int_{a}^{b}f(x)dx\n\\end{align*}\n\\tag{1}\\]\nwhere \\(f(x)\\) is a given function and \\([a,b]\\) is a finite interval. This problem is often called numerical quadrature, since it relates to the ancient problem of the quadrature of a circle i.e. constructing a square with equal area to that of a circle. The computation of the above quantity is equivalent to solving the IVP:\n\\[\n\\begin{align*}\ny'(x) = f(x), \\quad y(a)=0, \\quad x \\in[a,b]\n\\end{align*}\n\\tag{2}\\]\nfor \\(y(b)=I[f]\\).\nAs is well known, even many relatively simple integrals cannot be expressed in finite terms of elementary functions, and thus must be evaluated by numerical methods. Even when a closed form analytical solution exists, it may be preferable to use a numerical quadrature formula.\nSince \\(I[f]\\) is a linear functional, numerical integration is a special case of the problem of approximating a linear functional. We shall consider formulas of the form:\n\\[\n\\begin{align*}\nI[f] \\approx \\sum_{i=1}^n w_if(x_i)\n\\end{align*}\n\\tag{3}\\]\nwhere \\(x_1 &lt; x_2 &lt; \\ldots &lt; x_n\\) are distinct nodes and \\(w_1\\), \\(w_2\\), \\(\\ldots\\), \\(w_n\\) the corresponding weights. Often (but not always) all nodes lie in \\([a,b]\\).\n\nDefinition 1 (Order of accuracy of a Quadrature Rule) A quadrature rule (Equation 3) has order of accuracy (or degree of exactness) equal to \\(d\\), iff it is exact for all polynomials of degree \\(\\leq d\\), that is, for all \\(p\\in\\mathcal{P}_{d+1}\\).\n\n\n\n\nInterpolatory quadrature formulas, where the nodes are constrained to be equally spaced, are called Newton-Cotes formulas. These are especially suited for integrating a tabulated function, a task that was more common before the computer age. The midpoint, the trapezoidal and the Simpson’s rules, to be described here, are all special cases of (unweighted) Newton-Cotes formulas.\nThe trapezoidal rule is based on the linear interpolation of \\(f(x)\\) at \\(x_1 = a\\) and \\(x_2 = b\\), that is, \\(f(x)\\) is approximated by :\n\\[\n\\begin{align*}\np(x) = f(a) + (x-a)[a,b]f = f(a) + (x - a)\\frac{f(b) - f(a)}{b - a}\n\\end{align*}\n\\]\nThe integral of \\(p(x)\\) equals the area of a trapezoid with base \\((b-a)\\) times the average height \\(\\frac{1}{2}(f(a) + f(b))\\). Hence,\n\\[\n\\int_{a}^{b} f(x)dx \\approx \\frac{(b-a)}{2}(f(a) + f(b))\n\\]\nTo increase the accuracy, we subdivide the interval \\([a,b]\\) and assume that \\(f_i = f(x_i)\\) is known on a grid of equidistant points:\n\\[\n\\begin{align*}\nx_0 = a, \\quad x_i = x_0 + ih, \\quad x_n = b\n\\end{align*}\n\\tag{4}\\]\nwhere \\(h = (b - a)/n\\) is the step length. The trapezoidal approximation for the \\(i\\)th subinterval is:\n\\[\n\\begin{align*}\n\\int_{x_i}^{x_{i+1}} f(x)dx = T(h) + R_i, \\quad T(h) = \\frac{h}{2}(f_i + f_{i+1})\n\\end{align*}\n\\tag{5}\\]\nLet \\(p_2(x)\\in\\mathcal{P}_2\\) be the unique interpolating polynomial (Newton polynomial) passing through the points \\((x_i,f_i)\\) and \\((x_{i+1},f_{i+1})\\), that is, \\(p_2(x_i)=f(x_i)\\) and \\(p_2(x_{i+1}) = f(x_{i+1})\\). The exact remainder in Newton’s interpolation formula is given by:\n\\[\n\\begin{align*}\nf(x) - p_2(x) &= [x_i,x_{i+1},x]f\\cdot \\Phi_2(x)\\\\\n&=[x_i,x_{i+1},x]f \\cdot (x - x_{i+1})\\Phi_1(x)\\\\\n&=[x_i,x_{i+1},x]f \\cdot (x - x_{i+1})(x - x_{i})\\Phi_0\\\\\n&=[x_i,x_{i+1},x]f \\cdot (x - x_{i+1})(x - x_{i})\n\\end{align*}\n\\]\nSo, we have:\n\\[\n\\begin{align*}\nR_i &= \\int_{x_i}^{x_{i+1}} (f(x) - p_2(x))dx = \\int_{x_i}^{x_{i+1}}(x - x_i)(x - x_{i+1})[x_i,x_{i+1},x]f dx\n\\end{align*}\n\\tag{6}\\]\nBy the theorem on the remainder term for interpolation, we can write:\n\\[\n[x_1,\\ldots,x_n,x_{n+1}]f = \\frac{f^{(n)}(\\xi)}{n!}\n\\]\nConsequently,\n\\[\n[x_i,x_{i+1},x]f = \\frac{f''(\\xi)}{2}\n\\]\nSo, we have:\n\\[\n\\begin{align*}\nR_i &= \\frac{f''(\\xi)}{2}\\int_{x_i}^{x_{i+1}} (x - x_i)(x - x_{i+1})dx\n\\end{align*}\n\\tag{7}\\]\nSetting \\(x = x_i + ht\\), \\(dx = hdt\\) such that the limits of integration are from \\(t=0\\) to \\(t=1\\), $we get:\n\\[\n\\begin{align*}\nR_i &=  \\frac{f''(\\xi)}{2} \\int_{0}^{1}(ht)(x_i + ht - x_{i+1})h dt\\\\\n&= \\frac{f''(\\xi)}{2} \\int_{0}^{1}(ht)(ht - h)h dt\\\\\n&= \\frac{f''(\\xi)}{2} \\int_{0}^{1}(ht)(ht - h)h dt\\\\\n&= \\frac{f''(\\xi) h^3}{2} \\int_{0}^{1}(t^2 - t) dt\\\\\n&= \\frac{f''(\\xi) h^3}{2} \\left[\\frac{t^3}{3} - \\frac{t^2}{2}\\right]_{0}^{1} \\\\\n&= \\frac{f''(\\xi) h^3}{2} \\left[\\frac{1}{3} - \\frac{1}{2}\\right] \\\\\n&= -\\frac{1}{12}h^3 f''(\\xi)\n\\end{align*}\n\\tag{8}\\]\nSumming the contributions for each subinterval \\([x_i,x_{i+1}]\\), \\(i=0...n\\), gives:\n\\[\n\\begin{align*}\n\\int_{a}^{b}f(x)dx = T(h) + R_T, \\quad T(h) = \\frac{h}{2}(f_0 + f_n) + h\\sum_{i=1}^{n-1}f_i\n\\end{align*}\n\\tag{9}\\]\nwhich is the composite trapezoidal rule. The global truncation error is:"
  },
  {
    "objectID": "posts/numerical-integration/index.html#interpolatory-quadrature-rules",
    "href": "posts/numerical-integration/index.html#interpolatory-quadrature-rules",
    "title": "Numerical Integration",
    "section": "",
    "text": "We are interested in an approximate calculation of the definite integral\n\\[\n\\begin{align*}\nI[f] = \\int_{a}^{b}f(x)dx\n\\end{align*}\n\\tag{1}\\]\nwhere \\(f(x)\\) is a given function and \\([a,b]\\) is a finite interval. This problem is often called numerical quadrature, since it relates to the ancient problem of the quadrature of a circle i.e. constructing a square with equal area to that of a circle. The computation of the above quantity is equivalent to solving the IVP:\n\\[\n\\begin{align*}\ny'(x) = f(x), \\quad y(a)=0, \\quad x \\in[a,b]\n\\end{align*}\n\\tag{2}\\]\nfor \\(y(b)=I[f]\\).\nAs is well known, even many relatively simple integrals cannot be expressed in finite terms of elementary functions, and thus must be evaluated by numerical methods. Even when a closed form analytical solution exists, it may be preferable to use a numerical quadrature formula.\nSince \\(I[f]\\) is a linear functional, numerical integration is a special case of the problem of approximating a linear functional. We shall consider formulas of the form:\n\\[\n\\begin{align*}\nI[f] \\approx \\sum_{i=1}^n w_if(x_i)\n\\end{align*}\n\\tag{3}\\]\nwhere \\(x_1 &lt; x_2 &lt; \\ldots &lt; x_n\\) are distinct nodes and \\(w_1\\), \\(w_2\\), \\(\\ldots\\), \\(w_n\\) the corresponding weights. Often (but not always) all nodes lie in \\([a,b]\\).\n\nDefinition 1 (Order of accuracy of a Quadrature Rule) A quadrature rule (Equation 3) has order of accuracy (or degree of exactness) equal to \\(d\\), iff it is exact for all polynomials of degree \\(\\leq d\\), that is, for all \\(p\\in\\mathcal{P}_{d+1}\\).\n\n\n\n\nInterpolatory quadrature formulas, where the nodes are constrained to be equally spaced, are called Newton-Cotes formulas. These are especially suited for integrating a tabulated function, a task that was more common before the computer age. The midpoint, the trapezoidal and the Simpson’s rules, to be described here, are all special cases of (unweighted) Newton-Cotes formulas.\nThe trapezoidal rule is based on the linear interpolation of \\(f(x)\\) at \\(x_1 = a\\) and \\(x_2 = b\\), that is, \\(f(x)\\) is approximated by :\n\\[\n\\begin{align*}\np(x) = f(a) + (x-a)[a,b]f = f(a) + (x - a)\\frac{f(b) - f(a)}{b - a}\n\\end{align*}\n\\]\nThe integral of \\(p(x)\\) equals the area of a trapezoid with base \\((b-a)\\) times the average height \\(\\frac{1}{2}(f(a) + f(b))\\). Hence,\n\\[\n\\int_{a}^{b} f(x)dx \\approx \\frac{(b-a)}{2}(f(a) + f(b))\n\\]\nTo increase the accuracy, we subdivide the interval \\([a,b]\\) and assume that \\(f_i = f(x_i)\\) is known on a grid of equidistant points:\n\\[\n\\begin{align*}\nx_0 = a, \\quad x_i = x_0 + ih, \\quad x_n = b\n\\end{align*}\n\\tag{4}\\]\nwhere \\(h = (b - a)/n\\) is the step length. The trapezoidal approximation for the \\(i\\)th subinterval is:\n\\[\n\\begin{align*}\n\\int_{x_i}^{x_{i+1}} f(x)dx = T(h) + R_i, \\quad T(h) = \\frac{h}{2}(f_i + f_{i+1})\n\\end{align*}\n\\tag{5}\\]\nLet \\(p_2(x)\\in\\mathcal{P}_2\\) be the unique interpolating polynomial (Newton polynomial) passing through the points \\((x_i,f_i)\\) and \\((x_{i+1},f_{i+1})\\), that is, \\(p_2(x_i)=f(x_i)\\) and \\(p_2(x_{i+1}) = f(x_{i+1})\\). The exact remainder in Newton’s interpolation formula is given by:\n\\[\n\\begin{align*}\nf(x) - p_2(x) &= [x_i,x_{i+1},x]f\\cdot \\Phi_2(x)\\\\\n&=[x_i,x_{i+1},x]f \\cdot (x - x_{i+1})\\Phi_1(x)\\\\\n&=[x_i,x_{i+1},x]f \\cdot (x - x_{i+1})(x - x_{i})\\Phi_0\\\\\n&=[x_i,x_{i+1},x]f \\cdot (x - x_{i+1})(x - x_{i})\n\\end{align*}\n\\]\nSo, we have:\n\\[\n\\begin{align*}\nR_i &= \\int_{x_i}^{x_{i+1}} (f(x) - p_2(x))dx = \\int_{x_i}^{x_{i+1}}(x - x_i)(x - x_{i+1})[x_i,x_{i+1},x]f dx\n\\end{align*}\n\\tag{6}\\]\nBy the theorem on the remainder term for interpolation, we can write:\n\\[\n[x_1,\\ldots,x_n,x_{n+1}]f = \\frac{f^{(n)}(\\xi)}{n!}\n\\]\nConsequently,\n\\[\n[x_i,x_{i+1},x]f = \\frac{f''(\\xi)}{2}\n\\]\nSo, we have:\n\\[\n\\begin{align*}\nR_i &= \\frac{f''(\\xi)}{2}\\int_{x_i}^{x_{i+1}} (x - x_i)(x - x_{i+1})dx\n\\end{align*}\n\\tag{7}\\]\nSetting \\(x = x_i + ht\\), \\(dx = hdt\\) such that the limits of integration are from \\(t=0\\) to \\(t=1\\), $we get:\n\\[\n\\begin{align*}\nR_i &=  \\frac{f''(\\xi)}{2} \\int_{0}^{1}(ht)(x_i + ht - x_{i+1})h dt\\\\\n&= \\frac{f''(\\xi)}{2} \\int_{0}^{1}(ht)(ht - h)h dt\\\\\n&= \\frac{f''(\\xi)}{2} \\int_{0}^{1}(ht)(ht - h)h dt\\\\\n&= \\frac{f''(\\xi) h^3}{2} \\int_{0}^{1}(t^2 - t) dt\\\\\n&= \\frac{f''(\\xi) h^3}{2} \\left[\\frac{t^3}{3} - \\frac{t^2}{2}\\right]_{0}^{1} \\\\\n&= \\frac{f''(\\xi) h^3}{2} \\left[\\frac{1}{3} - \\frac{1}{2}\\right] \\\\\n&= -\\frac{1}{12}h^3 f''(\\xi)\n\\end{align*}\n\\tag{8}\\]\nSumming the contributions for each subinterval \\([x_i,x_{i+1}]\\), \\(i=0...n\\), gives:\n\\[\n\\begin{align*}\n\\int_{a}^{b}f(x)dx = T(h) + R_T, \\quad T(h) = \\frac{h}{2}(f_0 + f_n) + h\\sum_{i=1}^{n-1}f_i\n\\end{align*}\n\\tag{9}\\]\nwhich is the composite trapezoidal rule. The global truncation error is:"
  },
  {
    "objectID": "posts/positive_definiteness/index.html",
    "href": "posts/positive_definiteness/index.html",
    "title": "Positive Definiteness",
    "section": "",
    "text": "Definition 1. A real-valued symmetric matrix \\(A\\) is positive-definite (written as \\(A \\succ 0\\)), if for every real valued vector \\(\\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}\\), the quadratic form\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} &gt; 0\n\\end{align*}\\]\nThe real-value symmetric matrix \\(A\\) is positive semi-definite (written as \\(A \\succeq 0\\)) if :\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\geq 0\n\\end{align*}\\]\n\\(\\forall \\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}\\).\nIntuitively, positive definite matrices are like strictly positive real numbers. Consider a scalar \\(a &gt; 0\\). The sign of \\(ax\\) will depend on the sign of \\(x\\). And \\(x\\cdot ax &gt; 0\\). However, if \\(a &lt; 0\\), multiplying it with \\(x\\) will flip the sign, so \\(x\\) and \\(ax\\) have opposite signs and \\(x \\cdot ax &lt; 0\\).\nIf \\(A \\succ 0\\), then by definition \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\). Thus, the vector \\(\\mathbf{x}\\) and it’s transformed self \\(A\\mathbf{x}\\) should make an angle \\(\\theta \\in \\left[-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right]\\). Both \\(\\mathbf{x}\\) and \\(A\\mathbf{x}\\) lie on the same side of the hyperplane \\(\\mathbf{x}^{\\perp}\\).\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\draw [-&gt;] (-2,0) -- (6,0) node [below] {\\huge $x_1$};\n\\draw [-&gt;] (0,-2) -- (0,6) node [above] {\\huge $x_2$};\n\\draw [-&gt;] (0,0) -- (3,1) node [above] {\\huge $\\mathbf{x}$};\n\\draw [-&gt;] (0,0) -- (2,5) node [above] {\\huge $A\\mathbf{x}$};\n\\draw [dashed] (-2,6) -- (1,-3) node [] {\\huge $\\mathbf{x}^{\\perp}$};\n\\draw[draw=green!50!black,thick] (1,1/3) arc (18.43:68.19:1) node [midway,above] {\\huge $\\theta$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#positive-definite-matrices",
    "href": "posts/positive_definiteness/index.html#positive-definite-matrices",
    "title": "Positive Definiteness",
    "section": "",
    "text": "Definition 1. A real-valued symmetric matrix \\(A\\) is positive-definite (written as \\(A \\succ 0\\)), if for every real valued vector \\(\\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}\\), the quadratic form\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} &gt; 0\n\\end{align*}\\]\nThe real-value symmetric matrix \\(A\\) is positive semi-definite (written as \\(A \\succeq 0\\)) if :\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} \\geq 0\n\\end{align*}\\]\n\\(\\forall \\mathbf{x} \\in \\mathbf{R}^n \\setminus \\{\\mathbf{0}\\}\\).\nIntuitively, positive definite matrices are like strictly positive real numbers. Consider a scalar \\(a &gt; 0\\). The sign of \\(ax\\) will depend on the sign of \\(x\\). And \\(x\\cdot ax &gt; 0\\). However, if \\(a &lt; 0\\), multiplying it with \\(x\\) will flip the sign, so \\(x\\) and \\(ax\\) have opposite signs and \\(x \\cdot ax &lt; 0\\).\nIf \\(A \\succ 0\\), then by definition \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\). Thus, the vector \\(\\mathbf{x}\\) and it’s transformed self \\(A\\mathbf{x}\\) should make an angle \\(\\theta \\in \\left[-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right]\\). Both \\(\\mathbf{x}\\) and \\(A\\mathbf{x}\\) lie on the same side of the hyperplane \\(\\mathbf{x}^{\\perp}\\).\n\n%load_ext itikz\n\n\n\nShow the code\n%%itikz --temp-dir --tex-packages=tikz --tikz-libraries=arrows --implicit-standalone\n\\begin{tikzpicture}[scale=1.5]\n\\draw [-&gt;] (-2,0) -- (6,0) node [below] {\\huge $x_1$};\n\\draw [-&gt;] (0,-2) -- (0,6) node [above] {\\huge $x_2$};\n\\draw [-&gt;] (0,0) -- (3,1) node [above] {\\huge $\\mathbf{x}$};\n\\draw [-&gt;] (0,0) -- (2,5) node [above] {\\huge $A\\mathbf{x}$};\n\\draw [dashed] (-2,6) -- (1,-3) node [] {\\huge $\\mathbf{x}^{\\perp}$};\n\\draw[draw=green!50!black,thick] (1,1/3) arc (18.43:68.19:1) node [midway,above] {\\huge $\\theta$};\n\\end{tikzpicture}"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#convex-functions",
    "href": "posts/positive_definiteness/index.html#convex-functions",
    "title": "Positive Definiteness",
    "section": "Convex functions",
    "text": "Convex functions\nThere is a second geometric way to think about positive definite matrices : a quadratic form is convex when the matrix is symmetric and positive definite.\nDefinition 2. (Convex function) A function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex, if for all \\(\\mathbf{x},\\mathbf{y}\\in \\mathbf{R}^n\\) and \\(0 \\leq \\lambda \\leq 1\\), we have:\n\\[\\begin{align*}\nf(\\lambda \\mathbf{x} + (1-\\lambda)\\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}) \\tag{1}\n\\end{align*}\\]\nProposition 3. Assume that the function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is differentiable. Then, \\(f\\) is convex, if and only if, for all \\(\\mathbf{x},\\mathbf{y} \\in \\mathbf{R}^n\\), the inequality\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{y})^T (\\mathbf{y} - \\mathbf{x}) \\tag{2}\n\\end{align*}\\]\nis satisfied.\nProof.\n\\(\\Longrightarrow\\) direction.\nAssume that \\(f\\) is convex and let \\(\\mathbf{x} \\neq \\mathbf{y} \\in \\mathbf{R}^n\\). The convexity of \\(f\\) implies that:\n\\[\\begin{align*}\nf((\\mathbf{x} + \\mathbf{y})/2) \\leq \\frac{1}{2}f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{y})\n\\end{align*}\\]\nDenote now \\(\\mathbf{h} = \\mathbf{y}-\\mathbf{x}\\). Then this inequality reads:\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}/2) \\leq \\frac{1}{2} f(\\mathbf{x}) + \\frac{1}{2}f(\\mathbf{x} + \\mathbf{h})\n\\end{align*}\\]\nUsing elementary transformations, we have:\n\\[\\begin{align*}\n\\frac{f(\\mathbf{x} + \\mathbf{h}/2)}{1/2} &\\leq f(\\mathbf{x}) + f(\\mathbf{x} + \\mathbf{h}) \\\\\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) &\\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2}\n\\end{align*}\\]\nRepeating this line of argumentation:\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/2) - f(\\mathbf{x})}{1/2} \\geq \\frac{f(\\mathbf{x} + \\mathbf{h}/4) - f(\\mathbf{x})}{1/4}\n\\end{align*}\\]\nConsequently,\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} \\tag{2}\n\\end{align*}\\]\nBy the order limit theorem,\n\\[\\begin{align*}\nf(\\mathbf{x} + \\mathbf{h}) - f(\\mathbf{x}) \\geq \\lim_{k \\to \\infty}\\frac{f(\\mathbf{x} + 2^{-k}\\mathbf{h}) - f(\\mathbf{x})}{2^{-k}} = D_{\\mathbf{h}}f(\\mathbf{x}) = \\nabla f(\\mathbf{x}) \\cdot (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\nReplacing \\(\\mathbf{y}-\\mathbf{x}\\) by \\(\\mathbf{h}\\), we have:\n\\[\\begin{align*}\nf(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\n\\(\\Longleftarrow\\) direction.\nLet \\(\\mathbf{w}, \\mathbf{z} \\in \\mathbf{R}^n\\). Moreover, denote:\n\\[\\begin{align*}\n\\mathbf{x} := \\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z}\n\\end{align*}\\]\nThen, the inequality in (1) implies that:\n\\[\\begin{align*}\nf(\\mathbf{w}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{w} - \\mathbf{x})\\\\\nf(\\mathbf{z}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T (\\mathbf{z} - \\mathbf{x}) \\tag{3}\n\\end{align*}\\]\nNote moreover that:\n\\[\\begin{align*}\n\\mathbf{w} - \\mathbf{x} &= (1-\\lambda)(\\mathbf{w}-\\mathbf{z})\\\\\n\\mathbf{z} - \\mathbf{x} &= \\lambda(\\mathbf{z}-\\mathbf{w})\n\\end{align*}\\]\nThus, if we multiply the first line in (3) with \\(\\lambda\\) and the second line with \\((1-\\lambda)\\) and then add the two inequalities, we obtain:\n\\[\\begin{align*}\n\\lambda f(\\mathbf{w}) + (1-\\lambda)f(\\mathbf{z}) &\\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})[\\lambda(1-\\lambda)(\\mathbf{w} - \\mathbf{z} + \\mathbf{z} - \\mathbf{w})\\\\\n&=f(\\lambda \\mathbf{w} + (1-\\lambda)\\mathbf{z})\n\\end{align*}\\]\nSince \\(\\mathbf{w}\\) and \\(\\mathbf{z}\\) were arbitrary, this proves the convexity of \\(f\\).\nThe convexity of a differentiable function can either be characterized by the fact that all secants lie above the graph or that all tangents lie below the graph.\nWe state the next corollary without proof.\nCorollary 4. Assume that \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex and differentiable. Then \\(\\mathbf{x}^*\\) is a global minimizer of \\(f\\), if and only if \\(\\nabla f(\\mathbf{x}^{*}) = 0\\).\n\nHessians of convex functions.\nProposition 5. (Second derivative test) Let \\(f:X\\subseteq\\mathbf{R}^n \\to \\mathbf{R}\\) be a \\(C^2\\) function and suppose that \\(\\mathbf{a}\\in X\\) is a critical point of \\(f\\). If the hessian \\(\\nabla^2 f(\\mathbf{a})\\) is positive definite, then \\(f\\) has a local minimum at \\(\\mathbf{a}\\).\nProof.\nLet \\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\) be a quadratic form. We have:\n\\[\\begin{align*}\nq(\\lambda \\mathbf{h}) &= (\\lambda \\mathbf{x}^T) A (\\lambda \\mathbf{x})\\\\\n&= \\lambda^2 \\mathbf{x}^T A \\mathbf{x}\\\\\n&= \\lambda^2 q(\\mathbf{x}) \\tag{4}\n\\end{align*}\\]\nWe show that if \\(A\\) is the symmetric matrix associated with a positive definite quadratic form \\(q(\\mathbf{x})\\), then there exists \\(M &gt; 0\\) such that:\n\\[\\begin{align*}\nq(\\mathbf{h}) \\geq M ||\\mathbf{h}||^2 \\tag{5}\n\\end{align*}\\]\nfor all \\(\\mathbf{h} \\in \\mathbf{R}^n\\).\nFirst note that when \\(\\mathbf{h} = \\mathbf{0}\\), then \\(q(\\mathbf{h})=q(\\mathbf{0})=0\\), so the conclusion holds trivially in this case.\nNext, suppose that when \\(\\mathbf{h}\\) is a unit vector, that is \\(||\\mathbf{h}||=1\\). The set of all unit vectors in \\(\\mathbf{R}^n\\) is an \\((n-1)\\)-dimensional hypersphere, which is a compact set. By the extreme-value theorem, the restriction of \\(q\\) to \\(S\\) must achieve a global minimum value \\(M\\) somewhere on \\(S\\). Thus, \\(q(\\mathbf{h}) \\geq M\\) for all \\(\\mathbf{h} \\in S\\).\nFinally, let \\(\\mathbf{h}\\) be any nonzero vector in \\(\\mathbf{R}^n\\). Then, its normalization \\(\\mathbf{h}/||\\mathbf{h}||\\) is a unit vector and also lies in \\(S\\). Therefore, by the result of step 1, we have:\n\\[\\begin{align*}\nq(\\mathbf{h}) &= q\\left(||\\mathbf{h}|| \\cdot \\frac{\\mathbf{h}}{||\\mathbf{h}||} \\right)\\\\\n&= ||\\mathbf{h}||^2 q\\left(\\frac{\\mathbf{h}}{||\\mathbf{h}||}\\right)\\\\\n&\\geq M ||\\mathbf{h}||^2\n\\end{align*}\\]\nWe can now prove the theorem.\nBy the second order Taylor’s formula, we have that, for the critical point \\(\\mathbf{a}\\) of \\(f\\),\n\\[\\begin{align*}\nf(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{x})\\cdot(\\mathbf{x} - \\mathbf{a}) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^T \\nabla^2 f(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}) + R_2(\\mathbf{x},\\mathbf{a}) \\tag{6}\n\\end{align*}\\]\nwhere \\(R_2(\\mathbf{x},\\mathbf{a})/||\\mathbf{x}-\\mathbf{a}||^2 \\to 0\\) as \\(\\mathbf{x} \\to \\mathbf{a}\\).\nIf \\(\\nabla^2 f(\\mathbf{a}) \\succ 0\\), then\n\\[\\begin{align*}\n\\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^T \\nabla^2 f(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}) \\geq M||\\mathbf{x} - \\mathbf{a}||^2 \\tag{7}\n\\end{align*}\\]\nPick \\(\\epsilon  = M\\). By the definition of limits, since \\(R_2(\\mathbf{x},\\mathbf{a})/||\\mathbf{x} - \\mathbf{a}||^2 \\to 0\\) as \\(\\mathbf{x} \\to \\mathbf{a}\\), there exists \\(\\delta &gt; 0\\), such that for all \\(||\\mathbf{x} - \\mathbf{a}||&lt;\\delta\\), \\(|R_2(\\mathbf{x},\\mathbf{a})|/||\\mathbf{x} - \\mathbf{a}||^2 &lt; M\\). Or equivalently,\n\\[\\begin{align*}\n|R_2(\\mathbf{x},\\mathbf{a})| &lt; M||\\mathbf{x}-\\mathbf{a}||^2\n\\end{align*}\\]\nthat is:\n\\[\\begin{align*}\n-M||\\mathbf{x}-\\mathbf{a}||^2 &lt; R_2(\\mathbf{x},\\mathbf{a}) &lt; M||\\mathbf{x}-\\mathbf{a}||^2 \\tag{8}\n\\end{align*}\\]\nPutting together (6), (7) and (8),\n\\[\\begin{align*}\nf(\\mathbf{x}) - f(\\mathbf{a}) &gt; 0\n\\end{align*}\\]\nso that \\(f\\) has a minimum at \\(\\mathbf{a}\\).\nProposition 6. A twice differentiable function \\(f:\\mathbf{R}^n \\to \\mathbf{R}\\) is convex, if and only if, the hessian \\(\\nabla^2 f(\\mathbf{x})\\) is positive semi-definite for all \\(\\mathbf{x}\\in\\mathbf{R}^n\\).\nProof.\nAssume first that \\(f\\) is convex and let \\(\\mathbf{x}\\in\\mathbf{R}^n\\). Define the \\(g:\\mathbf{R}^n \\to \\mathbf{R}\\) as a function of the vector \\(\\mathbf{y}\\) setting:\n\\[\\begin{align*}\ng(\\mathbf{y}) := f(\\mathbf{y}) - \\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\n\\end{align*}\\]\nConsider the mapping \\(T(\\mathbf{y}) = -\\nabla f(\\mathbf{x})^T (\\mathbf{y} - \\mathbf{x})\\). We have:\n\\[\\begin{align*}\nT(\\lambda \\mathbf{y}_1 + (1-\\lambda)\\mathbf{y}_2) &= -\\nabla f(\\mathbf{x})^T (\\lambda \\mathbf{y}_1 + (1-\\lambda)\\mathbf{y}_2 - \\mathbf{x}) \\\\\n&= \\lambda [-\\nabla f(\\mathbf{x})^T (\\mathbf{y}_1 - \\mathbf{x})] + (1-\\lambda)[-\\nabla f(\\mathbf{x})^T (\\mathbf{y}_2 - \\mathbf{x})]\\\\\n&=\\lambda T(\\mathbf{y}_1) + (1-\\lambda)T(\\mathbf{y}_2)\n\\end{align*}\\]\nThus, \\(T\\) is an affine transformation.\nSince an affine transformation is convex and \\(f\\) is convex, their sum \\(g\\) is also convex. Moreover \\(g\\) is a function of \\(\\mathbf{y}\\), treating \\(\\mathbf{x}\\) as a constant, we have:\n\\[\\begin{align*}\n\\nabla g(\\mathbf{y}) = \\nabla f(\\mathbf{y}) - \\nabla f(\\mathbf{x})\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n\\nabla^2 g(\\mathbf{y}) = \\nabla^2 f(\\mathbf{y})\n\\end{align*}\\]\nfor all \\(\\mathbf{y} \\in \\mathbf{R}^n\\). In particular, \\(\\nabla g(\\mathbf{x}) = 0\\). Thus, corollary (4) implies that \\(\\mathbf{x}\\) is a global minimizer of \\(g\\). Now, the second order necessary condition for a minimizer implies that \\(\\nabla^2 g(\\mathbf{x})\\) is positive semi-definite. Since \\(\\nabla^2 g(\\mathbf{x}) = \\nabla^2 f(\\mathbf{x})\\), this proves that the Hessian of \\(f\\) is positive semi-definite for all \\(\\mathbf{x} \\in \\mathbf{R}^n\\).\nThus, a function \\(f\\) is convex, if its Hessian is everywhere positive semi-definite. This allows us to test whether a given function is convex."
  },
  {
    "objectID": "posts/positive_definiteness/index.html#tests-for-positive-definiteness",
    "href": "posts/positive_definiteness/index.html#tests-for-positive-definiteness",
    "title": "Positive Definiteness",
    "section": "Tests for positive definiteness",
    "text": "Tests for positive definiteness\nOne of the most important theorems of finite dimensional vector spaces is the spectral theorem. Every real symmetric matrix \\(A\\) is orthogonally diagonalizable. It admits \\(A = Q\\Lambda Q^T\\) factorization, where \\(Q\\) is an orthogonal matrix and \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\).\nFrom basic algebra, we know that, if \\(A\\) is a non-singular matrix, with all it’s pivot elements \\(a_{kk}^{(k)}\\) non-zero in the Gaussian elimination process, then \\(A=LDU\\) where \\(L\\) and \\(U\\) are lower and upper unitriangular matrices and \\(D\\) is a diagonal matrix consisting of the pivots of \\(A\\). If \\(A\\) is symmetric, then it admits the unique factorization \\(A = LDL^T\\).\nConsider the quadratic form \\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\). Substituting \\(A = Q \\Lambda Q^T\\), we have:\n\\[\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} \\\\\n&= \\mathbf{x}^T Q \\Lambda Q^T \\mathbf{x} \\\\\n&= (Q^T \\mathbf{x})^T \\Lambda (Q^T \\mathbf{x}) \\tag{9}\n\\end{align*}\\]\nBut, the matrix \\(Q = [\\mathbf{q}_1,\\mathbf{q}_2,\\ldots,\\mathbf{q}_n]\\). Moreover, \\(A=Q\\Lambda Q^T\\) implies that \\(AQ^{-1} = AQ^T = \\Lambda Q^T\\). Therefore:\n\\[\\begin{align*}\nA\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}=\\begin{bmatrix}\n\\lambda_1 \\\\\n& \\lambda_2 \\\\\n& & \\ddots \\\\\n& & & \\lambda_n\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{q}_1\\\\\n\\mathbf{q}_2\\\\\n\\vdots\\\\\n\\mathbf{q}_n\n\\end{bmatrix}\n\\end{align*}\\]\nSo, \\(\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\) are the eigenvectors of \\(A\\). Now,\n\\[\\begin{align*}\n\\mathbf{q}_1 &= [q_{11}, q_{21}, \\ldots,q_{n1}]^T = q_{11} \\mathbf{e}_1 + q_{21} \\mathbf{e}_2 + \\ldots + q_{n1} \\mathbf{e}_n\\\\\n\\mathbf{q}_2 &= [q_{12}, q_{22}, \\ldots,q_{n2}]^T = q_{12} \\mathbf{e}_1 + q_{22} \\mathbf{e}_2 + \\ldots + q_{n2} \\mathbf{e}_n\\\\\n\\vdots \\\\\n\\mathbf{q}_n &= [q_{1n}, q_{2n}, \\ldots,q_{nn}]^T = q_{1n} \\mathbf{e}_1 + q_{2n} \\mathbf{e}_2 + \\ldots + q_{nn} \\mathbf{e}_n\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\nQ = \\begin{bmatrix}\nq_{11} & \\ldots & q_{1n}\\\\\n\\vdots & & \\vdots \\\\\nq_{n1} & \\ldots & q_{nn}\n\\end{bmatrix}\n\\end{align*}\\]\nis the change of basis matrix from the standard basis \\(\\mathcal{B}_{old}=\\{\\mathbf{e}_1,\\ldots,\\mathbf{e}_n\\}\\) to the eigenvector basis \\(\\mathcal{B}_{new}=\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\}\\).\nIf \\(\\mathbf{x}\\) are the coordinates of a vector in the standard basis and \\(\\mathbf{y}\\) are its coordinates in the eigenvector basis, then \\(\\mathbf{x}=Q\\mathbf{y}\\).\nHence, substituting \\(\\mathbf{y}=Q^{-1}\\mathbf{x}=Q^T \\mathbf{x}\\) in equation (9), the quadratic form becomes:\n\\[\\begin{align*}\nq(\\mathbf{x}) &= \\mathbf{x}^T A \\mathbf{x} = \\mathbf{y}^T \\Lambda \\mathbf{y}\\\\\n&=\\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\ldots + \\lambda_n y_n^2\n\\end{align*}\\]\nwhere we have changed the axes to be aligned across the eigenvectors of \\(A\\).\nThe coefficients \\(\\lambda_i\\) are the diagonal entries of \\(\\Lambda\\) and are the pivots of \\(A\\). The quadratic form is strictly positive for all \\(\\mathbf{y}\\), if and only if the eigenvalues \\(\\lambda_1 &gt; 0\\), \\(\\lambda_2 &gt;0\\), \\(\\ldots\\), \\(\\lambda_n &gt; 0\\).\nTheorem 7. (Positive definiteness) Let \\(A \\in \\mathbf{R}^{n \\times n}\\) be a real symmetric positive definite(SPD) matrix. Then, the following statements are equivalent:\n\n\\(A\\) is non-singular and has positive pivot elements when performing Gaussian elimination (without row exchanges).\n\\(A\\) admits a factorization \\(A = Q \\Lambda Q^T\\), where \\(\\Lambda = diag(\\lambda_1,\\ldots,\\lambda_n)\\) such that \\(\\lambda_i &gt; 0\\) for all \\(i=1,2,3,\\ldots,n\\)."
  },
  {
    "objectID": "posts/positive_definiteness/index.html#cholesky-factorization",
    "href": "posts/positive_definiteness/index.html#cholesky-factorization",
    "title": "Positive Definiteness",
    "section": "Cholesky Factorization",
    "text": "Cholesky Factorization\nWe can push the result above slightly further in the positive definite case. Since, each eigen value \\(\\lambda_i\\) is positive, the quadratic form can be written as a sum of squares:\n\\[\\begin{align*}\n\\lambda_1 y_1^2 + \\lambda_2 y_2^2 + \\ldots + \\lambda_n y_n^2 &= (\\sqrt{\\lambda_1} y_1)^2 + \\ldots + (\\sqrt{\\lambda_n} y_n)^2\\\\\n&= z_1^2 + z^2 + \\ldots + z_n^2\n\\end{align*}\\]\nwhere \\(z_i =\\sqrt{\\lambda_i}y_i\\). In the matrix form, we are writing:\n\\[\\begin{align*}\n\\hat{q}(\\mathbf{y}) &= \\mathbf{y}^T \\Lambda \\mathbf{y}\\\\\n&= \\mathbf{z}^T \\mathbf{z}\\\\\n&= ||\\mathbf{z}||^2\n\\end{align*}\\]\nwhere \\(\\mathbf{z} = S\\mathbf{y}\\) with \\(S=diag(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda_n})\\). Since \\(\\Lambda = S^2=SS^T\\), \\(S\\) can be thought as the square root of the original matrix \\(\\Lambda\\). Substituting back into the equation \\(A=Q\\Lambda Q^T\\), we deduce the Cholesky factorization:\n\\[\\begin{align*}\nA &= Q\\Lambda Q^T\\\\\n&= QS S^T Q^T\\\\\n&= MM^T\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/positive_definiteness/index.html#level-plots-of-a-positive-definite-quadratic-form-are-ellipsoids",
    "href": "posts/positive_definiteness/index.html#level-plots-of-a-positive-definite-quadratic-form-are-ellipsoids",
    "title": "Positive Definiteness",
    "section": "Level plots of a positive definite quadratic form are ellipsoids",
    "text": "Level plots of a positive definite quadratic form are ellipsoids\nConsider the level plot of a positive definite quadratic form \\(q(\\mathbf{x})\\):\n\\[\\begin{align*}\nq(\\mathbf{x}) = \\hat{q}(\\mathbf{y}) &= 1 \\\\\n\\lambda_1 y_1^2 + \\ldots + \\lambda_n y_n^2 &= 1\\\\\n\\frac{y_1^2}{\\left(\\frac{1}{\\sqrt{\\lambda_1}}\\right)^2}+\\frac{y_2^2}{\\left(\\frac{1}{\\sqrt{\\lambda_2}}\\right)^2} + \\ldots + \\frac{y_n^2}{\\left(\\frac{1}{\\sqrt{\\lambda_n}}\\right)^2} &= 1\n\\end{align*}\\]\nThus, the level plot of a positive definite quadratic form is an ellipse (if \\(n=2\\)) or an ellipsoid (if \\(n &gt; 2\\)) with axes aligned along the eigenvectors and lengths \\(\\frac{1}{\\sqrt{\\lambda_i}}\\), \\(i=1,2,3,\\ldots,n\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nA = np.array([[4, 3], [3, 4]])\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# The parameteric equation of an ellipse is\n# (x(theta),y(theta))=(a cos theta, b sin theta)\n# where a and b are semi-major and semi-minor axes\ntheta = np.linspace(0, 2 * np.pi, 10000)\ny1 = np.sqrt(1 / eigenvalues[0]) * np.cos(theta)\ny2 = np.sqrt(1 / eigenvalues[1]) * np.sin(theta)\n\nY = np.array([y1,y2])\n\n# The change of basis matrix from the standard basis to the eigen vector basis\n# is Q. So, x = Q y, where Q = [q_1,q_2]; q_1, q_2 are the eigenvectors of A.\n\nQ = eigenvectors.T\nX = np.dot(Q, Y)\nx1 = X[0,:]\nx2 = X[1,:]\n\nplt.xlim([-1, 1])\nplt.grid(True)\nplt.title(r'$q(\\mathbf{x})=\\mathbf{x}^T A \\mathbf{x} = 1$')\nplt.xlabel(r'$x_1$')\nplt.ylabel(r'$x_2$')\nplt.plot(x1, x2)\nplt.show()"
  },
  {
    "objectID": "posts/spectral_theorem/index.html",
    "href": "posts/spectral_theorem/index.html",
    "title": "The Spectral Theorem",
    "section": "",
    "text": "Spectral Theorem\nEvery real, symmetric matrix is orthogonally diagonalizable.\n\nTheorem 1 (Spectral Theorem) Every real symmetric matrix is diagonalizable.\nLet \\(A\\) be a \\(n \\times n\\) real symmetric matrix. Then,\n\nThe eigenvalues of \\(A\\) are real.\nThere exists an orthonormal basis \\(\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_n\\}\\) for \\(\\mathbb{R}^n\\) consisting of the eigenvectors of \\(A\\). That is, there is an orthogonal matrix \\(Q\\) so that \\(A = QAQ^{-1}\\).\n\n\n\n\n\n\n\n\nTip 1: Spectral values\n\n\n\nThe term spectrum refers to the eigenvalues of a matrix, or more, generally a linear operator. In Physics, the spectral energy lines of atoms (e.g. Balmer lines of the Hydrogen atom), are characterized as the eigenvalues of the governing quantum mechanical Schrodinger operator.\n\n\nProof.\nClaim. The eigenvalues of \\(A\\) are real.\n\\[\n\\begin{align*}\n\\langle A\\mathbf{x}, \\mathbf{y} \\rangle &= (A \\mathbf{x})' \\mathbf{y}\\\\\n&= \\mathbf{x}'A' \\mathbf{y}\\\\\n&= \\langle \\mathbf{x},A'\\mathbf{y}\\rangle\n\\end{align*}\n\\]\nSince, for a symmetric matrix \\(A\\), \\(A = A'\\), it follows that:\n\\[\n\\langle A\\mathbf{x},\\mathbf{y}\\rangle = \\langle \\mathbf{x}, A\\mathbf{y} \\rangle\n\\]\nOr using the dot-product notation, we could write:\n\\[\n(A\\mathbf{x})\\cdot \\mathbf{y} = \\mathbf{x}\\cdot (A\\mathbf{y})\n\\tag{1}\\]\nSuppose \\(\\mathbf{v}\\neq\\mathbf{0}\\) is a non-zero vector in \\(\\mathbf{R}^n\\) such that there exists a complex scalar \\(\\lambda\\), satisfying:\n\\[\nA \\mathbf{v} = \\lambda \\mathbf{v}\n\\tag{2}\\]\nWe can now take the complex conjugate of the eigenvalue equation. Remember that \\(A\\) is a real matrix, so \\(\\bar{A} = A\\). Thus, we have the conjugated version of the eigenvalue equation:\n\\[\n\\overline{(A\\mathbf{v})}=\\overline{A}\\overline{\\mathbf{v}} = A\\overline{\\mathbf{v}} = \\overline{\\lambda \\mathbf{v}} = \\overline{\\lambda}\\overline{\\mathbf{v}}\n\\tag{3}\\]\nUsing the eigenvalue equation (Equation 2), we can write:\n\\[\n(A \\mathbf{v})\\cdot \\overline{\\mathbf{v}} = (\\lambda \\mathbf{v}) \\cdot \\overline{\\mathbf{v}} = \\lambda (\\mathbf{v} \\cdot \\overline{\\mathbf{v}})\n\\]\nAlternatively, using Equation 1 and Equation 3, we have:\n\\[\n(A \\mathbf{v})\\cdot \\overline{\\mathbf{v}} = \\mathbf{v} \\cdot (A\\overline{\\mathbf{v}}) = \\mathbf{v} \\cdot (\\overline{\\lambda} \\overline{\\mathbf{v}}) = \\overline{\\lambda} (\\mathbf{v} \\cdot \\overline{\\mathbf{v}})\n\\]\nConsequently,\n\\[\n(\\lambda - \\overline{\\lambda})(\\mathbf{v}\\cdot \\overline{\\mathbf{v}}) = 0\n\\]\nSince, \\(\\mathbf{v} \\neq \\mathbf{0}\\), \\(\\lambda = \\overline{\\lambda}\\). Therefore, \\(\\lambda \\in \\mathbb{R}\\).\nClaim. \\(A\\) is orthogonally diagonalizable.\nWe proceed by induction.\nFor \\(n=1\\), \\(A\\) and \\(v\\) are scalars, so \\(Av = \\lambda v\\), where \\(\\lambda = A\\). Thus, we can pick any non-zero scalar \\(v\\) to form a basis in \\(\\mathbf{R}\\). And \\(A=P^{-1}\\Lambda P\\), where \\(P=I\\) and \\(\\Lambda = A\\).\nInductive hypotheis. Every \\(k \\times k\\) matrix is diagonalisable for \\(k=1,2,3,\\ldots,n-1\\).\nClaim. Let \\(A \\in \\mathbf{R}^{n \\times n}\\) be symmetric. Then, we are interested to prove that \\(A\\) is diagonalizable. We break the induction part into 3 steps.\nEvery square matrix \\(A\\) has atleast one eigenvalue. Suppose \\(\\lambda_{1}\\) is an eigenvalue of the matrix \\(A\\) and has a corresponding eigenvector \\(\\mathbf{v}_1\\). By part (I), we know that \\(\\lambda_{1}\\in\\mathbf{R}\\). We can normalize \\(\\mathbf{v}_1\\) as \\(\\mathbf{q}_{1} = \\mathbf{v}_1/||\\mathbf{v}_1||\\), so that it is an eigenvector with eigenvalue \\(\\lambda_{1}\\). (Obviously, this is no problem, since if \\(A\\mathbf{v}_1 = \\lambda_1 \\mathbf{v}_1\\), it implies \\(A (\\mathbf{v}_1/||\\mathbf{v}_1||) = \\lambda_1 (\\mathbf{v}_1/||\\mathbf{v}_1||)\\). It follows that, \\(A \\mathbf{q}_1 = \\lambda_1 \\mathbf{q}_1\\). )\nNow, we can extend this to a basis \\(\\{\\mathbf{q}_{1},\\mathbf{w}_{2},\\ldots,\\mathbf{w}_{n}\\}\\) of \\(\\mathbf{R}^n\\). By the Gram-Schmidt orthogonalization algorithm, given the basis \\(\\{\\mathbf{q}_{1},\\mathbf{w}_{2},\\ldots,\\mathbf{w}_{n}\\}\\), we can find a corresponding orthonormal basis \\(\\{\\mathbf{q}_{1},\\mathbf{q}_{2},\\ldots,\\mathbf{q}_{n}\\}\\) of \\(\\mathbf{R}^n\\).\nNow, we huddle these basis vectors together as column-vectors of a matrix and formulate the matrix \\(P\\).\n\\[\n\\begin{align*}\nP & =\\left[\\begin{array}{cccc}\n\\mathbf{\\mathbf{q}_{1}} & \\mathbf{q}_{2} & \\ldots & \\mathbf{q}_{n}\\end{array}\\right]\n\\end{align*}\n\\]\nBy definition, \\(P\\) is an orthogonal matrix. So, \\(P^{-1} = P^T\\).\nDefine\n\\[\n\\begin{align*}\nB & =P^{-1}AP\n\\end{align*}\n\\]\nStep I. \\(B\\) is symmetric.\nWe have:\n\\[\n\\begin{align*}\nB^{T} & =(P^{-1}AP)^{T}\\\\\n& =(P^{T}AP)^{T} & \\{P^{-1}=P^{T}\\}\\\\\n& =P^{T}A^{T}(P^{T})^{T}\\\\\n& =P^{T}A^{T}P\\\\\n& =P^{T}AP & \\{A\\text{ is symmetric}\\}\\\\\n& =B\n\\end{align*}\n\\]\nWe are now going to try and write \\(B\\) in the block form to try to see the structure that this matrix must have and hope that it looks like, it is going to be diagonal.\nStep II. The structure of \\(B\\).\nThe way we do this, is to consider the matrix \\(B\\) post-multiplied by \\(\\mathbf{e}_{1}\\). Consider \\(B\\mathbf{e}_{1}\\). This should actually give us the first column of \\(B\\). Now, we also know that \\(B=P^{T}AP\\). So, we could actually say, well,\n\\[\n\\begin{align*}\nP^{T}AP\\mathbf{e}_{1} & =P^{T}A\\mathbf{q}_{1}\n\\end{align*}\n\\]\nNow, remember that \\(\\mathbf{q}_{1}\\) is the normalized eigenvector corresponding to the eigenvalue \\(\\lambda_{1}\\). So, \\(A\\mathbf{q}_{1}=\\lambda_{1}\\mathbf{q}_{1}\\). That means, this is equal to:\n\\[\\begin{align*}\nP^{T}A\\mathbf{q}_{1} & =P^{T}\\lambda_{1}\\mathbf{q}_{1}\\\\\n& =\\lambda_{1}P^{t}\\mathbf{q}_{1}\\\\\n& =\\lambda_{1}\\left[\\begin{array}{c}\n\\mathbf{q}_{1}^{T}\\\\\n\\mathbf{q}_{2}^{T}\\\\\n\\vdots\\\\\n\\mathbf{q}_{n}^{T}\n\\end{array}\\right]\\mathbf{q}_{1}\\\\\n& =\\lambda_{1}\\left[\\begin{array}{c}\n\\mathbf{q}_{1}^{T}\\mathbf{q}_{1}\\\\\n\\mathbf{q}_{2}^{T}\\mathbf{q}_{1}\\\\\n\\vdots\\\\\n\\mathbf{q}_{n}^{T}\\mathbf{q}_{1}\n\\end{array}\\right]\\\\\n& =\\lambda_{1}\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{c}\n\\lambda_{1}\\\\\n0\\\\\n0\\\\\n0\n\\end{array}\\right]\n\\end{align*}\\]\nThis is the first column of the matrix \\(B\\). Since \\(B=B^{T}\\), the first row should also be\n\\[\n\\begin{bmatrix}\n\\lambda_1 & 0 & 0 & \\ldots & 0\n\\end{bmatrix}\n\\]\nSo, we can write the matrix \\(B\\) in the block form:\n\\[\\begin{align*}\nB & =\\left[\\begin{array}{cc}\n\\lambda_{1} & O\\\\\nO & C\n\\end{array}\\right]\n\\end{align*}\\]\nThe first row and the first column are satisying the need to be diagonal.\nStep III.\nWe know that \\(C\\) is a \\(n-1\\times n-1\\) symmetric matrix. By the induction hypothesis, there exists an orthogonal matrix \\(Q\\) such that \\(D=Q^{-1}CQ = Q^T C Q\\).\nNow, define the matrix \\(R\\) as:\n\\[\\begin{equation}\nR:=P\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\n\\end{equation}\\]\nClaim. Our claim is that \\(R\\) is orthogonal and \\(R^{-1}AR\\) is diagonal.\n\nWe have:\n\n\\[\\begin{align*}\nR^{-1} & =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{-1}\n\\end{array}\\right]P^{-1} & \\{\\text{Reverse order law}\\}\\\\\n& =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]P^{T} & \\{P\\text{ and }Q\\text{ are orthogonal}\\}\n\\end{align*}\\]\nBut,\n\\[\\begin{align*}\nR^{T} & =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]P^{T}\n\\end{align*}\\]\nSo,\n\\[\\begin{align*}\nR^{T} & =R^{-1}\n\\end{align*}\\]\nThus, \\(R\\) is orthogonal.\n\nWell, let’s compute \\(R^{-1}AR\\).\n\n\\[\\begin{align*}\nR^{-1}AR & =R^{T}AR & \\{R\\text{ is orthogonal}\\}\\\\\n& =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]P^{T}AP\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]B\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\lambda_{1} & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & C\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{cc}\n\\lambda_{1} & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}C\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q\n\\end{array}\\right]\\\\\n& =\\left[\\begin{array}{cc}\n\\lambda_{1} & 0_{1\\times n-1}\\\\\n0_{n-1\\times1} & Q^{T}CQ\n\\end{array}\\right]\n\\end{align*}\\]\nSince \\(Q^{T}CQ\\) is diagonal, it follows that \\(R^{-1}AR\\) is diagonal. This closes the proof. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/the_markov_property/index.html",
    "href": "posts/the_markov_property/index.html",
    "title": "The Markov Property",
    "section": "",
    "text": "Let’s start by exhibiting the Markov property of Brownian motion. To see this, consider \\((\\mathcal{F}_t,t\\geq 0)\\), the natural filtration of the Brownian motion \\((B_t,t\\geq 0)\\). Consider \\(g(B_t)\\) for some time \\(t\\) and bounded function \\(g\\). (For example, \\(g\\) could be an indicator function.) Consider also a random variable \\(W\\) that is \\(\\mathcal{F}_s\\) measurable for \\(s &lt; t\\). (For example, \\(W\\) could be \\(B_s\\) or \\(1_{B_s &gt; 0}\\).) Let’s compute \\(\\mathbb{E}[g(B_t)W]\\).\n\\[\\begin{align*}\n\\mathbb{E}[g(B_t)W] &= \\mathbb{E}[\\mathbb{E}[Wg(B_t - B_s + B_s)|\\mathcal{F}_s]]\n\\end{align*}\\]\nThe random variable \\((B_t - B_s)\\) follows a \\(\\mathcal{N}(0,t-s)\\) distribution. By LOTUS,\n\\[\\begin{align*}\n\\mathbb{E}[g(B_t)W]\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)|\\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\\\\n&= \\{\\text{ Using the fact that }B_t - B_s \\perp B_s\\}\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]\\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\\]\nBy Fubini’s theorem, the integral and the expectation operator can be interchanged, and since \\(W\\) is \\(\\mathcal{F}_s\\) measurable, it follows from the definition of conditional expectations that:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] &= \\int_{\\mathbb{R}} g(y + B_s) \\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\n\\tag{1}\\]\nWe make two important observations. First, the right hand side is a function of \\(s,t\\) and \\(B_s\\) only (and not of the Brownian motion before time s). In particular, we have:\n\\[\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] = \\mathbb{E}[g(B_t)|B_s]\n\\]\nThis holds for any bounded function \\(g\\). In particular, it holds for all indicator functions. This implies that the conditional distribution of \\(B_t\\) given \\(\\mathcal{F}_s\\) depends solely on \\(B_s\\), and not on other values before time \\(s\\). Second, the right-hand side is time-homogenous in the sense that it depends on the time difference \\(t-s\\).\nWe have just shown that Brownian motion is a time-homogenous Markov process.\n\nDefinition 1 (Markov process.) Consider a stochastic process \\((X_t,t\\geq 0)\\) and its natural filtration \\((\\mathcal{F}_t,t\\geq 0)\\). It is said to be a Markov process if and only if for any (bounded) function \\(g: \\mathbb{R} \\to \\mathbb{R}\\), we have:\n\\[\n\\mathbb{E}[g(X_t) | \\mathcal{F}_s] = \\mathbb{E}[g(X_t) | X_s], \\quad \\forall t \\geq 0, \\forall s \\leq t\n\\tag{2}\\]\n\nThis implies that \\(\\mathbb{E}[g(X_t)|\\mathcal{F}_s]\\) is an explicit function of \\(s\\), \\(t\\) and \\(X_s\\). It is said to be time-homogenous, if it is a function of \\(t-s\\) and \\(X_s\\). Since the above holds for all bounded \\(g\\), the conditional distribution of \\(X_t\\) given \\(\\mathcal{F}_s\\) is the same as the conditional distribution of \\(X_t\\) given \\(X_s\\).\nOne way to compute the conditional distribution of \\(X_t\\) given \\(\\mathcal{F}_s\\) is to compute the conditional MGF given \\(\\mathcal{F}_s\\), that is:\n\\[\n\\mathbb{E}[e^{a X_t}|\\mathcal{F}_s], \\quad a \\geq 0\n\\tag{3}\\]\nThe process would be Markov, if the conditional MGF is an explicit function of \\(s\\), \\(t\\) and \\(X_s\\).\n\nExample 1 (Brownian Motion is Markov) Let \\((B_t,t\\geq 0)\\) be a standard brownian motion. Our claim is that the brownian motion is a markov process.\n\nProof.\nWe have:\n\\[\\begin{align*}\n\\mathbb{E}[e^{a B_t}|\\mathcal{F}_s] &= \\mathbb{E}[e^{a (B_t - B_s + B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_s \\text{ is }\\mathcal{F}_s-\\text{ measurable }\\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_t - B_s \\perp \\mathcal{F}_s \\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}]\\\\\n&= e^{a B_s} e^{\\frac{1}{2}a^2(t-s)}\n\\end{align*}\\]\nThis closes the proof. \\(\\blacksquare\\)\nAn equivalent (but more symmetric) way to express the Markov property is to say that the future of the process is independent of the past, when conditioned on the present. Concretely, this means that for any \\(r &lt; s&lt; t\\), we have that \\(X_t\\) is independent of \\(X_r\\), when we condition on \\(X_s\\).\nThe conditional distribution of \\(X_t\\) given \\(X_s\\) is well described using transition probabilities. We will more interested in a case well these probabilities admit a density \\(f_{X_t|X_s=x}(y)\\). More precisely, for such a Markov process, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(X_t)|X_s = x] &= \\int_{\\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\\\\n&=\\int_{\\mathbb{R}} g(y) p(y,t|x,s) dy\n\\end{align*}\n\\]\nHere, we explicitly write the left-hand side as a function of space, that is, the position \\(X_s\\), by fixing \\(X_s = x\\). In words, the transition probability density \\(p(y,t|x,s)\\) represents the probability density that starting from \\(X_s = x\\) at time \\(s\\), the process ends up at \\(X_t = y\\) at time \\(t &gt; s\\). If the process is time-homogenous, this only depends on the time difference \\((t-s)\\) and we write \\(p(y,t|x,s)\\). From Equation 1, we can write: \\[\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(u + x) \\frac{e^{-\\frac{u^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} du\n\\]\nIn the above expression, the random variable \\(B_t - B_s\\) takes some value \\(u \\in \\mathbb{R}\\) and \\(B_s = x\\) is fixed. Then, \\(B_t\\) takes the value \\(u + x\\). Let \\(y = u + x\\). Then, \\(u = y - x\\). Consequently, we may write:\n\\[\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(y) \\frac{e^{-\\frac{(y-x)^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} dy\n\\]\nSo, the transition density function for standard Brownian motion is:\n\\[\np(y,t|x,0)= \\frac{e^{-\\frac{(y-x)^2}{2s}}}{\\sqrt{2\\pi s}}, \\quad s&gt;0, x,y\\in\\mathbb{R}\n\\tag{4}\\]\nThis function is sometimes called the heat kernel, as it relates to the heat equation.\nThe Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process \\((X_t,t\\geq 0)\\) at different times. Consider the functions \\(f = \\mathbf{1}_A\\) and \\(g = \\mathbf{1}_B\\) from \\(\\mathbb{R} \\to \\mathbb{R}\\), where \\(A\\) and \\(B\\) are two intervals in \\(\\mathbb{R}\\). Let’s compute \\(\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) = \\mathbb{E}[\\mathbf{1}_{A} \\mathbf{1}_{B}] = \\mathbb{E}[f(X_{t_1}) g(X_{t_2})]\\) for \\(t_1 &lt; t_2\\). By the properties of conditional expectation and the Markov property, we have:\n\\[\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\mathbb{E}[f(X_{t_1})g(X_{t_2})]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|\\mathcal{F}_{t_1}]]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|X_{t_1}]]\n\\end{align*}\\]\nAssuming that the process is time-homogenous and admits a transition density \\(p(y,t|x,0)\\) as for Brownian motion, this becomes:\n\\[\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\int_{\\mathbb{R}} f(x_1) \\left(\\int_{\\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\\\\\n&= \\int_{A} \\left(\\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\n\\end{align*}\\]\nThis easily generalizes to any finite-dimensional distribution of \\((X_t, t\\geq 0)\\).\n\nExample 2 (Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift \\((X_t, t \\geq 0)\\), where \\(X_t = \\sigma B_t + \\mu t\\). Conversely, take \\(Y_t = \\int_0^t X_s dB_s\\), where \\(X_s = \\int_0^s B_u dB_u\\). The integrand \\(X_s\\) depends on whole Brownian motion path upto time \\(s\\) and not just on \\(B_s\\).\n\n\n\n\n\n\n\nTip 1: Functions of Markov Processes\n\n\n\nIt might be tempting to think that if \\((X_t,t\\geq 0)\\) is a Markov process, then the process defined by \\(Y_t = f(X_t)\\) for some reasonable function \\(f\\) is also Markov. Indeed, one could hope to write for an arbitrary bounded function \\(g\\):\n\\[\n\\begin{align*}\n\\mathbb{E}[g(Y_t)|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{X}_s]\n\\end{align*}\n\\tag{5}\\]\nby using the Markov property of \\((X_t,t\\geq 0)\\). The flaw in this reasoning is that the Markov property should hold for the natural fitration \\((\\mathcal{F}_t^Y,t\\geq 0)\\) of the process \\((Y_t,t\\geq 0)\\) and not the one of \\((X_t,t\\geq 0)\\), \\((\\mathcal{F}_t^X,t\\geq 0)\\). It might be that the filtration \\((\\mathcal{F}_t^Y,t\\geq 0)\\) has less information that \\((\\mathcal{F}_t^X,t\\geq 0)\\), especially, if the function \\(f\\) is not one-to-one. For example, if \\(f(x)=x^2\\), then \\(\\mathcal{F}_t^Y\\) has less information than \\(\\mathcal{F}_t^X\\) as we cannot recover the sign of \\(X_t\\) knowing \\(Y_t\\). In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when \\(f\\) is not one-to-one.\n\n\nIt turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.\n\nTheorem 1 (Diffusions are Markov processes.) Let \\((B_t,t\\geq 0)\\) be a standard Brownian motion. Let \\(\\mu : \\mathbb{R} \\to \\mathbb{R}\\) and \\(\\sigma: \\mathbb{R} \\to \\mathbb{R}\\) be differentiable functions with bounded derivatives on \\([0,T]\\). Then, the diffusion with the SDE\n\\[\ndX_t = \\mu(X_t) dt + \\sigma(X_t)dB_t, \\quad X_0 = x_0\n\\]\ndefines a time-homogenous markov process on \\([0,T]\\).\n\nAn analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments.\nProof.\nBy the existence and uniqueness theorem, this stochastic initial value problem(SIVP) defines a unique continous adapted process \\((X_t,t\\leq T)\\). Let \\((\\mathcal{F}_t^X,t\\geq 0)\\) be the natural filtration of \\((X_t,t\\leq T)\\). For a fixed \\(t &gt; 0\\), consider the process \\(W_s = B_{t+s} - B_t, s \\geq 0\\). Let \\((\\mathcal{F}_t,t \\geq 0)\\) be the natural filtration of \\((B_t,t \\geq 0)\\). It turns out that the process \\((W_s,s \\geq 0)\\) is a standard brownian motion independent of \\(\\mathcal{F}_t\\) (Exercise 1). For \\(s \\geq 0\\), we consider the SDE:\n\\[\ndY_s = \\mu (Y_s) ds + \\sigma(Y_s) dW_s, \\quad Y_0 = X_t\n\\]\nAgain by the existence and uniqueness theorem, there exists a unique solution to the SIVP that is adapted to the natural filtration of \\(W\\). Note that, the shifted process \\((X_{t+s},s\\geq 0)\\) is the solution to this SIVP since:\n\\[\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{t}^{t+s}\\mu(X_u) du + \\int_{t}^{t+s}\\sigma(X_u) dB_u\n\\end{align*}\\]\nPerform a change of variable \\(v = u - t\\). Then, \\(dv = du\\), \\(dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v\\). So,\n\\[\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{0}^{s}\\mu(X_{t+v}) dv + \\int_{0}^{s}\\sigma(X_{t+v}) dW_v\n\\end{align*}\\]\nLet \\(Y_v= X_{t+v}\\), \\(Y_0 = X_t\\). Then,\n\\[\\begin{align*}\nY_s &= Y_0 + \\int_{0}^{s}\\mu(Y_v) dv + \\int_{0}^{s}\\sigma(Y_v) dW_v\n\\end{align*}\\]\nThus, we conclude that for any interval \\(A\\):\n\\[\n\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(Y_s \\in A | \\mathcal{F}_t^X)\n\\]\nBut, since \\((Y_s,s \\geq 0)\\) depends on \\(\\mathcal{F}_t^X\\) only through \\(X_t\\) (because \\((W_s,s \\geq 0)\\) is independent of \\(\\mathcal{F}_t\\)), we conclude that \\(\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(X_{t+s} \\in A|X_t)\\), so \\((X_t,t \\geq 0)\\) is a time-homogenous markov process. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/the_markov_property/index.html#the-markov-property-for-diffusions",
    "href": "posts/the_markov_property/index.html#the-markov-property-for-diffusions",
    "title": "The Markov Property",
    "section": "",
    "text": "Let’s start by exhibiting the Markov property of Brownian motion. To see this, consider \\((\\mathcal{F}_t,t\\geq 0)\\), the natural filtration of the Brownian motion \\((B_t,t\\geq 0)\\). Consider \\(g(B_t)\\) for some time \\(t\\) and bounded function \\(g\\). (For example, \\(g\\) could be an indicator function.) Consider also a random variable \\(W\\) that is \\(\\mathcal{F}_s\\) measurable for \\(s &lt; t\\). (For example, \\(W\\) could be \\(B_s\\) or \\(1_{B_s &gt; 0}\\).) Let’s compute \\(\\mathbb{E}[g(B_t)W]\\).\n\\[\\begin{align*}\n\\mathbb{E}[g(B_t)W] &= \\mathbb{E}[\\mathbb{E}[Wg(B_t - B_s + B_s)|\\mathcal{F}_s]]\n\\end{align*}\\]\nThe random variable \\((B_t - B_s)\\) follows a \\(\\mathcal{N}(0,t-s)\\) distribution. By LOTUS,\n\\[\\begin{align*}\n\\mathbb{E}[g(B_t)W]\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)|\\mathcal{F_s}]f_{(B_t - B_s)|B_s}(y) dy\\\\\n&= \\{\\text{ Using the fact that }B_t - B_s \\perp B_s\\}\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]f_{(B_t - B_s)}(y)dy\\\\\n&= \\int_{\\mathbb{R}} \\mathbb{E}[W g(y + B_s)]\\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\\]\nBy Fubini’s theorem, the integral and the expectation operator can be interchanged, and since \\(W\\) is \\(\\mathcal{F}_s\\) measurable, it follows from the definition of conditional expectations that:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] &= \\int_{\\mathbb{R}} g(y + B_s) \\frac{e^{-\\frac{y^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}}dy\n\\end{align*}\n\\tag{1}\\]\nWe make two important observations. First, the right hand side is a function of \\(s,t\\) and \\(B_s\\) only (and not of the Brownian motion before time s). In particular, we have:\n\\[\n\\mathbb{E}[g(B_t)|\\mathcal{F}_s] = \\mathbb{E}[g(B_t)|B_s]\n\\]\nThis holds for any bounded function \\(g\\). In particular, it holds for all indicator functions. This implies that the conditional distribution of \\(B_t\\) given \\(\\mathcal{F}_s\\) depends solely on \\(B_s\\), and not on other values before time \\(s\\). Second, the right-hand side is time-homogenous in the sense that it depends on the time difference \\(t-s\\).\nWe have just shown that Brownian motion is a time-homogenous Markov process.\n\nDefinition 1 (Markov process.) Consider a stochastic process \\((X_t,t\\geq 0)\\) and its natural filtration \\((\\mathcal{F}_t,t\\geq 0)\\). It is said to be a Markov process if and only if for any (bounded) function \\(g: \\mathbb{R} \\to \\mathbb{R}\\), we have:\n\\[\n\\mathbb{E}[g(X_t) | \\mathcal{F}_s] = \\mathbb{E}[g(X_t) | X_s], \\quad \\forall t \\geq 0, \\forall s \\leq t\n\\tag{2}\\]\n\nThis implies that \\(\\mathbb{E}[g(X_t)|\\mathcal{F}_s]\\) is an explicit function of \\(s\\), \\(t\\) and \\(X_s\\). It is said to be time-homogenous, if it is a function of \\(t-s\\) and \\(X_s\\). Since the above holds for all bounded \\(g\\), the conditional distribution of \\(X_t\\) given \\(\\mathcal{F}_s\\) is the same as the conditional distribution of \\(X_t\\) given \\(X_s\\).\nOne way to compute the conditional distribution of \\(X_t\\) given \\(\\mathcal{F}_s\\) is to compute the conditional MGF given \\(\\mathcal{F}_s\\), that is:\n\\[\n\\mathbb{E}[e^{a X_t}|\\mathcal{F}_s], \\quad a \\geq 0\n\\tag{3}\\]\nThe process would be Markov, if the conditional MGF is an explicit function of \\(s\\), \\(t\\) and \\(X_s\\).\n\nExample 1 (Brownian Motion is Markov) Let \\((B_t,t\\geq 0)\\) be a standard brownian motion. Our claim is that the brownian motion is a markov process.\n\nProof.\nWe have:\n\\[\\begin{align*}\n\\mathbb{E}[e^{a B_t}|\\mathcal{F}_s] &= \\mathbb{E}[e^{a (B_t - B_s + B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_s \\text{ is }\\mathcal{F}_s-\\text{ measurable }\\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}|\\mathcal{F}_s]\\\\\n& \\{ \\text{ since }B_t - B_s \\perp \\mathcal{F}_s \\}\\\\\n&= e^{a B_s} \\mathbb{E}[e^{a (B_t - B_s)}]\\\\\n&= e^{a B_s} e^{\\frac{1}{2}a^2(t-s)}\n\\end{align*}\\]\nThis closes the proof. \\(\\blacksquare\\)\nAn equivalent (but more symmetric) way to express the Markov property is to say that the future of the process is independent of the past, when conditioned on the present. Concretely, this means that for any \\(r &lt; s&lt; t\\), we have that \\(X_t\\) is independent of \\(X_r\\), when we condition on \\(X_s\\).\nThe conditional distribution of \\(X_t\\) given \\(X_s\\) is well described using transition probabilities. We will more interested in a case well these probabilities admit a density \\(f_{X_t|X_s=x}(y)\\). More precisely, for such a Markov process, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(X_t)|X_s = x] &= \\int_{\\mathbb{R}} g(y) f_{X_t|X_s=x}(y) dy\\\\\n&=\\int_{\\mathbb{R}} g(y) p(y,t|x,s) dy\n\\end{align*}\n\\]\nHere, we explicitly write the left-hand side as a function of space, that is, the position \\(X_s\\), by fixing \\(X_s = x\\). In words, the transition probability density \\(p(y,t|x,s)\\) represents the probability density that starting from \\(X_s = x\\) at time \\(s\\), the process ends up at \\(X_t = y\\) at time \\(t &gt; s\\). If the process is time-homogenous, this only depends on the time difference \\((t-s)\\) and we write \\(p(y,t|x,s)\\). From Equation 1, we can write: \\[\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(u + x) \\frac{e^{-\\frac{u^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} du\n\\]\nIn the above expression, the random variable \\(B_t - B_s\\) takes some value \\(u \\in \\mathbb{R}\\) and \\(B_s = x\\) is fixed. Then, \\(B_t\\) takes the value \\(u + x\\). Let \\(y = u + x\\). Then, \\(u = y - x\\). Consequently, we may write:\n\\[\n\\mathbb{E}[g(B_t)|B_s = x] = \\int_{\\mathbb{R}} g(y) \\frac{e^{-\\frac{(y-x)^2}{2(t-s)}}}{\\sqrt{2\\pi(t-s)}} dy\n\\]\nSo, the transition density function for standard Brownian motion is:\n\\[\np(y,t|x,0)= \\frac{e^{-\\frac{(y-x)^2}{2s}}}{\\sqrt{2\\pi s}}, \\quad s&gt;0, x,y\\in\\mathbb{R}\n\\tag{4}\\]\nThis function is sometimes called the heat kernel, as it relates to the heat equation.\nThe Markov property is very convenient to compute quantities, as we shall see throughout the chapter. As a first example, we remark that it is easy to express joint probabilities of a markov process \\((X_t,t\\geq 0)\\) at different times. Consider the functions \\(f = \\mathbf{1}_A\\) and \\(g = \\mathbf{1}_B\\) from \\(\\mathbb{R} \\to \\mathbb{R}\\), where \\(A\\) and \\(B\\) are two intervals in \\(\\mathbb{R}\\). Let’s compute \\(\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) = \\mathbb{E}[\\mathbf{1}_{A} \\mathbf{1}_{B}] = \\mathbb{E}[f(X_{t_1}) g(X_{t_2})]\\) for \\(t_1 &lt; t_2\\). By the properties of conditional expectation and the Markov property, we have:\n\\[\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\mathbb{E}[f(X_{t_1})g(X_{t_2})]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|\\mathcal{F}_{t_1}]]\\\\\n&= \\mathbb{E}[f(X_{t_1})\\mathbb{E}[g(X_{t_2})|X_{t_1}]]\n\\end{align*}\\]\nAssuming that the process is time-homogenous and admits a transition density \\(p(y,t|x,0)\\) as for Brownian motion, this becomes:\n\\[\\begin{align*}\n\\mathbb{P}(X_{t_1} \\in A, X_{t_2} \\in B) &= \\int_{\\mathbb{R}} f(x_1) \\left(\\int_{\\mathbb{R}} g(x_2) p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\\\\\n&= \\int_{A} \\left(\\int_{B} p(x_2,t_2|x_1,t_1) dx_2 \\right) p(x_1,t_1|x_0,0) dx_1\n\\end{align*}\\]\nThis easily generalizes to any finite-dimensional distribution of \\((X_t, t\\geq 0)\\).\n\nExample 2 (Markov versus Martingale.) Martingales are not markov processes in general and markov processes are not martingales in general. There are processes such as brownian motion that enjoy both. An example of a markov process that is not a martingale is a Brownian motion with a drift \\((X_t, t \\geq 0)\\), where \\(X_t = \\sigma B_t + \\mu t\\). Conversely, take \\(Y_t = \\int_0^t X_s dB_s\\), where \\(X_s = \\int_0^s B_u dB_u\\). The integrand \\(X_s\\) depends on whole Brownian motion path upto time \\(s\\) and not just on \\(B_s\\).\n\n\n\n\n\n\n\nTip 1: Functions of Markov Processes\n\n\n\nIt might be tempting to think that if \\((X_t,t\\geq 0)\\) is a Markov process, then the process defined by \\(Y_t = f(X_t)\\) for some reasonable function \\(f\\) is also Markov. Indeed, one could hope to write for an arbitrary bounded function \\(g\\):\n\\[\n\\begin{align*}\n\\mathbb{E}[g(Y_t)|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{F}_s] = \\mathbb{E}[g(f(X_t))|\\mathcal{X}_s]\n\\end{align*}\n\\tag{5}\\]\nby using the Markov property of \\((X_t,t\\geq 0)\\). The flaw in this reasoning is that the Markov property should hold for the natural fitration \\((\\mathcal{F}_t^Y,t\\geq 0)\\) of the process \\((Y_t,t\\geq 0)\\) and not the one of \\((X_t,t\\geq 0)\\), \\((\\mathcal{F}_t^X,t\\geq 0)\\). It might be that the filtration \\((\\mathcal{F}_t^Y,t\\geq 0)\\) has less information that \\((\\mathcal{F}_t^X,t\\geq 0)\\), especially, if the function \\(f\\) is not one-to-one. For example, if \\(f(x)=x^2\\), then \\(\\mathcal{F}_t^Y\\) has less information than \\(\\mathcal{F}_t^X\\) as we cannot recover the sign of \\(X_t\\) knowing \\(Y_t\\). In other words, the second equality may not hold. In some cases, a function of a Brownian motion might be Markov, even when \\(f\\) is not one-to-one.\n\n\nIt turns out that diffusions such as the Ornstein-Uhlenbeck process and the Brownian bridge are Markov processes.\n\nTheorem 1 (Diffusions are Markov processes.) Let \\((B_t,t\\geq 0)\\) be a standard Brownian motion. Let \\(\\mu : \\mathbb{R} \\to \\mathbb{R}\\) and \\(\\sigma: \\mathbb{R} \\to \\mathbb{R}\\) be differentiable functions with bounded derivatives on \\([0,T]\\). Then, the diffusion with the SDE\n\\[\ndX_t = \\mu(X_t) dt + \\sigma(X_t)dB_t, \\quad X_0 = x_0\n\\]\ndefines a time-homogenous markov process on \\([0,T]\\).\n\nAn analogous statement holds for time-inhomogenous diffusions. The proof is generalization of the Markov property of Brownian motion. We take advantage of the independence of Brownian increments.\nProof.\nBy the existence and uniqueness theorem, this stochastic initial value problem(SIVP) defines a unique continous adapted process \\((X_t,t\\leq T)\\). Let \\((\\mathcal{F}_t^X,t\\geq 0)\\) be the natural filtration of \\((X_t,t\\leq T)\\). For a fixed \\(t &gt; 0\\), consider the process \\(W_s = B_{t+s} - B_t, s \\geq 0\\). Let \\((\\mathcal{F}_t,t \\geq 0)\\) be the natural filtration of \\((B_t,t \\geq 0)\\). It turns out that the process \\((W_s,s \\geq 0)\\) is a standard brownian motion independent of \\(\\mathcal{F}_t\\) (Exercise 1). For \\(s \\geq 0\\), we consider the SDE:\n\\[\ndY_s = \\mu (Y_s) ds + \\sigma(Y_s) dW_s, \\quad Y_0 = X_t\n\\]\nAgain by the existence and uniqueness theorem, there exists a unique solution to the SIVP that is adapted to the natural filtration of \\(W\\). Note that, the shifted process \\((X_{t+s},s\\geq 0)\\) is the solution to this SIVP since:\n\\[\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{t}^{t+s}\\mu(X_u) du + \\int_{t}^{t+s}\\sigma(X_u) dB_u\n\\end{align*}\\]\nPerform a change of variable \\(v = u - t\\). Then, \\(dv = du\\), \\(dB_u = B(u_2) - B(u_1)= B(t + v_2) - B(t + v_1) = W(v_2) - W(v_1) = dW_v\\). So,\n\\[\\begin{align*}\nX_{t+s} &= X_{t} + \\int_{0}^{s}\\mu(X_{t+v}) dv + \\int_{0}^{s}\\sigma(X_{t+v}) dW_v\n\\end{align*}\\]\nLet \\(Y_v= X_{t+v}\\), \\(Y_0 = X_t\\). Then,\n\\[\\begin{align*}\nY_s &= Y_0 + \\int_{0}^{s}\\mu(Y_v) dv + \\int_{0}^{s}\\sigma(Y_v) dW_v\n\\end{align*}\\]\nThus, we conclude that for any interval \\(A\\):\n\\[\n\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(Y_s \\in A | \\mathcal{F}_t^X)\n\\]\nBut, since \\((Y_s,s \\geq 0)\\) depends on \\(\\mathcal{F}_t^X\\) only through \\(X_t\\) (because \\((W_s,s \\geq 0)\\) is independent of \\(\\mathcal{F}_t\\)), we conclude that \\(\\mathbb{P}(X_{t+s} \\in A|\\mathcal{F}_t^X) = \\mathbb{P}(X_{t+s} \\in A|X_t)\\), so \\((X_t,t \\geq 0)\\) is a time-homogenous markov process. \\(\\blacksquare\\)"
  },
  {
    "objectID": "posts/the_markov_property/index.html#the-strong-markov-property",
    "href": "posts/the_markov_property/index.html#the-strong-markov-property",
    "title": "The Markov Property",
    "section": "The Strong Markov Property",
    "text": "The Strong Markov Property\nThe Doob’s Optional Stopping theorem extended some properties of martingales to stopping times. The Markov property can also be extended to stopping times for certain processes. These processes are called strong Markov processes.\nWe know, that the sigma-algebra \\(\\mathcal{F}_t\\) represents the set of all observable events upto time \\(t\\). What is the sigma-algebra of observable events at a random stopping time \\(\\tau\\)?\n\nDefinition 2 (\\(\\sigma\\)-algebra of \\(\\tau\\)-past) Let \\((\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\geq 0},\\mathbb{P})\\) be a filtered probability space. The sigma-algebra at the stopping time \\(\\tau\\) is then:\n\\[\n\\mathcal{F}_{\\tau} = \\{A \\in \\mathcal{F}_\\infty : A \\cap \\{\\tau \\leq t\\} \\in \\mathcal{F}_t, \\forall t \\geq 0 \\}\n\\tag{6}\\]\n\nIn words, an event \\(A\\) is in \\(\\mathcal{F}_\\tau\\), if we can determine if \\(A\\) and \\(\\{\\tau \\leq t\\}\\) both occurred or not based on the information \\(\\mathcal{F}_t\\) known at any arbitrary time \\(t\\). You should be able to tell the value of the random variable \\(\\mathbf{1}_A \\cdot \\mathbf{1}_{\\{\\tau \\leq t\\}}\\) given \\(\\mathcal{F}_t\\) for any arbitrary time \\(t \\geq 0\\).\nFor example, if \\(\\tau &lt; \\infty\\), the event \\(\\{B_\\tau &gt; 0\\}\\) is in \\(\\mathcal{F}_\\tau\\). However, the event \\(\\{B_1 &gt; 0\\}\\) is not in \\(\\mathcal{F}_\\tau\\) in general, since \\(A \\cap \\{\\tau \\leq t\\}\\) is not in \\(\\mathcal{F}_t\\) for \\(t &lt; 1\\). Roughly speaking, a random variable that is \\(\\mathcal{F}_\\tau\\)-measurable should be thought of as an explicit function of \\(X_\\tau\\). With this new object, we are ready to define the strong markov property.\n\nDefinition 3 (Strong Markov Property) Let \\((X_t,t\\geq 0)\\) be a stochastic process and let \\((\\mathcal{F}_t,t\\geq 0)\\) be its natural filtration. The process \\((X_t,t\\geq 0)\\) is said to be strong markov if for any stopping time \\(\\tau\\) for the filtration of the process and any bounded function \\(g\\):\n\\[\n\\mathbb{E}[g(X_{t+\\tau})|\\mathcal{F}_\\tau] = \\mathbb{E}[g(X_{t+\\tau})|X_\\tau]\n\\]\n\nThis means that \\(X_{t+\\tau}\\) depends on \\(\\mathcal{F}_\\tau\\) solely through \\(X_\\tau\\) (whenever \\(\\tau &lt; \\infty\\)). It turns out that Brownian motion is a strong markov process. In fact a stronger statement holds which generalizes Exercise 1.\n\nTheorem 2 Let \\(\\tau\\) be a stopping time for the filtration of the Brownian motion \\((B_t,t\\geq 0)\\) such that \\(\\tau &lt; \\infty\\). Then, the process:\n\\[\n(B_{t+\\tau} - B_{\\tau},t\\geq 0)\n\\]\nis a standard brownian motion independent of \\(\\mathcal{F}_\\tau\\).\n\n\nExample 3 (Brownian motion is strong Markov) To see this, let’s compute the conditional MGF as in Equation 3. We have:\n\\[\n\\begin{align*}\n\\mathbb{E}[e^{aB_{t+\\tau}}|\\mathcal{F}_\\tau] &= \\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau + B_\\tau)}|\\mathcal{F}_\\tau]\\\\\n&= e^{aB_\\tau} \\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau)}|\\mathcal{F}_\\tau]\\\\\n& \\{ B_\\tau \\text{ is }\\mathcal{F}_\\tau-\\text{measurable }\\}\\\\\n&= e^{aB_\\tau}\\mathbb{E}[e^{a(B_{t+\\tau} - B_\\tau)}]\\\\\n& \\{ (B_{t+\\tau} - B_\\tau) \\perp \\mathcal{F}_\\tau\\}\\\\\n&= e^{aB_\\tau}e^{\\frac{1}{2}a^2 t}\\\\\n\\end{align*}\n\\]\nThus, the conditional MGF is an explicit function of \\(B_\\tau\\) and \\(t\\). This proves the proposition. \\(\\blacksquare\\)\n\nProof of Theorem 2.\nWe first consider for fixed \\(n\\) the discrete valued stopping time:\n\\[\n\\tau_n = \\frac{k + 1}{2^n}, \\quad \\text{ if } \\frac{k}{2^n} \\leq \\tau &lt; \\frac{k+1}{2^n}, k\\in \\mathbb{N}\n\\]\nIn other words, if \\(\\tau\\) occurs in the interval \\([\\frac{k}{2^n},\\frac{k+1}{2^n})\\), we stop at the next dyadic \\(\\frac{k+1}{2^n}\\). By construction \\(\\tau_n\\) depends only on the process in the past. Consider the process \\(W_t = B_{t + \\tau_n} - B_{\\tau_n}, t \\geq 0\\). We show it is a standard brownian motion independent of \\(\\tau_n\\). This is feasible as we can decompose over the discrete values taken by \\(\\tau_n\\). More, precisely, take \\(E \\in \\mathcal{F}_{\\tau_n}\\), and some generic event \\(\\{W_t \\in A\\}\\) for the process \\(W\\). Then, by decomposing over the values of \\(\\tau_n\\), we have:\n\\[\n\\begin{align*}\n\\mathbb{P}(\\{W_t \\in A\\} \\cap E) &= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{W_t \\in A\\} \\cap E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\\\\\n&= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\} \\cap E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\\\\\n&= \\sum_{k=0}^\\infty \\mathbb{P}\\left(\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\}\\right) \\times \\mathbb{P}\\left( E \\cap \\{\\tau_n = \\frac{k}{2^n}\\}\\right)\n\\end{align*}\n\\]\nsince \\((B_{t+k/2^n} - B_{k/2^n})\\) is independent of \\(\\mathcal{F}_{k/2^n}\\) by Exercise 1 and since \\(E \\cap \\{\\tau_n = \\frac{k}{2^n}\\} \\in \\mathcal{F}_{k/2^n}\\) by definition of stopping time. But, given \\(\\{\\tau_n = k/2^n\\}\\), the event \\(\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\}\\) is the same as \\(\\{B_t \\in A\\} = \\{W_t \\in A\\}\\), since this process is now a standard brownian motion. Thus, \\(\\mathbb{P}\\{(B_{t+k/2^n} - B_{k/2^n}) \\in A\\} = \\mathbb{P}\\{B_t \\in A\\} = \\mathbb{P}\\{W_t \\in A\\}\\), dropping the dependence on \\(k\\). The sum over \\(k\\) then yields:\n\\[\n\\mathbb{P}\\left(\\{W_t \\in A\\}\\cap E\\right) = \\mathbb{P}(W_t \\in A) \\mathbb{P}(E)\n\\]\nas claimed. The extension to \\(\\tau\\) is done by using continuity of paths. We have:\n\\[\n\\lim_{n \\to \\infty} B_{t + \\tau_n} - B_{\\tau_n} = B_{t+\\tau} - B_{\\tau} \\text{ almost surely}\n\\]\nNote, that this only uses right continuity! Moreover, this implies that \\(B_{t+\\tau} - B_\\tau\\) is independent of \\(\\mathcal{F}_{\\tau_n}\\) for all \\(n\\). Again by (right-)continuity this extends to independence of \\(\\mathcal{F}_\\tau\\). The limiting distribution of the process is obtained by looking at the finite dimensional distributions of the increments of \\(B_{t+\\tau_n} - B_{\\tau_n}\\) for a finite number of \\(t\\)’s and taking the limit as above. \\(\\blacksquare\\)\nMost diffusions also enjoy the strong markov property, as long as the functions \\(\\sigma\\) and \\(\\mu\\) encoding the volatility and drift are nice enough. This is the case for the diffusions we have considered.\n\nTheorem 3 (Most diffusions are strong markov) Consider a diffusion \\((X_t,t\\leq T)\\) as as in Theorem 1. Then, the diffusion has strong markov property.\n\nThe proof follows the line of the one of Theorem 1\nProof.\nConsider the time-homogenous diffusion:\n\\[\ndX_t = \\mu(X_t)dt + \\sigma(X_t)dB_t\n\\]\nBy the existence and uniqueness theorem, this SIVP defines a unique continuous adapted process \\((X_t,t \\geq 0)\\). Let \\(\\mathfrak{F}=(\\mathcal{F}_t^X,t \\geq 0)\\) be the natural filtration of \\((X_t, t\\leq T)\\). Let \\(\\tau\\) be a stopping time for the filtration \\(\\mathfrak{F}\\) and consider the process \\(W_t = B_{t+\\tau} - B_\\tau\\). From Theorem 2, we know that the process \\((W_t,t\\geq 0)\\) is a standard brownian motion independent \\(\\mathcal{F}_\\tau\\). For \\(s \\geq 0\\), we consider the SDE:\n\\[\ndY_s = \\mu(Y_s)ds + \\sigma(Y_s)dW_s, \\quad Y_0 = X_\\tau\n\\tag{7}\\]\nAgain by the existence and uniqueness theorem, there exists a unique solution to the SIVP that is adapted to the natural filtration of \\(W\\). We claim that \\((X_{s+\\tau},s \\geq 0)\\) is the solution to this equation, since:\n\\[\nX_{s+\\tau} = X_\\tau + \\int_\\tau^{s+\\tau} \\mu(X_u)du + \\int_{\\tau}^{s+\\tau} \\sigma(X_u)dB_u\n\\]\nPerform a change of variable \\(v = u - \\tau\\). Then, the limits of integration bare, \\(v = 0\\) and \\(v = s\\). And \\(dv = du\\).\n\\(dB_u  \\approx B_{u_2} - B_{u_1} = B(v_1 + \\tau) - B(v_2 + \\tau) = W(v_2) - W(v_1) =dW_v\\).\n\\[\nX_{s+\\tau} = X_\\tau + \\int_0^{s} \\mu(X_{v+\\tau})dv + \\int_{0}^{s} \\sigma(X_{v+\\tau})dW_v\n\\]\nIf we let \\(Y_0 = X_\\tau\\), \\(Y_v = X_{v+\\tau}\\), we recover the dynamics of \\((Y_v,v \\geq 0)\\) in Equation 7. So, \\((X_{s+\\tau},s\\geq 0)\\) is the solution to the SIVP in Equation 7. Thus, we conclude for any interval \\(A\\):\n\\[\n\\mathbb{P}(X_{s+\\tau} \\in A | \\mathcal{F}_\\tau^X) = \\mathbb{P}(Y_v \\in A| \\mathcal{F}_\\tau^X)\n\\]\nBut, since \\((Y_v,v\\geq 0)\\) depends on \\(\\mathcal{F}_\\tau^X\\) only through \\(X_\\tau\\), we conclude that \\(\\mathbb{P}(X_{s + \\tau} \\in A | \\mathcal{F}_\\tau^X) = \\mathbb{P}(X_{s + \\tau} \\in A| X_\\tau)\\). Consequently, \\((X_t,t \\geq 0)\\) is a strong-markov process. \\(\\blacksquare\\)\n\n\n\n\n\n\nTip 2: Extension of optional sampling\n\n\n\nConsider a continuous martingale \\((M_t, t\\leq T)\\) for a filtration \\((\\mathcal{F}_t, t\\geq 0)\\) and a stopping time \\(\\tau\\) for the same filtration. Suppose we would like to compute for some \\(T\\):\n\\[\n\\mathbb{E}[M_T \\mathbf{1}_{\\{\\tau \\leq T\\}}]\n\\]\nIt would be tempting to condition on \\(\\mathcal{F}_\\tau\\) and write \\(\\mathbb{E}[M_T |\\mathcal{F}_\\tau] = M_\\tau\\) on the event \\(\\{\\tau \\leq T\\}\\). We would then conclude that:\n\\[\n\\mathbb{E}[M_T 1_{\\{\\tau \\leq T\\}}] = \\mathbb{E}[1_{\\{\\tau \\leq T\\}} \\mathbb{E}[M_T|\\mathcal{F}_\\tau] ] = \\mathbb{E}[M_\\tau 1_{\\{\\tau \\leq T\\}}]\n\\]\nIn some sense, we have extended the martingale property to stopping times. This property can be proved under reasonable assumptions on \\((M_t,t\\leq T)\\) (for example, if it is positive). Indeed, it suffices to approximate \\(\\tau\\) by discrete valued stopping time \\(\\tau_n\\) as in the proof of Theorem 2. One can then apply martingale property at a fixed time."
  },
  {
    "objectID": "posts/the_markov_property/index.html#the-heat-equation",
    "href": "posts/the_markov_property/index.html#the-heat-equation",
    "title": "The Markov Property",
    "section": "The Heat Equation",
    "text": "The Heat Equation\nWe look at more detail on how PDEs come up when computing quantities related to Markov processes.\n\nExample 4 (Heat Equation and Brownian motion) Let \\(f(t,x)\\) be a function of time and space. The heat equation in \\(1+1\\)-dimension (one dimension of time, one dimension of space) is the PDE:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\n\\end{align*}\n\\tag{8}\\]\nIn \\(1+d\\) (one dimension of time, \\(d\\) dimensions of space), the heat equation is:\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t} &= \\frac{1}{2}\\nabla^2 f\n\\end{align*}\n\\tag{9}\\]\nwhere \\(\\nabla^2\\) is the Laplacian operator.\nLet \\((X_t,t \\geq 0)\\) be a brownian motion starting at \\(X_0 = x\\) with probability density:\n\\[\nf(0,x) = g(x)\n\\tag{10}\\]\nwhere \\(g\\) is a function of space.\nLet \\(f(t,u)\\) be the probability density that the process ends up at \\(X_t=u\\) at time \\(t\\). By the law of total probability, we have:\n\\[\n\\begin{align*}\nf(t,x) &\\approx \\sum_{y} \\mathbb{P}\\left\\{X_0 = y \\right\\} \\times \\mathbb{P}\\left\\{X_t = x | X_0 = y \\right\\}\\\\\n&= \\int_{-\\infty}^\\infty g(y)\\cdot p(x,t|y,0)dy\\\\\n\\end{align*}\n\\]\nObserve that:\n\\[\n\\begin{align*}\n\\mathbb{E}[g(X_t)|X_0 = x] &= \\int_{-\\infty}^\\infty g(y) \\cdot p(y,t|x,0)dy\\\\\n&=\\int_{-\\infty}^\\infty g(y) \\cdot p(x,t|y,0)dy\n\\end{align*}\n\\]\nThus, the function \\(f\\) can be represented as a specific type of space average. It can be represented as an average of \\(g(B_t)\\) over brownian paths starting at \\(x\\):\n\\[\nf(t,x) = \\mathbb{E}[g(B_t)|B_0 = x]\n\\tag{11}\\]\nOur claim is that \\(f\\) indeed satisfies the PDE (Equation 8).\nThe gaussian transition probability density function (heat kernel) \\(p(x,t|y,0)\\) is given by:\n\\[\np(x,t|y,0) = \\frac{1}{\\sqrt{2\\pi t}}\\exp\\left(-\\frac{(x-y)^2}{2t}\\right)\n\\]\nDifferentiating \\(p\\) with respect to \\(t\\), we have:\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial t} p(x,t|y,0) &= \\frac{\\sqrt{2\\pi t} \\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{\\partial}{\\partial t}\\left(-\\frac{(x-y)^2}{2t}\\right) - \\exp\\left(-\\frac{(x-y)^2}{2t}\\right)\\sqrt{2\\pi}\\left(\\frac{1}{2\\sqrt{t}}\\right)}{2\\pi t}\\\\\n&=\\sqrt{2\\pi}\\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{\\frac{(x-y)^2}{2t^{3/2}} - \\frac{t}{2t^{3/2}}}{2\\pi t}\\\\\n&= \\exp\\left(-\\frac{(x-y)^2}{2t}\\right) \\frac{(x-y)^2 - t}{\\sqrt{2\\pi} (2t^{5/2}) }\n\\end{align*}\n\\tag{12}\\]\nDifferentiating \\(p\\) with respect to \\(x\\), we have:\n\\[\n\\begin{align*}\n\\frac{\\partial }{\\partial x} p(x,t|y,0) &= \\frac{1}{\\sqrt{2\\pi t}}\\exp\\left[-\\frac{(x-y)^2}{2t}\\right]\\frac{\\partial}{\\partial x}\\left(-\\frac{(x-y)^2}{2t}\\right)\\\\\n&= \\frac{1}{\\sqrt{2\\pi t}} \\cdot \\left(-\\frac{1}{\\cancel{2} t}\\right) \\exp\\left[-\\frac{(x-y)^2}{2t}\\right] \\cdot \\cancel{2}(x-y)\\\\\n&= -\\frac{1}{t\\sqrt{2\\pi t}} (x-y)\\exp\\left[-\\frac{(x-y)^2}{2t}\\right]\n\\end{align*}\n\\tag{13}\\]\nDifferentiating again with respect to space, we have:\n\\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial x^2} p(x,t|y,0) &= -\\frac{1}{t\\sqrt{2\\pi t}} \\left[\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} + (x-y)\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\}\\left(-\\frac{2(x-y)}{2y}\\right)\\right]\\\\\n&=-\\frac{1}{t\\sqrt{2\\pi t}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\left[1 - \\frac{(x-y)^2}{t}\\right]\\\\\n&=\\frac{1}{t\\sqrt{2\\pi t}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\left[\\frac{(x-y)^2 - t}{t}\\right]\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{(x-y)^2}{2}\\right\\} \\cdot \\frac{(x-y)^2 - t}{t^{5/2}}\n\\end{align*}\n\\tag{14}\\]\nFrom Equation 12 and Equation 14, it follows that:\n\\[\n\\frac{\\partial}{\\partial t} p(x,t|y,0) = \\frac{1}{2}\\frac{\\partial ^2}{\\partial x^2} p(x,t|y,0)\n\\]\nThus,\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial t} \\int_{-\\infty}^\\infty g(y) p(x,t|y,0)dy &= \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} \\int_{-\\infty}^\\infty g(y) p(x,t|y,0)dy \\\\\n\\frac{\\partial }{\\partial t}f(t,x) &= \\frac{1}{2}\\frac{\\partial^2 }{\\partial x^2} f(t,x)\n\\end{align*}\n\\]\n\n\nRobert Brown’s erratic motion of pollen\nIn the summer of 1827, the Scottish botanist Robert Brown observed that microscopic pollen grains suspended in water move in an erratic, highly irregular, zigzag pattern. It was only in 1905, that Albert Einstein could provide a satisfactory explanation of Brownian motion. He asserted that Brownian motion originates in the continual bombardment of the pollen grains by the molecules of the surrounding water. As a result of continual collisions, the particles themselves had the same kinetic energy as the water molecules. Thus, he showed that Brownian motion provided a solution (in a certain sense) to Fourier’s famous heat equation\n\\[\n\\frac{\\partial u}{\\partial t}(t,x) = \\kappa \\frac{\\partial^2 u}{\\partial x^2}(t,x)\n\\]\n\n\nAlbert Einstein’s proof of the existence of Brownian motion\nWe now summarize Einstein’s original 1905 argument. Let’s say that we are interested in the motion along the horizontal \\(x\\)-axis. Let’s say we drop brownian particles in a liquid. Let \\(f(t,x)\\) represent the number of particles per unit volume (density) at position \\(x\\) at time \\(t\\). So, the number of particles in a small interval \\(I=[x,x+dx]\\) of width \\(dx\\) will be \\(f(t,x)dx\\).\nNow, as time progresses, the number of particles in this interval \\(I\\) will change. The brownian particles will zig-zag upon bombardment by the molecules of the liquid. Some particles will move out of the interval \\(I\\), while other particles will move in.\nLet’s consider a timestep of length \\(\\tau\\). Einstein’s probabilistic approach was to model the distance travelled by the particles or displacement of the particles as a random variable \\(\\Delta\\). To determine how many particles end up in the interval \\(I\\), we start with the area to the right of the interval \\(I\\).\nThe density of particles at \\(x+\\Delta\\) is \\(f(t,x+\\Delta)\\); the number of particles in a small interval of length \\(dx\\) is \\(f(t,x+\\Delta)dx\\). If we represent the probability density of the displacement by \\(\\phi(\\Delta)\\), then the number of particles at \\(x+\\Delta\\) that will move to \\(x\\) will be \\(dx \\cdot f(t,x+\\Delta)\\phi(\\Delta)\\). We can apply the same logic to the left hand side. The number of particles at \\(x - \\Delta\\) that will move to \\(x\\) will be \\(dx \\cdot f(t,x-\\Delta)\\phi(-\\Delta)\\). Assume that \\(\\phi(\\Delta) = \\phi(-\\Delta)\\).\nNow, if we integrate these movements across the real line, then we get the number of particles at \\(x\\) at a short time later \\(t + \\tau\\).\n\\[\nf(t+ \\tau,x) dx = dx \\int_{-\\infty}^{\\infty} f(t,x+\\Delta) \\phi(\\Delta) d\\Delta\n\\]\nNow, we can get rid of \\(dx\\).\n\\[\nf(t+ \\tau,x) = \\int_{-\\infty}^{\\infty} f(t,x+\\Delta) \\phi(\\Delta) d\\Delta\n\\tag{15}\\]\nThe Taylor’s series expansion of \\(f(t+\\tau,x)\\) centered at \\(t\\) (holding \\(x\\) constant) is:\n\\[\nf(t + \\tau,x) = f(t,x) + \\frac{\\partial f}{\\partial t}\\tau + O(\\tau^2)\n\\]\nThe Taylor’s series expansion of \\(f(t,x+\\Delta)\\) centered at \\(x\\) (holding \\(t\\) constant) is:\n\\[\nf(t,x+\\Delta) = f(t,x) + \\frac{\\partial f}{\\partial x}\\Delta + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\Delta^2 + O(\\Delta^3)\n\\]\nWe can now substitute these into Equation 15 to get:\n\\[\n\\begin{align*}\nf(t,x) + \\frac{\\partial f}{\\partial t}\\tau &= \\int_{-\\infty}^{\\infty}\\left(f(t,x) + \\frac{\\partial f}{\\partial x}\\Delta + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\Delta^2\\right) \\phi(\\Delta)d\\Delta\\\\\n&= f(t,x) \\int_{-\\infty}^{\\infty} \\phi(\\Delta)d\\Delta \\\\\n&+ \\frac{\\partial f} {\\partial x} \\int_{-\\infty}^{\\infty} \\Delta \\phi(\\Delta)d\\Delta \\\\\n&+ \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta\n\\end{align*}\n\\]\nNow, since the probability distribution of displacement \\(\\phi(\\cdot)\\) is symmetric around the origin, the second term is zero. And we know, that if we integrate the density over \\(\\mathbb{R}\\), we should get one, so the first term equals one. So, we get:\n\\[\nf(t,x) + \\frac{\\partial f}{\\partial t}\\tau = f(t,x) + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta\n\\]\nNow, we can cancel the \\(f\\) on both sides and then shift \\(\\tau\\) to the right hand side:\n\\[\n\\frac{\\partial f}{\\partial t} =  \\left(\\frac{1}{2\\tau} \\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta \\right)\\frac{\\partial^2 f}{\\partial x^2}\n\\]\nDefine \\(D:= \\left(\\frac{1}{2\\tau} \\int_{-\\infty}^{\\infty}\\Delta^2 \\phi(\\Delta)d\\Delta \\right)\\). Then, we have:\n\\[\n\\frac{\\partial f}{\\partial t} =  D\\frac{\\partial^2 f}{\\partial x^2}\n\\]\nThe microscopic interpretation of the diffusion coefficient is, that its just the average of the squared displacements. The larger the \\(D\\), the faster the brownian particles move."
  },
  {
    "objectID": "posts/the_markov_property/index.html#kolmogorovs-backward-equation",
    "href": "posts/the_markov_property/index.html#kolmogorovs-backward-equation",
    "title": "The Markov Property",
    "section": "Kolmogorov’s Backward Equation",
    "text": "Kolmogorov’s Backward Equation\nThink of \\(y\\) and \\(t\\) as being current values and \\(y'\\) and \\(t'\\) being future values. The transition probability density function \\(p(y',t'|y,t)\\) of a diffusion satisfies two equations - one involving derivatives with respect to a future state and time (\\(y'\\) and \\(t'\\)) called forward equation and the other involving derivatives with respect to the current state and current time (\\(y\\) and \\(t\\)) called the backward equation. These two equations are parabolic partial differential equations not dissimilar to the Black-Scholes equation.\n\nTheorem 4 (Backward equation with initial value) Let \\((X_t,t\\geq 0)\\) be a diffusion in \\(\\mathbb{R}\\) with the SDE:\n\\[\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t) dt\n\\]\nLet \\(g\\in C^2(\\mathbb{R})\\) be such that \\(g\\) is \\(0\\) outside an interval. Then, the solution of the PDE with initial value\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(x)\\frac{\\partial f}{\\partial x}\\\\\nf(0,x) &= g(x)\n\\end{align*}\n\\tag{16}\\]\nhas the representation:\n\\[\nf(t,x) = \\mathbb{E}[g(X_t)|X_0 = x]\n\\]\n\nProof.\nStep 1. Let’s fix \\(t\\) and consider the function of space \\(h(x)=f(t,x)=\\mathbb{E}[g(X_t)|X_0=x]\\). Applying Ito’s formula to \\(h\\), we have:\n\\[\\begin{align}\ndh(X_s) &= h'(X_s) dX_s + \\frac{1}{2}h''(X_s) (dX_s)^2\\\\\n&= h'(X_s) (\\sigma(X_s)dB_s + \\mu(X_s) ds) + \\frac{\\sigma(X_s)^2}{2}h''(X_s)ds\\\\\n&= \\sigma(X_s)h'(X_s)dB_s + \\left(\\frac{\\sigma(X_s)^2}{2}h''(X_s) + \\mu(X_s)h'(X_s)\\right)ds\n\\end{align}\\]\nIn the integral form this is:\n\\[\\begin{align*}\nh(X_s) - h(X_0) &= \\int_0^s \\sigma(X_u)h'(X_u)dB_u \\\\\n&+ \\int_0^s \\left(\\frac{\\sigma(X_u)^2}{2}h''(X_u) + \\mu(X_u)h'(X_u)\\right)du \\tag{1}\n\\end{align*}\\]\nStep 2. Take expectations on both sides, divide by \\(s\\) and let \\(s \\to 0\\). We are interested in taking the derivative with respect to \\(s\\) at \\(s_0=0\\).\nThe expectation of the first term on the right hand side is zero, by the properties of the Ito integral.\nThe integrand of the second term (RHS) is a conditional expectation \\(\\mathbb{E}[\\xi(X_u)|X_0 = x]\\), it is an average at time \\(u\\), of the paths of the process starting at initial position \\(X_0 = x\\), so it is a function of \\(u\\) and \\(x\\). So, \\(\\mathbb{E}[\\xi(X_u)|X_0 = x] = p(u,x)\\). Suppressing the argument \\(x\\), we have the representation:\n\\[\\begin{align}\n\\int_0^s p(u) du\n\\end{align}\\]\nRecall that, if \\(p\\) is a continuous function, then it is Riemann integrable. Further, since integration and differentiation are inverse operations, there exists a unique antiderivative \\(P\\) given by\n\\[\nP(s) = \\int_{0}^{s}p(u)du\n\\]\nsatisfying \\(P'(0) = p(0)\\).\nBy the definition of the derivative:\n\\[P'(0) = \\lim_{s \\to 0} \\frac{P(s) - P(0)}{s} = \\lim_{s\\to 0} \\frac{P(s)}{s} = p(0) \\quad \\{ P(0)=0 \\text{ by definition }\\}\\]\nThus, we have:\n\\[\np(0,x) = \\mathbb{E}[\\xi(X_0)|X_0 = x] = \\frac{\\sigma(x)^2}{2} h''(x) + \\mu(x)h'(x)\n\\]\nStep 3. As for the left-hand side, we have:\n\\[\n\\lim_{s \\to 0} \\frac{\\mathbb{E}[h(X_s)|X_0 = x] - h(X_0)}{s} = \\lim_{s \\to 0} \\frac{\\mathbb{E}[h(X_s)|X_0 = x] - f(t,x)}{s}\n\\]\nTo prove that this limit is \\(\\frac{\\partial f}{\\partial t}(t,x)\\), it remains to show that \\(\\mathbb{E}[h(X_s)|X_0 = x]=\\mathbb{E}[g(X_{t+s})|X_0 = x]=f(t+s,x)\\).\nTo see this, note that \\(h(X_s) = \\mathbb{E}[g(X_{t+s})|X_s]\\). We deduce:\n\\[\\begin{align*}\n\\mathbb{E}[h(X_s)|X_0 = x] &= \\mathbb{E}[\\mathbb{E}[g(X_{t+s})|X_s]|X_0 = x]\\\\\n&= \\mathbb{E}[\\mathbb{E}[g(X_{t+s})|\\mathcal{F}_s]|X_0 = x]\\\\\n& \\{ (X_t,t\\geq 0) \\text{ is Markov }\\} \\\\\n&= \\mathbb{E}[g(X_{t+s})|X_0 = x]\\\\\n& \\{ \\text{ Tower property }\\} \\\\\n&= f(t+s,x)\n\\end{align*}\\]\nThis closes the proof. \\(\\blacksquare\\)\nThe backward equation (Equation 16) can be conveniently written in terms of the generator of the diffusion.\n\nDefinition 4 (Generator of a diffusion) The generator of a diffusion with SDE \\(dX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt\\) is the differential operator acting on functions of space defined by :\n\\[\nA = \\frac{\\sigma(x)^2}{2}\\frac{\\partial }{\\partial x^2} + \\mu(x)\\frac{\\partial}{\\partial x}\n\\]\n\nWith this notation, the backward equation for the function \\(f(t,x)\\) takes the form:\n\\[\n\\frac{\\partial f}{\\partial x}(t,x) = Af(t,x)\n\\]\nwhere it is understood that \\(A\\) acts only on the space variable. Theorem 4 gives a nice interpretation of the generator: it quantifies how much the function \\(f(t,x) = \\mathbb{E}[g(X_t)|X_0 = x]\\) changes in a small time interval.\n\nExample 5 (Generator of the Ornstein Uhlenbeck Process) The SDE of the Ornstein-Uhlenbeck process is:\n\\[\ndX_t = dB_t - X_t dt\n\\]\nThis means that its generator is:\n\\[\nA = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} - x \\frac{\\partial}{\\partial x}\n\\]\n\n\nExample 6 (Generator of Geometric Brownian Motion) Recall that the geometric Brownian motion\n\\[\nS_t = S_0 \\exp(\\sigma B_t + \\mu t)\n\\]\nsatisfies the SDE:\n\\[\ndS_t = \\sigma S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right) S_t dt\n\\]\nIn particular, the generator of geometric Brownian motion is :\n\\[\nA = \\frac{\\sigma^2 x^2}{2} x \\frac{\\partial^2}{\\partial x^2} + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\frac{\\partial}{\\partial x}\n\\]\n\nFor applications, in particular in mathematical finance, it is important to solve the backward equation with terminal value instead of with initial value. The reversal of time causes the appearance of an extra minus sign in the equation.\n\nTheorem 5 (Backward equation with terminal value) Let \\((X_t,t\\leq T)\\) be a diffusion with the dynamics:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt\n\\]\nLet \\(g\\in C^2(\\mathbb{R})\\) be such that \\(g\\) is \\(0\\) outside an interval. Then, the solution of the PDE with terminal value at time \\(T\\)\n\\[\n\\begin{align*}\n-\\frac{\\partial f}{\\partial t} &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(x)\\frac{\\partial f}{\\partial x}\\\\\nf(T,x) &= g(x)\n\\end{align*}\n\\tag{17}\\]\nhas the representation:\n\\[\nf(t,x) = \\mathbb{E}[g(X_T)|X_t = x]\n\\]\n\n\n\n\n\n\n\nTip 3: Backward equation with terminal value appears in the martingale condition\n\n\n\nOne way to construct a martingale for the filtration \\((\\mathcal{F}_t,t\\geq 0)\\) is to take\n\\[\nM_t = \\mathbb{E}[Y | \\mathcal{F}_t]\n\\]\nwhere \\(Y\\) is some integrable random variable. The martingale property then follows from the tower property of the conditional expectation. In the setup of Theorem 5, the random variable \\(Y\\) is \\(g(X_T)\\). By the Markov property of diffusion, we therefore have:\n\\[\nf(t,X_t) = \\mathbb{E}[g(X_T)|X_t] = \\mathbb{E}[g(X_T)|\\mathcal{F}_t]\n\\]\nIn other words, the solution to the backward equation with terminal value evaluated at \\(X_t = x\\) yields a martingale for the natural filtration of the process. This is a different point of view on the procedure we have used many times now: To get a martingale of the form \\(f(t,X_t)\\), apply the Ito’s formula to \\(f(t,X_t)\\) and set the \\(dt\\) term to zero. The PDE we obtain is the backward equation with terminal value. In fact, the proof of the theorem takes this exact route.\n\n\nProof.\nConsider \\(f(t,X_t)\\) and apply Ito’s formula.\n\\[\n\\begin{align*}\ndf(t,X_t) &= \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial x}dX_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2} dX_t \\cdot dX_t\\\\\n&= \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial x}(\\sigma(X_t) dB_t + \\mu(X_t)dt) + \\frac{\\sigma(X_t)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} dt\\\\\n&= \\sigma(X_t) dB_t + \\left(\\frac{\\partial f}{\\partial t} + \\frac{\\sigma(X_t)^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + \\mu(X_t)\\frac{\\partial f}{\\partial x}\\right)dt\n\\end{align*}\n\\]\nSince \\(f(t,x)\\) is a solution to the equation, we get that the \\(dt\\) term is \\(0\\) and \\(f(t,X_t)\\) is a martingale for the Brownian filtration (and thus also for the natural filtration of the diffusion, which contains less information). In particular we have:\n\\[\nf(t,X_t) = \\mathbb{E}[f(T,X_T)|\\mathcal{F}_t] = \\mathbb{E}[g(X_T)|\\mathcal{F}_t]\n\\]\nSince \\((X_t,t\\leq T)\\) is a Markov process, we finally get:\n\\[\nf(t,x) = \\mathbb{E}[g(X_T)|X_t = x]\n\\]\n\nExample 7 (Martingales of geometric Brownian motion) Let \\((S_t, \\geq 0)\\) be a geometric brownian motion with SDE:\n\\[\ndS_t = \\sigma S_t dB_t + \\left(\\mu + \\frac{\\sigma^2}{2}\\right)dt\n\\]\nAs we saw in Example 6, its generator is:\n\\[\nA = \\frac{\\sigma^2 x^2}{2}\\frac{\\partial^2}{\\partial x^2} + x\\left(\\mu+\\frac{\\sigma^2}{2}\\right)\\frac{\\partial}{\\partial x}\n\\]\nIn view of Theorem 5, if \\(f(t,x)\\) satisfies the PDE\n\\[\n\\frac{\\partial f}{\\partial t} + \\frac{\\sigma^2 x^2}{2}\\frac{\\partial^2 f}{\\partial x^2} + x\\left(\\mu+\\frac{\\sigma^2}{2}\\right)\\frac{\\partial f}{\\partial x}\n\\]\nthen processes of the form \\(f(t,S_t)\\) will be martingales for the natural filtration."
  },
  {
    "objectID": "posts/the_markov_property/index.html#kolmogorovs-forward-equation",
    "href": "posts/the_markov_property/index.html#kolmogorovs-forward-equation",
    "title": "The Markov Property",
    "section": "Kolmogorov’s forward equation",
    "text": "Kolmogorov’s forward equation\nThe companion equation to the backward equation is the Kolmogorov forward equation or forward equation. It is also known as the Fokker-Planck equation from its physics origin. The equation is very useful as it is satisfied by the transition density function \\(p(y',t'|y,t)\\) of a time-homogenous diffusion. It involves the adjoint of the generator.\n\nDefinition 5 (Adjoint of the generator) The adjoint \\(A^*\\) of the generator of a diffusion \\((X_t,t\\geq 0)\\) with SDE:\n\\[\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t)dt\n\\]\nis the differential operator acting on a function of space \\(f(x)\\) as follows:\n\\[\nA^*f(x) = \\frac{1}{2}\\frac{\\partial^2 }{\\partial x^2} \\frac{\\sigma(x)^2}{2} f(x) - \\frac{\\partial }{\\partial x}\\mu(x)f(x)\n\\tag{18}\\]\n\nNote the differences with the generator in Definition 4: there is an extra minus sign and the derivatives also act on the volatility and the drift.\n\nExample 8 (The generator of Brownian motion is self-adjoint) In the case of standard brownian motion, it is easy to check that:\n\\[\nA^* = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}\n\\]\nand\n\\[\nA^* = \\frac{1}{2}\\nabla^2\n\\]\nin the multivariate case. In other words, the generator and its adjoint are the same. In this case, the operator is self-adjoint.\n\n\nExample 9 We see that the adjoint of the generator acting on \\(f(x)\\) for geometric Brownian motion is:\n\\[\nA^*f(x) = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} (\\sigma^2 x^2 f(x)) - \\frac{\\partial}{\\partial x} \\left(\\left(\\mu + \\frac{\\sigma^2}{2}\\right) x f(x)\\right)\n\\]\nUsing the product rule in differentiating we get:\n\\[\nA^*[f(x)] = \\frac{\\sigma^2}{2}\\left(2x f(x) + x^2 f''(x)\\right) - \\left(\\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\left(f(x) + x f'(x)\\right)\\right)\n\\]\n\n\nExample 10 The generator for the Ornstein-Uhlenbeck process was given in Example 5. The adjoint acting on \\(f\\) is therefore:\n\\[\n\\begin{align*}\nA^*f(x) &= \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}(f(x)) - \\frac{\\partial}{\\partial x}(- x f(x))\\\\\n&= \\frac{f''(x)}{2} + (f(x)+xf'(x))\n\\end{align*}\n\\]\n\nThe forward equation takes the following form for a function \\(f(t,x)\\) of time and space:\n\\[\n\\frac{\\partial f}{\\partial t} = A^* f\n\\tag{19}\\]\nFor brownian motion, since \\(A^* = A\\), the backward and forward equations are the same. As advertised earlier, the forward equation is satisfied by the transition \\(p_t(y',t'|y,t)\\) of a diffusion. Before showing this in general, we verify it in the Brownian case.\n\nExample 11 Recall that the transition probability density \\(p(y,t|x,0)\\) for Brownian motion, or heat kernel, is:\n\\[\np(y,t|x,0) = \\frac{e^{-\\frac{(y-x)^2}{2}}}{\\sqrt{2\\pi t}}\n\\]\nHere, the space variable will be \\(y\\) and \\(x\\) will be fixed. The relevant function is thus \\(f(t,y) = p(y,t|x,0)\\). The adjoint operator acting on the space variable \\(y\\) is \\(A^* = A = \\frac{1}{2}\\frac{\\partial^2}{\\partial y^2}\\). The relevant time and space derivatives are given by Equation 12 and Equation 14.\nWe conclude that \\(f(t,y)=p(y,t|x,0)\\) is a solution of the forward equation.\n\nWhere does the form of the adjoint operator Equation 18 come from? In some sense, the adjoint operator plays a role similar to that of the transpose of a matrix in linear algebra. The adjoint acts on the function on the left. To see this, consider two functions \\(f,g\\) of space on which the generator \\(A\\) of a diffusion is well-defined. In particular, let’s assume that the functions are zero outside an interval. Consider the quantity\n\\[\n\\int_{\\mathbb{R}}g(x)A(f(x))dx = \\int_{\\mathbb{R}} g(x)\\left(\\frac{\\sigma(x)^2 }{2}f''(x) + \\mu(x)f'(x)\\right)dx\n\\]\nThis quantity can represent for example the average of \\(Af(x)\\) over some PDF \\(g(x)\\). In the above, \\(A\\) acts on the function on the right. To make the operator act on \\(g\\), we integrate by parts. This gives for the second term:\n\\[\n\\int_{\\mathbb{R}} g(x)\\mu(x)f'(x)dx = g(x)\\mu(x)f(x)\\Bigg|_{-\\infty}^{\\infty}-\\int_{\\mathbb{R}}f(x)\\frac{d}{dx}(g(x)\\mu(x))dx\n\\]\nThe boundary term \\(g(x)f(x)\\mu(x)\\Bigg|_{-\\infty}^\\infty\\) is \\(0\\) by the assumptions on \\(f,g\\). This term on \\(\\sigma\\) is obtained by integrating by parts twice:\n\\[\n\\begin{align*}\n\\int_{\\mathbb{R}} g(x) \\frac{\\sigma(x)^2}{2}f''(x)dx &= g(x) \\frac{\\sigma(x)^2}{2}f'(x)\\Bigg|_{-\\infty}^{\\infty} - \\int_{\\mathbb{R}}\\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right) f'(x)dx\\\\\n-\\int_{\\mathbb{R}} \\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f'(x)dx &= -\\frac{d}{dx}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f(x) \\Bigg|_{-\\infty}^{\\infty} + \\int_{\\mathbb{R}}\\frac{d^2}{dx^2}\\left(g(x) \\frac{\\sigma(x)^2}{2}\\right)f(x)dx\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\n\\int_{\\mathbb{R}}g(x) Af(x)dx &= \\int_{\\mathbb{R}}\\left(\\frac{1}{2}\\frac{d^2}{dx^2}(g(x) \\sigma(x)^2) - \\frac{d}{dx}(g(x)\\mu(x))\\right)f(x)dx\\\\\n&= \\int_{\\mathbb{R}}(A^*g(x))f(x)dx\n\\end{align*}\n\\tag{20}\\]\n\nTheorem 6 (Forward equation and transition probability) Let \\((X_t,t\\geq 0)\\) be a diffusion with SDE:\n\\[\ndX_t = \\sigma(X_t)dB_t + \\mu(X_t)dt, \\quad X_0 = x_0\n\\]\nLet \\(p(x,t|x_0,0)\\) be the transition probability density function for a fixed \\(x_0\\). Then, the function \\(f(t,y) = p(y,t|x_0,0)\\) is a solution of the PDE\n\\[\n\\frac{\\partial f}{\\partial t} = A^* f\n\\]\nwhere \\(A^*\\) is the adjoint of \\(A\\).\n\nProof.\nLet \\(h(x)\\) be some arbitrary function of space that is \\(0\\) outside an interval. We compute :\n\\[\n\\frac{1}{\\epsilon}(\\mathbb{E}[h(X_{t+\\epsilon}) - \\mathbb{E}[h(X_t)]])\n\\]\ntwo different ways and take the limit as \\(\\epsilon \\to 0\\).\nOn one hand, we have by the definition of the transition density\n\\[\n\\frac{1}{\\epsilon}\\left(\\mathbb{E}[h(X_{t+\\epsilon})]-\\mathbb{E}[h(X_t)]\\right) = \\int_{\\mathbb{R}}\\frac{1}{\\epsilon}(p(x,t+\\epsilon|x,0) - p(x,t|x_0,0))h(x)dx\n\\]\nBy taking the limit \\(\\epsilon \\to 0\\) inside the integral (assuming this is fine), we get:\n\\[\n\\int_{\\mathbb{R}} \\frac{\\partial}{\\partial t}p(x,t|x_0,0)h(x)dx\n\\tag{21}\\]\nOn the other hand, Ito’s formula implies\n\\[\n\\begin{align*}\ndh(X_s) &= \\frac{\\partial h}{\\partial x} dX_s + \\frac{1}{2} \\frac{\\partial^2 h}{\\partial x^2} (dX_s)^2\\\\\n&= \\frac{\\partial h}{\\partial x} (\\sigma(X_s) dB_s + \\mu(X_s)ds) + \\frac{1}{2} \\frac{\\partial^2 h}{\\partial x^2} (\\sigma(X_s)^2 ds)\\\\\n&= \\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s + \\left(\\mu(X_s) \\frac{\\partial h}{\\partial x} + \\frac{\\sigma(X_s)^2}{2}\\frac{\\partial^2 h}{\\partial x^2}\\right)ds\\\\\nh(X_{t+\\epsilon}) - h(X_t) &= \\int_{t}^{t+\\epsilon}\\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s + \\int_{t}^{t+\\epsilon}(Ah(x))ds\\\\\n\\mathbb{E}[h(X_{t+\\epsilon})] - \\mathbb{E}[h(X_t)] &= \\underbrace{\\mathbb{E}\\left[\\int_{t}^{t+\\epsilon}\\sigma(X_s)\\frac{\\partial h}{\\partial x} dB_s\\right]}_{0} + \\int_{t}^{t+\\epsilon}\\mathbb{E}[Ah(X_s)]ds\n\\end{align*}\n\\]\nDividing by \\(\\epsilon\\) and taking the limit as \\(\\epsilon \\to 0\\), we have:\n\\[\n\\begin{align*}\n\\lim_{\\epsilon \\to 0} \\frac{1}{\\epsilon} (\\mathbb{E}[h(X_{t+\\epsilon})] - \\mathbb{E}[h(X_t)]) &= \\mathbb{E}[Ah(X_t)]\\\\\n&= \\int_{\\mathbb{R}} p(x,t|x_0,0) Ah(x) dx\n\\end{align*}\n\\]\nThis can be written using Equation 20 as,\n\\[\n\\int_{\\mathbb{R}}(A^* p(x,t|x_0,0)) h(x) dx\n\\]\nSince \\(h\\) is arbitrary, we conclude that:\n\\[\n\\frac{\\partial}{\\partial t}p(x,t|x_0,0) = A^* p(x,t|x_0,0)\n\\tag{22}\\]\n\nExample 12 (Forward equation and invariant probability.) The Ornstein-Uhlenbeck process converges to a stationary distribution as noted in the example here. For example, for the SDE of the form\n\\[\ndX_t = -X_t dt + dB_t\n\\]\nwith \\(X_0\\) a Gaussian of mean \\(0\\) and variance \\(1/2\\), the PDF of \\(X_t\\), is, for all \\(t\\) is:\n\\[\nf(x) = \\frac{1}{\\sqrt{\\pi}} e^{-x^2}\n\\tag{23}\\]\nThis invariant distribution can be seen from the point of view of the forward equation. Indeed since the PDF is constant in time, the forward equation simply becomes:\n\\[\nA^* f = 0\n\\tag{24}\\]\n\n\nExample 13 The SDE of the Ornstein-Uhlenbeck process can be generated as follows. Consider \\(V(x)\\), a smooth function of space such that \\(\\int_{\\mathbb{R}} e^{-2V(x)}dx&lt;\\infty\\). The Smoluchowski equation is the SDE of the form:\n\\[\ndX_t = dB_t - V'(X_t) dt\n\\tag{25}\\]\nThe SDE can be interpreted as follows: \\(X_t\\) represents the position of a particle on \\(\\mathbb{R}\\). The position varies due to the Brownian fluctuations and also due to a force \\(V'(X_t)\\) that depends on the position. The function \\(V(x)\\) should then be thought of as the potential with which the particle moves, since the force (field) is the (negative) derivative of the potential function in Newtonian physics. The generator of this diffusion is:\n\\[\nA = \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} - V'(x)\\frac{\\partial}{\\partial x}\n\\]\nThis diffusion admits an invariant distribution :\n\\[\nf(x) = Ce^{-2V(x)}\n\\]\nwhere \\(C\\) is such that \\(\\int_{\\mathbb{R}}f(x)dx = 1\\)."
  },
  {
    "objectID": "posts/the_markov_property/index.html#the-feynman-kac-formula",
    "href": "posts/the_markov_property/index.html#the-feynman-kac-formula",
    "title": "The Markov Property",
    "section": "The Feynman-Kac Formula",
    "text": "The Feynman-Kac Formula\nWe saw in Example 4 that the solution of the heat equation:\n\\[\n\\frac{\\partial f}{\\partial t} = \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\n\\]\ncan be represented as an average over Brownian paths. This representation was extended to diffusions in theorem Theorem 4 where the second derivative in the equation is replaced by the generator of the corresponding diffusion. How robust is this representation? In other words, is it possible to slightly change the PDE and still get a stochastic representation representation for the solution? The answer to this question is yes, when a term of the form \\(r(x)f(t,x)\\) is added to the equation, where \\(r(x)\\) is a well-behaved function of space (for example, piecewise continuous). The stochastic representation of the PDE in this case bears the name Feynman-Kac formula, making a fruitful collaboration between the physicist Richard Feynman and the mathematician Mark Kac. By the way, you pronounce “Kac” as “cats”. His name is Polish. People who immigrated from Poland before him spelled their names as “Katz”. The case when \\(r(x)\\) is linear will be important in the applications to mathematical finance, where it represents the contribution of the interest rate.\n\nTheorem 7 (Initial Value Problem) Let \\((X_t,t\\geq 0)\\) be a diffusion in \\(\\mathbb{R}\\) with the SDE:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t)dt\n\\]\nLet \\(g\\in C^2(\\mathbb{R})\\) be such that \\(g\\) is \\(0\\) outside an interval. Then, the solution of the PDE with initial value\n\\[\n\\begin{align*}\n\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,x) + \\mu(x)\\frac{\\partial f}{\\partial x}(t,x) - r(x)f(x)\\\\\nf(0,x) &= g(x)\n\\end{align*}\n\\tag{26}\\]\nhas the stochastic representation:\n\\[\nf(t,x) = \\mathbb{E}\\left[g(X_t)\\exp\\left(-\\int_0^t r(X_s) ds\\right)\\Bigg| X_0 = x\\right]\n\\]\n\nProof.\nThe proof is again based on Ito’s formula. For a fixed \\(t\\), we consider the process:\n\\[\nM_s = f(t-s, X_s) \\exp\\left(-\\int_0^s r(X_u) du\\right), \\quad s \\leq t\n\\]\nWrite \\(Z_s = \\exp\\left(-\\int_0^s r(X_u) du\\right)\\) and \\(V_s = f(t-s,X_s)\\). A direct application of Ito’s formula yields:\nLet \\(R_s = -\\int_0^s r(X_u) du\\). So, \\(dR_t = r(X_t) dt\\). \\((R_t,t\\geq 0)\\) is a random variable, because \\(r(X_s)\\) depends on how \\((X_s, s \\leq t)\\) evolves, it is stochastic, but for very small intervals of time \\(r(X_s)\\) is a constant, and hence the process \\((R_t,t\\geq 0)\\) is said to be locally deterministic.\n\\[\n\\begin{align*}\nZ_s &= e^{-R_s}\\\\\ndZ_s &= -e^{-R_s} dR_s + \\frac{1}{2}e^{R_s} (dR_s)^2\\\\\n&= -Z_s r(X_s) ds\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\ndV_s &= \\frac{\\partial}{\\partial s}f(t-s, X_s)ds + \\frac{\\partial}{\\partial x}f(t-s, X_s)dX_s + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}f(t-s,X_s)(dX_s)^2\\\\\n&= -f_s ds + f_x (\\sigma(X_s)dB_s + \\mu(X_s)ds) + \\frac{1}{2}f_{xx} \\sigma(X_s)^2 ds \\\\\n&= \\sigma(X_s) f_x dB_s + \\\\\n&+ \\left\\{-f_s + \\mu(X_s)f_x + \\frac{\\sigma(X_s)^2}{2}f_{xx}\\right\\}ds\n\\end{align*}\n\\]\nRecall that \\(t\\) is fixed here, and we differentiate with respect to \\(s\\) in time. Since \\(f(t,x)\\) is a solution of the PDE, we can write the second equation as:\n\\[\ndV_s = \\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds\n\\]\nNow, by Ito’s product rule, we finally have:\n\\[\n\\begin{align*}\ndM_s &= V_s dZ_s + Z_s dV_s + dZ_s dV_s\\\\\n&= -f(t-s,X_s)Z_s r(X_s) ds + Z_s (\\sigma(X_s) f_x dB_s + r(X_s) f(t-s,X_s)ds) + 0\\\\\n&= \\sigma(X_s)Z_s f_x dB_s\n\\end{align*}\n\\]\nThis proves that \\((M_s, s \\leq t)\\) is a martingale. We conclude that:\n\\[\n\\mathbb{E}[M_t] = \\mathbb{E}[M_0]\n\\]\nUsing the definition of \\(M_t\\), this yields:\n\\[\n\\mathbb{E}[M_t] = \\mathbb{E}\\left[f(0,X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\\right] = \\mathbb{E}\\left[g(X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\\right] = \\mathbb{E}[M_0] = f(t,x)\n\\]\nThis proves the theorem. \\(\\blacksquare\\)\nAs for the backward equation, it is natural to consider the terminal value problem for the same PDE.\n\nTheorem 8 (Terminal Value Problem) Let \\((X_t,t \\leq T)\\) be a diffusion in \\(\\mathbb{R}\\) with the SDE:\n\\[\ndX_t = \\sigma(X_t) dB_t + \\mu(X_t) dt\n\\]\nLet \\(g\\in C^2(\\mathbb{R})\\) be such that \\(g\\) is \\(0\\) outside an interval. Then, the solution of the PDE with initial value\n\\[\n\\begin{align*}\n-\\frac{\\partial f}{\\partial t}(t,x) &= \\frac{\\sigma(x)^2}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,x) + \\mu(x)\\frac{\\partial f}{\\partial x}(t,x) - r(x)f(t,x)\\\\\nf(T,x) &= g(x)\n\\end{align*}\n\\]\nhas the stochastic representation :\n\\[\nf(t,x) = \\mathbb{E}\\left[g(X_T)\\exp\\left(-\\int_t^T r(X_u) du\\right)\\Bigg|X_t = x\\right]\n\\]\n\nProof.\nThe proof is similar by considering instead\n\\[\nM_t = f(t,X_t)\\exp\\left(-\\int_0^t r(X_u) du\\right)\n\\]"
  },
  {
    "objectID": "posts/the_markov_property/index.html#numerical-projects",
    "href": "posts/the_markov_property/index.html#numerical-projects",
    "title": "The Markov Property",
    "section": "Numerical Projects",
    "text": "Numerical Projects\n\nTemperature of a rod\nConsider the initial function \\(g(x) = 1 - |x|\\) for \\(|x| \\leq 1\\) and \\(0\\) if \\(|x| &gt; 1\\). This function may represent the temperature of a rod at time \\(0\\).\n\nApproximate the solution \\(f(t,x)\\) to the heat equation at time \\(t=0.25\\) at every \\(0.01\\) in \\(x\\) using the representation Equation 11. Use a sample of \\(100\\) paths for each \\(x\\).\n\nSolution.\n\nimport numpy as np\n\n\n# initial temperature of the rod as a function of position x\ndef g(x):\n    if x &gt;= -1.0 and x &lt;= 1.0:\n        return 1.0 - np.abs(x)\n    else:\n        return 0.0\n\n\n# helper function to generate brownian paths starting at B_0 = x_0\ndef brownian_paths(num_paths, step_size, t_0, t_n, x_0):\n    num_steps = int((t_n - t_0) / step_size)\n    db_t = np.sqrt(step_size) * np.random.standard_normal(size=(num_paths, num_steps))\n    db_t = np.concatenate([np.full((num_paths, 1), x_0), db_t], axis=1)\n    b_t = np.cumsum(db_t, axis=1)\n    return b_t\n\n\nx = np.linspace(-5.0, 5.0, 1001)  # space variable\nt = np.linspace(0.0, 1.0, 101)  # time variable\n\nNow, let’s use the data from the problem to compute the specific space average.\n\n# generate sample paths\npaths = brownian_paths(num_paths=100, step_size=0.01, t_0=0.0, t_n=1.0, x_0=0.0)\n\n# look at the value of B(t) at t=0.25 and average them"
  },
  {
    "objectID": "posts/the_markov_property/index.html#exercises",
    "href": "posts/the_markov_property/index.html#exercises",
    "title": "The Markov Property",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 (Shifted Brownian Motion) Let \\((B_t,t\\geq 0)\\) be a standard brownian motion. Fix \\(t &gt; 0\\). Show that the process \\((W_s,s \\geq 0)\\) with \\(W_s = B_{t+s} - B_t\\) is a standard brownian motion independent of \\(\\mathcal{F}_t\\).\n\nSolution.\nAt \\(s = 0\\), \\(W(0) = B(t) - B(t) = 0\\).\nConsider any arbitrary times \\(t_1 &lt; t_2\\). We have:\n\\[\\begin{align*}\nW(t_2) - W(t_1) &= (B(t + t_2) - B(t)) - (B(t + t_1) - B(t))\\\\\n&= B(t + t_2) - B(t + t_1)\n\\end{align*}\\]\nNow, \\(B(t + t_2) - B(t + t_1) \\sim \\mathcal{N}(0,t_2 - t_1)\\). So, \\(W(t_2) - W(t_1)\\) is a Gaussian random variable with mean \\(0\\) and variance \\(t_2 - t_1\\).\nFinally, consider any finite set of times \\(0=t_0 &lt; t_1 &lt; t_2 &lt; \\ldots &lt; t_n = T\\). Then, \\(t &lt; t + t_1 &lt; t + t_2 &lt; \\ldots &lt; t + t_n\\). We have that, \\(B(t + t_1) - B(t)\\), \\(B(t + t_2) - B(t + t_1)\\), \\(B(t + t_3) - B(t + t_2)\\), \\(\\ldots\\), \\(B(t+T) - B(t+t_{n-1})\\) are independent random variables. Consequently, \\(W(t_1) - W(0)\\), \\(W(t_2) - W(t_1)\\), \\(W(t_3) - W(t_2)\\), \\(\\ldots\\), \\(W(t_n) - W(t_{n-1})\\) are independent random variables. So, \\((W_s,s\\geq 0)\\) is a standard brownian motion.\nAlso, we have:\n\\[\\begin{align*}\n\\mathbb{E}[W(s)|\\mathcal{F}_t] &= \\mathbb{E}[B(t + s) - B(t)|\\mathcal{F}_t]\\\\\n& \\{ B(t+s) - B(t) \\perp \\mathcal{F}_t \\}\\\\\n&= \\mathbb{E}[B(t + s) - B(t)]\\\\\n&= \\mathbb{E}[W(s)]\n\\end{align*}\\]\nThus, \\(W(s)\\) is independent of \\(\\mathcal{F}_t\\), it does not depend upon the information available upto time \\(t\\)."
  },
  {
    "objectID": "posts/template-programming/index.html#implementing-getn-for-the-tuple",
    "href": "posts/template-programming/index.html#implementing-getn-for-the-tuple",
    "title": "Template programming",
    "section": "Implementing get<N> for the tuple",
    "text": "Implementing get&lt;N&gt; for the tuple\nWe can implement get&lt;N&gt; that takes a tuple as an argument and returns a reference to the element at the index n. Its prototype could look like the following:\ntemplate &lt;int n, typename... Ts&gt;\ntypename nth_type&lt;n, Ts...&gt;::value_type& get(tuple&lt;Ts...&gt;& t);\nThe template arguments are the index and a parameter pack of the tuple types. Its implementation, however, requires some helper types. First, we need to know what the type of the element is at the n-th index. This can be done with the help of the following nth_type variadic class template:\ntemplate&lt;int n, typename T, typename... Ts&gt;\nstruct nth_type : nth_type&lt;n-1,Ts...&gt;{\n};\n\ntemplate&lt;typename T, typename... Ts&gt;\nstruct nth_type&lt;0,T,Ts...&gt;{\n    using value_type = T;\n};\nAgain, we have a primary template that uses recursive inheritance, and the specialization for the index 0 (which is the head of the list of templates). This type is only used as a mechanism for determining the type of a tuple element. We need another variadic class template for retrieving the value.\ntemplate&lt;int n&gt;\nstruct getter{\n    template&lt;typename... Ts&gt;\n    static typename nth_type&lt;n, Ts...&gt;::value_type&\n    get(tuple&lt;Ts...&gt;& t){\n        return getter&lt;n-1&gt;::get(t.rest_);\n    }\n};\n\ntemplate&lt;&gt;\nstruct getter&lt;0&gt;{\n    template&lt;typename T, typename... Ts&gt;\n    static T& get(tuple&lt;T, Ts...&gt;& t){\n        return t.first_;\n    }\n};\nWith all these defined, we can now provide an implementation for the helper variadic function template get. This implementation relies on the getter class template and calls the get variadic function template.\ntemplate&lt;int n, typename... Ts&gt;\ntypename nth_type&lt;n, Ts...&gt;::value_type &\nget(tuple&lt;Ts...&gt;& t){\n    return getter&lt;n&gt;::get(t);\n}\nCompiler Explorer\nAnalysing the example above in cppinsights.io will be very illuminating. Here’s the listing:\n/*************************************************************************************\n * NOTE: This an educational hand-rolled transformation. Things can be incorrect or  *\n * buggy.                                                                            *\n *************************************************************************************/\n#include &lt;iostream&gt;\n\ntemplate&lt;typename T, typename ... Ts&gt;\nstruct tuple\n{\n  T first_;\n  tuple&lt;Ts...&gt; rest_;\n  inline tuple(T first, Ts... rest)\n  : first_{first}\n  , rest_{tuple&lt;Ts...&gt;(rest... )}\n  {\n  }\n  \n};\n\n/* First instantiated from: insights.cpp:6 */\n#ifdef INSIGHTS_USE_TEMPLATE\ntemplate&lt;&gt;\nstruct tuple&lt;int, double&gt;\n{\n  int first_;\n  tuple&lt;double&gt; rest_;\n  inline tuple(int first, double __rest1)\n  : first_{first}\n  , rest_{tuple&lt;double&gt;(__rest1)}\n  {\n  }\n  \n};\n\n#endif\n/* First instantiated from: insights.cpp:6 */\n#ifdef INSIGHTS_USE_TEMPLATE\ntemplate&lt;&gt;\nstruct tuple&lt;double&gt;\n{\n  double first_;\n  inline tuple(double first)\n  : first_{first}\n  {\n  }\n  \n};\n\n#endif\n/* First instantiated from: insights.cpp:53 */\n#ifdef INSIGHTS_USE_TEMPLATE\ntemplate&lt;&gt;\nstruct tuple&lt;char, int, double&gt;\n{\n  char first_;\n  tuple&lt;int, double&gt; rest_;\n  inline tuple(char first, int __rest1, double __rest2)\n  : first_{first}\n  , rest_{tuple&lt;int, double&gt;(__rest1, __rest2)}\n  {\n  }\n  \n};\n\n#endif\n\ntemplate&lt;typename T&gt;\nstruct tuple&lt;T&gt;\n{\n  T first_;\n  inline tuple(T first)\n  : first_(first)\n  {\n  }\n  \n};\nThe tuple&lt;char, int, double&gt; contains an int and a tuple&lt;int, double&gt;, which contains a int and tuple&lt;double&gt;, which in turn contains a double value.\nNext, we have the nth_type class template, for which, again, we have a primary template and several specializations, as follows:\n\ntemplate&lt;int n, typename T, typename ... Ts&gt;\nstruct nth_type : public nth_type&lt;n - 1, Ts...&gt;\n{\n};\n\n/* First instantiated from: insights.cpp:19 */\n#ifdef INSIGHTS_USE_TEMPLATE\ntemplate&lt;&gt;\nstruct nth_type&lt;1, int, double&gt; : public nth_type&lt;0, double&gt;\n{\n};\n\n#endif\n/* First instantiated from: insights.cpp:19 */\n#ifdef INSIGHTS_USE_TEMPLATE\ntemplate&lt;&gt;\nstruct nth_type&lt;0, double&gt;\n{\n  using value_type = double;\n};\n\n#endif\n/* First instantiated from: insights.cpp:45 */\n#ifdef INSIGHTS_USE_TEMPLATE\ntemplate&lt;&gt;\nstruct nth_type&lt;2, char, int, double&gt; : public nth_type&lt;1, int, double&gt;\n{\n};\n\n#endif\n\ntemplate&lt;typename T, typename ... Ts&gt;\nstruct nth_type&lt;0, T, Ts...&gt;\n{\n  using value_type = T;\n};\nThe nth_type&lt;2, char, int, double&gt; specialization is derived from nth_type&lt;1, int, double&gt; which in turn is derived from nth_type&lt;0, double&gt;, which is the last class in the base hierarchy.\nThe nth_type structure is used as the return type in the getter helper class template, which is instantiated as follows:\n/* First instantiated from: insights.cpp:32 */\n#ifdef INSIGHTS_USE_TEMPLATE\ntemplate&lt;&gt;\nstruct getter&lt;1&gt;\n{\n  template&lt;typename ... Ts&gt;\n  static inline typename nth_type&lt;1, Ts...&gt;::value_type & get(tuple&lt;Ts...&gt; & t);\n  \n  /* First instantiated from: insights.cpp:32 */\n  #ifdef INSIGHTS_USE_TEMPLATE\n  template&lt;&gt;\n  static inline typename nth_type&lt;1, int, double&gt;::value_type & get&lt;int, double&gt;(tuple&lt;int, double&gt; & t)\n  {\n    return getter&lt;0&gt;::get(t.rest_);\n  }\n  #endif\n  \n};\n\n#endif\n/* First instantiated from: insights.cpp:47 */\n#ifdef INSIGHTS_USE_TEMPLATE\ntemplate&lt;&gt;\nstruct getter&lt;2&gt;\n{\n  template&lt;typename ... Ts&gt;\n  static inline typename nth_type&lt;2, Ts...&gt;::value_type & get(tuple&lt;Ts...&gt; & t);\n  \n  /* First instantiated from: insights.cpp:47 */\n  #ifdef INSIGHTS_USE_TEMPLATE\n  template&lt;&gt;\n  static inline typename nth_type&lt;2, char, int, double&gt;::value_type & get&lt;char, int, double&gt;(tuple&lt;char, int, double&gt; & t)\n  {\n    return getter&lt;1&gt;::get(t.rest_);\n  }\n  #endif\n  \n};\n\n#endif\n\ntemplate&lt;&gt;\nstruct getter&lt;0&gt;\n{\n  template&lt;typename T, typename ... Ts&gt;\n  static inline T & get(tuple&lt;T, Ts...&gt; & t)\n  {\n    return t.first_;\n  }\n  \n  /* First instantiated from: insights.cpp:32 */\n  #ifdef INSIGHTS_USE_TEMPLATE\n  template&lt;&gt;\n  static inline double & get&lt;double&gt;(tuple&lt;double&gt; & t)\n  {\n    return t.first_;\n  }\n  #endif\n  \n};\nWe see the use of the keyword typename to prefix the nth_type&lt;N, Ts...&gt;::value_type which is a dependent type. In C++ 20, this is however no longer necessary.\nBecause implementing variadic templates is often verbose and can be cumbersome, the C++17 added fold expressions."
  },
  {
    "objectID": "posts/template-programming/index.html#fold-expressions",
    "href": "posts/template-programming/index.html#fold-expressions",
    "title": "Template programming",
    "section": "Fold Expressions",
    "text": "Fold Expressions\nA special form of pack expansions is folds introduced in C++17. Above, we showed a function sum that summed a set of integers. This function can be implemented far more concisely with a fold:\nint sum(auto... i){\n    return (... + i);\n}\nLet \\(p_1,\\ldots,p_n\\) be the instances of the parameter pack \\(p\\). Let \\(\\bigoplus\\) stand for any binary operator in the C++ grammar.\nA binary left fold has the form \\((e \\bigoplus \\ldots \\bigoplus p)\\) and is equivalent to \\((((e \\bigoplus p_1) \\bigoplus p_2)\\ldots ) \\bigoplus p_n\\).\nA unary left fold has the form \\((\\ldots \\bigoplus p)\\) and is equivalent to \\((((p_1 \\bigoplus p_2)\\bigoplus p_3) \\ldots )\\bigoplus p_n\\).\nA binary right fold has the form \\((p \\bigoplus \\ldots \\bigoplus e)\\) and is equivalent to \\(p_1 \\bigoplus (\\ldots (p_{n-1} \\bigoplus (p_n \\bigoplus e)))\\).\nA binary right fold has the form \\((p \\bigoplus \\ldots \\bigoplus p_n)\\) and is equivalent to \\(p_1 \\bigoplus (p_2 \\bigoplus (\\ldots \\bigoplus p_n))\\).\nIn the above expressions, \\(e\\) stands for the initial value."
  },
  {
    "objectID": "posts/template-programming/index.html#idioms",
    "href": "posts/template-programming/index.html#idioms",
    "title": "Template programming",
    "section": "Idioms",
    "text": "Idioms\nBelow is a collection of idioms for working with parameter packs."
  },
  {
    "objectID": "posts/template-programming/index.html#function-template-argument-deduction.",
    "href": "posts/template-programming/index.html#function-template-argument-deduction.",
    "title": "Template Programming",
    "section": "Function Template Argument Deduction.",
    "text": "Function Template Argument Deduction.\nIts super-helpful to have a good low-level hang of the template argument deduction rules (more on deduction guides in another post). Essentially, since C++17, you can just declare, std::vector{1.0, 2.0, 3.0, 4.0, 5.0} - clean and nice instead of std::vector&lt;double&gt;{1.0, 2.0, 3.0, 4.0, 5.0}.\nWhen the compiler tries to deduce the template arguments, it performs matching of the type template parameters with the types of arguments used to invoke the function.\nVery succinctly, the compiler can match: - Types of the form 𝚃, 𝚃 𝚌𝚘𝚗𝚜𝚝, 𝚃 𝚟𝚘𝚕𝚊𝚝𝚒𝚕𝚎 - Pointers 𝚃*, lvalue references 𝚃& and universal references 𝚃&& - Arrays such as 𝚃[𝟻] - Pointers to functions - Pointers to member-functions and data-members.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;chrono&gt;\n#include &lt;tuple&gt;\n\nusing namespace std::chrono;\n\ntemplate&lt;typename T&gt;\nstd::ostream& operator&lt;&lt;(std::ostream& out, const std::vector&lt;T&gt; vec){\n    for(int i{0}; i &lt; vec.size(); ++ i){\n        out &lt;&lt; \"\\n[\" &lt;&lt; i &lt;&lt; \"] = \" &lt;&lt; vec[i];\n    }\n    return out;\n}\n\nstruct BondDiscountingCurve{\n    std::vector&lt;std::pair&lt;year_month_day,double&gt;&gt; discountFactorCurve;\n};\n\nstruct CustomBond{\n    std::vector&lt;double&gt; cashflows;\n    std::vector&lt;std::chrono::year_month_day&gt; cashflow_dates;\n\n    double pv(BondDiscountingCurve disc){\n        double result{0.0};\n        for(int i{0};i &lt; cashflows.size(); ++i)\n        {\n            double df = std::get&lt;1&gt;(disc.discountFactorCurve[i]);\n            result += cashflows[i] * df;\n        }\n        return result;\n    }\n};\n\nstruct Leg{\n    // A cashflow is a 4-tuple (Cashflow date, Cashflow amount, Cap, Floor)\n    using flow = std::tuple&lt;year_month_day, double, double, double&gt;;\n    std::vector&lt;flow&gt; flows;\n};\n\nstruct AssetSwap{\n    Leg bondLeg;\n    Leg fundingLeg;\n\n    Leg getBondLeg() { return bondLeg; }\n    Leg getFundingLeg() { return fundingLeg; }\n};\n\ntemplate&lt;typename T&gt;\nvoid process01(T bond){ std::cout &lt;&lt; \"T\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process02(T const){ std::cout &lt;&lt; \"T const\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process03(T volatile) { std::cout &lt;&lt; \"T volatile\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T*) { std::cout &lt;&lt; \"T*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T&) { std::cout &lt;&lt; \"T&\\n\"; }\n\n// Universal reference\ntemplate&lt;typename T&gt;\nvoid process05(T&&) { std::cout &lt;&lt; \"T&&\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process06(T[5]) { std::cout &lt;&lt; \"T[5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process07(T[3][5]) { std::cout &lt;&lt; \"T[3][5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(T(*)()) { std::cout &lt;&lt; \"T (*)()\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(CustomBond (*)(T)) { std::cout &lt;&lt; \"C (*)(T)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process08(T(*)(U)) { std::cout &lt;&lt; \"T (*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(T(CustomBond::*)()) { std::cout &lt;&lt; \"T (C::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(CustomBond::*)(U)) { std::cout &lt;&lt; \"T (C::*)(U)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(U::*)()) { std::cout &lt;&lt; \"T (U::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U, typename V&gt;\nvoid process09(T(U::*)(V)) { std::cout &lt;&lt; \"T (U::*)(V)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(T::*)()) { std::cout &lt;&lt; \"C (T::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(Leg(T::*)(U)) { std::cout &lt;&lt; \"C (T::*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(AssetSwap::*)(T)) { std::cout &lt;&lt; \"D (C::*)(T)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(T CustomBond::*) { std::cout &lt;&lt; \"T C::*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(Leg T::*) { std::cout &lt;&lt; \"C T::*\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process10(T U::*) { std::cout &lt;&lt; \"T U::*\\n\"; }\n\nint main()\n{\n    CustomBond bond(\n        {0.05, 0.05, 0.05, 1.05},\n        {2024y/June/20d, 2024y/December/20d, 2025y/June/20d, 2025y/December/20d}\n    );\n\n    AssetSwap assetSwap;\n\n    process01(bond);    // T\n    process02(bond);    // T const\n    process03(bond);    // T volatile\n    process04(&bond);   // T*\n    process04(bond);    // T&\n    process05(bond);    // T&&; deduced as CustomBond& \n\n    CustomBond bondsArray[5] {};  //Create an array of custom bonds\n    process06(bondsArray);  // T[5]\n    process06(&bond);       // T[5]\n\n    CustomBond bondsByMaturityAndRating[3][5] {}; \n    process07(bondsByMaturityAndRating);    //C[5][n]\n\n    CustomBond (*funcptr1)() = nullptr;\n    CustomBond (*funcptr2)(int) = nullptr;\n    double     (*funcptr3)(int) = nullptr;\n\n    process08(funcptr1);    //T(*)()\n    process08(funcptr2);    //C(*)(T)\n    process08(funcptr3);    //T(*)(U)\n\n    double (CustomBond::*ptrmemfunc1) () = nullptr;\n    double (CustomBond::*ptrmemfunc2)(BondDiscountingCurve) = &CustomBond::pv;\n    std::vector&lt;double&gt; (Leg::*ptrmemfunc3)() = nullptr;\n    double(AssetSwap::*getLegPv)(Leg) = nullptr;\n    Leg(AssetSwap::*ptrmemfunc4)() = &AssetSwap::getFundingLeg;\n    Leg(Leg::*applyScaleToAllCoupons)(double) = nullptr;\n    Leg(AssetSwap::*applyFixedSpreadToAllCoupons)(double) = nullptr;\n    //Leg(AssetSwap::*)()\n\n    process09(ptrmemfunc1);     // T(C::*)()\n    process09(ptrmemfunc2);     // T(C::*)(U)\n    process09(ptrmemfunc3);     // T(U::*)()\n    process09(getLegPv);        // T(U::*)(V)\n    process09(ptrmemfunc4);     // C(T::*)()\n    process09(applyScaleToAllCoupons);          //C(T::*)(U)\n    process09(applyFixedSpreadToAllCoupons);    //D(C::*)(T)\n\n    process10(&CustomBond::cashflows);\n    process10(&AssetSwap::bondLeg);\n    process10(&Leg::flows);\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/template-argument-deduction/index.html",
    "href": "posts/template-argument-deduction/index.html",
    "title": "Class Template Argument Deduction(CTAD) Explained simply",
    "section": "",
    "text": "Its super-helpful to have a good low-level hang of the template argument deduction rules (more on deduction guides in another post). Essentially, since C++17, you can just declare, std::vector{1.0, 2.0, 3.0, 4.0, 5.0} - clean and nice instead of std::vector&lt;double&gt;{1.0, 2.0, 3.0, 4.0, 5.0}.\nWhen the compiler tries to deduce the template arguments, it performs matching of the type template parameters with the types of arguments used to invoke the function.\nVery succinctly, the compiler can match: - Types of the form 𝚃, 𝚃 𝚌𝚘𝚗𝚜𝚝, 𝚃 𝚟𝚘𝚕𝚊𝚝𝚒𝚕𝚎 - Pointers 𝚃*, lvalue references 𝚃& and universal references 𝚃&& - Arrays such as 𝚃[𝟻] - Pointers to functions - Pointers to member-functions and data-members.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;chrono&gt;\n#include &lt;tuple&gt;\n\nusing namespace std::chrono;\n\ntemplate&lt;typename T&gt;\nstd::ostream& operator&lt;&lt;(std::ostream& out, const std::vector&lt;T&gt; vec){\n    for(int i{0}; i &lt; vec.size(); ++ i){\n        out &lt;&lt; \"\\n[\" &lt;&lt; i &lt;&lt; \"] = \" &lt;&lt; vec[i];\n    }\n    return out;\n}\n\nstruct BondDiscountingCurve{\n    std::vector&lt;std::pair&lt;year_month_day,double&gt;&gt; discountFactorCurve;\n};\n\nstruct CustomBond{\n    std::vector&lt;double&gt; cashflows;\n    std::vector&lt;std::chrono::year_month_day&gt; cashflow_dates;\n\n    double pv(BondDiscountingCurve disc){\n        double result{0.0};\n        for(int i{0};i &lt; cashflows.size(); ++i)\n        {\n            double df = std::get&lt;1&gt;(disc.discountFactorCurve[i]);\n            result += cashflows[i] * df;\n        }\n        return result;\n    }\n};\n\nstruct Leg{\n    // A cashflow is a 4-tuple (Cashflow date, Cashflow amount, Cap, Floor)\n    using flow = std::tuple&lt;year_month_day, double, double, double&gt;;\n    std::vector&lt;flow&gt; flows;\n};\n\nstruct AssetSwap{\n    Leg bondLeg;\n    Leg fundingLeg;\n\n    Leg getBondLeg() { return bondLeg; }\n    Leg getFundingLeg() { return fundingLeg; }\n};\n\ntemplate&lt;typename T&gt;\nvoid process01(T bond){ std::cout &lt;&lt; \"T\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process02(T const){ std::cout &lt;&lt; \"T const\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process03(T volatile) { std::cout &lt;&lt; \"T volatile\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T*) { std::cout &lt;&lt; \"T*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T&) { std::cout &lt;&lt; \"T&\\n\"; }\n\n// Universal reference\ntemplate&lt;typename T&gt;\nvoid process05(T&&) { std::cout &lt;&lt; \"T&&\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process06(T[5]) { std::cout &lt;&lt; \"T[5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process07(T[3][5]) { std::cout &lt;&lt; \"T[3][5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(T(*)()) { std::cout &lt;&lt; \"T (*)()\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(CustomBond (*)(T)) { std::cout &lt;&lt; \"C (*)(T)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process08(T(*)(U)) { std::cout &lt;&lt; \"T (*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(T(CustomBond::*)()) { std::cout &lt;&lt; \"T (C::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(CustomBond::*)(U)) { std::cout &lt;&lt; \"T (C::*)(U)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(U::*)()) { std::cout &lt;&lt; \"T (U::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U, typename V&gt;\nvoid process09(T(U::*)(V)) { std::cout &lt;&lt; \"T (U::*)(V)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(T::*)()) { std::cout &lt;&lt; \"C (T::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(Leg(T::*)(U)) { std::cout &lt;&lt; \"C (T::*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(AssetSwap::*)(T)) { std::cout &lt;&lt; \"D (C::*)(T)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(T CustomBond::*) { std::cout &lt;&lt; \"T C::*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(Leg T::*) { std::cout &lt;&lt; \"C T::*\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process10(T U::*) { std::cout &lt;&lt; \"T U::*\\n\"; }\n\nint main()\n{\n    CustomBond bond(\n        {0.05, 0.05, 0.05, 1.05},\n        {2024y/June/20d, 2024y/December/20d, 2025y/June/20d, 2025y/December/20d}\n    );\n\n    AssetSwap assetSwap;\n\n    process01(bond);    // T\n    process02(bond);    // T const\n    process03(bond);    // T volatile\n    process04(&bond);   // T*\n    process04(bond);    // T&\n    process05(bond);    // T&&; deduced as CustomBond& \n\n    CustomBond bondsArray[5] {};  //Create an array of custom bonds\n    process06(bondsArray);  // T[5]\n    process06(&bond);       // T[5]\n\n    CustomBond bondsByMaturityAndRating[3][5] {}; \n    process07(bondsByMaturityAndRating);    //C[5][n]\n\n    CustomBond (*funcptr1)() = nullptr;\n    CustomBond (*funcptr2)(int) = nullptr;\n    double     (*funcptr3)(int) = nullptr;\n\n    process08(funcptr1);    //T(*)()\n    process08(funcptr2);    //C(*)(T)\n    process08(funcptr3);    //T(*)(U)\n\n    double (CustomBond::*ptrmemfunc1) () = nullptr;\n    double (CustomBond::*ptrmemfunc2)(BondDiscountingCurve) = &CustomBond::pv;\n    std::vector&lt;double&gt; (Leg::*ptrmemfunc3)() = nullptr;\n    double(AssetSwap::*getLegPv)(Leg) = nullptr;\n    Leg(AssetSwap::*ptrmemfunc4)() = &AssetSwap::getFundingLeg;\n    Leg(Leg::*applyScaleToAllCoupons)(double) = nullptr;\n    Leg(AssetSwap::*applyFixedSpreadToAllCoupons)(double) = nullptr;\n    //Leg(AssetSwap::*)()\n\n    process09(ptrmemfunc1);     // T(C::*)()\n    process09(ptrmemfunc2);     // T(C::*)(U)\n    process09(ptrmemfunc3);     // T(U::*)()\n    process09(getLegPv);        // T(U::*)(V)\n    process09(ptrmemfunc4);     // C(T::*)()\n    process09(applyScaleToAllCoupons);          //C(T::*)(U)\n    process09(applyFixedSpreadToAllCoupons);    //D(C::*)(T)\n\n    process10(&CustomBond::cashflows);\n    process10(&AssetSwap::bondLeg);\n    process10(&Leg::flows);\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/template-argument-deduction/index.html#function-template-argument-deduction.",
    "href": "posts/template-argument-deduction/index.html#function-template-argument-deduction.",
    "title": "Class Template Argument Deduction(CTAD) Explained simply",
    "section": "",
    "text": "Its super-helpful to have a good low-level hang of the template argument deduction rules (more on deduction guides in another post). Essentially, since C++17, you can just declare, std::vector{1.0, 2.0, 3.0, 4.0, 5.0} - clean and nice instead of std::vector&lt;double&gt;{1.0, 2.0, 3.0, 4.0, 5.0}.\nWhen the compiler tries to deduce the template arguments, it performs matching of the type template parameters with the types of arguments used to invoke the function.\nVery succinctly, the compiler can match: - Types of the form 𝚃, 𝚃 𝚌𝚘𝚗𝚜𝚝, 𝚃 𝚟𝚘𝚕𝚊𝚝𝚒𝚕𝚎 - Pointers 𝚃*, lvalue references 𝚃& and universal references 𝚃&& - Arrays such as 𝚃[𝟻] - Pointers to functions - Pointers to member-functions and data-members.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;chrono&gt;\n#include &lt;tuple&gt;\n\nusing namespace std::chrono;\n\ntemplate&lt;typename T&gt;\nstd::ostream& operator&lt;&lt;(std::ostream& out, const std::vector&lt;T&gt; vec){\n    for(int i{0}; i &lt; vec.size(); ++ i){\n        out &lt;&lt; \"\\n[\" &lt;&lt; i &lt;&lt; \"] = \" &lt;&lt; vec[i];\n    }\n    return out;\n}\n\nstruct BondDiscountingCurve{\n    std::vector&lt;std::pair&lt;year_month_day,double&gt;&gt; discountFactorCurve;\n};\n\nstruct CustomBond{\n    std::vector&lt;double&gt; cashflows;\n    std::vector&lt;std::chrono::year_month_day&gt; cashflow_dates;\n\n    double pv(BondDiscountingCurve disc){\n        double result{0.0};\n        for(int i{0};i &lt; cashflows.size(); ++i)\n        {\n            double df = std::get&lt;1&gt;(disc.discountFactorCurve[i]);\n            result += cashflows[i] * df;\n        }\n        return result;\n    }\n};\n\nstruct Leg{\n    // A cashflow is a 4-tuple (Cashflow date, Cashflow amount, Cap, Floor)\n    using flow = std::tuple&lt;year_month_day, double, double, double&gt;;\n    std::vector&lt;flow&gt; flows;\n};\n\nstruct AssetSwap{\n    Leg bondLeg;\n    Leg fundingLeg;\n\n    Leg getBondLeg() { return bondLeg; }\n    Leg getFundingLeg() { return fundingLeg; }\n};\n\ntemplate&lt;typename T&gt;\nvoid process01(T bond){ std::cout &lt;&lt; \"T\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process02(T const){ std::cout &lt;&lt; \"T const\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process03(T volatile) { std::cout &lt;&lt; \"T volatile\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T*) { std::cout &lt;&lt; \"T*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T&) { std::cout &lt;&lt; \"T&\\n\"; }\n\n// Universal reference\ntemplate&lt;typename T&gt;\nvoid process05(T&&) { std::cout &lt;&lt; \"T&&\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process06(T[5]) { std::cout &lt;&lt; \"T[5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process07(T[3][5]) { std::cout &lt;&lt; \"T[3][5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(T(*)()) { std::cout &lt;&lt; \"T (*)()\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(CustomBond (*)(T)) { std::cout &lt;&lt; \"C (*)(T)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process08(T(*)(U)) { std::cout &lt;&lt; \"T (*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(T(CustomBond::*)()) { std::cout &lt;&lt; \"T (C::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(CustomBond::*)(U)) { std::cout &lt;&lt; \"T (C::*)(U)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(U::*)()) { std::cout &lt;&lt; \"T (U::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U, typename V&gt;\nvoid process09(T(U::*)(V)) { std::cout &lt;&lt; \"T (U::*)(V)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(T::*)()) { std::cout &lt;&lt; \"C (T::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(Leg(T::*)(U)) { std::cout &lt;&lt; \"C (T::*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(AssetSwap::*)(T)) { std::cout &lt;&lt; \"D (C::*)(T)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(T CustomBond::*) { std::cout &lt;&lt; \"T C::*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(Leg T::*) { std::cout &lt;&lt; \"C T::*\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process10(T U::*) { std::cout &lt;&lt; \"T U::*\\n\"; }\n\nint main()\n{\n    CustomBond bond(\n        {0.05, 0.05, 0.05, 1.05},\n        {2024y/June/20d, 2024y/December/20d, 2025y/June/20d, 2025y/December/20d}\n    );\n\n    AssetSwap assetSwap;\n\n    process01(bond);    // T\n    process02(bond);    // T const\n    process03(bond);    // T volatile\n    process04(&bond);   // T*\n    process04(bond);    // T&\n    process05(bond);    // T&&; deduced as CustomBond& \n\n    CustomBond bondsArray[5] {};  //Create an array of custom bonds\n    process06(bondsArray);  // T[5]\n    process06(&bond);       // T[5]\n\n    CustomBond bondsByMaturityAndRating[3][5] {}; \n    process07(bondsByMaturityAndRating);    //C[5][n]\n\n    CustomBond (*funcptr1)() = nullptr;\n    CustomBond (*funcptr2)(int) = nullptr;\n    double     (*funcptr3)(int) = nullptr;\n\n    process08(funcptr1);    //T(*)()\n    process08(funcptr2);    //C(*)(T)\n    process08(funcptr3);    //T(*)(U)\n\n    double (CustomBond::*ptrmemfunc1) () = nullptr;\n    double (CustomBond::*ptrmemfunc2)(BondDiscountingCurve) = &CustomBond::pv;\n    std::vector&lt;double&gt; (Leg::*ptrmemfunc3)() = nullptr;\n    double(AssetSwap::*getLegPv)(Leg) = nullptr;\n    Leg(AssetSwap::*ptrmemfunc4)() = &AssetSwap::getFundingLeg;\n    Leg(Leg::*applyScaleToAllCoupons)(double) = nullptr;\n    Leg(AssetSwap::*applyFixedSpreadToAllCoupons)(double) = nullptr;\n    //Leg(AssetSwap::*)()\n\n    process09(ptrmemfunc1);     // T(C::*)()\n    process09(ptrmemfunc2);     // T(C::*)(U)\n    process09(ptrmemfunc3);     // T(U::*)()\n    process09(getLegPv);        // T(U::*)(V)\n    process09(ptrmemfunc4);     // C(T::*)()\n    process09(applyScaleToAllCoupons);          //C(T::*)(U)\n    process09(applyFixedSpreadToAllCoupons);    //D(C::*)(T)\n\n    process10(&CustomBond::cashflows);\n    process10(&AssetSwap::bondLeg);\n    process10(&Leg::flows);\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/template-argument-deduction/index.html#ctad-class-template-argument-deduction.",
    "href": "posts/template-argument-deduction/index.html#ctad-class-template-argument-deduction.",
    "title": "Class Template Argument Deduction(CTAD) Explained simply",
    "section": "CTAD (Class Template Argument Deduction).",
    "text": "CTAD (Class Template Argument Deduction).\n\nThe basic mechanics.\nCTAD(Class Template Argument Deduction) has \\(2\\) phases:\n\nDeduction (CTAD) - The first step is, the compiler is going to deduce the types that you didn’t write.\nInitialization - The second step is, it’s going to initialize the object.\n\nLet’s take a templated class pair, this is just a fictional class, it is not actually how std::pair&lt;&gt; looks like:\n\ntemplate&lt;typename T, typename U&gt;\nstruct pair{\n    T first;\n    U second;\n\n    pair(const T& _first, const U& _second) \n    : first(_first)\n    , second(_second)\n    {}\n\n    pair(T&& _first, U&& _second) \n    : first(std::move&lt;T&gt;(_first))\n    , second(std::move&lt;U&gt;(_second))\n    {}\n\n    //other stuff\n};\n\nThis is an oversimplification that is enough for our purposes. So, you have a templated class with two template parameters T and U and then you have a bunch of constructors. Now, we want to instantiate one of these things:\npair p1{\"OptionVolQuote\"s, 0.50};\nYou want to construct an object of type pair. The next thing the compiler sees is, pair is a template. And we didn’t specify any template arguments. Probably, you wanna do class template argument deduction.\nThe next thing happens. pair has a bunch of constructors. Probably, you wanna call one of those constructors. And this where step 1 kicks in, which is the actual Class Template Argument Deduction(CTAD).\nSo, how does the compiler figure out, what you actually want to instantiate? So, it’s going to look at those constructors. Let’s pretend for a minute, that those constructors are ordinary functions - just free-standing functions. Now, these functions use class template parameters. Let’s pretend for a moment, that those template parameters are template parameters for the function.\n\ntemplate&lt;typename T, typename U&gt;\nstruct pair{\n    T first;\n    U second;\n\n    // Imagine this to be a function template\n    template&lt;typename T, typename U&gt;  \n    pair(const T& _first, const T& _second) \n    : first(_first)\n    , second(_second)\n    {}\n\n    // Imagine this to be a function template\n    template&lt;typename T, typename U&gt;\n    pair(T&& _first, U&& _second) \n    : first(std::move&lt;T&gt;(_first))\n    , second(std::move&lt;U&gt;(_second))\n    {}\n\n    //other stuff\n};\n\nSo, this code doesn’t exist. It’s just what the compiler temporarily does for you. And it generates these template functions from the constructors and they are called the deduction candidates.\nAnd now, if we have a call like this:\n\npair p1{\"OptionVolQuote\"s, 0.50};\n\nwe know, how to deal with functions right. So, it’s going to look at these functions and apply the usual template arguments deduction and the usual overload resolution.\n\"OptionVolQuote\"s is a lvalue that gets converted to an xvalue (by the std::string() constructor) and 0.50 is a prvalue. these arguments bind to universal references. So, the pair(T&&, U&&) version is chosen by the compiler from the overload set, during overload resolution. Further, T is deduced as std::string and U is deduced as double. The compiler literally inserts them as:\n\npair&lt;std::string,double&gt; p1{\"OptionVolQuote\"s, 0.50};\n\nThen, its going to do, what it would have done, if you would have written pair&lt;std::string,double&gt;. So, now we know, that this pair is actually pair&lt;std::string,int&gt;. So, the step 1 is done.\nNow, what we can do is, we can actually instantiate the function template! That’s step 2. So, you have an actual constructor and it will be called by the run-time to create an object of pair&lt;std::string,int&gt;. And we are done.\nIf we write:\n\nconst auto s{\"5YSwapRate\"s};\nconst auto rate{0.0125};\npair p2{s,rate};\n\nHere, s and rate are identifiers, so these are glvalues and can bind to const T&. So, the compiler instantiates the first overload of the constructor as pair(const std::string&, const double&).\nThere’s no need to use std::make_pair anymore. This make_pair thing is a basically a work-around for the fact that up until C++14, you could only do this with functions. So, you had to use a function to deduce the class template arguments. So, it was kind of hacky. And now we don’t need to use that anymore.\nThe same goes for std::tuple, you can instantiate a std::tuple with a bunch of arguments and it’s going to deduce the correct types for you, so you don’t need to use std::make_tuple anymore.\n\nstd::tuple point{1.00, -1.00}\n\nLet’s look at std::vector. So, for example, if you just give it an std::initializer_list of ints, its gonna correctly deduce back to std::vector&lt;int&gt;.\n\nstd::vector v{3, 5, 7, 11, 13};\n// deduces std::vector&lt;int&gt;\n\nOf course, with std::vector, there’s a trap. std::vector has this other constructor which takes a std::size_t, and it initializes a vector with that many elements in it.\n\nstd::vector&lt;int&gt; v1{3};\n// content is {3}\n\nstd::vector&lt;int&gt; v2(3);\n// content is {0,0,0}\n\nSo, in C++14, if you write std::vector&lt;int&gt; v{3} with curly braces, it’s going to be an initializer list, so its going to initialize the vector with one int, which is 3. If you std::vector&lt;int&gt; v(3) with parenthesis, it’s going to call the size_t constructor, and you’re gonna have 3 ints, which are initialized to 0.0.\nNow, what happens if you omit the int and use class-template argument deduction? Then if you write the curlies, its going to do the deduction. But if you use round parenthesis, it says, well you’re calling the constructor that takes a size_t, so you are going to have 3 elements, but 3 elements of what type? You didn’t specify! So, you get a compiler error.\n\nstd::vector v1{3};\n// Ok- deduces std::vector&lt;int&gt;, content is {3}\n\n// std::vector v2(3);\n// Error : 3 elements of what?\n\nstd::vector has another constructor, which is really cool! Now, some real magic happens here! So, if you have a range of ints, any range, then there’s this constructor that takes a pair of iterators like begin() and end() and if you don’t specify the int, it is still going to figure out, that those iterators are iterators to int range and it is going correctly deduce std::vector&lt;int&gt; for you.\n\nstd::vector range{2, 3, 5, 7, 11};\nstd::vector v(range.begin(), range.end());\n// deduces std::vector&lt;int&gt;\n\nHow does that work? It has this constructor which looks like the below. It takes two iterators.\n\ntemplate&lt;typename T&gt;\nstruct vector\n{\n    // range c'tor\n    template&lt;typename Iter&gt;\n    vector&lt;Iter begin, Iter end&gt; { /* ... */ }\n\n    // other stuff\n};\n\nIf you have a constructor that also has template arguments, the compiler is going to pretend that this is a function and it’s going to take the template argument of the class and concatenate it with the constructor’s template argument. It’s going to put them one after the other.\n\n// This is magic code, generated by the compiler\ntemplate&lt;typename T&gt;\nstruct vector\n{\n    // range c'tor\n    template&lt;typename T, typename Iter&gt;\n    vector&lt;Iter begin, Iter end&gt; { /* ... */ }\n\n    // other stuff\n};\n\nNow, if you call it like this:\n\nstd::vector range{2, 3, 5, 7, 11};\nstd::vector v(range.begin(), range.end());\n\nit’s going to say, well okay, you are giving me two iterators, so I can deduce the type of iterators as std::vector&lt;&gt;::iterator. But, you didn’t specify T, so I still don’t know what T is. So, how is it able to figure this out?"
  },
  {
    "objectID": "posts/template-argument-deduction/index.html#list-initialization-has-priority",
    "href": "posts/template-argument-deduction/index.html#list-initialization-has-priority",
    "title": "Class Template Argument Deduction(CTAD) Explained simply",
    "section": "List initialization has priority",
    "text": "List initialization has priority\nYou really have to be careful with the parenthesis and the curlies.\n\nstd::vector range{2, 3, 5, 7, 11};\nstd::vector v(range.begin(), range.end());\n// deduces std::vector&lt;int&gt;\n\nstd::vector v{range.begin(), range.end()};\n// list initialization has a priority, so the compiler deduces it\n// as std::vector&lt;std::vector&lt;int&gt;::iterator&gt;, which is \n// probably not what we want"
  },
  {
    "objectID": "posts/class-template-argument-deduction/index.html",
    "href": "posts/class-template-argument-deduction/index.html",
    "title": "Class Template Argument Deduction(CTAD)",
    "section": "",
    "text": "Its super-helpful to have a good low-level hang of the template argument deduction rules (more on deduction guides in another post). Essentially, since C++17, you can just declare, std::vector{1.0, 2.0, 3.0, 4.0, 5.0} - clean and nice instead of std::vector&lt;double&gt;{1.0, 2.0, 3.0, 4.0, 5.0}.\nWhen the compiler tries to deduce the template arguments, it performs matching of the type template parameters with the types of arguments used to invoke the function.\nVery succinctly, the compiler can match: - Types of the form T, T const, T volatile - Pointers T*, lvalue references T& and universal references T&& - Arrays such as T[5] and C[5][n] - Pointers to functions - Pointers to member-functions and data-members.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;chrono&gt;\n#include &lt;tuple&gt;\n\nusing namespace std::chrono;\n\ntemplate&lt;typename T&gt;\nstd::ostream& operator&lt;&lt;(std::ostream& out, const std::vector&lt;T&gt; vec){\n    for(int i{0}; i &lt; vec.size(); ++ i){\n        out &lt;&lt; \"\\n[\" &lt;&lt; i &lt;&lt; \"] = \" &lt;&lt; vec[i];\n    }\n    return out;\n}\n\nstruct BondDiscountingCurve{\n    std::vector&lt;std::pair&lt;year_month_day,double&gt;&gt; discountFactorCurve;\n};\n\nstruct CustomBond{\n    std::vector&lt;double&gt; cashflows;\n    std::vector&lt;std::chrono::year_month_day&gt; cashflow_dates;\n\n    double pv(BondDiscountingCurve disc){\n        double result{0.0};\n        for(int i{0};i &lt; cashflows.size(); ++i)\n        {\n            double df = std::get&lt;1&gt;(disc.discountFactorCurve[i]);\n            result += cashflows[i] * df;\n        }\n        return result;\n    }\n};\n\nstruct Leg{\n    // A cashflow is a 4-tuple (Cashflow date, Cashflow amount, Cap, Floor)\n    using flow = std::tuple&lt;year_month_day, double, double, double&gt;;\n    std::vector&lt;flow&gt; flows;\n};\n\nstruct AssetSwap{\n    Leg bondLeg;\n    Leg fundingLeg;\n\n    Leg getBondLeg() { return bondLeg; }\n    Leg getFundingLeg() { return fundingLeg; }\n};\n\ntemplate&lt;typename T&gt;\nvoid process01(T bond){ std::cout &lt;&lt; \"T\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process02(T const){ std::cout &lt;&lt; \"T const\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process03(T volatile) { std::cout &lt;&lt; \"T volatile\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T*) { std::cout &lt;&lt; \"T*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T&) { std::cout &lt;&lt; \"T&\\n\"; }\n\n// Universal reference\ntemplate&lt;typename T&gt;\nvoid process05(T&&) { std::cout &lt;&lt; \"T&&\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process06(T[5]) { std::cout &lt;&lt; \"T[5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process07(T[3][5]) { std::cout &lt;&lt; \"T[3][5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(T(*)()) { std::cout &lt;&lt; \"T (*)()\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(CustomBond (*)(T)) { std::cout &lt;&lt; \"C (*)(T)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process08(T(*)(U)) { std::cout &lt;&lt; \"T (*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(T(CustomBond::*)()) { std::cout &lt;&lt; \"T (C::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(CustomBond::*)(U)) { std::cout &lt;&lt; \"T (C::*)(U)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(U::*)()) { std::cout &lt;&lt; \"T (U::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U, typename V&gt;\nvoid process09(T(U::*)(V)) { std::cout &lt;&lt; \"T (U::*)(V)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(T::*)()) { std::cout &lt;&lt; \"C (T::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(Leg(T::*)(U)) { std::cout &lt;&lt; \"C (T::*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(AssetSwap::*)(T)) { std::cout &lt;&lt; \"D (C::*)(T)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(T CustomBond::*) { std::cout &lt;&lt; \"T C::*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(Leg T::*) { std::cout &lt;&lt; \"C T::*\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process10(T U::*) { std::cout &lt;&lt; \"T U::*\\n\"; }\n\nint main()\n{\n    CustomBond bond(\n        {0.05, 0.05, 0.05, 1.05},\n        {2024y/June/20d, 2024y/December/20d, 2025y/June/20d, 2025y/December/20d}\n    );\n\n    AssetSwap assetSwap;\n\n    process01(bond);    // T\n    process02(bond);    // T const\n    process03(bond);    // T volatile\n    process04(&bond);   // T*\n    process04(bond);    // T&\n    process05(bond);    // T&&; deduced as CustomBond& \n\n    CustomBond bondsArray[5] {};  //Create an array of custom bonds\n    process06(bondsArray);  // T[5]\n    process06(&bond);       // T[5]\n\n    CustomBond bondsByMaturityAndRating[3][5] {}; \n    process07(bondsByMaturityAndRating);    //C[5][n]\n\n    CustomBond (*funcptr1)() = nullptr;\n    CustomBond (*funcptr2)(int) = nullptr;\n    double     (*funcptr3)(int) = nullptr;\n\n    process08(funcptr1);    //T(*)()\n    process08(funcptr2);    //C(*)(T)\n    process08(funcptr3);    //T(*)(U)\n\n    double (CustomBond::*ptrmemfunc1) () = nullptr;\n    double (CustomBond::*ptrmemfunc2)(BondDiscountingCurve) = &CustomBond::pv;\n    std::vector&lt;double&gt; (Leg::*ptrmemfunc3)() = nullptr;\n    double(AssetSwap::*getLegPv)(Leg) = nullptr;\n    Leg(AssetSwap::*ptrmemfunc4)() = &AssetSwap::getFundingLeg;\n    Leg(Leg::*applyScaleToAllCoupons)(double) = nullptr;\n    Leg(AssetSwap::*applyFixedSpreadToAllCoupons)(double) = nullptr;\n    //Leg(AssetSwap::*)()\n\n    process09(ptrmemfunc1);     // T(C::*)()\n    process09(ptrmemfunc2);     // T(C::*)(U)\n    process09(ptrmemfunc3);     // T(U::*)()\n    process09(getLegPv);        // T(U::*)(V)\n    process09(ptrmemfunc4);     // C(T::*)()\n    process09(applyScaleToAllCoupons);          //C(T::*)(U)\n    process09(applyFixedSpreadToAllCoupons);    //D(C::*)(T)\n\n    process10(&CustomBond::cashflows);\n    process10(&AssetSwap::bondLeg);\n    process10(&Leg::flows);\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/class-template-argument-deduction/index.html#function-template-argument-deduction.",
    "href": "posts/class-template-argument-deduction/index.html#function-template-argument-deduction.",
    "title": "Class Template Argument Deduction(CTAD)",
    "section": "",
    "text": "Its super-helpful to have a good low-level hang of the template argument deduction rules (more on deduction guides in another post). Essentially, since C++17, you can just declare, std::vector{1.0, 2.0, 3.0, 4.0, 5.0} - clean and nice instead of std::vector&lt;double&gt;{1.0, 2.0, 3.0, 4.0, 5.0}.\nWhen the compiler tries to deduce the template arguments, it performs matching of the type template parameters with the types of arguments used to invoke the function.\nVery succinctly, the compiler can match: - Types of the form T, T const, T volatile - Pointers T*, lvalue references T& and universal references T&& - Arrays such as T[5] and C[5][n] - Pointers to functions - Pointers to member-functions and data-members.\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;chrono&gt;\n#include &lt;tuple&gt;\n\nusing namespace std::chrono;\n\ntemplate&lt;typename T&gt;\nstd::ostream& operator&lt;&lt;(std::ostream& out, const std::vector&lt;T&gt; vec){\n    for(int i{0}; i &lt; vec.size(); ++ i){\n        out &lt;&lt; \"\\n[\" &lt;&lt; i &lt;&lt; \"] = \" &lt;&lt; vec[i];\n    }\n    return out;\n}\n\nstruct BondDiscountingCurve{\n    std::vector&lt;std::pair&lt;year_month_day,double&gt;&gt; discountFactorCurve;\n};\n\nstruct CustomBond{\n    std::vector&lt;double&gt; cashflows;\n    std::vector&lt;std::chrono::year_month_day&gt; cashflow_dates;\n\n    double pv(BondDiscountingCurve disc){\n        double result{0.0};\n        for(int i{0};i &lt; cashflows.size(); ++i)\n        {\n            double df = std::get&lt;1&gt;(disc.discountFactorCurve[i]);\n            result += cashflows[i] * df;\n        }\n        return result;\n    }\n};\n\nstruct Leg{\n    // A cashflow is a 4-tuple (Cashflow date, Cashflow amount, Cap, Floor)\n    using flow = std::tuple&lt;year_month_day, double, double, double&gt;;\n    std::vector&lt;flow&gt; flows;\n};\n\nstruct AssetSwap{\n    Leg bondLeg;\n    Leg fundingLeg;\n\n    Leg getBondLeg() { return bondLeg; }\n    Leg getFundingLeg() { return fundingLeg; }\n};\n\ntemplate&lt;typename T&gt;\nvoid process01(T bond){ std::cout &lt;&lt; \"T\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process02(T const){ std::cout &lt;&lt; \"T const\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process03(T volatile) { std::cout &lt;&lt; \"T volatile\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T*) { std::cout &lt;&lt; \"T*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process04(T&) { std::cout &lt;&lt; \"T&\\n\"; }\n\n// Universal reference\ntemplate&lt;typename T&gt;\nvoid process05(T&&) { std::cout &lt;&lt; \"T&&\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process06(T[5]) { std::cout &lt;&lt; \"T[5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process07(T[3][5]) { std::cout &lt;&lt; \"T[3][5]\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(T(*)()) { std::cout &lt;&lt; \"T (*)()\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process08(CustomBond (*)(T)) { std::cout &lt;&lt; \"C (*)(T)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process08(T(*)(U)) { std::cout &lt;&lt; \"T (*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(T(CustomBond::*)()) { std::cout &lt;&lt; \"T (C::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(CustomBond::*)(U)) { std::cout &lt;&lt; \"T (C::*)(U)\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(T(U::*)()) { std::cout &lt;&lt; \"T (U::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U, typename V&gt;\nvoid process09(T(U::*)(V)) { std::cout &lt;&lt; \"T (U::*)(V)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(T::*)()) { std::cout &lt;&lt; \"C (T::*)()\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process09(Leg(T::*)(U)) { std::cout &lt;&lt; \"C (T::*)(U)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process09(Leg(AssetSwap::*)(T)) { std::cout &lt;&lt; \"D (C::*)(T)\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(T CustomBond::*) { std::cout &lt;&lt; \"T C::*\\n\"; }\n\ntemplate&lt;typename T&gt;\nvoid process10(Leg T::*) { std::cout &lt;&lt; \"C T::*\\n\"; }\n\ntemplate&lt;typename T, typename U&gt;\nvoid process10(T U::*) { std::cout &lt;&lt; \"T U::*\\n\"; }\n\nint main()\n{\n    CustomBond bond(\n        {0.05, 0.05, 0.05, 1.05},\n        {2024y/June/20d, 2024y/December/20d, 2025y/June/20d, 2025y/December/20d}\n    );\n\n    AssetSwap assetSwap;\n\n    process01(bond);    // T\n    process02(bond);    // T const\n    process03(bond);    // T volatile\n    process04(&bond);   // T*\n    process04(bond);    // T&\n    process05(bond);    // T&&; deduced as CustomBond& \n\n    CustomBond bondsArray[5] {};  //Create an array of custom bonds\n    process06(bondsArray);  // T[5]\n    process06(&bond);       // T[5]\n\n    CustomBond bondsByMaturityAndRating[3][5] {}; \n    process07(bondsByMaturityAndRating);    //C[5][n]\n\n    CustomBond (*funcptr1)() = nullptr;\n    CustomBond (*funcptr2)(int) = nullptr;\n    double     (*funcptr3)(int) = nullptr;\n\n    process08(funcptr1);    //T(*)()\n    process08(funcptr2);    //C(*)(T)\n    process08(funcptr3);    //T(*)(U)\n\n    double (CustomBond::*ptrmemfunc1) () = nullptr;\n    double (CustomBond::*ptrmemfunc2)(BondDiscountingCurve) = &CustomBond::pv;\n    std::vector&lt;double&gt; (Leg::*ptrmemfunc3)() = nullptr;\n    double(AssetSwap::*getLegPv)(Leg) = nullptr;\n    Leg(AssetSwap::*ptrmemfunc4)() = &AssetSwap::getFundingLeg;\n    Leg(Leg::*applyScaleToAllCoupons)(double) = nullptr;\n    Leg(AssetSwap::*applyFixedSpreadToAllCoupons)(double) = nullptr;\n    //Leg(AssetSwap::*)()\n\n    process09(ptrmemfunc1);     // T(C::*)()\n    process09(ptrmemfunc2);     // T(C::*)(U)\n    process09(ptrmemfunc3);     // T(U::*)()\n    process09(getLegPv);        // T(U::*)(V)\n    process09(ptrmemfunc4);     // C(T::*)()\n    process09(applyScaleToAllCoupons);          //C(T::*)(U)\n    process09(applyFixedSpreadToAllCoupons);    //D(C::*)(T)\n\n    process10(&CustomBond::cashflows);\n    process10(&AssetSwap::bondLeg);\n    process10(&Leg::flows);\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/class-template-argument-deduction/index.html#ctad-class-template-argument-deduction.",
    "href": "posts/class-template-argument-deduction/index.html#ctad-class-template-argument-deduction.",
    "title": "Class Template Argument Deduction(CTAD)",
    "section": "CTAD (Class Template Argument Deduction).",
    "text": "CTAD (Class Template Argument Deduction).\n\nThe basic mechanics.\nCTAD(Class Template Argument Deduction) has \\(2\\) phases:\n\nDeduction (CTAD) - The first step is, the compiler is going to deduce the types that you didn’t write.\nInitialization - The second step is, it’s going to initialize the object.\n\nLet’s take a templated class pair, this is just a fictional class, it is not actually how std::pair&lt;&gt; looks like:\n\ntemplate&lt;typename T, typename U&gt;\nstruct pair{\n    T first;\n    U second;\n\n    pair(const T& _first, const U& _second) \n    : first(_first)\n    , second(_second)\n    {}\n\n    pair(T&& _first, U&& _second) \n    : first(std::move&lt;T&gt;(_first))\n    , second(std::move&lt;U&gt;(_second))\n    {}\n\n    //other stuff\n};\n\nThis is an oversimplification that is enough for our purposes. So, you have a templated class with two template parameters T and U and then you have a bunch of constructors. Now, we want to instantiate one of these things:\npair p1{\"OptionVolQuote\"s, 0.50};\nYou want to construct an object of type pair. The next thing the compiler sees is, pair is a template. And we didn’t specify any template arguments. Probably, you wanna do class template argument deduction.\nThe next thing happens. pair has a bunch of constructors. Probably, you wanna call one of those constructors. And this where step 1 kicks in, which is the actual Class Template Argument Deduction(CTAD).\nSo, how does the compiler figure out, what you actually want to instantiate? So, it’s going to look at those constructors. Let’s pretend for a minute, that those constructors are ordinary functions - just free-standing functions. Now, these functions use class template parameters. Let’s pretend for a moment, that those template parameters are template parameters for the function.\n\ntemplate&lt;typename T, typename U&gt;\nstruct pair{\n    T first;\n    U second;\n\n    // Imagine this to be a function template\n    template&lt;typename T, typename U&gt;  \n    pair(const T& _first, const T& _second) \n    : first(_first)\n    , second(_second)\n    {}\n\n    // Imagine this to be a function template\n    template&lt;typename T, typename U&gt;\n    pair(T&& _first, U&& _second) \n    : first(std::move&lt;T&gt;(_first))\n    , second(std::move&lt;U&gt;(_second))\n    {}\n\n    //other stuff\n};\n\nSo, this code doesn’t exist. It’s just what the compiler temporarily does for you. And it generates these template functions from the constructors and they are called the deduction candidates.\nAnd now, if we have a call like this:\n\npair p1{\"OptionVolQuote\"s, 0.50};\n\nwe know, how to deal with functions right. So, it’s going to look at these functions and apply the usual template arguments deduction and the usual overload resolution.\n\"OptionVolQuote\"s is a lvalue that gets converted to an xvalue (by the std::string() constructor) and 0.50 is a prvalue. these arguments bind to universal references. So, the pair(T&&, U&&) version is chosen by the compiler from the overload set, during overload resolution. Further, T is deduced as std::string and U is deduced as double. The compiler literally inserts them as:\n\npair&lt;std::string,double&gt; p1{\"OptionVolQuote\"s, 0.50};\n\nThen, its going to do, what it would have done, if you would have written pair&lt;std::string,double&gt;. So, now we know, that this pair is actually pair&lt;std::string,int&gt;. So, the step 1 is done.\nNow, what we can do is, we can actually instantiate the function template! That’s step 2. So, you have an actual constructor and it will be called by the run-time to create an object of pair&lt;std::string,int&gt;. And we are done.\nIf we write:\n\nconst auto s{\"5YSwapRate\"s};\nconst auto rate{0.0125};\npair p2{s,rate};\n\nHere, s and rate are identifiers, so these are glvalues and can bind to const T&. So, the compiler instantiates the first overload of the constructor as pair(const std::string&, const double&).\nThere’s no need to use std::make_pair anymore. This make_pair thing is a basically a work-around for the fact that up until C++14, you could only do this with functions. So, you had to use a function to deduce the class template arguments. So, it was kind of hacky. And now we don’t need to use that anymore.\nThe same goes for std::tuple, you can instantiate a std::tuple with a bunch of arguments and it’s going to deduce the correct types for you, so you don’t need to use std::make_tuple anymore.\n\nstd::tuple point{1.00, -1.00}\n\nLet’s look at std::vector. So, for example, if you just give it an std::initializer_list of ints, its gonna correctly deduce back to std::vector&lt;int&gt;.\n\nstd::vector v{3, 5, 7, 11, 13};\n// deduces std::vector&lt;int&gt;\n\nOf course, with std::vector, there’s a trap. std::vector has this other constructor which takes a std::size_t, and it initializes a vector with that many elements in it.\n\nstd::vector&lt;int&gt; v1{3};\n// content is {3}\n\nstd::vector&lt;int&gt; v2(3);\n// content is {0,0,0}\n\nSo, in C++14, if you write std::vector&lt;int&gt; v{3} with curly braces, it’s going to be an initializer list, so its going to initialize the vector with one int, which is 3. If you std::vector&lt;int&gt; v(3) with parenthesis, it’s going to call the size_t constructor, and you’re gonna have 3 ints, which are initialized to 0.0.\nNow, what happens if you omit the int and use class-template argument deduction? Then if you write the curlies, its going to do the deduction. But if you use round parenthesis, it says, well you’re calling the constructor that takes a size_t, so you are going to have 3 elements, but 3 elements of what type? You didn’t specify! So, you get a compiler error.\n\nstd::vector v1{3};\n// Ok- deduces std::vector&lt;int&gt;, content is {3}\n\n// std::vector v2(3);\n// Error : 3 elements of what?\n\nstd::vector has another constructor, which is really cool! Now, some real magic happens here! So, if you have a range of ints, any range, then there’s this constructor that takes a pair of iterators like begin() and end() and if you don’t specify the int, it is still going to figure out, that those iterators are iterators to int range and it is going correctly deduce std::vector&lt;int&gt; for you.\n\nstd::vector range{2, 3, 5, 7, 11};\nstd::vector v(range.begin(), range.end());\n// deduces std::vector&lt;int&gt;\n\nHow does that work? It has this constructor which looks like the below. It takes two iterators.\n\ntemplate&lt;typename T&gt;\nstruct vector\n{\n    // range c'tor\n    template&lt;typename Iter&gt;\n    vector&lt;Iter begin, Iter end&gt; { /* ... */ }\n\n    // other stuff\n};\n\nIf you have a constructor that also has template arguments, the compiler is going to pretend that this is a function and it’s going to take the template argument of the class and concatenate it with the constructor’s template argument. It’s going to put them one after the other.\n\n// This is magic code, generated by the compiler\ntemplate&lt;typename T&gt;\nstruct vector\n{\n    // range c'tor\n    template&lt;typename T, typename Iter&gt;\n    vector&lt;Iter begin, Iter end&gt; { /* ... */ }\n\n    // other stuff\n};\n\nNow, if you call it like this:\n\nstd::vector range{2, 3, 5, 7, 11};\nstd::vector v(range.begin(), range.end());\n\nit’s going to say, well okay, you are giving me two iterators, so I can deduce the type of iterators as std::vector&lt;&gt;::iterator. But, you didn’t specify T, so I still don’t know what T is. So, how is it able to figure this out?"
  },
  {
    "objectID": "posts/class-template-argument-deduction/index.html#list-initialization-has-priority",
    "href": "posts/class-template-argument-deduction/index.html#list-initialization-has-priority",
    "title": "Class Template Argument Deduction(CTAD)",
    "section": "List initialization has priority",
    "text": "List initialization has priority\nYou really have to be careful with the parenthesis and the curlies.\n\nstd::vector range{2, 3, 5, 7, 11};\nstd::vector v(range.begin(), range.end());\n// deduces std::vector&lt;int&gt;\n\nstd::vector v{range.begin(), range.end()};\n// list initialization has a priority, so the compiler deduces it\n// as std::vector&lt;std::vector&lt;int&gt;::iterator&gt;, which is \n// probably not what we want"
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html",
    "href": "posts/type-traits-and-conditional-compilation/index.html",
    "title": "Type Traits",
    "section": "",
    "text": "Meta-programs are programs that treat other programs as data. They could be other programs or itself. A meta-function is not a function, but a class or struct. Metafunctions are not part of the language and have no formal language support. They exist purely as an idiomatic use of the existing language features. Now, since their use is not enforced by the language, their used has to be dictated by a convention. Over the years, the C++ community has created common set of standard conventions. Actually this work goes back all the way to Boost type_traits.\nMetafunctions are not functions. Technically, they are a class with zero or more template parameters and zero+ return types and values. The convention is that a metafunction should return just one thing, like a regular function. The convention was developed over time, so there are plenty of existing examples that do not follow this convention. More modern metafunctions do follow this convention."
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#how-do-we-return-from-a-metafunction",
    "href": "posts/type-traits-and-conditional-compilation/index.html#how-do-we-return-from-a-metafunction",
    "title": "Type Traits",
    "section": "How do we return from a metafunction",
    "text": "How do we return from a metafunction\nIf we have to return a value, basically, we are going to expose a public field named value.\ntemplate&lt;typename T&gt;\nstruct TheAnswer{\n    static constexpr int value = 42;\n};\nAnd if we are going to return a type, we are going to expose a public field named type.\ntemplate&lt;typename T&gt;\nstruct Echo{\n    using type = T;\n};\nNow, here’s kind of the difference between regular functions and metafunctions. A regular function in C++ always works on some form of data and it’s always going to return to you some piece of data as well. Amongst metafunctions, we have value metafunctions that work on values like we are used to and then we have metafunctions that work entirely on types and they yield back some type to you. And so in the both of the examples above, we return something by exposing the public members of a class."
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#value-metafunctions",
    "href": "posts/type-traits-and-conditional-compilation/index.html#value-metafunctions",
    "title": "Type Traits",
    "section": "Value metafunctions",
    "text": "Value metafunctions\nA value metafunction is kind of like a simple regular function. Let’s look at a simple regular function - the integer identity function.\nint int_identity(int x)\n{\n    return x;\n}\n\nassert(42 == int_identity(42));\nThis function just applies the identity transformation on any integer passed to it, and spits out the same number. A simple metafunction for identity - we can call it the intIdentity metafunction would look like this:\ntemplate&lt;int X&gt;\nstruct intIdentity{\n    static constexpr int value = X;\n};\n\nstatic_assert(42 == IntIdentity&lt;42&gt;::value);\nWe see that it’s not that much different. You return a value by having a static data-member called value and it has the metafunction’s return value. IntIdentity&lt;42&gt;::value is where we are calling the metafunction. Now, this convention needs to be adhered to, because if you give your metafunction some other name such as my_value, for example, if you write:\ntemplate&lt;int X&gt;\nstruct intIdentity{\n    static constexpr int my_value = X;\n};\n\nstatic_assert(42 == IntIdentity&lt;42&gt;::value);\nit’s not going to work well with other things.\n\nGeneric Identity Function\nLet’s look at the generic identity function.\ntemplate&lt;typename T&gt;\nT identity(T x){\n    return x;\n}\n\n//Returned type will be 42\nassert(42 == identity(42));\n\n// Returned type will be unsigned long long\nassert(42ul == identity(42ul))\nThis is just a function that will be an identity for any type. You give me a value of any type and I will give you that value back. Now we can create a generic identity metafunction as well:\ntemplate&lt;typename T, T x&gt;\nstruct ValueIdentity{\n    static constexpr T value = x;\n};\n\n// The type of value will be int\nstatic_assert(42 == identity&lt;int,42&gt;::value);\n\n// The type of value will unsigned long long\nstatic_assert(42ull == identity&lt;unsigned long long,42ull&gt;::value);\nValueIdentity is a generic metafunction, so we have to first feed it the type int and then the value 42. It’s a little cumbersome, but you get used to it after a while.\nIn C++17, things get a little bit easier with generic metafunctions, because we have this cool keyword called auto. I won’t go into all the details of auto. For now, it basically means that the template will accept and deduce the type of any non-type template parameter.\ntemplate&lt;auto X&gt;\nstruct ValueIdentity{\n    static constexpr auto value = X;\n};\n\n// The type of value will be int\nstatic_assert(42 == identity&lt;int,42&gt;::value);\n\n// The type of value will unsigned long long\nstatic_assert(42ull == identity&lt;unsigned long long,42ull&gt;::value);\nLet’s look at another function sum(). We can do this in a regular function, and we can do this in a metafunction as well.\nint sum(int x, int y){\n    return x + y;\n}\n\ntemplate&lt;int X, int Y&gt;\nstruct intSum{\n    static constexpr int value = X + Y;\n};\n\nstatic_assert(42 == IntSum&lt;30,12&gt;::value);\nSo, we can also create a generic version of this:\ntemplate&lt;typename X, typename Y&gt;\nauto sum(T x, Ty){\n    return x + y;\n}\n\ntemplate&lt;auto X, auto Y&gt;\nstruct Sum{\n    static constexpr auto value = X + Y;\n};"
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#type-metafunctions",
    "href": "posts/type-traits-and-conditional-compilation/index.html#type-metafunctions",
    "title": "Type Traits",
    "section": "Type metafunctions",
    "text": "Type metafunctions\nType metafunctions are the workhorse of doing type transformations. You can manipulate types through type metafunctions. Type *metafunctions are going to return just a type.\nHere’s our TypeIdentity function:\ntemplate&lt;typename T&gt;\nstruct TypeIdentity{\n    using type = T;\n}\nJust like we have ValueIdentity, where given any value, it’s going you the value back; we have TypeIdentity, where you give it any type, and it’s going to give you the type back.\nC++20 actually introduces std::type_identity, which is pretty much what we see above.\n\nCalling Type Metafunctions\nWhen we call a value metafunction, we can easily call the function:\nValueIdentity&lt;42&gt;::value\nValueIdentity is the metafunction, it’s passed the parameter 42 in the angle brackets (just like parentheses for a regular value function) and ::value is how I get it’s value back.\nWhen I call a type metafunction, it’s the same way. The function call consists of the metafunction name std::type_identity, the parameters to the metafunction in angle brackets (&lt;42&gt;) and ::type is how I get it’s value back.\n#include &lt;iostream&gt;\n#include &lt;type_traits&gt;\n\nint main()\n{\n    using T = std::type_identity&lt;int&gt;::type;\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#understanding-name-binding-and-dependent-types",
    "href": "posts/type-traits-and-conditional-compilation/index.html#understanding-name-binding-and-dependent-types",
    "title": "Type Traits",
    "section": "Understanding name binding and dependent types",
    "text": "Understanding name binding and dependent types\nName binding is the process of establishing, determining explicitly the type of each name (declaration) in a template. There are two kinds of names used within a template: dependent and non-dependent names. Names that depend on a template parameter are called dependent names.\n\nFor dependent names, name binding is performed at the point of template instantiation.\nFor non-dependent names, name binding is performed at the point of template definition.\n\nConsider the following C++ code:\n// Ref: Template metaprogramming with C++\n// Mariusz Bancilla, pages 123 \n#include &lt;iostream&gt;\n\ntemplate&lt;typename T&gt;\nstruct handler{\n    void handle(T value)    //[1] handle is a dependent name\n    {\n        std::cout &lt;&lt; \"handler&lt;T&gt;: \" &lt;&lt; value &lt;&lt; '\\n';\n    }\n};\n\ntemplate&lt;typename T&gt;\nstruct parser{\n    void parse(T arg){      //[2] parse is a dependent name\n        arg.handle(x);\n    }\n\n    double x;                  //[3] x is a non-dependent name\n};\n\nint main(){\n    handler&lt;double&gt; doubleHandler;                //[5] template instantiation\n    parser&lt;handler&lt;double&gt;&gt; doubleParser(3.14);   //[6] template instantiation\n    doubleParser.parse(doubleHandler);\n    return 0;\n}\nWhen the compiler sees dependent names, e.g. at points [1] and [2], it cannot determine the type-signature of these functions. So, parse() and handle() are not bound at this point.\nThe declaration double x at point [3] declares a non-dependent type. So, the type of the variable x is known and bound.\nContinuing with the code, at point [4], there is a template specialization for the handler class template for the type int.\nTemplate instantiation happens at points [5] and [6]. At point [5], handler&lt;double&gt;::handle is bound to handle and at point [6], parser&lt;handler&lt;double&gt;&gt;::parse is bound to parse.\n\nTwo-phase name lookup\nName binding happens differently for dependent names (those that depend on a template parameter) and non-dependent names. When the compiler passes throuh the definition of a template, it needs to figure out whether a name is dependent or non-dependent. Further, name binding depends upon this categorization. Thus, the instantiation of a template happens in 2-phases.\n\nThe first phase occurs at the point of the definition when the template syntax is checked and the names are categorized as dependent or non-dependent.\nThe second phase occurs at the point of template instantiation when the template arguments are substituted for the template parameters. Name binding for dependent names happens at this point.\n\nThis process in two steps is called the two-phase name lookup.\nConsider the following C++ code:\n// Ref: Template metaprogramming with C++\n// Mariusz Bancilla, pages 125\ntemplate&lt;typename T&gt;\nstruct base_parser\n{\n    void init()                     // [1] non-dependent name\n    {\n        std::cout &lt;&lt; \"init\\n\";\n    }\n};\n\ntemplate&lt;typename T&gt;\nstruct parser::public base_parser&lt;T&gt;\n{\n    void parse(){                  // [2] non-dependent name\n        // The compiler at [3] will try to bind init(), as it's a non-dependent name.\n        // However, base_parser has not yet been instantiated. This will result in a \n        // compile-error\n        //init();                  // [3] non-dependent name\n        std::cout &lt;&lt; \"parse\\n\";\n    }\n};\n\nint main(){\n    parser&lt;double&gt; p;\n    p.parse();\n}\nThe call to init() inside the parse() member function has been commented out. Uncommenting it will cause a compile error.\nThe intention here is to call the base-class init() function. However, the compiler will issue an error, because it’s not able to find init(). The reason is that init() is a non-dependent name. Therefore, it must be bound at the time of the definition of the parser template. Although, base_parser&lt;T&gt;::init() exists, this template has still not been instantiated. The compiler cannot assume its what we want to call, because the primary template base_parser can always be specialized and init() can be defined as something else.\nThis problem can be fixed, by making init a dependent name. This can be done by either prefixing it with this-&gt; or with base_parser&lt;T&gt;::.\n\n\nDependent type names\nThere are cases where a dependent name is a type. Consider the following C++ code:\n#include &lt;iostream&gt;\n\ntemplate&lt;typename T&gt;\nstruct base_parser{\n    using value_type = T;\n};\n\ntemplate&lt;typename T&gt;\nstruct parser: base_parser&lt;T&gt;{\n    void parse(){\n        // value_type v{};  // [1] : Error\n        // base_parser&lt;T&gt;::value_type v{};  //[2] :Error\n        std::cout &lt;&lt; \"parse\\n\";\n    }\n};\n\nint main()\n{\n    parser&lt;double&gt; p;\n    p.parse();\n    return 0;\n}\nCompiler Explorer\nIn this code snippet, the metafunction base_parser is an identity metafunction and returns back the type you give it. base_parser&lt;T&gt;::value_type is actually a dependent type, which depends on the template parameter T. At point [1] and [2], the compiler does not know what T will be. If it attempts to bind the name v, it will fail. We need to tell the compiler explicitly base_parser&lt;T&gt;::value_type is a dependent type. You do that using the typename keyword.\n#include &lt;iostream&gt;\n\ntemplate&lt;typename T&gt;\nstruct base_parser{\n    using value_type = T;\n};\n\ntemplate&lt;typename T&gt;\nstruct parser: base_parser&lt;T&gt;{\n    void parse(){\n        // value_type v{};  // [1] : Error\n        // base_parser&lt;T&gt;::value_type v{};  //[2] :Error\n        typename base_parser&lt;T&gt;::value_type v{};  //[3] :Ok\n        std::cout &lt;&lt; \"parse\\n\";\n    }\n};\n\nint main()\n{\n    parser&lt;double&gt; p;\n    p.parse();\n    return 0;\n}\nSo, any time, when calling a type metafunction, if the compiler does not know what ::type is, you must prefix it using the typename keyword, if we want to treat it as a type."
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#convenience-calling-functions",
    "href": "posts/type-traits-and-conditional-compilation/index.html#convenience-calling-functions",
    "title": "Type Traits",
    "section": "Convenience calling functions",
    "text": "Convenience calling functions\nValue metafunctions often use helper functions (variable templates) ending with _v. For example, we often define the helper function:\ntemplate &lt;auto X&gt;\ninline constexpr auto ValueIdentity_v = ValueIdentity&lt;X&gt;::value;\n\nstatic_assert(42 == ValueIdentity&lt;42&gt;::value);\nstatic_assert(42 == ValueIdentity_v&lt;42&gt;)\nWe are just calling the ValueIdentity&lt;&gt; metafunction, grabbing its value and storing it into this variable ValueIdentity_v. This is a convenient way of calling value metafunctions. It does require you to instantiate an extra variable template.\nIt’s really helpful when we start using it with types. Type metafunctions use alias templates ending with _t. It helps us get rid of the entire typename dance.\ntemplate &lt;typename T&gt;\nusing TypeIdentity_t = typename TypeIdentity&lt;T&gt;::type;\n\nstatic_assert(std::is_same_v&lt;int, TypeIdentity_t&lt;int&gt;);\nInstead of calling the TypeIdentity metafunction with the parameter int and writing ::type to get its value, I can just call the metafunction with _t and with its parameters in angle brackets(&lt;&gt;).\nThese calling conventions are easier to use. But each one must be explicitly hand-written. So, every time you write a metafunction, if you want to provide convenience capabilities, you also have to write the convenience variable template or alias template."
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#stdintegral_constant---a-very-useful-metafunction",
    "href": "posts/type-traits-and-conditional-compilation/index.html#stdintegral_constant---a-very-useful-metafunction",
    "title": "Type Traits",
    "section": "std::integral_constant - a very useful metafunction",
    "text": "std::integral_constant - a very useful metafunction\nThis is probably the most useful metafunction. This is what std::integral_constant looks like:\ntemplate&lt;class T, T v&gt;\nstruct integral_constant{\n    static constexpr T value = v;\n\n    using value_type = T;\n    using type = integral_constant&lt;T, v&gt;\n    \n    constexpr operator value_type() const noexcept{     //[1] : Implicit integral cast\n        return value;\n    }\n\n    constexpr operator value_type()() const noexcept{\n        return value;\n    }\n};\nintegral_constant is just a class, it has no data. Its just a wrapper over a compile-time constant of a specified type.\nIt’s got a value, so its a value metafunction. But, it’s also got type, so its a type metafunction as well. In fact, its a type metafunction which is an identity metafunction. It will tell exactly what the integral_constant thing is. It also has the value_type.\n#include &lt;iostream&gt;\n#include &lt;type_traits&gt;\n\ntemplate&lt;typename T&gt;\nstruct is_void{\n    using type = std::false_type;\n};\n\ntemplate&lt;&gt;\nstruct is_void&lt;void&gt;{\n    using type = std::true_type;\n};\n\nint main(){\n    static_assert(std::is_same&lt;is_void&lt;void&gt;::type,std::true_type&gt;::value);\n    static_assert(std::is_same&lt;is_void&lt;int&gt;::type,std::false_type&gt;::value);\n    return 0;\n}"
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#useful-metafunctions-to-think-of",
    "href": "posts/type-traits-and-conditional-compilation/index.html#useful-metafunctions-to-think-of",
    "title": "Type Traits",
    "section": "Useful metafunctions to think of",
    "text": "Useful metafunctions to think of\nHow is std::remove_pointer metafunction implemented? You can intuitively come up with what it must look like:\ntemplate&lt;typename T&gt;\nstruct remove_pointer{\n    using type = T;\n};\n\n// template specialization\ntemplate&lt;typename T&gt;\nstruct remove_pointer&lt;T*&gt;{\n    using type = T;\n};\nIf the std::remove_pointer metafunction receives int* as a parameter, the specialized version of the template kicks in, as int* is matched against T*. Here is the full implementation of std::remove_pointer&lt;T&gt;:\ntemplate &lt;class T&gt; struct remove_pointer                    { using type = T };\ntemplate &lt;class T&gt; struct remove_pointer&lt;T*&gt;                { using type = T };\ntemplate &lt;class T&gt; struct remove_pointer&lt;T* const&gt;          { using type = T };\ntemplate &lt;class T&gt; struct remove_pointer&lt;T* volatile&gt;       { using type = T };\ntemplate &lt;class T&gt; struct remove_pointer&lt;T* const volatile&gt; { using type = T };\nAs you can see, the same technique is applied to more subtle edge cases.\nHow about std::remove_reference?\ntemplate&lt;typename T&gt;\nstruct remove_reference{\n    using type = T;\n};\n\n// template specialization\ntemplate&lt;typename T&gt;\nstruct remove_reference&lt;T&&gt;{\n    using type = T;\n};\n\n// template specialization\ntemplate&lt;typename T&gt;\nstruct remove_reference&lt;T&&&gt;{\n    using type = T;\n};\nBut, removing qualifiers from types is only the tip of the iceberg. Take for instance, a jewel for type manipulation std::decay."
  },
  {
    "objectID": "posts/fun-with-numeraires/index.html",
    "href": "posts/fun-with-numeraires/index.html",
    "title": "Fun with numeraires!",
    "section": "",
    "text": "A proficiency in the change-of-measure technique is useful to the working quant. An excellent summary of the important results is the note Girsanov, Numeraires and all that, by Andrew Lesniewski. In this post, I would like to derive relevant results and then we can enjoy pricing some payoffs together!"
  },
  {
    "objectID": "posts/fun-with-numeraires/index.html#pricing-the-payoff-vt-s_ts_t---k",
    "href": "posts/fun-with-numeraires/index.html#pricing-the-payoff-vt-s_ts_t---k",
    "title": "Fun with numeraires!",
    "section": "Pricing the payoff \\(V(T) = S_T(S_T - K)^{+}\\)",
    "text": "Pricing the payoff \\(V(T) = S_T(S_T - K)^{+}\\)\nUnder the risk-neutral measure \\(\\mathbb{Q}^M\\) associated with the money-market account numeraire \\(M(t)\\), the stock price \\(S(t)\\) evolves according to:\n\\[\n\\begin{align*}\ndS_t = r S_t dt + \\sigma S_t dW^{\\mathbb{Q}^M}\n\\end{align*}\n\\tag{9}\\]\nwhich has the solution:\n\\[\n\\begin{align*}\nS_t = S_0\\exp\\left[\\left(r - \\frac{\\sigma^2}{2}\\right)t + \\sigma W^{\\mathbb{Q}^M}(t)\\right]\n\\end{align*}\n\\tag{10}\\]\nBy the risk-neutral pricing formula, we have:\n\\[\n\\begin{align*}\nV(0) &= \\mathbb{E}^{\\mathbb{Q}^M}\\left[\\frac{V(T)}{M(T)}\\right] \\\\\n&= \\mathbb{E}^{\\mathbb{Q}^M}\\left[\\frac{S_T}{M_T}(S_T - K)^{+}\\right]\n\\end{align*}\n\\tag{11}\\]\nBy the change-of-measure formula, if \\(N\\) is any other numeraire with associated probability measure \\(\\mathbb{Q}^N\\), we know that:\n\\[\n\\begin{align*}\n\\mathbb{E}^{\\mathbb{Q}^N}[V(T)]=\\mathbb{E}^{\\mathbb{Q}^M}\\left[\\frac{d\\mathbb{Q}^N}{d\\mathbb{Q}^M}V(T)\\right]\n\\end{align*}\n\\tag{12}\\]\nWe switch to the stock numeraire \\(S_t\\). The Radon-Nikodym derivative \\(L_T = d\\mathbb{Q}^S/d\\mathbb{Q}^M\\) is simply:\n\\[\n\\begin{align*}\nL_T = \\frac{d\\mathbb{Q}^S}{d\\mathbb{Q}^M} = \\frac{S(T)/S(0)}{M(T)/M(0)}  = \\frac{1}{S(0)}\\cdot \\frac{S(T)}{M(T)}\n\\end{align*}\n\\tag{13}\\]\nThe dynamics of \\(L_t\\) under the \\(\\mathbb{Q}_M\\) measure is given by:\n\\[\ndL_t = \\phi_t L_t dW^{\\mathbb{Q}^M}\n\\]\nwhere the Girsanov kernel \\(\\phi_t = \\sigma\\). Then, the Girsanov transformation is:\n\\[\nW^{\\mathbb{Q}^M}_T = W^{\\mathbb{Q}^S}_T + \\sigma T\n\\tag{14}\\]\nSo, the \\(\\mathbb{Q}^S\\) dynamics of the stock price is:\n\\[\n\\begin{align*}\nS_T &= S_0 \\exp\\left[\\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma (W^{\\mathbb{Q}^S}_T + \\sigma T)\\right]\\\\\n&=  S_0 \\exp\\left[\\left(r + \\frac{\\sigma^2}{2}\\right)T + \\sigma (W^{\\mathbb{Q}^S}_T)\\right]\n\\end{align*}\n\\tag{15}\\]\nWe can develop the expression in Equation 11 to be:\n\\[\n\\begin{align*}\nV(0) &= S_0\\mathbb{E}^{\\mathbb{Q}^M}\\left[\\frac{1}{S_0}\\frac{S_T}{M_T}(S_T - K)^{+}\\right]\n\\end{align*}\n\\tag{16}\\]\nApplying the change-of-measure (abstract Baye’s formula),we have:\n\\[\n\\begin{align*}\nV(0) &= S_0\\mathbb{E}^{\\mathbb{Q}^S}\\left[(S_T - K)^{+}\\right]\n\\end{align*}\n\\tag{17}\\]\nNow, we have:\n\\[\n\\begin{align*}\n\\mathbb{Q}^S(S_T &gt; K) &= \\mathbb{Q}^S\\left(S_0 \\exp\\left[\\left(r + \\frac{\\sigma^2}{2}\\right)T + \\sigma (W^{\\mathbb{Q}^S}_T)\\right] &gt; K\\right)\\\\\n&= \\mathbb{Q}^S\\left(S_0 \\exp\\left[\\left(r + \\frac{\\sigma^2}{2}\\right)T + \\sigma (-\\sqrt{T}Z)\\right] &gt; K\\right)\\\\\n&= \\mathbb{Q}^S\\left(\\log \\frac{S_0}{K}  + \\left(r + \\frac{\\sigma^2}{2}\\right)T   &gt; \\sigma\\sqrt{T}Z\\right)\\\\\n\\end{align*}\n\\tag{18}\\]\nwhere we subbed \\(-\\sqrt{T}Z=W^{\\mathbb{Q}^S}_T\\). Recall, the standard normals \\(Z\\) and \\(-Z\\) have the same distribution by symmetry.\nDefine\n\\[\nd_{-} = \\frac{\\log \\frac{S_0}{K}  + \\left(r + \\frac{\\sigma^2}{2}\\right)T}{\\sigma\\sqrt{T}}\n\\tag{19}\\]\nThen, since \\(Z \\sim \\mathcal{N}^{\\mathbb{Q}^S}(0,1)\\):\n\\[\n\\mathbb{Q}^S(S_T &gt; K) = \\mathbb{Q}^S(Z &lt; d_{-}) = \\Phi(d_{-})\n\\tag{20}\\]\nSo, we can expand the expectation in Equation 17. The indicator random variable \\(1_{\\{S_T &gt; K\\}}\\) is \\(1\\) for all points \\(Z &lt; d_{-}\\). So, the limits of integration will be \\(-\\infty\\) to \\(d_{-}\\).\n\\[\n\\begin{align*}\nV(0) &= S_0\\left[\\int_{-\\infty}^{d_{-}} S_T  d\\mathbb{Q}^S - K\\int_{-\\infty}^{d_{-}} dQ^S\\right]\\\\\n&= S_0\\left[\\int_{-\\infty}^{d_{-}} S_T f_Z^{\\mathbb{Q}^S}(z)dz - K\\int_{-\\infty}^{d_{-}} f_Z^{\\mathbb{Q}^S}(z)dz\\right]\\\\\n&= S_0\\left[\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S_0 \\exp\\left[\\left(r + \\frac{\\sigma^2}{2}\\right)T - \\sigma \\sqrt{T}z\\right] \\exp(-\\frac{z^2}{2})dz - K\\Phi(d_{-})\\right]\n\\end{align*}\n\\]\nCompleting the square, we have:\n\\[\n\\begin{align*}\nV(0) &= S_0\\left[\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} S_0 \\exp\\left[-\\frac{1}{2}(z^2 + 2\\sigma\\sqrt{T}z + (\\sigma \\sqrt{T})^2)\\right] \\exp\\left[\\left(r + \\sigma^2\\right)T\\right]dz - K\\Phi(d_{-})\\right]\\\\\n&=S_0 \\left[S_0\\frac{e^{(r + \\sigma^2) T}}{\\sqrt{2\\pi}}\\int_{-\\infty}^{d_{-}} \\exp\\left(-\\frac{1}{2}(z + \\sigma\\sqrt{T})^2\\right) dz - K\\Phi(d_{-})\\right]\\\\\n\\end{align*}\n\\]\nLet \\(u = z + \\sigma\\sqrt{T}\\). And define:\n\\[\nd_{+} = d_{-} + \\sigma \\sqrt{T} = \\frac{\\log \\frac{S_0}{K} + \\left(r + \\frac{3}{2}\\sigma^2\\right)}{\\sigma \\sqrt{T}}\n\\]\nWe have the closed-form formula:\n\\[\nV(0) = S_0^2 e^{(r+\\sigma^2)T} \\Phi(d_{+}) - KS_0 \\Phi(d_{-})\n\\]"
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#value-metafunctions-1",
    "href": "posts/type-traits-and-conditional-compilation/index.html#value-metafunctions-1",
    "title": "Type Traits",
    "section": "Value metafunctions",
    "text": "Value metafunctions\nA value metafunction is kind of like a simple regular function. Let’s look at a simple regular function - the integer identity function.\nint int_identity(int x)\n{\n    return x;\n}\n\nassert(42 == int_identity(42));\nThis function just applies the identity transformation on any integer passed to it, and spits out the same number. A simple metafunction for identity - we can call it the intIdentity metafunction would look like this:\ntemplate&lt;int X&gt;\nstruct intIdentity{\n    static constexpr int value = X;\n};\n\nstatic_assert(42 == IntIdentity&lt;42&gt;::value);\nWe see that it’s not that much different. You return a value by having a static data-member called value and it has the metafunction’s return value. IntIdentity&lt;42&gt;::value is where we are calling the metafunction. Now, this convention needs to be adhered to, because if you give your metafunction some other name such as my_value, for example, if you write:\ntemplate&lt;int X&gt;\nstruct intIdentity{\n    static constexpr int my_value = X;\n};\n\nstatic_assert(42 == IntIdentity&lt;42&gt;::value);\nit’s not going to work well with other things.\n\nGeneric Identity Function\nLet’s look at the generic identity function.\ntemplate&lt;typename T&gt;\nT identity(T x){\n    return x;\n}\n\n//Returned type will be 42\nassert(42 == identity(42));\n\n// Returned type will be unsigned long long\nassert(42ul == identity(42ul))\nThis is just a function that will be an identity for any type. You give me a value of any type and I will give you that value back. Now we can create a generic identity metafunction as well:\ntemplate&lt;typename T, T x&gt;\nstruct ValueIdentity{\n    static constexpr T value = x;\n};\n\n// The type of value will be int\nstatic_assert(42 == identity&lt;int,42&gt;::value);\n\n// The type of value will unsigned long long\nstatic_assert(42ull == identity&lt;unsigned long long,42ull&gt;::value);\nValueIdentity is a generic metafunction, so we have to first feed it the type int and then the value 42. It’s a little cumbersome, but you get used to it after a while.\nIn C++17, things get a little bit easier with generic metafunctions, because we have this cool keyword called auto. I won’t go into all the details of auto. For now, it basically means that the template will accept and deduce the type of any non-type template parameter.\ntemplate&lt;auto X&gt;\nstruct ValueIdentity{\n    static constexpr auto value = X;\n};\n\n// The type of value will be int\nstatic_assert(42 == identity&lt;int,42&gt;::value);\n\n// The type of value will unsigned long long\nstatic_assert(42ull == identity&lt;unsigned long long,42ull&gt;::value);\nLet’s look at another function sum(). We can do this in a regular function, and we can do this in a metafunction as well.\nint sum(int x, int y){\n    return x + y;\n}\n\ntemplate&lt;int X, int Y&gt;\nstruct intSum{\n    static constexpr int value = X + Y;\n};\n\nstatic_assert(42 == IntSum&lt;30,12&gt;::value);\nSo, we can also create a generic version of this:\ntemplate&lt;typename X, typename Y&gt;\nauto sum(T x, Ty){\n    return x + y;\n}\n\ntemplate&lt;auto X, auto Y&gt;\nstruct Sum{\n    static constexpr auto value = X + Y;\n};"
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#type-metafunctions-1",
    "href": "posts/type-traits-and-conditional-compilation/index.html#type-metafunctions-1",
    "title": "Type Traits",
    "section": "Type metafunctions",
    "text": "Type metafunctions\nType metafunctions are the workhorse of doing type transformations. You can manipulate types through type metafunctions. Type *metafunctions are going to return just a type.\nHere’s our TypeIdentity function:\ntemplate&lt;typename T&gt;\nstruct TypeIdentity{\n    using type = T;\n}\nJust like we have ValueIdentity, where given any value, it’s going you the value back; we have TypeIdentity, where you give it any type, and it’s going to give you the type back.\nC++20 actually introduces std::type_identity, which is pretty much what we see above.\n\nCalling Type Metafunctions\nWhen we call a value metafunction, we can easily call the function:\nValueIdentity&lt;42&gt;::value\nValueIdentity is the metafunction, it’s passed the parameter 42 in the angle brackets (just like parentheses for a regular value function) and ::value is how I get it’s value back.\nWhen I call a type metafunction, it’s the same way. The function call consists of the metafunction name std::type_identity, the parameters to the metafunction in angle brackets (&lt;42&gt;) and ::type is how I get it’s value back.\n#include &lt;iostream&gt;\n#include &lt;type_traits&gt;\n\nint main()\n{\n    using T = std::type_identity&lt;int&gt;::type;\n    return 0;\n}\nCompiler Explorer"
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#understanding-name-binding-and-dependent-types-1",
    "href": "posts/type-traits-and-conditional-compilation/index.html#understanding-name-binding-and-dependent-types-1",
    "title": "Type Traits",
    "section": "Understanding name binding and dependent types",
    "text": "Understanding name binding and dependent types\nName binding is the process of establishing, determining explicitly the type of each name (declaration) in a template. There are two kinds of names used within a template: dependent and non-dependent names. Names that depend on a template parameter are called dependent names.\n\nFor dependent names, name binding is performed at the point of template instantiation.\nFor non-dependent names, name binding is performed at the point of template definition.\n\nConsider the following C++ code:\n// Ref: Template metaprogramming with C++\n// Mariusz Bancilla, pages 123 \n#include &lt;iostream&gt;\n\ntemplate&lt;typename T&gt;\nstruct handler{\n    void handle(T value)    //[1] handle is a dependent name\n    {\n        std::cout &lt;&lt; \"handler&lt;T&gt;: \" &lt;&lt; value &lt;&lt; '\\n';\n    }\n};\n\ntemplate&lt;typename T&gt;\nstruct parser{\n    void parse(T arg){      //[2] parse is a dependent name\n        arg.handle(x);\n    }\n\n    double x;                  //[3] x is a non-dependent name\n};\n\nint main(){\n    handler&lt;double&gt; doubleHandler;                //[5] template instantiation\n    parser&lt;handler&lt;double&gt;&gt; doubleParser(3.14);   //[6] template instantiation\n    doubleParser.parse(doubleHandler);\n    return 0;\n}\nWhen the compiler sees dependent names, e.g. at points [1] and [2], it cannot determine the type-signature of these functions. So, parse() and handle() are not bound at this point.\nThe declaration double x at point [3] declares a non-dependent type. So, the type of the variable x is known and bound.\nContinuing with the code, at point [4], there is a template specialization for the handler class template for the type int.\nTemplate instantiation happens at points [5] and [6]. At point [5], handler&lt;double&gt;::handle is bound to handle and at point [6], parser&lt;handler&lt;double&gt;&gt;::parse is bound to parse.\n\nTwo-phase name lookup\nName binding happens differently for dependent names (those that depend on a template parameter) and non-dependent names. When the compiler passes throuh the definition of a template, it needs to figure out whether a name is dependent or non-dependent. Further, name binding depends upon this categorization. Thus, the instantiation of a template happens in 2-phases.\n\nThe first phase occurs at the point of the definition when the template syntax is checked and the names are categorized as dependent or non-dependent.\nThe second phase occurs at the point of template instantiation when the template arguments are substituted for the template parameters. Name binding for dependent names happens at this point.\n\nThis process in two steps is called the two-phase name lookup.\nConsider the following C++ code:\n// Ref: Template metaprogramming with C++\n// Mariusz Bancilla, pages 125\ntemplate&lt;typename T&gt;\nstruct base_parser\n{\n    void init()                     // [1] non-dependent name\n    {\n        std::cout &lt;&lt; \"init\\n\";\n    }\n};\n\ntemplate&lt;typename T&gt;\nstruct parser::public base_parser&lt;T&gt;\n{\n    void parse(){                  // [2] non-dependent name\n        // The compiler at [3] will try to bind init(), as it's a non-dependent name.\n        // However, base_parser has not yet been instantiated. This will result in a \n        // compile-error\n        //init();                  // [3] non-dependent name\n        std::cout &lt;&lt; \"parse\\n\";\n    }\n};\n\nint main(){\n    parser&lt;double&gt; p;\n    p.parse();\n}\nThe call to init() inside the parse() member function has been commented out. Uncommenting it will cause a compile error.\nThe intention here is to call the base-class init() function. However, the compiler will issue an error, because it’s not able to find init(). The reason is that init() is a non-dependent name. Therefore, it must be bound at the time of the definition of the parser template. Although, base_parser&lt;T&gt;::init() exists, this template has still not been instantiated. The compiler cannot assume its what we want to call, because the primary template base_parser can always be specialized and init() can be defined as something else.\nThis problem can be fixed, by making init a dependent name. This can be done by either prefixing it with this-&gt; or with base_parser&lt;T&gt;::.\n\n\nDependent type names\nThere are cases where a dependent name is a type. Consider the following C++ code:\n#include &lt;iostream&gt;\n\ntemplate&lt;typename T&gt;\nstruct base_parser{\n    using value_type = T;\n};\n\ntemplate&lt;typename T&gt;\nstruct parser: base_parser&lt;T&gt;{\n    void parse(){\n        // value_type v{};  // [1] : Error\n        // base_parser&lt;T&gt;::value_type v{};  //[2] :Error\n        std::cout &lt;&lt; \"parse\\n\";\n    }\n};\n\nint main()\n{\n    parser&lt;double&gt; p;\n    p.parse();\n    return 0;\n}\nCompiler Explorer\nIn this code snippet, the metafunction base_parser is an identity metafunction and returns back the type you give it. base_parser&lt;T&gt;::value_type is actually a dependent type, which depends on the template parameter T. At point [1] and [2], the compiler does not know what T will be. If it attempts to bind the name v, it will fail. We need to tell the compiler explicitly base_parser&lt;T&gt;::value_type is a dependent type. You do that using the typename keyword.\n#include &lt;iostream&gt;\n\ntemplate&lt;typename T&gt;\nstruct base_parser{\n    using value_type = T;\n};\n\ntemplate&lt;typename T&gt;\nstruct parser: base_parser&lt;T&gt;{\n    void parse(){\n        // value_type v{};  // [1] : Error\n        // base_parser&lt;T&gt;::value_type v{};  //[2] :Error\n        typename base_parser&lt;T&gt;::value_type v{};  //[3] :Ok\n        std::cout &lt;&lt; \"parse\\n\";\n    }\n};\n\nint main()\n{\n    parser&lt;double&gt; p;\n    p.parse();\n    return 0;\n}\nSo, any time, when calling a type metafunction, if the compiler does not know what ::type is, you must prefix it using the typename keyword, if we want to treat it as a type."
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#convenience-calling-functions-1",
    "href": "posts/type-traits-and-conditional-compilation/index.html#convenience-calling-functions-1",
    "title": "Type Traits",
    "section": "Convenience calling functions",
    "text": "Convenience calling functions\nValue metafunctions often use helper functions (variable templates) ending with _v. For example, we often define the helper function:\ntemplate &lt;auto X&gt;\ninline constexpr auto ValueIdentity_v = ValueIdentity&lt;X&gt;::value;\n\nstatic_assert(42 == ValueIdentity&lt;42&gt;::value);\nstatic_assert(42 == ValueIdentity_v&lt;42&gt;)\nWe are just calling the ValueIdentity&lt;&gt; metafunction, grabbing its value and storing it into this variable ValueIdentity_v. This is a convenient way of calling value metafunctions. It does require you to instantiate an extra variable template.\nIt’s really helpful when we start using it with types. Type metafunctions use alias templates ending with _t. It helps us get rid of the entire typename dance.\ntemplate &lt;typename T&gt;\nusing TypeIdentity_t = typename TypeIdentity&lt;T&gt;::type;\n\nstatic_assert(std::is_same_v&lt;int, TypeIdentity_t&lt;int&gt;);\nInstead of calling the TypeIdentity metafunction with the parameter int and writing ::type to get its value, I can just call the metafunction with _t and with its parameters in angle brackets(&lt;&gt;).\nThese calling conventions are easier to use. But each one must be explicitly hand-written. So, every time you write a metafunction, if you want to provide convenience capabilities, you also have to write the convenience variable template or alias template."
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#useful-metafunctions-to-think-of-1",
    "href": "posts/type-traits-and-conditional-compilation/index.html#useful-metafunctions-to-think-of-1",
    "title": "Type Traits",
    "section": "Useful metafunctions to think of",
    "text": "Useful metafunctions to think of\nHow is std::remove_pointer metafunction implemented? You can intuitively come up with what it must look like:\ntemplate&lt;typename T&gt;\nstruct remove_pointer{\n    using type = T;\n};\n\n// template specialization\ntemplate&lt;typename T&gt;\nstruct remove_pointer&lt;T*&gt;{\n    using type = T;\n};\nIf the std::remove_pointer metafunction receives int* as a parameter, the specialized version of the template kicks in, as int* is matched against T*. Here is the full implementation of std::remove_pointer&lt;T&gt;:\ntemplate &lt;class T&gt; struct remove_pointer                    { using type = T };\ntemplate &lt;class T&gt; struct remove_pointer&lt;T*&gt;                { using type = T };\ntemplate &lt;class T&gt; struct remove_pointer&lt;T* const&gt;          { using type = T };\ntemplate &lt;class T&gt; struct remove_pointer&lt;T* volatile&gt;       { using type = T };\ntemplate &lt;class T&gt; struct remove_pointer&lt;T* const volatile&gt; { using type = T };\nAs you can see, the same technique is applied to more subtle edge cases.\nHow about std::remove_reference?\ntemplate&lt;typename T&gt;\nstruct remove_reference{\n    using type = T;\n};\n\n// template specialization\ntemplate&lt;typename T&gt;\nstruct remove_reference&lt;T&&gt;{\n    using type = T;\n};\n\n// template specialization\ntemplate&lt;typename T&gt;\nstruct remove_reference&lt;T&&&gt;{\n    using type = T;\n};\nBut, removing qualifiers from types is only the tip of the iceberg. Take for instance, a jewel for type manipulation std::decay."
  },
  {
    "objectID": "posts/type-traits-and-conditional-compilation/index.html#stdintegral_constant---a-very-useful-metafunction-1",
    "href": "posts/type-traits-and-conditional-compilation/index.html#stdintegral_constant---a-very-useful-metafunction-1",
    "title": "Type Traits",
    "section": "std::integral_constant - a very useful metafunction",
    "text": "std::integral_constant - a very useful metafunction\nThis is probably the most useful metafunction. This is what std::integral_constant looks like:\ntemplate&lt;class T, T v&gt;\nstruct integral_constant{\n    static constexpr T value = v;\n\n    using value_type = T;\n    using type = integral_constant&lt;T, v&gt;\n    \n    constexpr operator value_type() const noexcept{     //[1] : Implicit integral cast\n        return value;\n    }\n\n    constexpr operator value_type()() const noexcept{\n        return value;\n    }\n};\nintegral_constant is just a class, it has no data. Its just a wrapper over a compile-time constant of a specified type.\nIt’s got a value, so its a value metafunction. But, it’s also got type, so its a type metafunction as well. In fact, its a type metafunction which is an identity metafunction. It will tell exactly what the integral_constant thing is. It also has the value_type.\n#include &lt;iostream&gt;\n#include &lt;type_traits&gt;\n\ntemplate&lt;typename T&gt;\nstruct is_void{\n    using type = std::false_type;\n};\n\ntemplate&lt;&gt;\nstruct is_void&lt;void&gt;{\n    using type = std::true_type;\n};\n\nint main(){\n    static_assert(std::is_same&lt;is_void&lt;void&gt;::type,std::true_type&gt;::value);\n    static_assert(std::is_same&lt;is_void&lt;int&gt;::type,std::false_type&gt;::value);\n    return 0;\n}\n\n:::"
  },
  {
    "objectID": "posts/fun-with-numeraires/index.html#introduction",
    "href": "posts/fun-with-numeraires/index.html#introduction",
    "title": "Fun with numeraires!",
    "section": "",
    "text": "A proficiency in the change-of-measure technique is useful to the working quant. An excellent summary of the important results is the note Girsanov, Numeraires and all that, by Andrew Lesniewski. In this post, I would like to derive relevant results and then we can enjoy pricing some payoffs together!"
  },
  {
    "objectID": "posts/fun-with-numeraires/index.html#girsanov-theorem",
    "href": "posts/fun-with-numeraires/index.html#girsanov-theorem",
    "title": "Fun with numeraires!",
    "section": "Girsanov Theorem",
    "text": "Girsanov Theorem\n\nTheorem 1 (Girsanov Theorem) Let \\((W^{\\mathbb{P}},t\\geq 0)\\) be a \\(\\mathbb{P}\\) standard brownian motion on \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) and let \\(\\phi\\) be any adapted process. Choose fixed \\(T\\) and defined the process \\(L\\) on \\([0,T]\\) by:\n\\[\n\\begin{align*}\ndL_t = \\phi_t L_t dW^{\\mathbb{P}}_t\n\\end{align*}\n\\tag{1}\\]\n\\[\n\\begin{align*}\nL_0 = 1\n\\end{align*}\n\\tag{2}\\]\nthat is:\n\\[\n\\begin{align*}\nL_t = \\exp\\left(\\int_0^t \\phi_s dW^{\\mathbb{P}}_s - \\int_0^t \\phi^2_s ds\\right)\n\\end{align*}\n\\tag{3}\\]\nAssume that:\n\\[\n\\begin{align*}\n\\mathbb{E}^{\\mathbb{P}}[L_T] = 1\n\\end{align*}\n\\tag{4}\\]\nand define the new probability measure \\(\\mathbb{Q}\\) on \\(\\mathcal{F}_t\\) by:\n\\[\nL_T = \\frac{d\\mathbb{Q}}{d\\mathbb{P}}\n\\tag{5}\\]\nThen,\n\\[\ndW^{\\mathbb{P}}_t = dW^{\\mathbb{Q}}_t + \\phi_t dt\n\\tag{6}\\]\nwhere \\(dW^{\\mathbb{Q}}_t\\) is a \\(\\mathbb{Q}\\)-standard brownian motion.\n\nProof.\nOur claim is that, under the \\(\\mathbb{Q}\\) measure, the increments \\((W^{\\mathbb{Q}}_t - W^{\\mathbb{Q}}_s)\\) are normally distributed with mean \\(0\\) and variance \\((t-s)\\). We start with the special case \\(s=0\\). Using moment generating functions, it is enough to show that:\nIt is straightforward to derive Equation 3 using Ito’s lemma. Let \\(f(x) = \\ln x\\). Then, \\(f_x = -\\frac{1}{x}\\), \\(f_{xx} = -\\frac{1}{x^2}\\).\n\\[\n\\begin{align*}\nd(\\ln L_t) &= -\\frac{1}{L_t}dL_t + \\frac{1}{L_t^2}(dL_t)^2 \\\\\n&= -\\frac{1}{L_t}{\\phi_t L_t dW^{\\mathbb{P}}_t} + \\frac{1}{L_t^2}\\phi_t^2 L_t^2 dt\\\\\n&= - \\phi_t dW^{\\mathbb{P}}_t + \\phi_t^2 dt \\\\\nL_t &= \\exp\\left(-\\int_0^t \\phi_s dW^{\\mathbb{P}}_s + \\frac{1}{2}\\int_0^t \\phi_s^2 ds \\right)\n\\end{align*}\n\\]\nTo prove our main result, we will now use the MGF of the increments. For \\(n \\in \\mathbb{N}\\) and \\((t_j,j\\leq n)\\) a partition of \\([0,T]\\), with \\(t_n = T\\), I will show that :\n\\[\n\\begin{align*}\n\\mathbb{E}^{\\mathbb{Q}}\\left([\\exp\\left(\\sum_{j=0}^{n-1}\\lambda_j (W^{\\mathbb{Q}}_{t_{j+1}} - W^{\\mathbb{Q}}_{t_{j}})\\right)\\right] = \\exp\\left(\\lambda_j^2(t_{j+1}-t_j)\\right)\n\\end{align*}\n\\tag{7}\\]\nThis proves that the increments are the ones of standard brownian motion.\nLet \\((\\mathcal{F}_{t_j},j\\leq n)\\) be the filtrations of the Brownian motion at the time of the partition. The proof is by successively conditioning from \\(t_{n-1}\\) to \\(t_1\\). We have:\n\\[\n\\begin{aligned}\n\\mathbb{E}^{\\mathbb{Q}}\\left[\\exp\\left(\\sum _{j=0}^{n-1} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\right)\\right] & =\\mathbb{E}^{\\mathbb{P}}\\left[\\mathbb{E}^{\\mathbb{P}}\\left[ M_{t_{n}}\\exp\\left(\\sum _{j=0}^{n-1} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\right) |\\mathcal{F}_{t_{n-1}}\\right]\\right]\\\\\n& =\\mathbb{E}^{\\mathbb{P}}\\left[\\sum _{j=0}^{n-2} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\mathbb{E}^{\\mathbb{P}}\\left[ M_{t_{n}}\\exp\\left( \\lambda _{n-1} (W_{t_{n}}^{\\mathbb{Q}} -W_{t_{n-1}}^{\\mathbb{Q}} )\\right) |\\mathcal{F}_{t_{n-1}}\\right]\\right]\\\\\n& =\\mathbb{E}^{\\mathbb{P}}\\left[\\sum _{j=0}^{n-2} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\mathbb{E}^{\\mathbb{P}}\\left[\\exp\\left( -\\int _{t_{n-1}}^{t_{n}} \\theta dW_{s}^{\\mathbb{Q}} +\\frac{1}{2}\\int _{t_{n-1}}^{t_{n}} \\theta _{s}^{2} ds\\right)\\exp\\left( \\lambda _{n-1}\\int _{t_{n-1}}^{t_{n}} dW_{s}^{\\mathbb{Q}}\\right) |\\mathcal{F}_{t_{n-1}}\\right]\\right]\\\\\n& =\\mathbb{E}^{\\mathbb{P}}\\left[\\sum _{j=0}^{n-2} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\mathbb{E}^{\\mathbb{P}}\\left[\\exp\\left(\\int _{t_{n-1}}^{t_{n}}( -\\theta _{s} +\\lambda _{n-1}) dW_{s}^{\\mathbb{Q}} +\\frac{1}{2}\\int _{t_{n-1}}^{t_{n}} \\theta _{s}^{2} ds\\right) |\\mathcal{F}_{t_{n-1}}\\right]\\right]\\\\\n& =\\mathbb{E}^{\\mathbb{P}}\\left[\\sum _{j=0}^{n-2} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\mathbb{E}^{\\mathbb{P}}\\left[\\exp\\left(\\int _{t_{n-1}}^{t_{n}}( -\\theta _{s} +\\lambda _{n-1})\\left( dW_{s}^{\\mathbb{P}} +\\theta ds\\right) +\\frac{1}{2}\\int _{t_{n-1}}^{t_{n}} \\theta _{s}^{2} ds\\right) |\\mathcal{F}_{t_{n-1}}\\right]\\right]\\\\\n& =\\mathbb{E}^{\\mathbb{P}}\\left[\\sum _{j=0}^{n-2} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\exp\\left(\\frac{1}{2}\\int _{t_{n-1}}^{t_{n}} \\theta _{s}^{2} +\\int _{t_{n-1}}^{t_{n}} \\theta _{s}( -\\theta _{s} +\\lambda _{n-1}) ds\\right) \\ \\exp\\left(\\frac{1}{2}\\int _{t_{n-1}}^{t_{n}}( -\\theta _{s} +\\lambda _{n-1})^{2} ds\\right)\\right]\\\\\n& \\left\\{\\ \\int XdW_{s}^{\\mathbb{P}} \\ \\text{ is a }\\mathcal{N}^{\\mathbb{P}}\\left( 0,\\int \\mathbb{E}\\left[ X^{2}\\right] ds\\right) \\ \\text{gaussian random variable.}\\right\\} \\ \\\\\n& =\\mathbb{E}^{\\mathbb{P}}\\left[\\sum _{j=0}^{n-2} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\exp\\left(\\int _{t_{n-1}}^{t_{n}}\\left( -\\frac{1}{2} \\theta _{s}^{2} +\\lambda _{n-1} \\theta _{s} +\\frac{1}{2} \\theta _{s}^{2} -\\lambda _{n-1} \\theta +\\lambda _{n-1}^{2}\\right) ds\\right)\\right]\\\\\n& =\\mathbb{E}^{\\mathbb{P}}\\left[\\sum _{j=0}^{n-2} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\exp\\left( \\lambda _{n-1}\\int _{t_{n-1}}^{t_{n}} ds\\right)\\right]\\\\\n& =\\exp( \\lambda _{n-1}( t_{n} -t_{n-1})) \\cdot \\mathbb{E}^{\\mathbb{P}}\\left[\\sum _{j=0}^{n-2} \\lambda _{j} (W_{t_{j+1}}^{\\mathbb{Q}} -W_{t_{j}}^{\\mathbb{Q}} )\\right]\n\\end{aligned}\n\\]\nHere, I used the fact that \\(M_{t_{n-1}}\\) is \\(\\mathcal{F}_{t_{n-1}}\\) measurable. I can now condition on \\(\\mathcal{F}_{t_{n-2}}\\) down to \\(\\mathcal{F}_{t_1}\\) and proceed as above to obtain the desired result.\nThe process \\(\\phi_t\\) is called the Girsanov kernel."
  },
  {
    "objectID": "posts/implementing-vanna-volga/index.html",
    "href": "posts/implementing-vanna-volga/index.html",
    "title": "Implementing Vanna Volga",
    "section": "",
    "text": "It is possible to calculate analytically the values of vanillas or barrier options using the Black-Scholes model, however, they are far from quoted prices. This is because the BS-model is based on the assumption that the volatility \\(\\sigma\\) of the stock price process remains constant throughout the lifetime of the option.\nThe vanna-volga method also known as the trader’s rule of thumb is based on adding an analytical correction to the Black-Scholes price of the instrument. In this note, I derive and implement the original paper The vanna-volga method for implied volatilities, by Castagna and Mercurio."
  },
  {
    "objectID": "posts/implementing-vanna-volga/index.html#background.",
    "href": "posts/implementing-vanna-volga/index.html#background.",
    "title": "Implementing Vanna Volga",
    "section": "",
    "text": "It is possible to calculate analytically the values of vanillas or barrier options using the Black-Scholes model, however, they are far from quoted prices. This is because the BS-model is based on the assumption that the volatility \\(\\sigma\\) of the stock price process remains constant throughout the lifetime of the option.\nThe vanna-volga method also known as the trader’s rule of thumb is based on adding an analytical correction to the Black-Scholes price of the instrument. In this note, I derive and implement the original paper The vanna-volga method for implied volatilities, by Castagna and Mercurio."
  },
  {
    "objectID": "posts/implementing-vanna-volga/index.html#replicating-portfolio.",
    "href": "posts/implementing-vanna-volga/index.html#replicating-portfolio.",
    "title": "Implementing Vanna Volga",
    "section": "Replicating Portfolio.",
    "text": "Replicating Portfolio.\nConsider a Black-Scholes world with two assets : a locally risk free domestic bank account \\(B(t)\\) and a stock \\(S(t)\\). We assume that the volatility of the stock is stochastic, but strike-independent(flat). We have the asset dynamics:\n\\[\n\\begin{align*}\ndB_t &= r_{DOM}B_t dt \\\\\\\\\ndS_t &= \\mu S_t dt + \\sigma_t S_t dW_t\n\\end{align*}\n\\]\nOur aim is to value an arbitrary option contract \\(O=f(t,S_t,\\sigma_t;K)\\) with a strike \\(K\\). We price \\(O\\) using a standard hedging argument. We build a hedge aka replicating portfolio such that it zeroes out the greeks of our net position upto the second order.\nConsider a self-financing portfolio \\(\\Pi_t\\) consisting of:\n\nA long position in \\(1\\) unit of the option \\(O(t;K)\\).\nA short position in \\(\\Delta_t\\) units of the stock \\(S_t\\).\nShort positions in three European vanilla pivot options \\(C_i\\), \\(i \\in \\{1,2,3\\}\\). We short \\(x_i\\) units of \\(C_i\\). It is standard practice, to take \\(C_1,C_2,C_3\\) as a 25-delta put, an ATM call and a 25-delta call option respectively.\n\nThe pivot options have strikes \\(K_1 = K_{25P}\\), \\(K_2 = K_{ATM}\\) and \\(K_3 = K_{25C}\\) and implied volatility quotes (market prices) \\(\\sigma_1\\), \\(\\sigma_2\\) and \\(\\sigma_3\\) which are known to us.\nThe value of the portfolio at time \\(t\\) is:\n\\[\\Pi_t = O_t - \\Delta_t S_t - \\sum_{i=1}^{3} x_i C_t^i \\tag{1}\\]\nBy self-financing, I mean, there is no exogenous infusion or withdrawal of cash, once the portfolio has been setup at time zero. Therefore, the changes in the portfolio are solely due to gains/losses on the constituents. The self-financing condition is:\n\\[d\\Pi_t = dO_t - \\Delta_t dS_t - \\sum_{i=1}^{3} x_i dC_t^i \\tag{2}\\]\nBy Ito’s lemma, the differential of the option price \\(O_t\\) can be written as:\n\\[\n\\begin{align*}\ndO_t &= \\frac{\\partial O}{\\partial t} dt + \\frac{\\partial O}{\\partial S_t} dS_t + \\frac{\\partial O}{\\partial \\sigma_t} d\\sigma_t \\\\\\\\\n&+ \\frac{1}{2}\\frac{\\partial^2 O}{\\partial t^2} (dt)^2 \\\\\\\\\n&+ \\frac{1}{2}\\frac{\\partial^2 O}{\\partial S_t^2} (dS_t)^2 \\\\\\\\\n&+ \\frac{1}{2}\\frac{\\partial^2 O}{\\partial \\sigma_t^2} (d\\sigma_t)^2 \\\\\\\\\n&+ \\frac{\\partial^2 O}{\\partial t \\partial S_t} dt \\cdot dS_t \\\\\\\\\n&+ \\frac{\\partial^2 O}{\\partial S_t \\partial \\sigma_t} dS_t \\cdot d\\sigma_t \\\\\\\\\n&+ \\frac{\\partial^2 O}{\\partial t \\partial \\sigma_t} dt \\cdot d\\sigma_t \\tag{3}\n\\end{align*}\n\\]\nSince \\((dt)^2\\), \\(dt \\cdot dS_t\\), \\(dt \\cdot d\\sigma_t\\) are equal to zero, we can write:\n\\[\n\\begin{align*}\ndO_t &= \\frac{\\partial O}{\\partial t} dt + \\frac{\\partial O}{\\partial S_t} dS_t + \\frac{\\partial O}{\\partial \\sigma_t} d\\sigma_t \\\\\\\\\n&+ \\frac{1}{2}\\frac{\\partial^2 O}{\\partial S_t^2} (dS_t)^2 \\\\\\\\\n&+ \\frac{1}{2}\\frac{\\partial^2 O}{\\partial \\sigma_t^2} (d\\sigma_t)^2 \\\\\\\\\n&+ \\frac{\\partial^2 O}{\\partial S_t \\partial \\sigma_t} dS_t \\cdot d\\sigma_t  \\tag{4}\n\\end{align*}\n\\]\nSimilarly, we can apply Ito’s lemma to the European vanilla pivot options to find the differential \\(dC^i_t\\). Putting it together we have:\n\\[\n\\begin{align*}\nd\\Pi_t &= \\left(\\frac{\\partial O(t;K)}{\\partial t} - \\sum_{i=1}^{3}x_i\\frac{\\partial C^i(t;K_i)}{\\partial t} \\right) dt  \\\\\\\\\n&+ \\left(\\frac{\\partial O(t;K)}{\\partial S_t}  - \\Delta_t - \\sum_{i=1}^{3}x_i\\frac{\\partial C^i(t;K_i)}{\\partial S_t}\\right) dS_t \\\\\\\\\n&+ \\left(\\frac{\\partial O(t;K)}{\\partial \\sigma_t} - \\sum_{i=1}^{3}x_i\\frac{\\partial C^i(t;K_i)}{\\partial \\sigma_t} \\right)d\\sigma_t\\\\\\\\\n&+ \\frac{1}{2}\\left(\\frac{\\partial^2 O(t;K)}{\\partial S_t^2} - \\sum_{i=1}^{3}x_i\\frac{\\partial^2 C^i(t;K_i)}{\\partial S_t^2}\\right)(dS_t)^2 \\\\\\\\\n&+ \\frac{1}{2}\\left(\\frac{\\partial^2 O(t;K)}{\\partial \\sigma_t^2}  - \\sum_{i=1}^{3}x_i\\frac{\\partial^2 C^i(t;K_i)}{\\partial \\sigma_t^2} \\right)(d\\sigma_t)^2 \\\\\\\\\n&+ \\left(\\frac{\\partial^2 O(t;K)}{\\partial S_t \\partial \\sigma_t} - \\sum_{i=1}^{3}x_i\\frac{\\partial^2 C^i(t;K_i)}{\\partial S_t \\partial \\sigma_t}\\right)  dS_t \\cdot d\\sigma_t  \\tag{5}\n\\end{align*}\n\\]\nWe claim that we can choose the weights \\(\\Delta_t\\) and \\(\\mathbf{x}=(x_1,x_2,x_3)\\) of the replicating portfolio, such that the coefficient of the terms \\(dS_t\\), \\(d\\sigma_t\\), \\((d\\sigma_t)^2\\) and \\(dS_t \\cdot d\\sigma_t\\) are zeroed out.\nWe are therefore left with:\n\\[\n\\begin{align*}\nd\\Pi_t &= \\left(\\frac{\\partial O}{\\partial t} - \\sum_{i=1}^{3}x_i\\frac{\\partial C^i_t}{\\partial t} \\right) dt \\\\\\\\\n&+ \\frac{1}{2}\\left(\\frac{\\partial^2 O}{\\partial S_t^2} - \\sum_{i=1}^{3}x_i\\frac{\\partial^2 C^i_t}{\\partial S_t^2}\\right)\\sigma_t^2 S_t^2 dt \\tag{6}\n\\end{align*}\n\\]\nThe portfolio value process has no driving Brownian motion \\(dW_t\\) term, and hence the source of randomness has been eliminated. Therefore, \\(\\Pi_t\\) must be a locally risk-free portfolio. That is, it satisfies:\n\\[d\\Pi_t = r_{DOM}\\Pi_t dt \\tag{7}\\]"
  },
  {
    "objectID": "posts/implementing-vanna-volga/index.html#calculating-the-vv-weights",
    "href": "posts/implementing-vanna-volga/index.html#calculating-the-vv-weights",
    "title": "Implementing Vanna Volga",
    "section": "Calculating the VV weights",
    "text": "Calculating the VV weights\nWe assume hereafter, that the constant BS volatility is the at-the-money one; \\(\\sigma = \\sigma_2 = \\sigma_{ATM}\\). We assume \\(t=0\\), so we can drop the argument \\(t\\) in the call prices \\(C_i(t;K)\\) in equation (5). The weights \\(\\mathbf{x}=(x_1,x_2,x_3)\\) are determined by solving the system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) where:\n\\[\nA = \\begin{bmatrix}\n\\frac{\\partial C_1(K_1)}{\\partial \\sigma_t} & \\frac{\\partial C_1(K_2)}{\\partial \\sigma_t} &  \\frac{\\partial C_3(K_3)}{\\partial \\sigma_t} \\\\\\\\\n\\frac{\\partial^2 C_1(K_1)}{\\partial S_t \\partial \\sigma_t} & \\frac{\\partial^2 C_2(K_2)}{\\partial S_t \\partial \\sigma_t} & \\frac{\\partial^2 C_3(K_3)}{\\partial S_t \\partial \\sigma_t}\\\\\\\\\n\\frac{\\partial^2 C_1(K_1)}{\\partial \\sigma_t^2} & \\frac{\\partial^2 C_2(K_2)}{\\partial \\sigma_t^2} & \\frac{\\partial^2 C_3(K_3)}{\\partial \\sigma_t^2}\n\\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix}\n\\frac{\\partial O(K)}{\\partial \\sigma_t} \\\\\\\\\n\\frac{\\partial^2 O(K)}{\\partial S_t \\partial \\sigma_t} \\\\\\\\\n\\frac{\\partial^2 O(K)}{\\partial \\sigma_t^2}\n\\end{bmatrix}\n\\]\nThe entries in the first, second and third rows of \\(A\\) and \\(\\mathbf{b}\\) are the option vega, the option vanna and the option volga.\nI derived the expressions for option vega, vanna and volga here. They are:\n\\[\n\\begin{align*}\n\\text{Vega} &= S_0 e^{-r_{FOR}T} \\phi(d_{+}) \\sqrt{T} \\\\\\\\\n\\text{Vanna} &= -e^{-r_{FOR}T} \\phi(d_{+})\\frac{d_{-}}{\\sigma}\\\\\\\\\n\\text{Volga} &= S_0 e^{-r_{FOR}T}\\sqrt{T}\\phi(d_{+}) \\frac{d_{+}d_{-}}{\\sigma}\n\\end{align*}\n\\]\nWe can re-phrase the other greeks in terms of vega \\(\\mathcal{V}\\). Recall, that \\(d_{+}\\) varies with the option strike \\(K\\), so all other things equal, we can write \\(\\mathcal{V} = \\mathcal{V}(K)\\).\n\\[\n\\begin{align*}\n\\text{Vanna} &= -\\frac{d_{-}}{\\sigma S_0 \\sqrt{T}} \\mathcal{V}(K)\\\\\\\\\n\\text{Volga} &= \\frac{d_{+}d_{-}}{\\sigma} \\mathcal{V}(K)\n\\end{align*}\n\\]\nThe augmented matrix \\([A | b]\\), therefore is:\n\\[\n\\begin{bmatrix}\n\\mathcal{V}(K_1) & \\mathcal{V}(K_2) &  \\mathcal{V}(K_3) & | & \\mathcal{V}(K)\\\\\\\\\n-\\frac{d_{-}(K_1)}{\\sigma S_0 \\sqrt{T}}\\mathcal{V}(K_1) & -\\frac{d_{-}(K_2)}{\\sigma S_0 \\sqrt{T}}\\mathcal{V}(K_2) & -\\frac{d_{-}(K_3)}{\\sigma S_0 \\sqrt{T}}\\mathcal{V}(K_3) & | &\\frac{d_{-}(K)}{\\sigma S_0 \\sqrt{T}}\\mathcal{V}(K)\\\\\\\\\n\\frac{d_{+}(K_1) d_{-}(K_1)}{\\sigma}\\mathcal{V}(K_1) & \\frac{d_{+}(K_2) d_{-}(K_2)}{\\sigma}\\mathcal{V}(K_2) & \\frac{d_{+}(K_3) d_{-}(K_3)}{\\sigma}\\mathcal{V}(K_3) & | & \\frac{d_{+}(K) d_{-}(K)}{\\sigma}\\mathcal{V}(K)\n\\end{bmatrix}\n\\]\nCancelling out the constant terms, we get:\n\\[\n\\begin{bmatrix}\n\\mathcal{V}(K_1) & \\mathcal{V}(K_2) &  \\mathcal{V}(K_3) & | & \\mathcal{V}(K)\\\\\\\\\nd_{-}(K_1)\\mathcal{V}(K_1) & d_{-}(K_2)\\mathcal{V}(K_2) & d_{-}(K_3)\\mathcal{V}(K_3) & | & d_{-}(K)\\mathcal{V}(K)\\\\\\\\\nd_{+}(K_1) d_{-}(K_1)\\mathcal{V}(K_1) & d_{+}(K_2) d_{-}(K_2)\\mathcal{V}(K_2) & d_{+}(K_3) d_{-}(K_3)\\mathcal{V}(K_3) & | & d_{+}(K) d_{-}(K)\\mathcal{V}(K)\n\\end{bmatrix}\n\\]\nPerforming the elementary row operation \\(R_3 \\leftarrow R_3 - d_{+}(K_1) R_2\\), we get:\n\\[\n\\begin{bmatrix}\n\\mathcal{V}(K_1) & \\mathcal{V}(K_2) &  \\mathcal{V}(K_3) & | & \\mathcal{V}(K)\\\\\\\\\nd_{-}(K_1)\\mathcal{V}(K_1) & d_{-}(K_2)\\mathcal{V}(K_2) & d_{-}(K_3)\\mathcal{V}(K_3) & | & d_{-}(K)\\mathcal{V}(K)\\\\\\\\\n0 & \\log\\frac{K_2}{K_1} d_{-}(K_2)\\mathcal{V}(K_2) & \\log \\frac{K_3}{K_1} d_{-}(K_3)\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1} d_{-}(K)\\mathcal{V}(K)\n\\end{bmatrix}\n\\]\nPerforming the elementary row operation \\(R_2 \\leftarrow R_2 - d_{-}(K_1)R_1\\), we get:\n\\[\n\\begin{bmatrix}\n\\mathcal{V}(K_1) & \\mathcal{V}(K_2) &  \\mathcal{V}(K_3) & | & \\mathcal{V}(K)\\\\\\\\\n0 & \\log\\frac{K_2}{K_1}\\mathcal{V}(K_2) & \\log \\frac{K_3}{K_1}\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1}\\mathcal{V}(K)\\\\\\\\\n0 & \\log\\frac{K_2}{K_1} d_{-}(K_2)\\mathcal{V}(K_2) & \\log \\frac{K_3}{K_1} d_{-}(K_3)\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1} d_{-}(K)\\mathcal{V}(K)\n\\end{bmatrix}\n\\]\nPerforming the elementary row operation \\(R_3 \\leftarrow R_3 - d_{-}(K_2) R_2\\), we get:\n\\[\n\\begin{bmatrix}\n\\mathcal{V}(K_1) & \\mathcal{V}(K_2) &  \\mathcal{V}(K_3) & | & \\mathcal{V}(K)\\\\\\\\\n0 & \\log\\frac{K_2}{K_1}\\mathcal{V}(K_2) & \\log \\frac{K_3}{K_1}\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1}\\mathcal{V}(K)\\\\\\\\\n0 & 0 & \\log \\frac{K_3}{K_1} \\frac{K_3}{K_2}\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1} \\log \\frac{K}{K_2}\\mathcal{V}(K)\n\\end{bmatrix}\n\\]\nPerforming the elementary row operation \\(R_1 \\leftarrow \\log(K_2/K_1) R_1 - R_2\\), we get:\n\\[\n\\begin{bmatrix}\n\\log \\frac{K_2}{K_1}\\mathcal{V}(K_1) & 0 & - \\log \\frac{K_3}{K_2}\\mathcal{V}(K_3) & | & -\\log \\frac{K}{K_2}\\mathcal{V}(K)\\\\\\\\\n0 & \\log\\frac{K_2}{K_1}\\mathcal{V}(K_2) & \\log \\frac{K_3}{K_1}\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1}\\mathcal{V}(K)\\\\\\\\\n0 & 0 & \\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1} \\log \\frac{K}{K_2}\\mathcal{V}(K)\n\\end{bmatrix}\n\\]\nPerforming the elementary row operation \\(R_1 \\leftarrow \\log(K_3/K_1)R_1 + R_3\\), we get:\n\\[\n\\begin{bmatrix}\n\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}\\mathcal{V}(K_1) & 0 & 0 & | & \\log \\frac{K}{K_3}\\log \\frac{K}{K_2}\\mathcal{V}(K)\\\\\\\\\n0 & \\log\\frac{K_2}{K_1}\\mathcal{V}(K_2) & \\log \\frac{K_3}{K_1}\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1}\\mathcal{V}(K)\\\\\\\\\n0 & 0 & \\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1} \\log \\frac{K}{K_2}\\mathcal{V}(K)\n\\end{bmatrix}\n\\]\nPerforming the elementary row operation \\(R_2 \\leftarrow \\log(K_3/K_2) R_2 - R_3\\), we get:\n\\[\n\\begin{bmatrix}\n\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}\\mathcal{V}(K_1) & 0 & 0 & | & \\log \\frac{K}{K_3}\\log \\frac{K}{K_2}\\mathcal{V}(K)\\\\\\\\\n0 & \\log\\frac{K_2}{K_1} \\log \\frac{K_3}{K_2}\\mathcal{V}(K_2) & 0 & | & \\log \\frac{K}{K_1} \\log \\frac{K_3}{K}\\mathcal{V}(K)\\\\\\\\\n0 & 0 & \\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}\\mathcal{V}(K_3) & | & \\log \\frac{K}{K_1} \\log \\frac{K}{K_2}\\mathcal{V}(K)\n\\end{bmatrix}\n\\]\nThus, the solution vector \\(\\mathbf{x}\\) is:\n\\[\n\\begin{align*}\nx_1(K) &= \\frac{\\log \\frac{K_3}{K}\\log \\frac{K_2}{K}\\mathcal{V}(K)}{\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}\\mathcal{V}(K_1)}\\\\\\\\\nx_2(K) &= \\frac{\\log \\frac{K}{K_1} \\log \\frac{K_3}{K}\\mathcal{V}(K)}{\\log\\frac{K_2}{K_1} \\log \\frac{K_3}{K_2}\\mathcal{V}(K_2)}\\\\\\\\\nx_3(K) &= \\frac{\\log \\frac{K}{K_1} \\log \\frac{K}{K_2}\\mathcal{V}(K)}{\\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}\\mathcal{V}(K_3)} \\tag{9}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/implementing-vanna-volga/index.html#the-vv-option-price.",
    "href": "posts/implementing-vanna-volga/index.html#the-vv-option-price.",
    "title": "Implementing Vanna Volga",
    "section": "The VV Option price.",
    "text": "The VV Option price.\nWe can now proceed to the definition of an option price that is consistent with the market prices of the basic options. The above replication argument shows that a portfolio comprising of \\(x_i(K)\\) units of the option with strike \\(K_i\\) and \\(\\Delta_0\\) units of the underlying gives a local perfect hedge in a Black-Scholes world. The hedging strategy has to be implemented at prevailing market prices, which generates an extra cost with respect to the Black-Scholes value of the options portfolio. Such a cost has to be added to the Black-Scholes price \\(O^{BS}(K)\\), with \\(t=0\\), to produce an arbitrage free price which is consistent with the quoted option prices \\(C_1^{MKT}(K_1)\\), \\(C_2^{MKT}(K_2)\\) and \\(C_3^{MKT}(K_3)\\).\nIn fact, in the case of a short-maturity \\(T\\), the equation (7) can be written as:\n\\[\n\\begin{align*}\n&((S\\_T - K)^{+} - O^{BS}(K)) - \\Delta\\_0(S\\_T - S\\_0)\\\\\\\\\n-& \\sum_{i=1}^{3} x_i(K) (C_i^{MKT}(K_i) - C_i^{BS}(K_i))\\\\\\\\\n&= r_{DOM} (O^{BS}(K) - \\Delta_0 S_0 - \\sum_{i=1}^{3} x_i(K_i)C_i^{BS}(K_i))T \\tag{10}\n\\end{align*}\\]\nTherefore, setting\n\\[O_{VV}^{MKT}(K) = O^{BS}(K) + \\sum_{i=1}^{3}x_i(K)(C_i^{MKT}(K_i) - C_i^{BS}(K_i))\\tag{11}\\]\nWe get:\n\\[\n\\begin{align*}\n(S_T - K)^{+} &= O^{MKT}_{VV}(K) + \\Delta_0(S_T - S_0) \\\\\\\\\n&+ r_{DOM} (O^{BS}(K) - \\Delta_0 S_0 - \\sum_{i=1}^{3} x_i(K)C_i^{BS}(K_i))T \\tag{12}\n\\end{align*}\\]\nThus, when actual market prices are considered, the option payout \\((S_T-K)^{+}\\) can still be replicated by starting with an initial capital of \\(O_{VV}^{MKT}(K)\\), buying \\(\\Delta_0\\) units of the stock and \\(x_i(K)\\) units of the pivot options with strike \\(K_i\\), and investing the rest at the cash rate \\(r_{DOM}\\).\nHence, implicitly ignoring the replication error over longer maturities, the price of the option must the initial capital required to setup the hedge portfolio \\(O_{VV}^{MKT}(K)\\).\nThe option premium \\(O_{VV}^{MKT}(K)\\) equals the Black-Scholes price of the option \\(O^{BS}(K)\\) plus a vanna-volga correction term, or overhedge \\(O_{VV}\\), which is the difference between the market quoted prices of the pivot options and the Black-Scholes prices of the pivot options under the constant BS volatility \\(\\sigma = \\sigma_{ATM}\\)."
  },
  {
    "objectID": "posts/implementing-vanna-volga/index.html#deriving-the-implied-volatility-smile.",
    "href": "posts/implementing-vanna-volga/index.html#deriving-the-implied-volatility-smile.",
    "title": "Implementing Vanna Volga",
    "section": "Deriving the implied volatility smile.",
    "text": "Deriving the implied volatility smile.\nWe can now derive an easy approximation for the vanna-volga implied volatility smile curve \\(\\xi(K)\\).\nIn formula (11), we Taylor expand the market quotes \\(C_1^{MKT}(K_1)\\), \\(C_2^{MKT}(K_2)\\), \\(C_3^{MKT}(K_3)\\) and \\(O^{MKT}_{VV}(K)\\), about \\(\\sigma = \\sigma_{2}\\). We have:\n\\[\n\\begin{align*}\nC_i^{MKT}(\\sigma_i,K_i) &= C_i^{BS}(\\sigma,K_i)+ \\frac{\\partial C_i^{BS}(\\sigma,K_i)}{\\partial \\sigma}(\\sigma_i - \\sigma)\\\\\\\\\nO_{VV}^{MKT}(\\xi(K),K) &= O^{BS}(\\sigma,K)+ \\frac{\\partial O^{BS}(\\sigma,K)}{\\partial \\sigma}(\\xi(K) - \\sigma) \\tag{13}\n\\end{align*}\n\\]\nEquivalently,\n\\[\n\\begin{align*}\nC_i^{MKT}(\\sigma_i,K_i) - C_i^{BS}(\\sigma,K_i) &= \\mathcal{V}(K_i)(\\sigma_i - \\sigma)\\\\\\\\\nO_{VV}^{MKT}(\\xi(K),K) - O^{BS}(\\sigma,K) &= \\mathcal{V}(K)(\\xi(K) - \\sigma) \\tag{14}\n\\end{align*}\n\\]\nSubstituting (13) and (14) in formula (11), we get:\n\\[\n\\begin{align*}\n\\mathcal{V}(K)(\\xi(K) - \\sigma) &= \\sum_{i=1}^{3} x_i(K) \\mathcal{V}(K_i)(\\sigma_i - \\sigma)\n\\end{align*}\n\\]\nSince \\(\\sigma_2 = \\sigma\\), the second term in the summation vanishes. Simplifying, we have:\n\\[\n\\begin{align*}\n\\mathcal{V}(K)(\\xi(K) - \\sigma_2) &= x_1(K)\\mathcal{V}(K_1)(\\sigma_1 - \\sigma_2) + x_3(K)\\mathcal{V}(K_3)(\\sigma_3 - \\sigma_2)\\\\\\\\\n\\xi(K) - \\sigma_2 &=  x_1(K)\\frac{\\mathcal{V}(K_1)}{\\mathcal{V}(K)}(\\sigma_1 - \\sigma_2) +  x_3(K)\\frac{\\mathcal{V}(K_3)}{\\mathcal{V}(K)}(\\sigma_3 - \\sigma_2)\\\\\\\\\n\\xi(K) &= \\frac{\\log \\frac{K_3}{K}\\log \\frac{K_2}{K}}{\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}} \\sigma_1 + \\frac{\\log \\frac{K}{K_1} \\log \\frac{K}{K_2}}{\\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}}\\sigma_3 \\\\\\\\\n&+ \\left(1 - x_1(K)\\frac{\\mathcal{V}(K_1)}{\\mathcal{V}(K)} - x_3(K)\\frac{\\mathcal{V}(K_3)}{\\mathcal{V}(K)} \\right)\\sigma_2 \\tag{15}\n\\end{align*}\n\\]\nBut, we know from the matrix system \\(A\\mathbf{x}=b\\), that the weights \\(\\mathbf{x}=(x_1,x_2,x_3)\\) satisfy:\n\\[x_1(K) \\mathcal{V}(K_1) + x_2(K) \\mathcal{V}(K_2) + x_3(K) \\mathcal{V}(K_3) = \\mathcal{V}(K) \\tag{16}\\]\nSo,\n\\[\n\\begin{align*}\n\\left(1 - x_1(K)\\frac{\\mathcal{V}(K_1)}{\\mathcal{V}(K)} - x_3(K)\\frac{\\mathcal{V}(K_3)}{\\mathcal{V}(K)} \\right) &= x_2(K) \\frac{\\mathcal{V}(K_2)}{\\mathcal{V}(K)}\\\\\\\\\n&=\\frac{\\log \\frac{K}{K_1} \\log \\frac{K_3}{K}}{\\log\\frac{K_2}{K_1} \\log \\frac{K_3}{K_2}} \\tag{17}\n\\end{align*}\n\\]\nSubstituting (17) in the expression (15), we have the result:\n\\[\n\\xi^{1}(K) := \\xi(K) = \\frac{\\log \\frac{K_3}{K}\\log \\frac{K_2}{K}}{\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}} \\sigma_1 + \\frac{\\log \\frac{K}{K_1} \\log \\frac{K}{K_2}}{\\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}}\\sigma_3 + \\frac{\\log \\frac{K}{K_1} \\log \\frac{K_2}{K_1}}{\\log \\frac{K_2}{K_1} \\log \\frac{K_3}{K_2}} \\tag{18}\n\\]\nThe VV-implied volatility smile \\(\\xi(K)\\) is thus approximated by a linear combination of the implied vol quotes \\(\\sigma_1\\), \\(\\sigma_2\\) and \\(\\sigma_3\\) of the vanilla pivot options with coefficients that add up to \\(1\\). This approximation is extremely accurate in the interval \\(\\[K_1,K_3\\]\\). The wings, however, tend to be overvalued."
  },
  {
    "objectID": "posts/implementing-vanna-volga/index.html#a-second-order-approximation-for-vv-smile.",
    "href": "posts/implementing-vanna-volga/index.html#a-second-order-approximation-for-vv-smile.",
    "title": "Implementing Vanna Volga",
    "section": "A second order approximation for VV-smile.",
    "text": "A second order approximation for VV-smile.\nLet’s Taylor expand the market quotes \\(C_i^{MKT}(\\sigma_i,K_i)\\) and \\(O_{VV}^{MKT}(\\xi(K),K)\\) upto the second order.\n\\[\n\\begin{align*}\nC_i^{MKT}(\\sigma_i,K_i) - C_i^{BS}(\\sigma,K_i) &= \\frac{\\partial C_i^{BS}(\\sigma,K_i)}{\\partial \\sigma}(\\sigma_i - \\sigma) + \\frac{1}{2}\\frac{\\partial^2 C_i^{BS}(\\sigma,K_i)}{\\partial \\sigma^2}(\\sigma_i - \\sigma)^2 \\\\\\\\\nO^{MKT}_{VV}(\\xi(K),K) - O^{BS}(\\sigma,K) &= \\frac{\\partial O^{BS}(\\sigma,K)}{\\partial \\sigma}(\\xi(K) - \\sigma) + \\frac{1}{2}\\frac{\\partial^2 O^{BS}(\\sigma,K)}{\\partial \\sigma^2}(\\xi(K) - \\sigma)^2 \\tag{19}\n\\end{align*}\n\\]\nEquivalently,\n\\[\n\\begin{align*}\nC_i^{MKT}(\\sigma_i,K_i) - C_i^{BS}(\\sigma,K_i) &= \\mathcal{V}(K_i)(\\sigma_i - \\sigma) + \\frac{1}{2}\\frac{d_{+}(K_i)d_{-}(K_i)}{\\sigma}\\mathcal{V}(K_i)(\\sigma_i - \\sigma)^2 \\\\\\\\\nO^{MKT}_{VV}(\\xi(K),K) - O^{BS}(\\sigma,K) &= \\mathcal{V}(K)(\\xi(K) - \\sigma) + \\frac{1}{2}\\frac{d\\_{+}(K) d\\_{-}(K)}{\\sigma}\\mathcal{V}(K)(\\xi(K) - \\sigma)^2 \\tag{20}\n\\end{align*}\n\\]\nSubstituting (20) in formula (11):\n\\[\n\\mathcal{V}(K)(\\xi(K) - \\sigma) + \\frac{1}{2}\\frac{d\\_{+}(K)d\\_{-}(K)}{\\sigma}\\mathcal{V}(K)(\\xi(K) - \\sigma)^2 \\\\\\\\\n= x\\_1\\mathcal{V}(K\\_1)(\\sigma\\_1 - \\sigma) + \\frac{1}{2}x\\_1\\frac{d\\_{+}(K\\_1)d\\_{-}(K\\_1)}{\\sigma}\\mathcal{V}(K\\_1)(\\sigma\\_1 - \\sigma)^2\\\\\\\\\n+(\\sigma\\_3 - \\sigma) + \\frac{1}{2}x\\_3\\frac{d\\_{+}(K\\_3)d\\_{-}(K\\_3)}{\\sigma}\\mathcal{V}(K\\_3)(\\sigma\\_3 - \\sigma)^2 \\tag{21}\n\\]\nSimplifying we have:\n\\[\n\\begin{align*}\n&(\\xi(K) - \\sigma_2) + \\frac{1}{2}\\frac{d_{+}(K)d_{-}(K)}{\\sigma}(\\xi(K) - \\sigma_2)^2 \\\\\\\\\n=& \\frac{\\log \\frac{K_3}{K}\\log \\frac{K_2}{K}}{\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}}(\\sigma_1 - \\sigma_2) + \\frac{1}{2}\\frac{d_{+}(K_1)d_{-}(K_1)}{\\sigma}\\frac{\\log \\frac{K_3}{K}\\log \\frac{K_2}{K}}{\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}}(\\sigma_1 - \\sigma_2)^2\\\\\\\\\n+& \\frac{\\log \\frac{K}{K_1} \\log \\frac{K}{K_2}}{\\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}}(\\sigma_3 - \\sigma_2) + \\frac{1}{2}\\frac{d_{+}(K_3)d_{-}(K_3)}{\\sigma}\\frac{\\log \\frac{K}{K_1} \\log \\frac{K}{K_2}}{\\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}}(\\sigma_3 - \\sigma_2)^2\n\\end{align*}\\tag{22}\n\\]\nLet\n\\[\n\\begin{align*}\nD_1(K) &:= \\frac{\\log \\frac{K_3}{K}\\log \\frac{K_2}{K}}{\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}}(\\sigma_1 - \\sigma_2) + \\frac{\\log \\frac{K}{K_1} \\log \\frac{K}{K_2}}{\\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}}(\\sigma_3 - \\sigma_2)\\\\\\\\\n&= \\xi^1(K) - \\sigma_2 \\tag{23}\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\nD_2(K) &:= d_{+}(K_1)d_{-}(K_1)\\frac{\\log \\frac{K_3}{K}\\log \\frac{K_2}{K}}{\\log \\frac{K_3}{K_1}\\log \\frac{K_2}{K_1}}(\\sigma_1 - \\sigma_2)^2 \\\\\\\\\n&+ d_{+}(K_3)d_{-}(K_3)\\frac{\\log \\frac{K}{K_1} \\log \\frac{K}{K_2}}{\\log \\frac{K_3}{K_1} \\log \\frac{K_3}{K_2}}(\\sigma_3 - \\sigma_2)^2 \\tag{24}\n\\end{align*}\n\\]\nSubstituting (23) and (24) in equation (22), we get:\n\\[(\\xi(K) - \\sigma_2) + \\frac{d_{+}(K)d_{-}(K)}{2\\sigma_2}(\\xi(K) - \\sigma_2)^2 = D_1(K) + \\frac{D_2(K)}{2\\sigma_2}\\tag{25}\\]\nMultiplying throughout by \\(2\\sigma_2\\), we get:\n\\[2\\sigma_2(\\xi(K) - \\sigma_2) + d_{+}(K)d_{-}(K)(\\xi(K) - \\sigma_2)^2 = 2\\sigma_2 D_1(K) + D_2(K)\\tag{26}\\]\nSolving for \\(\\xi(K) - \\sigma_2\\), we have:\n\\[\n\\begin{align*}\n\\xi(K) - \\sigma_2 &= \\frac{-2\\sigma_2 \\pm \\sqrt{4\\sigma_2^2-4d_{+}(K)d_{-}(K)(2\\sigma_2 D_1(K) + D_2(K))}}{2d_{+}(K)d_{-}(K)}\\\\\\\\\n\\xi^2(K) := \\xi(K) &=\\sigma_2 + \\frac{-\\sigma_2 \\pm \\sqrt{\\sigma_2^2-d_{+}(K)d_{-}(K)(2\\sigma_2 D_1(K) + D_2(K))}}{d_{+}(K)d_{-}(K)} \\tag{27}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/fun-with-numeraires/index.html#what-is-a-numeraire",
    "href": "posts/fun-with-numeraires/index.html#what-is-a-numeraire",
    "title": "Fun with numeraires!",
    "section": "What is a numeraire?",
    "text": "What is a numeraire?\nAs Shreve puts it, a numeraire is the unit of account in which other assets are denominated. In practice, we tend to choose numeraires that simply the payoff expression.\nAny strictly positive (non-dividend paying) price process can be chosen as a numeraire. A numeraire must be a tradable asset. For example, a unit of stock \\(S_t\\) is a numeraire. But, powers of the stock price \\(S_t^\\alpha\\) are not numeraires."
  },
  {
    "objectID": "posts/fun-with-numeraires/index.html#changing-the-numeraire",
    "href": "posts/fun-with-numeraires/index.html#changing-the-numeraire",
    "title": "Fun with numeraires!",
    "section": "Changing the numeraire",
    "text": "Changing the numeraire\nWhen switching numeraires from \\(M\\) to \\(N\\), we are switching the probability measure (weights) \\(\\mathbb{Q}^M\\) to \\(\\mathbb{Q}^N\\) attached to the outcomes. We want to find the Girsanov transformation which will take us from \\(\\mathbb{Q}^M\\) to \\(\\mathbb{Q}^N\\)."
  },
  {
    "objectID": "posts/fun-with-numeraires/index.html#abstract-bayes-formula",
    "href": "posts/fun-with-numeraires/index.html#abstract-bayes-formula",
    "title": "Fun with numeraires!",
    "section": "Abstract Bayes Formula",
    "text": "Abstract Bayes Formula\n\nTheorem 2 (Abstract Bayes Formula) Let \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) be a probability space and let \\(\\mathbb{Q}\\) be any other probability measure on it. By the Radon-Nikodym theorem, \\(\\exists L = \\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\), \\(L \\geq 0\\) with \\(\\mathbb{E}^{\\mathbb{P}}[L]=1\\). Then we have:\n\\[\n\\begin{align*}\n\\mathbb{E}^{\\mathbb{Q}}[X|\\mathcal{G}] = \\frac{\\mathbb{E}^{\\mathbb{P}}[LX|\\mathcal{G}]}{\\mathbb{E}^{\\mathbb{P}}[L|\\mathcal{G}]}\n\\end{align*}\n\\tag{8}\\]\n\nProof.\nBy the definition of conditional expectations, recall that if \\(W\\) is any \\(\\mathcal{G}\\)-measurable random variable, then the conditional expectation must satisfy the relationship:\n\\[\n\\mathbb{E}[WX] = \\mathbb{E}[W\\mathbb{E}[X|\\mathcal{G}]]\n\\]\nIt is sufficient to prove that:\n\\[\n\\mathbb{E}^{\\mathbb{P}}[X|\\mathcal{G}]\\mathbb{E}^{\\mathbb{Q}}[L|\\mathcal{G}] = \\mathbb{E}^{\\mathbb{P}}[LX|\\mathcal{G}]\n\\]\nLet \\(G\\) be an arbitrary set in \\(\\mathcal{G}\\). We have:\n\\[\n\\begin{align*}\n& \\int_G \\mathbb{E}^{\\mathbb{Q}}[X|\\mathcal{G}]\\mathbb{E}^{\\mathbb{P}}[L|\\mathcal{G}] d\\mathbb{P} \\\\\n&= \\int_G \\mathbb{E}^{\\mathbb{P}}[L\\cdot \\mathbb{E}^{\\mathbb{Q}}[X|\\mathcal{G}]|\\mathcal{G}] d\\mathbb{P} \\\\\n& \\quad \\{ \\mathbb{E}^{\\mathbb{Q}}[X|\\mathcal{G}] \\text{ is }\\mathcal{G}\\text{-measurable } \\}\\\\\n&= \\int_G  L\\cdot \\mathbb{E}^{\\mathbb{Q}}[X|\\mathcal{G}] d\\mathbb{P}\\\\\n&= \\int_G  \\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\cdot \\mathbb{E}^{\\mathbb{Q}}[X|\\mathcal{G}] d\\mathbb{P}\\\\\n&= \\int_G \\mathbb{E}^{\\mathbb{Q}}[X|\\mathcal{G}] d\\mathbb{Q}\n\\end{align*}\n\\]\nHence, proved.\nNote that, the filtration \\(\\mathcal{G}\\) is the same irrespective of what probability measure we construct on \\(\\Omega\\)."
  },
  {
    "objectID": "posts/fun-with-numeraires/index.html#martingale-property",
    "href": "posts/fun-with-numeraires/index.html#martingale-property",
    "title": "Fun with numeraires!",
    "section": "Martingale property",
    "text": "Martingale property\n\nAssume that there exists a numeraire \\(M\\) and a probability measure \\(\\mathbb{Q}^M\\), such that the price of any traded asset \\(X\\) (without intermediate payments) relative to \\(M\\) is a martingale under \\(\\mathbb{Q}^M\\).That is:\n\\[\n\\frac{X_t}{M_t} = \\mathbb{E}^{\\mathbb{Q}^M} \\left\\{\\frac{X_T}{M_T}|\\mathcal{F}_t\\right\\}\n\\]\nLet \\(N_t\\) be an arbitrary numeraire. Then, there exists a probability measure \\(\\mathbb{Q}^N\\) such that the price of \\(X\\) normalized by \\(N\\) is a martingale under \\(\\mathbb{Q}^N\\).\n\\[\n\\frac{X_t}{N_t} = \\mathbb{E}^{\\mathbb{Q}^N} \\left\\{\\frac{X_T}{N_T}|\\mathcal{F}_t\\right\\}\n\\]\nMoreover, the Radon-Nikodym derivative defining the measure \\(\\mathbb{Q}^N\\) is given by:\n\\[\n\\frac{d\\mathbb{Q}^N}{d\\mathbb{Q}^M} = \\frac{N_T/N_0}{M_T/M_0}\n\\]\n\nProof.\nWe have:\n\\[\nX_0 = M_0 \\mathbb{E}^{\\mathbb{Q}^M}\\left[\\frac{X_T}{M_T}\\right]\n\\]\nImposing the simple fact that, the price of the derivative contract should be the same, even if we switch numeraires from \\(M\\) to \\(N\\), we should have:\n\\[\nX_0 = N_0 \\mathbb{E}^{\\mathbb{Q}^N}\\left[\\frac{X_T}{N_T}\\right]\n\\]\nThus,\n\\[\n\\begin{aligned}\nN_{0}\\mathbb{E}^{\\mathbb{Q}^{N}}\\left[\\frac{X_{T}}{N_{T}}\\right] & =M_{0}\\mathbb{E}^{\\mathbb{Q}^{M}}\\left[\\frac{X_{T}}{M_{T}}\\right]\\\\\n\\frac{N_{T}}{N_{0}} \\times N_{0}\\mathbb{E}^{\\mathbb{Q}^{N}}\\left[\\frac{X_{T}}{N_{T}}\\right] & =\\frac{N_{T}}{N_{0}} \\times M_{0} \\ \\mathbb{E}^{\\mathbb{Q}^{M}}\\left[\\frac{X_{T}}{M_{T}}\\right] \\quad \\left\\{\\text{Multiplying both sides by }\\frac{N_{T}}{N_{0}}\\right\\}\\\\\n\\Longrightarrow \\mathbb{E}^{\\mathbb{Q}^{N}}[ X_{T}] & =\\mathbb{E}^{\\mathbb{Q}^{M}}\\left[\\frac{N_{T} /N_{0}}{M_{T} /M_{0}} X_{T}\\right]\n\\end{aligned}\n\\]\nBut, we know that:\n\\[\n\\mathbb{E}^{\\mathbb{Q}^N}[X_T] = \\mathbb{E}^{\\mathbb{Q}^M}\\left[\\frac{d\\mathbb{Q}^N}{d\\mathbb{Q}^M}X_T\\right]\n\\]\nConsequently, our candidate for the Radon-Nikodym derivative should be:\n\\[\nL_T = \\frac{d\\mathbb{Q}^N}{d\\mathbb{Q}^M} = \\frac{N_T/N_0}{M_T/M_0}\n\\]\nFurther \\((X_t/N_t)\\) is a martingale under \\(\\mathbb{Q}^N\\). Its easy to see that:\n\\[\n\\begin{aligned}\n\\mathbb{E}^{\\mathbb{Q}^{N}}\\left[\\frac{X_{T}}{N_{T}} |\\mathcal{F}_{t}\\right] & =\\frac{\\mathbb{E}^{\\mathbb{Q}^{M}}\\left[ L_{T} \\cdot \\frac{X_{T}}{N_{T}} |\\mathcal{F}_{t}\\right]}{\\mathbb{E}^{\\mathbb{Q}^{M}}[ L_{T} |\\mathcal{F}_{t}]} \\quad \\left\\{\\text{ Abstract bayes formula }\\right\\}\\\\\n& =\\frac{\\mathbb{E}^{\\mathbb{Q}^{M}}\\left[ L_{T} \\cdot \\frac{X_{T}}{N_{T}} |\\mathcal{F}_{t}\\right]}{L_{t}}\\\\\n& =\\frac{\\mathbb{E}^{\\mathbb{Q}^{M}}\\left[\\frac{N_{T}}{N_{0}} \\cdot \\frac{M_{0}}{M_{T}} \\cdot \\frac{X_{T}}{N_{T}} |\\mathcal{F}_{t}\\right]}{\\frac{N_{t}}{N_{0}} \\cdot \\frac{M_{0}}{M_{t}}}\\\\\n& =\\frac{\\mathbb{E}^{\\mathbb{Q}^{M}}\\left[\\frac{N_{T}}{N_{0}} \\cdot \\frac{M_{0}}{M_{T}} \\cdot \\frac{X_{T}}{N_{T}} |\\mathcal{F}_{t}\\right]}{\\frac{N_{t}}{N_{0}} \\cdot \\frac{M_{0}}{M_{t}}}\\\\\n& =\\frac{M_{t}}{N_{t}} \\cdot \\mathbb{E}^{\\mathbb{Q}^{M}}\\left[\\frac{X_{T}}{M_{T}}\\right]\\\\\n& =\\frac{M_{t}}{N_{t}} \\cdot \\frac{X_{t}}{M_{t}}\\\\\n& =\\frac{X_{t}}{N_{t}}\n\\end{aligned}\n\\]"
  }
]